bug_id,summary,description,fix_pr_titles,type,labels,source,created_at,closed_at,text,severity_raw,severity,confidence,explanation
rust-lang/rust#151411,"[ice]: rustdoc panics with ""no resolutions for a doc link"" when resolving intra-doc link in #[deprecated] note via glob re-export","### command ### code ### meta : ### error output note: in a larger codebase (the original context where this was discovered), the ice was reported at a different location: ### backtrace full backtrace ### additional context the ice occurs when: 1. a function has a attribute containing an intra-doc link (e.g., ) 2. the function is in a private module 3. the function is re-exported via a glob import ( ) the issue appears to be that rustdoc attempts to resolve the intra-doc link in the deprecation note but fails to properly handle the resolution when the item is accessed through a glob re-export. the warning ""no item named in scope"" suggests the link resolution is happening in the wrong scope (the private module's scope rather than the crate root where is defined). even though the link is unresolvable (which should just be a warning), rustdoc should not ice. it should gracefully handle the unresolved link case. ### discovered in this was discovered while running the following command on the [stellar/rs-soroban-sdk]( repository:",[],['BUG'],"['T-rustdoc', 'I-ICE', 'C-bug', 'A-intra-doc-links']",github,2026-01-20T13:50:47Z,2026-01-21T23:13:21Z,"[ice]: rustdoc panics with ""no resolutions for a doc link"" when resolving intra-doc link in #[deprecated] note via glob re-export ### command ### code ### meta : ### error output note: in a larger codebase (the original context where this was discovered), the ice was reported at a different location: ### backtrace full backtrace ### additional context the ice occurs when: 1. a function has a attribute containing an intra-doc link (e.g., ) 2. the function is in a private module 3. the function is re-exported via a glob import ( ) the issue appears to be that rustdoc attempts to resolve the intra-doc link in the deprecation note but fails to properly handle the resolution when the item is accessed through a glob re-export. the warning ""no item named in scope"" suggests the link resolution is happening in the wrong scope (the private module's scope rather than the crate root where is defined). even though the link is unresolvable (which should just be a warning), rustdoc should not ice. it should gracefully handle the unresolved link case. ### discovered in this was discovered while running the following command on the [stellar/rs-soroban-sdk]( repository:",4.6,Critical,1.0,crash-like behavior
tensorflow/tensorflow#108503,spam/duplicate: python 3.14 support,### issue type bug ### have you reproduced the bug with tensorflow nightly? yes ### source source ### tensorflow version 2.2 ### custom code yes ### os platform and distribution windows 11 ### mobile device _no response_ ### python version _no response_ ### bazel version _no response_ ### gcc/compiler version _no response_ ### cuda/cudnn version _no response_ ### gpu model and memory _no response_ ### current behavior? pip install tensorflow error: could not find a version that satisfies the requirement tensorflow (from versions: none) error: no matching distribution found for tensorflow ### standalone code to reproduce the issue ### relevant log output,[],['BUG'],"['type:bug', 'TF 2.2']",github,2026-01-20T13:52:25Z,2026-01-20T17:34:36Z,spam/duplicate: python 3.14 support ### issue type bug ### have you reproduced the bug with tensorflow nightly? yes ### source source ### tensorflow version 2.2 ### custom code yes ### os platform and distribution windows 11 ### mobile device _no response_ ### python version _no response_ ### bazel version _no response_ ### gcc/compiler version _no response_ ### cuda/cudnn version _no response_ ### gpu model and memory _no response_ ### current behavior? pip install tensorflow error: could not find a version that satisfies the requirement tensorflow (from versions: none) error: no matching distribution found for tensorflow ### standalone code to reproduce the issue ### relevant log output,4.2,Critical,1.0,system-wide impact
cockroachdb/cockroach#161420,include system.transaction_diagnostics and system.transaction_diagnostics_requests tables in debug zip,**is your feature request related to a problem? please describe.** the and are missing from the debug zip. this makes troubleshooting issues with transaction diagnostic bundles impossible. **describe the solution you'd like** add these two tables to the debug zip. jira issue: crdb-58872,[],['FEATURE'],"['C-enhancement', 'O-support', 'T-observability', 'target-release-26.2.0']",github,2026-01-20T14:20:21Z,2026-01-24T01:49:49Z,include system.transaction_diagnostics and system.transaction_diagnostics_requests tables in debug zip **is your feature request related to a problem? please describe.** the and are missing from the debug zip. this makes troubleshooting issues with transaction diagnostic bundles impossible. **describe the solution you'd like** add these two tables to the debug zip. jira issue: crdb-58872,1.4,Low,0.538,localized low-impact
pandas-dev/pandas#63774,bug: ignores exclude argument when,"### pandas version checks - [x] i have checked that this issue has not already been reported. - [x] i have confirmed this bug exists on the [latest version]( of pandas. - [x] i have confirmed this bug exists on the [main branch]( of pandas. ### reproducible example ### issue description in pandas 2.3.3, was empty when . this issue was reported in issue and fixed in . however, this pr seem to accidentally cause regression in which dataframe.from_records ignored argument. in pandas 3.0.0rc2, result.columns will be [""a"", ""b"", ""c""] and do not exclude [""b""] ### expected behavior result.columns should be [""a"", ""c""] ### installed versions installed versions ------------------ commit : bb69be8a6c6af980f6dc89d641515502c41880a4 python : 3.14.2 python-bits : 64 os : darwin os-release : 24.6.0 version : darwin kernel version 24.6.0: mon jul 14 11:30:29 pdt 2025; root:xnu-11417.140.69~1/release_arm64_t6000 machine : arm64 processor : arm byteorder : little lc_all : none lang : en_us.utf-8 locale : en_us.utf-8 pandas : 3.0.0rc2 numpy : 2.4.1 dateutil : 2.9.0.post0 pip : 25.3 cython : none sphinx : none ipython : none adbc-driver-postgresql: none adbc-driver-sqlite : none bs4 : none bottleneck : none fastparquet : none fsspec : none html5lib : none hypothesis : none gcsfs : none jinja2 : none lxml.etree : none matplotlib : none numba : none numexpr : none odfpy : none openpyxl : none psycopg2 : none pymysql : none pyarrow : none pyiceberg : none pyreadstat : none pytest : none python-calamine : none pytz : 2025.2 pyxlsb : none s3fs : none scipy : none sqlalchemy : none tables : none tabulate : none xarray : none xlrd : none xlsxwriter : none zstandard : none qtpy : none pyqt5 : none",[],['BUG'],"['Bug', 'IO Data']",github,2026-01-20T15:06:53Z,,"bug: ignores exclude argument when ### pandas version checks - [x] i have checked that this issue has not already been reported. - [x] i have confirmed this bug exists on the [latest version]( of pandas. - [x] i have confirmed this bug exists on the [main branch]( of pandas. ### reproducible example ### issue description in pandas 2.3.3, was empty when . this issue was reported in issue and fixed in . however, this pr seem to accidentally cause regression in which dataframe.from_records ignored argument. in pandas 3.0.0rc2, result.columns will be [""a"", ""b"", ""c""] and do not exclude [""b""] ### expected behavior result.columns should be [""a"", ""c""] ### installed versions installed versions ------------------ commit : bb69be8a6c6af980f6dc89d641515502c41880a4 python : 3.14.2 python-bits : 64 os : darwin os-release : 24.6.0 version : darwin kernel version 24.6.0: mon jul 14 11:30:29 pdt 2025; root:xnu-11417.140.69~1/release_arm64_t6000 machine : arm64 processor : arm byteorder : little lc_all : none lang : en_us.utf-8 locale : en_us.utf-8 pandas : 3.0.0rc2 numpy : 2.4.1 dateutil : 2.9.0.post0 pip : 25.3 cython : none sphinx : none ipython : none adbc-driver-postgresql: none adbc-driver-sqlite : none bs4 : none bottleneck : none fastparquet : none fsspec : none html5lib : none hypothesis : none gcsfs : none jinja2 : none lxml.etree : none matplotlib : none numba : none numexpr : none odfpy : none openpyxl : none psycopg2 : none pymysql : none pyarrow : none pyiceberg : none pyreadstat : none pytest : none python-calamine : none pytz : 2025.2 pyxlsb : none s3fs : none scipy : none sqlalchemy : none tables : none tabulate : none xarray : none xlrd : none xlsxwriter : none zstandard : none qtpy : none pyqt5 : none",4.6,Critical,1.0,crash-like behavior
openssl/openssl#29684,it is impossible to set provider added signature schemes via ssl_set1_sigalgs,"it is common with newer signature schemes that the hash is undefined (this comes from the underlying standards). ssl_set1_sigalgs should be able to set these schemes even when hash is nid_undef, eg. if the list is previously read via ssl_get_shared_sigalgs. for instance it seems that if postquantum crypto is used, ssl_get_shared_sigalgs can return something like this list (hash:sign): 0:5b2,0:5b3,0:5b1,2a0:198,2a1:198,2a2:198,0:43f,0:440,2a0:198,2a1:198,2a2:198,2a0:390,2a1:390,2a2:390,2a0:390,2a1:390,2a2:390,2a0:6,2a1:6,2a2:6,2a3:198,40:198,2a3:6,40:6 this list can not be set via ssl_set1_sigalgs. asked me to create this within discussion.",[],['FEATURE'],"['branch: master', 'triaged: feature']",github,2026-01-20T15:20:51Z,,"it is impossible to set provider added signature schemes via ssl_set1_sigalgs it is common with newer signature schemes that the hash is undefined (this comes from the underlying standards). ssl_set1_sigalgs should be able to set these schemes even when hash is nid_undef, eg. if the list is previously read via ssl_get_shared_sigalgs. for instance it seems that if postquantum crypto is used, ssl_get_shared_sigalgs can return something like this list (hash:sign): 0:5b2,0:5b3,0:5b1,2a0:198,2a1:198,2a2:198,0:43f,0:440,2a0:198,2a1:198,2a2:198,2a0:390,2a1:390,2a2:390,2a0:390,2a1:390,2a2:390,2a0:6,2a1:6,2a2:6,2a3:198,40:198,2a3:6,40:6 this list can not be set via ssl_set1_sigalgs. asked me to create this within discussion.",1.4,Low,0.538,localized low-impact
cockroachdb/cockroach#161425,sql: reduce discoverability of unsafe internals,"in 26.1, we began blocking the unsafe internals by default. however, they are still discoverable via a number of means (show databases, etc). initial discovery of possible error states includes database crawlers, which intentionally discover available database objects and attempt to query them. this ticket seeks to remove these objects from discovery, so that can't be accidentally stumbled upon, nor crawled. epic: none jira issue: crdb-58874",[],['FEATURE'],"['C-enhancement', 'T-observability']",github,2026-01-20T15:58:05Z,,"sql: reduce discoverability of unsafe internals in 26.1, we began blocking the unsafe internals by default. however, they are still discoverable via a number of means (show databases, etc). initial discovery of possible error states includes database crawlers, which intentionally discover available database objects and attempt to query them. this ticket seeks to remove these objects from discovery, so that can't be accidentally stumbled upon, nor crawled. epic: none jira issue: crdb-58874",1.4,Low,0.538,localized low-impact
kubernetes/kubernetes#136346,post-quantum cryptography readiness observations & recommendations (research),"this issue shares research-based observations on post-quantum cryptography (pqc) readiness in kubernetes, based solely on publicly available code and documentation. this is not a vulnerability report and does not include exploitation, attack paths, or sensitive configuration details. the intent is awareness, future-readiness, and community discussion aligned with nist pqc transition guidance. observations: ‚Ä¢ cryptographic usage is distributed across multiple layers ‚Ä¢ there is no centralized crypto inventory or ownership map ‚Ä¢ no defined crypto agility or pqc migration framework ‚Ä¢ no governance-level visibility into algorithm lifecycle recommendations: ‚Ä¢ consider adding a crypto inventory layer ‚Ä¢ define crypto ownership and lifecycle policy ‚Ä¢ track nist pqc migration readiness ‚Ä¢ introduce crypto agility abstractions ‚Ä¢ document pqc roadmap for operators why this matters: quantum-safe transitions are multi-year efforts. early visibility reduces long-term risk. happy to refine or contribute to documentation if helpful.",[],['SECURITY'],"['sig/security', 'needs-triage']",github,2026-01-20T15:59:37Z,2026-01-22T04:02:42Z,"post-quantum cryptography readiness observations & recommendations (research) this issue shares research-based observations on post-quantum cryptography (pqc) readiness in kubernetes, based solely on publicly available code and documentation. this is not a vulnerability report and does not include exploitation, attack paths, or sensitive configuration details. the intent is awareness, future-readiness, and community discussion aligned with nist pqc transition guidance. observations: ‚Ä¢ cryptographic usage is distributed across multiple layers ‚Ä¢ there is no centralized crypto inventory or ownership map ‚Ä¢ no defined crypto agility or pqc migration framework ‚Ä¢ no governance-level visibility into algorithm lifecycle recommendations: ‚Ä¢ consider adding a crypto inventory layer ‚Ä¢ define crypto ownership and lifecycle policy ‚Ä¢ track nist pqc migration readiness ‚Ä¢ introduce crypto agility abstractions ‚Ä¢ document pqc roadmap for operators why this matters: quantum-safe transitions are multi-year efforts. early visibility reduces long-term risk. happy to refine or contribute to documentation if helpful.",7.8,Critical,1.0,"security risk, crash-like behavior"
rust-lang/rust#151417,[ice]:,"<!-- thank you for finding an internal compiler error! üßä if possible, try to provide a minimal verifiable example. you can read ""rust bug minimization patterns"" for how to create smaller examples. --> ### code ### meta <!-- if you're using the stable version of the compiler, you should also check if the bug also exists in the beta or nightly versions. --> : ### error output <!-- include a backtrace in the code block by setting in your environment. e.g. . --> backtrace",[],['BUG'],"['I-ICE', 'T-compiler', 'C-bug']",github,2026-01-20T16:28:53Z,2026-01-20T16:52:42Z,"[ice]: <!-- thank you for finding an internal compiler error! üßä if possible, try to provide a minimal verifiable example. you can read ""rust bug minimization patterns"" for how to create smaller examples. --> ### code ### meta <!-- if you're using the stable version of the compiler, you should also check if the bug also exists in the beta or nightly versions. --> : ### error output <!-- include a backtrace in the code block by setting in your environment. e.g. . --> backtrace",2.446,Medium,0.776,functional impact
tensorflow/tensorflow#108521,spam: tf.concat does not consistently validate non-scalar axis,"### issue type bug ### have you reproduced the bug with tensorflow nightly? yes ### source source ### tensorflow version master ### custom code yes ### os platform and distribution na ### mobile device na ### python version na ### bazel version _no response_ ### gcc/compiler version _no response_ ### cuda/cudnn version _no response_ ### gpu model and memory _no response_ ### current behavior? tf.concat validates that is a scalar only in the single-input path. when multiple tensors are provided, invalid axis shapes result in confusing errors from lower-level ops instead of an early validation error. ### standalone code to reproduce the issue ### relevant log output",[],['BUG'],['type:bug'],github,2026-01-20T17:10:33Z,2026-01-20T17:22:54Z,"spam: tf.concat does not consistently validate non-scalar axis ### issue type bug ### have you reproduced the bug with tensorflow nightly? yes ### source source ### tensorflow version master ### custom code yes ### os platform and distribution na ### mobile device na ### python version na ### bazel version _no response_ ### gcc/compiler version _no response_ ### cuda/cudnn version _no response_ ### gpu model and memory _no response_ ### current behavior? tf.concat validates that is a scalar only in the single-input path. when multiple tensors are provided, invalid axis shapes result in confusing errors from lower-level ops instead of an early validation error. ### standalone code to reproduce the issue ### relevant log output",4.2,Critical,1.0,system-wide impact
rust-lang/rust#151420,"rustdoc doesn't have ""skip navigation"" links","<!-- thank you for filing a rustdoc issue! rustdoc is the tool that handles the generation of docs. it is usually invoked via , but can also be used directly. if you have an issue with the actual content of the docs, use the ""documentation problem"" template instead. --> # problem synopsis skip navigation links allow users using keyboard navigation (i.e. the key) to be able to skip long repeated sections and get directly to the main content. currently, while using navigation, one has to get past the entire left contents bar, before being able to access the main content (see video demo below). wcag success criterion 2.4.1 (level a) states > a [mechanism]( is available to bypass blocks of content that are repeated on multiple [web pages]( this feature is currently missing within , making keyboard navigation a pain to work with on large crates. ## useful links 1. 2. 3. # additional details",[],"['BUG', 'ACCESSIBILITY']","['E-easy', 'T-rustdoc', 'C-bug', 'A-a11y']",github,2026-01-20T17:46:34Z,2026-01-24T13:31:48Z,"rustdoc doesn't have ""skip navigation"" links <!-- thank you for filing a rustdoc issue! rustdoc is the tool that handles the generation of docs. it is usually invoked via , but can also be used directly. if you have an issue with the actual content of the docs, use the ""documentation problem"" template instead. --> # problem synopsis skip navigation links allow users using keyboard navigation (i.e. the key) to be able to skip long repeated sections and get directly to the main content. currently, while using navigation, one has to get past the entire left contents bar, before being able to access the main content (see video demo below). wcag success criterion 2.4.1 (level a) states > a [mechanism]( is available to bypass blocks of content that are repeated on multiple [web pages]( this feature is currently missing within , making keyboard navigation a pain to work with on large crates. ## useful links 1. 2. 3. # additional details",2.599,Medium,0.811,functional impact
numpy/numpy#30681,bug: doc build on 3.14t interpreter leads to bus error,"when i run with a free-threaded interpreter on my arm macbook pro, the doc build crashes with a bus error and the following faulthandler output: the most relevant line in the python traceback is: which corresponds to this line: i'm pretty sure that writing to like this via ctypes is undefined behavior. for static types is embedded in the binary. i'm a little surprised it works at all in the gil-enabled build. ping since you last touched this code.",[],"['BUG', 'DOCUMENTATION']","['00 - Bug', '04 - Documentation', '39 - free-threading']",github,2026-01-20T17:52:05Z,2026-01-20T20:49:55Z,"bug: doc build on 3.14t interpreter leads to bus error when i run with a free-threaded interpreter on my arm macbook pro, the doc build crashes with a bus error and the following faulthandler output: the most relevant line in the python traceback is: which corresponds to this line: i'm pretty sure that writing to like this via ctypes is undefined behavior. for static types is embedded in the binary. i'm a little surprised it works at all in the gil-enabled build. ping since you last touched this code.",2.911,Medium,0.882,crash-like behavior
electron/electron#49462,v40.0.0 crash on macos 26.2 periodically.,"### preflight checklist - [x] i have read the [contributing guidelines]( for this project. - [x] i agree to follow the [code of conduct]( that this project adheres to. - [x] i have searched the [issue tracker]( for a bug report that matches the one i want to file, without success. ### electron version 40.0.0 ### what operating system(s) are you using? macos ### operating system version macos 26.2 (25c56) ### what arch are you using? arm64 (including apple silicon) ### last known working electron version 38.2.2 ### does the issue also appear in chromium / google chrome? no ### expected behavior electron v40.0.0 based app is working. ### actual behavior electron v40.0.0 based app for macos 26.2 is periodically crash: thread 0 crashed:: crbrowsermain dispatch queue: com.apple.main-thread 0 electron framework 0x111b260a0 v8::cpuprofilenode::getnodeid() const + 121836 1 electron framework 0x1106626b8 v8::isolate::setfailedaccesscheckcallbackfunction(void (*)(v8::local , v8::accesstype, v8::local )) + 104148 2 electron framework 0x11385c9bc _v8_internal_node_print(void*) + 9595396 3 electron framework 0x1105ad234 v8::trycatch::hasterminated() const + 103224 4 electron framework 0x1105a9c38 v8::trycatch::hasterminated() const + 89404 full log: [crash_report.log]( ### testcase gist url _no response_ ### additional information _no response_",[],['BUG'],"['platform/macOS', 'bug :beetle:', 'blocked/need-repro', '40-x-y']",github,2026-01-20T17:53:06Z,,"v40.0.0 crash on macos 26.2 periodically. ### preflight checklist - [x] i have read the [contributing guidelines]( for this project. - [x] i agree to follow the [code of conduct]( that this project adheres to. - [x] i have searched the [issue tracker]( for a bug report that matches the one i want to file, without success. ### electron version 40.0.0 ### what operating system(s) are you using? macos ### operating system version macos 26.2 (25c56) ### what arch are you using? arm64 (including apple silicon) ### last known working electron version 38.2.2 ### does the issue also appear in chromium / google chrome? no ### expected behavior electron v40.0.0 based app is working. ### actual behavior electron v40.0.0 based app for macos 26.2 is periodically crash: thread 0 crashed:: crbrowsermain dispatch queue: com.apple.main-thread 0 electron framework 0x111b260a0 v8::cpuprofilenode::getnodeid() const + 121836 1 electron framework 0x1106626b8 v8::isolate::setfailedaccesscheckcallbackfunction(void (*)(v8::local , v8::accesstype, v8::local )) + 104148 2 electron framework 0x11385c9bc _v8_internal_node_print(void*) + 9595396 3 electron framework 0x1105ad234 v8::trycatch::hasterminated() const + 103224 4 electron framework 0x1105a9c38 v8::trycatch::hasterminated() const + 89404 full log: [crash_report.log]( ### testcase gist url _no response_ ### additional information _no response_",6.0,Critical,1.0,crash-like behavior
rust-lang/rust#151422,rustdoc fails contrast check for a certain element.,"<!-- thank you for filing a rustdoc issue! rustdoc is the tool that handles the generation of docs. it is usually invoked via , but can also be used directly. if you have an issue with the actual content of the docs, use the ""documentation problem"" template instead. --> # issue synopsis the at the start of the docs page has a contrast ratio of 3.1:1 with the background on the dark theme, which fails both aa and aaa specifications for wcag 2. the colors for the theme pass wcag level aa, but fail level aaa with a contrast ratio of 4.69:1. the light theme fails both aa and aaa specifications with a contrast ratio of 3.94:1. # useful links: 1. wcag contrast guidelines: 2. webaim contrast checker:",[],"['BUG', 'ACCESSIBILITY', 'UI']","['E-easy', 'T-rustdoc', 'C-bug', 'A-rustdoc-ui', 'T-rustdoc-frontend', 'A-a11y']",github,2026-01-20T17:55:22Z,,"rustdoc fails contrast check for a certain element. <!-- thank you for filing a rustdoc issue! rustdoc is the tool that handles the generation of docs. it is usually invoked via , but can also be used directly. if you have an issue with the actual content of the docs, use the ""documentation problem"" template instead. --> # issue synopsis the at the start of the docs page has a contrast ratio of 3.1:1 with the background on the dark theme, which fails both aa and aaa specifications for wcag 2. the colors for the theme pass wcag level aa, but fail level aaa with a contrast ratio of 4.69:1. the light theme fails both aa and aaa specifications with a contrast ratio of 3.94:1. # useful links: 1. wcag contrast guidelines: 2. webaim contrast checker:",2.536,Medium,0.796,user-visible issue
pytorch/pytorch#172854,[test] add unittest for nanmean output dtype validation (follow-up to ),"## summary this is a follow-up issue to add a unittest for the fix in pr , which addresses issue . ## background issue reported that with invalid output dtype ( , ) behaved inconsistently: - non-empty tensor + ‚Üí runtimeerror with unclear message - empty tensor + ‚Üí ‚ùå returns without error (bug!) pr fixes this by adding upfront validation of in and . ## this issue per reviewer feedback from on pr : > it'll be nice to have a unittest (via opinfo probably?) this issue tracks adding a unittest to verify the fix works correctly. ## test coverage needed 1. should reject invalid output dtypes ( , ) for: - non-empty tensors - empty tensors (the bug case) - scalar tensors 2. should still work with valid output dtypes: - - (default) ## related prs - **fix pr:** - - **test pr:** - ## references - original issue: - reviewer comment: cc",[],['TESTING'],"['module: tests', 'triaged', 'module: reductions']",github,2026-01-20T17:56:51Z,,"[test] add unittest for nanmean output dtype validation (follow-up to ) ## summary this is a follow-up issue to add a unittest for the fix in pr , which addresses issue . ## background issue reported that with invalid output dtype ( , ) behaved inconsistently: - non-empty tensor + ‚Üí runtimeerror with unclear message - empty tensor + ‚Üí ‚ùå returns without error (bug!) pr fixes this by adding upfront validation of in and . ## this issue per reviewer feedback from on pr : > it'll be nice to have a unittest (via opinfo probably?) this issue tracks adding a unittest to verify the fix works correctly. ## test coverage needed 1. should reject invalid output dtypes ( , ) for: - non-empty tensors - empty tensors (the bug case) - scalar tensors 2. should still work with valid output dtypes: - - (default) ## related prs - **fix pr:** - - **test pr:** - ## references - original issue: - reviewer comment: cc",1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161433,kvserver: add a roachtest repro for,"**describe the problem** we should add a roachtest that repros what we saw in it should run 20k warehouses, tpcc inserts, 3 nodes. - claude said the closest roachtest we have is tpcc/published/headroom/25k) which runs tpcc, 20k warehouses, 3 nodes, 30 mins, 500 active workers. - find the relevant env files in the exact repro is: - it should do be similar to below: jira issue: crdb-58876",[],['BUG'],"['C-bug', 'A-kv', 'branch-master', 'T-kv', 'P-3', 'branch-release-25.4', 'branch-release-26.1']",github,2026-01-20T18:01:18Z,,"kvserver: add a roachtest repro for **describe the problem** we should add a roachtest that repros what we saw in it should run 20k warehouses, tpcc inserts, 3 nodes. - claude said the closest roachtest we have is tpcc/published/headroom/25k) which runs tpcc, 20k warehouses, 3 nodes, 30 mins, 500 active workers. - find the relevant env files in the exact repro is: - it should do be similar to below: jira issue: crdb-58876",2.22,Medium,0.725,functional impact
cockroachdb/cockroach#161436,admission: compaction byte estimates can be too low,"when there is almost no foreground work, and background work is bursty e.g. an import or index backfill, the compaction concurrency can vary significantly and be much lower than the configured max (in the following, the max was 3). in that case, using the observed compaction bytes out of l0 to model the capacity can be an under-estimate. we could scale this up by the (compaction-time-capacity-over-adjustment-interval)/(compaction-time-observed-over-adjustment-interval): the numerator in this example would be 3*15s = 45s. note the ac throttline at 18:19:10 and 18:20:55 below ( in the former case) jira issue: crdb-58877",[],['FEATURE'],"['C-enhancement', 'A-admission-control', 'T-admission-control']",github,2026-01-20T18:35:28Z,,"admission: compaction byte estimates can be too low when there is almost no foreground work, and background work is bursty e.g. an import or index backfill, the compaction concurrency can vary significantly and be much lower than the configured max (in the following, the max was 3). in that case, using the observed compaction bytes out of l0 to model the capacity can be an under-estimate. we could scale this up by the (compaction-time-capacity-over-adjustment-interval)/(compaction-time-observed-over-adjustment-interval): the numerator in this example would be 3*15s = 45s. note the ac throttline at 18:19:10 and 18:20:55 below ( in the former case) jira issue: crdb-58877",1.4,Low,0.538,localized low-impact
flutter/flutter#181196,flutter test's bysemanticslabel() finder has unhelpful description,"### steps to reproduce 1. use in a test such that the matcher fails in some way. 2. see that the test failure message is very nondescript ### expected results the failure message should indicate the label trying to be matched. ### actual results the failure message does not indicate the label trying to be matched. ### code sample n/a, see above ### screenshots or video n/a ### logs n/a ### flutter doctor output n/a",[],['TESTING'],"['a: tests', 'tool', 'framework', 'f: material design', 'a: error message', 'has reproducible steps', 'r: fixed', 'team-framework', 'found in release: 3.38', 'found in release: 3.41']",github,2026-01-20T18:43:36Z,2026-01-22T00:14:28Z,"flutter test's bysemanticslabel() finder has unhelpful description ### steps to reproduce 1. use in a test such that the matcher fails in some way. 2. see that the test failure message is very nondescript ### expected results the failure message should indicate the label trying to be matched. ### actual results the failure message does not indicate the label trying to be matched. ### code sample n/a, see above ### screenshots or video n/a ### logs n/a ### flutter doctor output n/a",1.6,Low,0.584,localized low-impact
numpy/numpy#30684,doc: update sphinx and numpydoc pins,"currently sphinx and numpydoc are pinned to old versions in the requirements file: numpydoc 1.4 is from 2022 and sphinx 7.2.6 is from 2023. it would behoove us to update these to the current versions so our documentation build configuration doesn't hopelessly bitrot. i tried updating these pins in but punted when the update proved nontrivial. the build does succeed, but with more than a thousand warnings: is related too - that's using a sphinx version newer than our pin.",[],['DOCUMENTATION'],"['04 - Documentation', '03 - Maintenance']",github,2026-01-20T18:52:28Z,,"doc: update sphinx and numpydoc pins currently sphinx and numpydoc are pinned to old versions in the requirements file: numpydoc 1.4 is from 2022 and sphinx 7.2.6 is from 2023. it would behoove us to update these to the current versions so our documentation build configuration doesn't hopelessly bitrot. i tried updating these pins in but punted when the update proved nontrivial. the build does succeed, but with more than a thousand warnings: is related too - that's using a sphinx version newer than our pin.",1.2,Low,0.493,localized low-impact
cockroachdb/cockroach#161447,bulkmerge: support unique constraints in import,"this is a follow-on for . that added uniqueness support into distributed merge for the index backfiller. this task is to open it up for import. here is what was added previously: | # | phase | duplicate location | detection mechanism | status | |---|-------|-------------------|---------------------|--------| | 1 | **map phase** | same sst batch | | ‚úÖ detected | | 2 | **map phase** | across sst batches | none at map time | ‚ùå escapes to merge | | 3 | **merge intermediate** | same sst | pebble ordering error (""keys must be added in strictly increasing order"") | ‚úÖ detected | | 4 | **merge intermediate** | cross-sst | silently shadows | ‚ùå escapes (one copy dropped) | | 5 | **merge final** | same sst batch | consecutive key check | ‚ö†Ô∏è fragile (resets on flush) | | 6 | **merge final** | cross-sst | silently shadows | ‚ùå escapes (one copy dropped) | | 7 | **merge final** | sst vs. pre-existing kv | with | ‚úÖ detected | | 8 | **post-ingestion** | any that escaped above | (create index only) | ‚úÖ caught, but not for import | since import does not have a post validation step, it needs to properly handle cases 4, 5, and 6. jira issue: crdb-58878 epic crdb-48845",[],['FEATURE'],"['C-enhancement', 'T-sql-foundations']",github,2026-01-20T18:58:42Z,,"bulkmerge: support unique constraints in import this is a follow-on for . that added uniqueness support into distributed merge for the index backfiller. this task is to open it up for import. here is what was added previously: | # | phase | duplicate location | detection mechanism | status | |---|-------|-------------------|---------------------|--------| | 1 | **map phase** | same sst batch | | ‚úÖ detected | | 2 | **map phase** | across sst batches | none at map time | ‚ùå escapes to merge | | 3 | **merge intermediate** | same sst | pebble ordering error (""keys must be added in strictly increasing order"") | ‚úÖ detected | | 4 | **merge intermediate** | cross-sst | silently shadows | ‚ùå escapes (one copy dropped) | | 5 | **merge final** | same sst batch | consecutive key check | ‚ö†Ô∏è fragile (resets on flush) | | 6 | **merge final** | cross-sst | silently shadows | ‚ùå escapes (one copy dropped) | | 7 | **merge final** | sst vs. pre-existing kv | with | ‚úÖ detected | | 8 | **post-ingestion** | any that escaped above | (create index only) | ‚úÖ caught, but not for import | since import does not have a post validation step, it needs to properly handle cases 4, 5, and 6. jira issue: crdb-58878 epic crdb-48845",1.4,Low,0.538,localized low-impact
cockroachdb/cockroach#161452,kv: split failed while applying backpressure,"there is a long-standing, known issue involving . history on this issue: , , , , , . we recently ran into it again in for this issue we ran into specifically, our current hypothesis is that the reason why ranges can grow into 8gib is caused by . once a range exceeds the split backpressure threshold, the backpressure mechanism stops applying until the hard cap (8 gib by default) is reached. with bursty write traffic, writes can pass check at the layer above latches and then push the ranges well past the threshold (range_max_bytes * kv.range.backpressure_range_size_multiplier + kv.range.backpressure_byte_tolerance). once ranges reach this regime, backpressure stops applying until the hard cap. this matches our observations, where the largest ranges we observed in the bad runs have 8 gib. the relevant improvement is proposed in (not yet merged at the time of this comment), which checks whether writes should be backpressured at the layer below latching. next steps / follow-ups from this investigation: - add a roachtest repro for this issue: - test the patch in (or set kv.range.backpressure_byte_tolerance to a very large value) and evaluate whether behavior improves. observed symptoms: - splits unable to keep up under bursty write workloads, with splits retrying several times due to timeout and slow replica grpc: - both ‚Äúgood‚Äù and ‚Äúbad‚Äù clusters showed: high replication latency (~10s), slow latches, blocked replication streams, dropped proposals due to exceeding uncommitted entry size, failed splits due to backpressure, and ranges unable to split. - the key difference is that the bad cluster allowed ranges to grow to 8 gib, while the good cluster sustained the overload without hitting the hard cap. - both clusters had high goroutine scheduling latency (p99.9 ~30 ms), with the bad cluster slightly worse and exhibiting more slow latches. good: bad: jira issue: crdb-58880",[],['FEATURE'],"['C-enhancement', 'A-kv-transactions', 'A-kv', 'branch-master', 'T-kv', 'P-3']",github,2026-01-20T19:30:08Z,,"kv: split failed while applying backpressure there is a long-standing, known issue involving . history on this issue: , , , , , . we recently ran into it again in for this issue we ran into specifically, our current hypothesis is that the reason why ranges can grow into 8gib is caused by . once a range exceeds the split backpressure threshold, the backpressure mechanism stops applying until the hard cap (8 gib by default) is reached. with bursty write traffic, writes can pass check at the layer above latches and then push the ranges well past the threshold (range_max_bytes * kv.range.backpressure_range_size_multiplier + kv.range.backpressure_byte_tolerance). once ranges reach this regime, backpressure stops applying until the hard cap. this matches our observations, where the largest ranges we observed in the bad runs have 8 gib. the relevant improvement is proposed in (not yet merged at the time of this comment), which checks whether writes should be backpressured at the layer below latching. next steps / follow-ups from this investigation: - add a roachtest repro for this issue: - test the patch in (or set kv.range.backpressure_byte_tolerance to a very large value) and evaluate whether behavior improves. observed symptoms: - splits unable to keep up under bursty write workloads, with splits retrying several times due to timeout and slow replica grpc: - both ‚Äúgood‚Äù and ‚Äúbad‚Äù clusters showed: high replication latency (~10s), slow latches, blocked replication streams, dropped proposals due to exceeding uncommitted entry size, failed splits due to backpressure, and ranges unable to split. - the key difference is that the bad cluster allowed ranges to grow to 8 gib, while the good cluster sustained the overload without hitting the hard cap. - both clusters had high goroutine scheduling latency (p99.9 ~30 ms), with the bad cluster slightly worse and exhibiting more slow latches. good: bad: jira issue: crdb-58880",2.697,Medium,0.833,functional impact
cockroachdb/cockroach#161453,roachtest: schemachange/bulkingest/nodes=12/rows=4000000000/create_index failed,roachtest.schemachange/bulkingest/nodes=12/rows=4000000000/create_index [failed]( with [artifacts]( on release-26.1 @ [6d0c67ff41f6a930e66abe8c0cd18d1f413d081c]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-20996649-1768884161-240-n12cpu4-0001 | 163.75.83.108 | 10.249.128.160 | | teamcity-20996649-1768884161-240-n12cpu4-0002 | 163.75.93.143 | 10.249.128.161 | | teamcity-20996649-1768884161-240-n12cpu4-0003 | 163.75.82.233 | 10.249.128.156 | | teamcity-20996649-1768884161-240-n12cpu4-0004 | 163.75.86.62 | 10.249.128.157 | | teamcity-20996649-1768884161-240-n12cpu4-0005 | 163.75.81.134 | 10.249.128.159 | | teamcity-20996649-1768884161-240-n12cpu4-0006 | 163.75.93.155 | 10.249.128.162 | | teamcity-20996649-1768884161-240-n12cpu4-0007 | 163.75.88.161 | 10.249.128.158 | | teamcity-20996649-1768884161-240-n12cpu4-0008 | 163.75.82.242 | 10.249.128.164 | | teamcity-20996649-1768884161-240-n12cpu4-0009 | 163.75.87.221 | 10.249.128.154 | | teamcity-20996649-1768884161-240-n12cpu4-0010 | 163.75.88.156 | 10.249.128.163 | | teamcity-20996649-1768884161-240-n12cpu4-0011 | 163.75.85.211 | 10.249.128.155 | | teamcity-20996649-1768884161-240-n12cpu4-0012 | 163.75.85.204 | 10.249.128.165 | parameters: - arch=s390x - cloud=ibm - coveragebuild=false - cpu=4 - diskcount=4 - encrypted=false - fs=ext4 - localssd=false - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( _grafana is not yet available for ibm clusters_ /cc /sql-foundations [this test on roachdash]( | [improve this report!]( jira issue: crdb-58881,[],['TESTING'],"['C-test-failure', 'O-robot', 'O-roachtest', 'T-sql-foundations', 'P-2', 's390x-test-failure', 'branch-release-26.1']",github,2026-01-20T19:46:31Z,,roachtest: schemachange/bulkingest/nodes=12/rows=4000000000/create_index failed roachtest.schemachange/bulkingest/nodes=12/rows=4000000000/create_index [failed]( with [artifacts]( on release-26.1 @ [6d0c67ff41f6a930e66abe8c0cd18d1f413d081c]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-20996649-1768884161-240-n12cpu4-0001 | 163.75.83.108 | 10.249.128.160 | | teamcity-20996649-1768884161-240-n12cpu4-0002 | 163.75.93.143 | 10.249.128.161 | | teamcity-20996649-1768884161-240-n12cpu4-0003 | 163.75.82.233 | 10.249.128.156 | | teamcity-20996649-1768884161-240-n12cpu4-0004 | 163.75.86.62 | 10.249.128.157 | | teamcity-20996649-1768884161-240-n12cpu4-0005 | 163.75.81.134 | 10.249.128.159 | | teamcity-20996649-1768884161-240-n12cpu4-0006 | 163.75.93.155 | 10.249.128.162 | | teamcity-20996649-1768884161-240-n12cpu4-0007 | 163.75.88.161 | 10.249.128.158 | | teamcity-20996649-1768884161-240-n12cpu4-0008 | 163.75.82.242 | 10.249.128.164 | | teamcity-20996649-1768884161-240-n12cpu4-0009 | 163.75.87.221 | 10.249.128.154 | | teamcity-20996649-1768884161-240-n12cpu4-0010 | 163.75.88.156 | 10.249.128.163 | | teamcity-20996649-1768884161-240-n12cpu4-0011 | 163.75.85.211 | 10.249.128.155 | | teamcity-20996649-1768884161-240-n12cpu4-0012 | 163.75.85.204 | 10.249.128.165 | parameters: - arch=s390x - cloud=ibm - coveragebuild=false - cpu=4 - diskcount=4 - encrypted=false - fs=ext4 - localssd=false - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( _grafana is not yet available for ibm clusters_ /cc /sql-foundations [this test on roachdash]( | [improve this report!]( jira issue: crdb-58881,5.2,Critical,1.0,crash-like behavior
cockroachdb/cockroach#161455,add object names (table + index)to crdb_index_usage_statistics for index usage stats,"information_schema.crdb_index_usage_statistics currently exposes table_id and index_id as crdb internal ids (e.g., 106/107/108‚Ä¶) which are not meaningful to most users on their own: to resolve these ids to names, users need to join against crdb_internal.table_indexes (or similar), but: - that creates a ‚Äúcommon flow‚Äù dependency on crdb_internal - we want to avoid documenting workflows that rely on crdb_internal if the experience isn‚Äôt production-ready / stable - information_schema does not currently have an equivalent of crdb_internal.table_indexes ### proposed solution add the object names to the information_schema table so surface both ids and names, by joining the existing usage stats with the underlying metadata that maps table/index ids ‚Üí names example columns: - database_name - schema_name - table_name - index_name - table_id - index_id - total_reads - last_read this view should support the ‚Äútypical user flow‚Äù: show me index usage stats by object name, without requiring users to query or join crdb_internal directly. jira issue: crdb-58882",[],['FEATURE'],"['C-enhancement', 'T-observability']",github,2026-01-20T20:25:36Z,,"add object names (table + index)to crdb_index_usage_statistics for index usage stats information_schema.crdb_index_usage_statistics currently exposes table_id and index_id as crdb internal ids (e.g., 106/107/108‚Ä¶) which are not meaningful to most users on their own: to resolve these ids to names, users need to join against crdb_internal.table_indexes (or similar), but: - that creates a ‚Äúcommon flow‚Äù dependency on crdb_internal - we want to avoid documenting workflows that rely on crdb_internal if the experience isn‚Äôt production-ready / stable - information_schema does not currently have an equivalent of crdb_internal.table_indexes ### proposed solution add the object names to the information_schema table so surface both ids and names, by joining the existing usage stats with the underlying metadata that maps table/index ids ‚Üí names example columns: - database_name - schema_name - table_name - index_name - table_id - index_id - total_reads - last_read this view should support the ‚Äútypical user flow‚Äù: show me index usage stats by object name, without requiring users to query or join crdb_internal directly. jira issue: crdb-58882",1.4,Low,0.538,localized low-impact
kubernetes/kubernetes#136357,image build is failing - post-kubernetes-push-e2e-jessie-dnsutils-test-images,please see: last run:,[],"['NETWORK', 'TESTING']","['priority/important-soon', 'sig/network', 'sig/testing', 'triage/accepted']",github,2026-01-20T20:28:10Z,2026-01-22T19:58:12Z,image build is failing - post-kubernetes-push-e2e-jessie-dnsutils-test-images please see: last run:,2.216,Medium,0.724,affects communication layer
cockroachdb/cockroach#161456,ccl/schemachangerccl: testpausemixedversion_ccl_alter_table_alter_primary_key_rbr failed,ccl/schemachangerccl.testpausemixedversion_ccl_alter_table_alter_primary_key_rbr [failed]( with [artifacts]( on release-25.3 @ [09f2b47406eb506426d000ebf590245d03d2d030]( help see also: [how to investigate a go test failure \(internal\)]( /cc /sql-foundations [this test on roachdash]( | [improve this report!]( jira issue: crdb-58883,[],['TESTING'],"['C-test-failure', 'O-robot', 'T-sql-foundations', 'P-2', 'branch-release-24.3', 'branch-release-25.2', 'branch-release-25.3', 'branch-release-25.4', 'branch-release-26.1', 'target-release-26.2.0', 'target-release-26.1.1', 'target-release-25.3.9', 'target-release-25.2.13', 'target-release-24.3.27']",github,2026-01-20T20:36:49Z,2026-01-27T21:42:00Z,ccl/schemachangerccl: testpausemixedversion_ccl_alter_table_alter_primary_key_rbr failed ccl/schemachangerccl.testpausemixedversion_ccl_alter_table_alter_primary_key_rbr [failed]( with [artifacts]( on release-25.3 @ [09f2b47406eb506426d000ebf590245d03d2d030]( help see also: [how to investigate a go test failure \(internal\)]( /cc /sql-foundations [this test on roachdash]( | [improve this report!]( jira issue: crdb-58883,3.8,Critical,1.0,crash-like behavior
openssl/openssl#29691,detecting bio_read errors isn't possible,"from the man page: a return of -1 (or zero) means no data was written and: 1. it might or might not indicate an error and 2. there is no other return value for an error. this means that there is no way of detecting an error case reliably. this impacts , , .",[],['BUG'],['issue: bug report'],github,2026-01-20T20:46:48Z,,"detecting bio_read errors isn't possible from the man page: a return of -1 (or zero) means no data was written and: 1. it might or might not indicate an error and 2. there is no other return value for an error. this means that there is no way of detecting an error case reliably. this impacts , , .",2.33,Medium,0.75,functional impact
cockroachdb/cockroach#161458,sentry: conn_executor.go:1117: panic: √ó (1) attached stack trace -- stack trace: | github.com/cockroachdb/cockroach/pkg/sql.(*server).serveconn.func1 | pkg/sql/conn_executor.go:1117 | [...re...,this issue was auto filed by sentry. it represents a crash or reported error on a live cluster with telemetry enabled. sentry link: [ panic message: stacktrace (expand for inline code snippets): [pkg/sql/conn_executor_exec.go#l4506-l4508](pkg/sql/conn_executor_exec.go#l4506-l4508) [pkg/sql/conn_executor_exec.go#l177-l179](pkg/sql/conn_executor_exec.go#l177-l179) [pkg/sql/conn_executor_exec.go#l1087-l1089](pkg/sql/conn_executor_exec.go#l1087-l1089) [pkg/sql/conn_executor_exec.go#l2950-l2952](pkg/sql/conn_executor_exec.go#l2950-l2952) [pkg/sql/conn_executor_exec.go#l3451-l3453](pkg/sql/conn_executor_exec.go#l3451-l3453) [pkg/sql/distsql_running.go#l2043-l2045](pkg/sql/distsql_running.go#l2043-l2045) [pkg/sql/distsql_running.go#l2040-l2042](pkg/sql/distsql_running.go#l2040-l2042) [pkg/sql/distsql_running.go#l2341-l2343](pkg/sql/distsql_running.go#l2341-l2343) [pkg/sql/distsql_running.go#l1075-l1077](pkg/sql/distsql_running.go#l1075-l1077) [pkg/sql/colflow/vectorized_flow.go#l316-l318](pkg/sql/colflow/vectorized_flow.go#l316-l318) [pkg/sql/colflow/flow_coordinator.go#l268-l270](pkg/sql/colflow/flow_coordinator.go#l268-l270) [pkg/sql/colflow/flow_coordinator.go#l234-l236](pkg/sql/colflow/flow_coordinator.go#l234-l236) [pkg/sql/colexecerror/error.go#l161-l163](pkg/sql/colexecerror/error.go#l161-l163) [pkg/sql/colflow/flow_coordinator.go#l235-l237](pkg/sql/colflow/flow_coordinator.go#l235-l237) [pkg/sql/colflow/stats.go#l104-l106](pkg/sql/colflow/stats.go#l104-l106) [pkg/sql/colexecerror/error.go#l161-l163](pkg/sql/colexecerror/error.go#l161-l163) [pkg/sql/colflow/stats.go#l95-l97](pkg/sql/colflow/stats.go#l95-l97) [pkg/sql/colexec/columnarizer.go#l177-l179](pkg/sql/colexec/columnarizer.go#l177-l179) [pkg/sql/plan_node_to_row_source.go#l208-l210](pkg/sql/plan_node_to_row_source.go#l208-l210) [pkg/sql/plan.go#l557-l559](pkg/sql/plan.go#l557-l559) [pkg/sql/values.go#l85-l87](pkg/sql/values.go#l85-l87) [pkg/sql/sem/eval/expr.go#l21-l23](pkg/sql/sem/eval/expr.go#l21-l23) [bazel-out/k8-opt/bin/pkg/sql/sem/tree/eval_expr_generated.go#l290-l292](bazel-out/k8-opt/bin/pkg/sql/sem/tree/eval_expr_generated.go#l290-l292) [pkg/sql/sem/eval/expr.go#l501-l503](pkg/sql/sem/eval/expr.go#l501-l503) [pkg/sql/sem/builtins/builtins.go#l5996-l5998](pkg/sql/sem/builtins/builtins.go#l5996-l5998) [pkg/sql/colexecerror/error.go#l316-l318](pkg/sql/colexecerror/error.go#l316-l318) [goroot/src/runtime/panic.go#l782-l784](goroot/src/runtime/panic.go#l782-l784) [pkg/sql/colexecerror/error.go#l136-l138](pkg/sql/colexecerror/error.go#l136-l138) [goroot/src/runtime/panic.go#l782-l784](goroot/src/runtime/panic.go#l782-l784) [pkg/sql/colexecerror/error.go#l136-l138](pkg/sql/colexecerror/error.go#l136-l138) [goroot/src/runtime/panic.go#l782-l784](goroot/src/runtime/panic.go#l782-l784) [pkg/sql/conn_executor.go#l1116-l1118](pkg/sql/conn_executor.go#l1116-l1118) ### tags | tag | value | | --- | --- | | command | demo | | environment | v26.1.0-rc.1 | | go version | go1.25.5 | | platform | linux amd64 | | distribution | ccl | | cockroach release | v26.1.0-rc.1 | | cockroach sha | 6c9b2f778bd29895383d493a2fa4490323499ecf | | # of cpus | 8 | | # of goroutines | 455 | jira issue: crdb-58884,[],['BUG'],"['C-bug', 'O-sentry']",github,2026-01-20T20:52:51Z,2026-01-20T20:52:53Z,sentry: conn_executor.go:1117: panic: √ó (1) attached stack trace -- stack trace: | github.com/cockroachdb/cockroach/pkg/sql.(*server).serveconn.func1 | pkg/sql/conn_executor.go:1117 | [...re... this issue was auto filed by sentry. it represents a crash or reported error on a live cluster with telemetry enabled. sentry link: [ panic message: stacktrace (expand for inline code snippets): [pkg/sql/conn_executor_exec.go#l4506-l4508](pkg/sql/conn_executor_exec.go#l4506-l4508) [pkg/sql/conn_executor_exec.go#l177-l179](pkg/sql/conn_executor_exec.go#l177-l179) [pkg/sql/conn_executor_exec.go#l1087-l1089](pkg/sql/conn_executor_exec.go#l1087-l1089) [pkg/sql/conn_executor_exec.go#l2950-l2952](pkg/sql/conn_executor_exec.go#l2950-l2952) [pkg/sql/conn_executor_exec.go#l3451-l3453](pkg/sql/conn_executor_exec.go#l3451-l3453) [pkg/sql/distsql_running.go#l2043-l2045](pkg/sql/distsql_running.go#l2043-l2045) [pkg/sql/distsql_running.go#l2040-l2042](pkg/sql/distsql_running.go#l2040-l2042) [pkg/sql/distsql_running.go#l2341-l2343](pkg/sql/distsql_running.go#l2341-l2343) [pkg/sql/distsql_running.go#l1075-l1077](pkg/sql/distsql_running.go#l1075-l1077) [pkg/sql/colflow/vectorized_flow.go#l316-l318](pkg/sql/colflow/vectorized_flow.go#l316-l318) [pkg/sql/colflow/flow_coordinator.go#l268-l270](pkg/sql/colflow/flow_coordinator.go#l268-l270) [pkg/sql/colflow/flow_coordinator.go#l234-l236](pkg/sql/colflow/flow_coordinator.go#l234-l236) [pkg/sql/colexecerror/error.go#l161-l163](pkg/sql/colexecerror/error.go#l161-l163) [pkg/sql/colflow/flow_coordinator.go#l235-l237](pkg/sql/colflow/flow_coordinator.go#l235-l237) [pkg/sql/colflow/stats.go#l104-l106](pkg/sql/colflow/stats.go#l104-l106) [pkg/sql/colexecerror/error.go#l161-l163](pkg/sql/colexecerror/error.go#l161-l163) [pkg/sql/colflow/stats.go#l95-l97](pkg/sql/colflow/stats.go#l95-l97) [pkg/sql/colexec/columnarizer.go#l177-l179](pkg/sql/colexec/columnarizer.go#l177-l179) [pkg/sql/plan_node_to_row_source.go#l208-l210](pkg/sql/plan_node_to_row_source.go#l208-l210) [pkg/sql/plan.go#l557-l559](pkg/sql/plan.go#l557-l559) [pkg/sql/values.go#l85-l87](pkg/sql/values.go#l85-l87) [pkg/sql/sem/eval/expr.go#l21-l23](pkg/sql/sem/eval/expr.go#l21-l23) [bazel-out/k8-opt/bin/pkg/sql/sem/tree/eval_expr_generated.go#l290-l292](bazel-out/k8-opt/bin/pkg/sql/sem/tree/eval_expr_generated.go#l290-l292) [pkg/sql/sem/eval/expr.go#l501-l503](pkg/sql/sem/eval/expr.go#l501-l503) [pkg/sql/sem/builtins/builtins.go#l5996-l5998](pkg/sql/sem/builtins/builtins.go#l5996-l5998) [pkg/sql/colexecerror/error.go#l316-l318](pkg/sql/colexecerror/error.go#l316-l318) [goroot/src/runtime/panic.go#l782-l784](goroot/src/runtime/panic.go#l782-l784) [pkg/sql/colexecerror/error.go#l136-l138](pkg/sql/colexecerror/error.go#l136-l138) [goroot/src/runtime/panic.go#l782-l784](goroot/src/runtime/panic.go#l782-l784) [pkg/sql/colexecerror/error.go#l136-l138](pkg/sql/colexecerror/error.go#l136-l138) [goroot/src/runtime/panic.go#l782-l784](goroot/src/runtime/panic.go#l782-l784) [pkg/sql/conn_executor.go#l1116-l1118](pkg/sql/conn_executor.go#l1116-l1118) ### tags | tag | value | | --- | --- | | command | demo | | environment | v26.1.0-rc.1 | | go version | go1.25.5 | | platform | linux amd64 | | distribution | ccl | | cockroach release | v26.1.0-rc.1 | | cockroach sha | 6c9b2f778bd29895383d493a2fa4490323499ecf | | # of cpus | 8 | | # of goroutines | 455 | jira issue: crdb-58884,7.8,Critical,1.0,crash-like behavior
python/cpython#144085,missing coverage in inspect comments and tests,"some inspect documentation and tests do not reference or validate , even though the attribute exists and is exposed. this issue tracks adding minimal comments and test coverage for completeness. ### linked prs * gh-144086 * gh-144104 * gh-144134",[],['TESTING'],['tests'],github,2026-01-20T20:53:50Z,2026-01-22T09:20:41Z,"missing coverage in inspect comments and tests some inspect documentation and tests do not reference or validate , even though the attribute exists and is exposed. this issue tracks adding minimal comments and test coverage for completeness. ### linked prs * gh-144086 * gh-144104 * gh-144134",1.6,Low,0.584,localized low-impact
rust-lang/rust#151430,[ice]:,auto-reduced (treereduce-rust): ` original: ` version information ` possibly related line of code: command: program output label +wg-trait-system-refactor,[],"['CLEANUP', 'BUG']","['I-ICE', 'T-compiler', 'C-bug', 'requires-debug-assertions', 'WG-trait-system-refactor']",github,2026-01-20T21:00:16Z,,[ice]: auto-reduced (treereduce-rust): ` original: ` version information ` possibly related line of code: command: program output label +wg-trait-system-refactor,1.8,Low,0.629,localized low-impact
pytorch/pytorch#172886,libtorch_cpu.so have unresolved symbols,"### üêõ describe the bug when 2.9.1 is built with mpi and distributed the libtorch-cpu.so (on linux) has unresolved symbols. the symbols are pointing to that does not mean that pytorch does not work. it is just an anomaly. see i fixed adding this snippet to caffe2/cmakelists.txt but i'm not sure it works on all configurations: i do not have pytorch installed on this machine, so versions is about the machine where i was building it regards ### versions collecting environment information... pytorch version: n/a is debug build: n/a cuda used to build pytorch: n/a rocm used to build pytorch: n/a os: gentoo linux (x86_64) gcc version: (gentoo 15.2.1_p20251122 p3) 15.2.1 20251122 clang version: 21.1.8 cmake version: version 4.1.4 libc version: glibc-2.41 python version: 3.13.11 (main, jan 20 2026, 19:20:08) [gcc 15.2.1 20251122] (64-bit runtime) python platform: linux-6.12.41-gentoo-x86_64-x86_64-11th_gen_intel-r-_core-tm-_i9-11900k_ .50ghz-with-glibc2.41 is cuda available: n/a cuda runtime version: could not collect cuda_module_loading set to: n/a gpu models and configuration: could not collect nvidia driver version: could not collect cudnn version: could not collect is xpu available: n/a hip runtime version: n/a miopen runtime version: n/a is xnnpack available: n/a caching allocator config: n/a cpu: architecture: x86_64 cpu op-mode(s): 32-bit, 64-bit address sizes: 39 bits physical, 48 bits virtual byte order: little endian cpu(s): 16 on-line cpu(s) list: 0-15 vendor id: genuineintel model name: 11th gen intel(r) core(tm) i9-11900k @ 3.50ghz cpu family: 6 model: 167 thread(s) per core: 2 core(s) per socket: 8 socket(s): 1 stepping: 1 cpu(s) scaling mhz: 72% cpu max mhz: 5300.0000 cpu min mhz: 800.0000 bogomips: 7008.00 flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid mpx avx512f avx512dq rdseed adx smap avx512ifma clflushopt intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req vnmi avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid fsrm md_clear flush_l1d arch_capabilities virtualization: vt-x l1d cache: 384 kib (8 instances) l1i cache: 256 kib (8 instances) l2 cache: 4 mib (8 instances) l3 cache: 16 mib (1 instance) vulnerability gather data sampling: mitigation; microcode vulnerability indirect target selection: mitigation; aligned branch/return thunks vulnerability itlb multihit: not affected vulnerability l1tf: not affected vulnerability mds: not affected vulnerability meltdown: not affected vulnerability mmio stale data: mitigation; clear cpu buffers; smt vulnerable vulnerability reg file data sampling: not affected vulnerability retbleed: mitigation; enhanced ibrs vulnerability spec rstack overflow: not affected vulnerability spec store bypass: mitigation; speculative store bypass disabled via prctl vulnerability spectre v1: mitigation; usercopy/swapgs barriers and __user pointer sanitization vulnerability spectre v2: vulnerable: eibrs with unprivileged ebpf vulnerability srbds: not affected vulnerability tsa: not affected vulnerability tsx async abort: not affected versions of relevant libraries: [pip3] could not collect [conda] could not collect cc",[],['UI'],"['module: build', 'triaged', 'module: mpi', 'actionable']",github,2026-01-20T21:06:11Z,,"libtorch_cpu.so have unresolved symbols ### üêõ describe the bug when 2.9.1 is built with mpi and distributed the libtorch-cpu.so (on linux) has unresolved symbols. the symbols are pointing to that does not mean that pytorch does not work. it is just an anomaly. see i fixed adding this snippet to caffe2/cmakelists.txt but i'm not sure it works on all configurations: i do not have pytorch installed on this machine, so versions is about the machine where i was building it regards ### versions collecting environment information... pytorch version: n/a is debug build: n/a cuda used to build pytorch: n/a rocm used to build pytorch: n/a os: gentoo linux (x86_64) gcc version: (gentoo 15.2.1_p20251122 p3) 15.2.1 20251122 clang version: 21.1.8 cmake version: version 4.1.4 libc version: glibc-2.41 python version: 3.13.11 (main, jan 20 2026, 19:20:08) [gcc 15.2.1 20251122] (64-bit runtime) python platform: linux-6.12.41-gentoo-x86_64-x86_64-11th_gen_intel-r-_core-tm-_i9-11900k_ .50ghz-with-glibc2.41 is cuda available: n/a cuda runtime version: could not collect cuda_module_loading set to: n/a gpu models and configuration: could not collect nvidia driver version: could not collect cudnn version: could not collect is xpu available: n/a hip runtime version: n/a miopen runtime version: n/a is xnnpack available: n/a caching allocator config: n/a cpu: architecture: x86_64 cpu op-mode(s): 32-bit, 64-bit address sizes: 39 bits physical, 48 bits virtual byte order: little endian cpu(s): 16 on-line cpu(s) list: 0-15 vendor id: genuineintel model name: 11th gen intel(r) core(tm) i9-11900k @ 3.50ghz cpu family: 6 model: 167 thread(s) per core: 2 core(s) per socket: 8 socket(s): 1 stepping: 1 cpu(s) scaling mhz: 72% cpu max mhz: 5300.0000 cpu min mhz: 800.0000 bogomips: 7008.00 flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush dts acpi mmx fxsr sse sse2 ss ht tm pbe syscall nx pdpe1gb rdtscp lm constant_tsc art arch_perfmon pebs bts rep_good nopl xtopology nonstop_tsc cpuid aperfmperf tsc_known_freq pni pclmulqdq dtes64 monitor ds_cpl vmx smx est tm2 ssse3 sdbg fma cx16 xtpr pdcm pcid sse4_1 sse4_2 x2apic movbe popcnt tsc_deadline_timer aes xsave avx f16c rdrand lahf_lm abm 3dnowprefetch cpuid_fault epb ssbd ibrs ibpb stibp ibrs_enhanced tpr_shadow flexpriority ept vpid ept_ad fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid mpx avx512f avx512dq rdseed adx smap avx512ifma clflushopt intel_pt avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves dtherm ida arat pln pts hwp hwp_notify hwp_act_window hwp_epp hwp_pkg_req vnmi avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid fsrm md_clear flush_l1d arch_capabilities virtualization: vt-x l1d cache: 384 kib (8 instances) l1i cache: 256 kib (8 instances) l2 cache: 4 mib (8 instances) l3 cache: 16 mib (1 instance) vulnerability gather data sampling: mitigation; microcode vulnerability indirect target selection: mitigation; aligned branch/return thunks vulnerability itlb multihit: not affected vulnerability l1tf: not affected vulnerability mds: not affected vulnerability meltdown: not affected vulnerability mmio stale data: mitigation; clear cpu buffers; smt vulnerable vulnerability reg file data sampling: not affected vulnerability retbleed: mitigation; enhanced ibrs vulnerability spec rstack overflow: not affected vulnerability spec store bypass: mitigation; speculative store bypass disabled via prctl vulnerability spectre v1: mitigation; usercopy/swapgs barriers and __user pointer sanitization vulnerability spectre v2: vulnerable: eibrs with unprivileged ebpf vulnerability srbds: not affected vulnerability tsa: not affected vulnerability tsx async abort: not affected versions of relevant libraries: [pip3] could not collect [conda] could not collect cc",7.2,Critical,1.0,"user-visible issue, crash-like behavior"
cockroachdb/cockroach#161459,kvclient/rangefeed: onvalues discards prevvalue,"the onvalues() api allows for ""bulk"" delivery of many events at once, which can substantially reduce overhead during initial scans and catch-up scans compared to delivering events to the client one at a time. initial scans do no have a previous value -- they're a snapshot in time -- but _catch up scans_ do have prior values. the onvalues hook used to delivery bulk events to the client however discards these prev-values since its signature is solely in terms of key-value pairs. pcr added onvalues to keep up with bulk operations and it doesn't use prevvalues, but other users of this client, such as ldr, do use them, so we should ensure the onvalues hook can deliver them. jira issue: crdb-58885",[],['FEATURE'],"['C-enhancement', 'A-disaster-recovery', 'T-disaster-recovery']",github,2026-01-20T21:06:24Z,2026-01-23T20:43:35Z,"kvclient/rangefeed: onvalues discards prevvalue the onvalues() api allows for ""bulk"" delivery of many events at once, which can substantially reduce overhead during initial scans and catch-up scans compared to delivering events to the client one at a time. initial scans do no have a previous value -- they're a snapshot in time -- but _catch up scans_ do have prior values. the onvalues hook used to delivery bulk events to the client however discards these prev-values since its signature is solely in terms of key-value pairs. pcr added onvalues to keep up with bulk operations and it doesn't use prevvalues, but other users of this client, such as ldr, do use them, so we should ensure the onvalues hook can deliver them. jira issue: crdb-58885",1.4,Low,0.538,localized low-impact
python/cpython#144087,consider having int() support minus sign,"the function supports all unicode digit variants so that . likewise, the function supports all space character variants to that . however, the function does not support negation variants so that succeeds while fails. i found this issue while parsing a [table of jacobi symbol values]( all of the entries are a minus sign and digit one. i would have expected all minus sign variants to be supported. ### linked prs * gh-144095",[],['FEATURE'],"['type-feature', 'interpreter-core']",github,2026-01-20T21:11:36Z,,"consider having int() support minus sign the function supports all unicode digit variants so that . likewise, the function supports all space character variants to that . however, the function does not support negation variants so that succeeds while fails. i found this issue while parsing a [table of jacobi symbol values]( all of the entries are a minus sign and digit one. i would have expected all minus sign variants to be supported. ### linked prs * gh-144095",1.4,Low,0.538,localized low-impact
openssl/openssl#29694,x509_check_ip always fails?,"i am having an issue with a ip-based server cert used on an internal net for dev debugging. my kafkacat client always fails to validate the server cert. i am doing dev testing so my server cert is probably a bit odd (generated with a script in librdkafka testing directory). however: i tracked it down through the openssl code (3.1.0 in my case) and i am wondering whether x509_check_ip can ever succeed. it calls do_x509_check with gen_ipadd. when you get into that routine it sets cnid to nid_undef, then there is a call to x509_get_ext_d2i which returns 0 in my case. then the code checks for cnid being nid_undef and it is and this causes a return 0 -- the comment leads me to believe it is trying to declare the check all done -- nothing to check. is that return 0 correct? it is being interpreted by the caller (check_id) as a failure to validate -- seems like it should be succeeding though since there is apparently nothing to validate against in the x509 data? i checked the 3.6.0 code in case there was a fix in this area, don't see anything.",[],['BUG'],['issue: bug report'],github,2026-01-20T21:26:11Z,2026-01-20T21:52:21Z,"x509_check_ip always fails? i am having an issue with a ip-based server cert used on an internal net for dev debugging. my kafkacat client always fails to validate the server cert. i am doing dev testing so my server cert is probably a bit odd (generated with a script in librdkafka testing directory). however: i tracked it down through the openssl code (3.1.0 in my case) and i am wondering whether x509_check_ip can ever succeed. it calls do_x509_check with gen_ipadd. when you get into that routine it sets cnid to nid_undef, then there is a call to x509_get_ext_d2i which returns 0 in my case. then the code checks for cnid being nid_undef and it is and this causes a return 0 -- the comment leads me to believe it is trying to declare the check all done -- nothing to check. is that return 0 correct? it is being interpreted by the caller (check_id) as a failure to validate -- seems like it should be succeeding though since there is apparently nothing to validate against in the x509 data? i checked the 3.6.0 code in case there was a fix in this area, don't see anything.",2.326,Medium,0.749,functional impact
rust-lang/rust#151433,compiler allows marker trait bounds to be omitted in implementations when using rpitit,"i tried this code: i expected an error or at least a warning, but the code compiled with no diagnostics. omitting ~~ or~~ from the implementing function does produce an error. ### meta tested with both stable (1.92) and nightly (2026-01-19) in the playground.",[],['BUG'],"['T-compiler', 'A-impl-trait', 'C-bug', 'A-auto-traits', 'T-types']",github,2026-01-20T21:48:18Z,,"compiler allows marker trait bounds to be omitted in implementations when using rpitit i tried this code: i expected an error or at least a warning, but the code compiled with no diagnostics. omitting ~~ or~~ from the implementing function does produce an error. ### meta tested with both stable (1.92) and nightly (2026-01-19) in the playground.",2.616,Medium,0.815,functional impact
cockroachdb/cockroach#161460,sql: improve error handling for as of system time before gc threshold in create table as,"## summary when using with a timestamp that has already been garbage collected, the error messaging could be improved in two ways: 1. the error lacks a postgresql error code, making it harder for clients to programmatically handle this user error 2. the logs contain a full stack trace, which is excessive noise for what is fundamentally a user error (specifying a timestamp that's too old) ## reproduction **current error:** **current logs:** full stack trace is emitted at error level (see below), which is noisy for a user-caused error. full log output ## proposed improvements 1. **add a pg error code** - this should likely use an existing code like (invalid_parameter_value) or a more specific one if appropriate, so clients can detect and handle this case programmatically. 2. **reduce log verbosity** - since this is a user error (requesting data that has been garbage collected), the full stack trace is unnecessary. log at a lower severity or without the stack trace. ## code references - error origin: ( ) - error wrapping: ( ) - underlying error type: jira issue: crdb-58886 epic crdb-58150",[],['CLEANUP'],"['C-cleanup', 'T-sql-foundations', 'target-release-26.2.0']",github,2026-01-20T21:59:01Z,2026-01-21T01:28:13Z,"sql: improve error handling for as of system time before gc threshold in create table as ## summary when using with a timestamp that has already been garbage collected, the error messaging could be improved in two ways: 1. the error lacks a postgresql error code, making it harder for clients to programmatically handle this user error 2. the logs contain a full stack trace, which is excessive noise for what is fundamentally a user error (specifying a timestamp that's too old) ## reproduction **current error:** **current logs:** full stack trace is emitted at error level (see below), which is noisy for a user-caused error. full log output ## proposed improvements 1. **add a pg error code** - this should likely use an existing code like (invalid_parameter_value) or a more specific one if appropriate, so clients can detect and handle this case programmatically. 2. **reduce log verbosity** - since this is a user error (requesting data that has been garbage collected), the full stack trace is unnecessary. log at a lower severity or without the stack trace. ## code references - error origin: ( ) - error wrapping: ( ) - underlying error type: jira issue: crdb-58886 epic crdb-58150",1.2,Low,0.493,localized low-impact
pytorch/pytorch#172896,blank page,"### üêõ describe the bug going to (as explained in this tutorial: gives me a blank page (absolutely nothing on it, not even the buttons prompting me to add some files). if i remember correctly, it worked yesterday, but today it's just entirely blank on both brave and google chrome. i tried resetting the cookies associated with this page, without success. is it a problem on my end or do other people also see a blank page there? i also tried dragging and dropping my file, but it didn't work (it simply redirected me to a ). ### versions n/a cc",[],['DOCUMENTATION'],"['high priority', 'triage review', 'module: docs', 'module: regression', 'module: third_party', 'oncall: profiler']",github,2026-01-20T22:06:04Z,2026-01-22T18:51:11Z,"blank page ### üêõ describe the bug going to (as explained in this tutorial: gives me a blank page (absolutely nothing on it, not even the buttons prompting me to add some files). if i remember correctly, it worked yesterday, but today it's just entirely blank on both brave and google chrome. i tried resetting the cookies associated with this page, without success. is it a problem on my end or do other people also see a blank page there? i also tried dragging and dropping my file, but it didn't work (it simply redirected me to a ). ### versions n/a cc",1.2,Low,0.493,localized low-impact
rust-lang/rust#151434,compiling an infinite recursion call to main function in main function by modules,"<!-- thank you for filing a bug report! üêõ please provide a short summary of the bug, along with any information you feel relevant to replicating the bug. --> i tried this code: i expected to see an error while compilation for an infinite recursion call. instead, it compiled and i got main function stack overflowed. ### meta <!-- if you're using the stable version of the compiler, you should also check if the bug also exists in the beta or nightly versions. --> : <!-- include a backtrace in the code block by setting in your environment. e.g. . --> backtrace",[],['BUG'],['C-bug'],github,2026-01-20T22:11:44Z,2026-01-20T22:23:22Z,"compiling an infinite recursion call to main function in main function by modules <!-- thank you for filing a bug report! üêõ please provide a short summary of the bug, along with any information you feel relevant to replicating the bug. --> i tried this code: i expected to see an error while compilation for an infinite recursion call. instead, it compiled and i got main function stack overflowed. ### meta <!-- if you're using the stable version of the compiler, you should also check if the bug also exists in the beta or nightly versions. --> : <!-- include a backtrace in the code block by setting in your environment. e.g. . --> backtrace",4.6,Critical,1.0,crash-like behavior
cockroachdb/cockroach#161461,roachtest: sysbench/oltp_point_select/nodes=3/cpu=32/conc=256 failed,roachtest.sysbench/oltp_point_select/nodes=3/cpu=32/conc=256 [failed]( with [artifacts]( on release-26.1 @ [6c9b2f778bd29895383d493a2fa4490323499ecf]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-20996959-1768893086-310-n4cpu32-0001 | 34.73.165.151 | 10.142.0.18 | | teamcity-20996959-1768893086-310-n4cpu32-0002 | 34.23.43.25 | 10.142.0.2 | | teamcity-20996959-1768893086-310-n4cpu32-0003 | 35.190.154.30 | 10.142.0.40 | | teamcity-20996959-1768893086-310-n4cpu32-0004 | 34.73.58.164 | 10.142.0.43 | parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=32 - diskcount=0 - encrypted=false - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-58887,[],['TESTING'],"['C-test-failure', 'O-robot', 'O-roachtest', 'T-testeng', 'branch-release-26.1']",github,2026-01-20T22:18:04Z,,roachtest: sysbench/oltp_point_select/nodes=3/cpu=32/conc=256 failed roachtest.sysbench/oltp_point_select/nodes=3/cpu=32/conc=256 [failed]( with [artifacts]( on release-26.1 @ [6c9b2f778bd29895383d493a2fa4490323499ecf]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-20996959-1768893086-310-n4cpu32-0001 | 34.73.165.151 | 10.142.0.18 | | teamcity-20996959-1768893086-310-n4cpu32-0002 | 34.23.43.25 | 10.142.0.2 | | teamcity-20996959-1768893086-310-n4cpu32-0003 | 35.190.154.30 | 10.142.0.40 | | teamcity-20996959-1768893086-310-n4cpu32-0004 | 34.73.58.164 | 10.142.0.43 | parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=32 - diskcount=0 - encrypted=false - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-58887,2.977,Medium,0.897,functional impact
cockroachdb/cockroach#161463,roachprod: automate metrics export via datadog,"currently, datadog export automation is enabled only for drt clusters [1]. while both and are already exposed via , their configuration, including (datadog) secret management would have to be done out-of-band. thus, wiring everything together into something like would simplify things. (also see the internal slack thread [2].) [1] [2] jira issue: crdb-58888",[],"['TESTING', 'FEATURE']","['C-enhancement', 'T-testeng', 'A-testeng-infra']",github,2026-01-20T22:22:56Z,,"roachprod: automate metrics export via datadog currently, datadog export automation is enabled only for drt clusters [1]. while both and are already exposed via , their configuration, including (datadog) secret management would have to be done out-of-band. thus, wiring everything together into something like would simplify things. (also see the internal slack thread [2].) [1] [2] jira issue: crdb-58888",1.5,Low,0.561,localized low-impact
python/cpython#144093,cpython 3.14.2 build failure,"# bug report ## bug description i ran the configure script to build the [experimental jit compiler, but have it be disabled by default]( and the jit compiler and therefore cpython won't build. ## cpython versions tested on 3.14.2 ## operating systems tested on macos sequoia 15.7.4 (apple silicon)",[],['UI'],"['build', 'invalid']",github,2026-01-20T22:43:03Z,2026-01-21T00:25:49Z,"cpython 3.14.2 build failure # bug report ## bug description i ran the configure script to build the [experimental jit compiler, but have it be disabled by default]( and the jit compiler and therefore cpython won't build. ## cpython versions tested on 3.14.2 ## operating systems tested on macos sequoia 15.7.4 (apple silicon)",1.8,Low,0.629,user-visible issue
openssl/openssl#29696,"the reformatting has introduced some ""problems""",the big reformat has left some truly weird indentations around. for example: line 132 shouldn't be indented but kind of okay. line 134 is just plain wrong and will break anchored greps through the source ( e.g.).,[],['BUG'],['issue: bug report'],github,2026-01-20T22:49:37Z,,"the reformatting has introduced some ""problems"" the big reformat has left some truly weird indentations around. for example: line 132 shouldn't be indented but kind of okay. line 134 is just plain wrong and will break anchored greps through the source ( e.g.).",4.2,Critical,1.0,system-wide impact
cockroachdb/cockroach#161465,backup: update logic for backup ids,"the original purpose of the option in was to validate the files in the chain. with the new behavior of only displaying one backup, this behavior ends up only validating the backup in question. we should rethink how works under this new ux. epic: crdb-57536 jira issue: crdb-58889",[],['FEATURE'],"['C-enhancement', 'A-disaster-recovery', 'T-disaster-recovery']",github,2026-01-20T22:52:39Z,,"backup: update logic for backup ids the original purpose of the option in was to validate the files in the chain. with the new behavior of only displaying one backup, this behavior ends up only validating the backup in question. we should rethink how works under this new ux. epic: crdb-57536 jira issue: crdb-58889",1.4,Low,0.538,localized low-impact
tensorflow/tensorflow#108548,misleading output when an invalid encoding parameter passed to tf.compat.as_str_any,"### issue type bug ### have you reproduced the bug with tensorflow nightly? no ### source source ### tensorflow version tensorflow 2.20.0 ### custom code yes ### os platform and distribution _no response_ ### mobile device no ### python version 3.12.12 ### bazel version _no response_ ### gcc/compiler version _no response_ ### cuda/cudnn version _no response_ ### gpu model and memory _no response_ ### current behavior? tf.compat.as_str_any silently produces an output when an invalid encoding parameter is passed (e.g. np.str_('area')). while the other apis tf.compat.as_bytes, tf.compat.as_str, tf.compat.as_text raise lookuperror ### standalone code to reproduce the issue ### relevant log output",[],['BUG'],"['type:bug', 'comp:ops', 'TF 2.19']",github,2026-01-20T22:53:04Z,2026-01-22T16:25:54Z,"misleading output when an invalid encoding parameter passed to tf.compat.as_str_any ### issue type bug ### have you reproduced the bug with tensorflow nightly? no ### source source ### tensorflow version tensorflow 2.20.0 ### custom code yes ### os platform and distribution _no response_ ### mobile device no ### python version 3.12.12 ### bazel version _no response_ ### gcc/compiler version _no response_ ### cuda/cudnn version _no response_ ### gpu model and memory _no response_ ### current behavior? tf.compat.as_str_any silently produces an output when an invalid encoding parameter is passed (e.g. np.str_('area')). while the other apis tf.compat.as_bytes, tf.compat.as_str, tf.compat.as_text raise lookuperror ### standalone code to reproduce the issue ### relevant log output",4.2,Critical,1.0,system-wide impact
pytorch/pytorch#172901,"missing semicolon in rrelu function in activation.cpp potentially causing compile error? (not tested, just from looking at the code)","in aten/src/aten/native/activation.cpp, line with: torch_check(lower.to () (), ""lower bound should be less than or equal to the upper bound"") missing semicolon at the end of the torch_check statement. ‚Ä¢ potentially causes compiler errors and prevents building the c++ backend? i‚Äòm not quite sure, maybe i am mistaken because i‚Äòve not tested this, but just thought this when looking at the code. cc",[],['UI'],"['module: build', 'triaged']",github,2026-01-20T22:53:24Z,2026-01-25T19:33:32Z,"missing semicolon in rrelu function in activation.cpp potentially causing compile error? (not tested, just from looking at the code) in aten/src/aten/native/activation.cpp, line with: torch_check(lower.to () (), ""lower bound should be less than or equal to the upper bound"") missing semicolon at the end of the torch_check statement. ‚Ä¢ potentially causes compiler errors and prevents building the c++ backend? i‚Äòm not quite sure, maybe i am mistaken because i‚Äòve not tested this, but just thought this when looking at the code. cc",1.8,Low,0.629,user-visible issue
envoyproxy/envoy#43085,envoy offers chacha20 cipher after applying compliance policy,"*title*: *envoy offers chacha20 cipher after applying compliance policy* *description*: i use envoy in fips mode (i build it locally using the official toolchain image with appropriate tag for each release) and build it with . this method works well and i have fips compliant builds: until now, we have been limiting tls 1.2 to the 4 specific fips compliant ciphers using the directives , and - and completely disabling tls 1.3, since ciphers cannot be controlled and chacha20 is not fips compliant. this approach also works well and a ssl scanner only reports the expected one protocol and 4 ciphers. now the issue - envoy 1.34 introduced that can be set to ( i removed our protocol and cipher settings and applied the new compliance policy. a new ssl scan shows the correct 4 tls 1.2 ciphers but it also offers all three tls 1.3 ciphers, including the unapproved chacha20 the explanation of seems pretty clear on this - we should not be seeing chacha20: testssl.sh 3.2.2 (latest) shows *config*: cluster configuration for the entry point: this envoy (1.37.0) was built with , configured to build fips: and built",[],['BUG'],"['bug', 'triage']",github,2026-01-20T23:01:33Z,2026-01-21T15:56:04Z,"envoy offers chacha20 cipher after applying compliance policy *title*: *envoy offers chacha20 cipher after applying compliance policy* *description*: i use envoy in fips mode (i build it locally using the official toolchain image with appropriate tag for each release) and build it with . this method works well and i have fips compliant builds: until now, we have been limiting tls 1.2 to the 4 specific fips compliant ciphers using the directives , and - and completely disabling tls 1.3, since ciphers cannot be controlled and chacha20 is not fips compliant. this approach also works well and a ssl scanner only reports the expected one protocol and 4 ciphers. now the issue - envoy 1.34 introduced that can be set to ( i removed our protocol and cipher settings and applied the new compliance policy. a new ssl scan shows the correct 4 tls 1.2 ciphers but it also offers all three tls 1.3 ciphers, including the unapproved chacha20 the explanation of seems pretty clear on this - we should not be seeing chacha20: testssl.sh 3.2.2 (latest) shows *config*: cluster configuration for the entry point: this envoy (1.37.0) was built with , configured to build fips: and built",2.506,Medium,0.789,functional impact
flutter/flutter#181219,[impeller] unit test failure: play/aikstest.toimagefromimage/opengles,### steps to reproduce verified on current master ( ) 1. build the engine 2. run ### expected results the test should pass ### actual results test fails with ### code sample n/a ### screenshots or video _no response_ ### logs _no response_ ### flutter doctor output,[],['TESTING'],"['a: tests', 'engine', 'a: error message', 'P2', 'e: impeller', 'team-engine', 'triaged-engine']",github,2026-01-20T23:29:35Z,,[impeller] unit test failure: play/aikstest.toimagefromimage/opengles ### steps to reproduce verified on current master ( ) 1. build the engine 2. run ### expected results the test should pass ### actual results test fails with ### code sample n/a ### screenshots or video _no response_ ### logs _no response_ ### flutter doctor output,1.6,Low,0.584,localized low-impact
cilium/cilium#43896,multi-homed pod cannot reach cluster services,"### is there an existing issue for this? - [x] i have searched the existing issues ### version equal or higher than v1.18.6 and lower than v1.19.0 ### what happened? i have a weird pod networking issue - i am using multus to attach a macvlan to a pod. - the macvlan is configured explicitly with a route for only the lan it is attached to - , and all return - is the kubernetes service, is a pod ip - pinging or pod cidr ips always succeeds - pinging (or trying to reach it in any other way) fails **only** when macvlan is attached - traffic is blocked even when macvlan is not enslaving the physical nic used by cilium - only the cluster service network is blocked. pinging pod ips works at first, i thought it might be due to the kernel filtering the traffic because the macvlan is enslaving the host interface used by cilium. however, the traffic is leaving via the cilium managed interface, and also even when i changed the macvlan's master to (instead of the one used by cilium - ) i still got the same result, even though the macvlan is enslaving a completely different physical nic. additionally, the routing table inside the pod (shown below) appears correct, so i don't see a reason for this not to work unless cilium is somehow to blame. that being said, i am unsure whether i have found a bug or if this is expected behaviour, and i need to adjust a configuration option. ### how can we reproduce the issue? 1. install cilium with the following options: yaml 2. install multus or have another way to add a macvlan to a pod 3. apply the following config (assuming multus is used, otherwise modify accordingly) yaml 4. check the pod has the macvlan interface attached and that the routes are correct 5. test connectivity with public ips works: 6. test connectivity with pod ips works: 7. test connectivity with service ips fails: ### cilium version 1.18.6 ### kernel version 6.18.1 (os: talos 1.12.1) ### kubernetes version 1.35.0 ### regression _no response_ ### sysdump _no response_ ### relevant log output ### anything else? _no response_ ### cilium users document - [ ] are you a user of cilium? please add yourself to the [users doc]( ### code of conduct - [x] i agree to follow this project's code of conduct",[],['BUG'],"['kind/bug', 'area/datapath', 'kind/community-report']",github,2026-01-21T00:06:51Z,,"multi-homed pod cannot reach cluster services ### is there an existing issue for this? - [x] i have searched the existing issues ### version equal or higher than v1.18.6 and lower than v1.19.0 ### what happened? i have a weird pod networking issue - i am using multus to attach a macvlan to a pod. - the macvlan is configured explicitly with a route for only the lan it is attached to - , and all return - is the kubernetes service, is a pod ip - pinging or pod cidr ips always succeeds - pinging (or trying to reach it in any other way) fails **only** when macvlan is attached - traffic is blocked even when macvlan is not enslaving the physical nic used by cilium - only the cluster service network is blocked. pinging pod ips works at first, i thought it might be due to the kernel filtering the traffic because the macvlan is enslaving the host interface used by cilium. however, the traffic is leaving via the cilium managed interface, and also even when i changed the macvlan's master to (instead of the one used by cilium - ) i still got the same result, even though the macvlan is enslaving a completely different physical nic. additionally, the routing table inside the pod (shown below) appears correct, so i don't see a reason for this not to work unless cilium is somehow to blame. that being said, i am unsure whether i have found a bug or if this is expected behaviour, and i need to adjust a configuration option. ### how can we reproduce the issue? 1. install cilium with the following options: yaml 2. install multus or have another way to add a macvlan to a pod 3. apply the following config (assuming multus is used, otherwise modify accordingly) yaml 4. check the pod has the macvlan interface attached and that the routes are correct 5. test connectivity with public ips works: 6. test connectivity with pod ips works: 7. test connectivity with service ips fails: ### cilium version 1.18.6 ### kernel version 6.18.1 (os: talos 1.12.1) ### kubernetes version 1.35.0 ### regression _no response_ ### sysdump _no response_ ### relevant log output ### anything else? _no response_ ### cilium users document - [ ] are you a user of cilium? please add yourself to the [users doc]( ### code of conduct - [x] i agree to follow this project's code of conduct",4.6,Critical,1.0,crash-like behavior
rust-lang/rust#151438,lint is maybe overzealous for auto traits,"<!-- thank you for filing a bug report! üêõ please provide a short summary of the bug, along with any information you feel relevant to replicating the bug. --> i tried this code: i expected to see this happen: and are auto traits, and thus do not (currently) take any extra space in the vtable for [^1], so the lint should not fire. (similarly if had one auto and one non-auto supertrait). instead, this happened: the lint fires, since has multiple supertraits ( and ) ([playground]( tracking issue for : this could be determined to not be a false positive, as the diagnostic is *technically* correct. however, the reason for the lint when it was introduced was: > to support upcasting with multiple supertraits, we need to store multiple vtables and this can result in extra space overhead, even if no code actually uses upcasting. this lint allows users to identify when such scenarios occur and to decide whether the additional overhead is justified under that reasoning, i think should not be linted on. there could also be an argument for not linting on empty non-auto supertraits, but that is less convincing imo. non-auto supertraits there could also be an argument for not linting on below, as and also do not take any extra vtable space. however, this is less convincing imo, since they could semver-compatibly get new items with default implementations that would give them vtable entries, whereas auto traits like / cannot (currently). ### meta <!-- if you're using the stable version of the compiler, you should also check if the bug also exists in the beta or nightly versions. --> : (no backtrace) [^1]: upcasting to just re-uses the vtable since it has a common prefix of",[],['BUG'],"['A-lints', 'T-lang', 'C-bug', 'A-dyn-trait']",github,2026-01-21T02:11:40Z,,"lint is maybe overzealous for auto traits <!-- thank you for filing a bug report! üêõ please provide a short summary of the bug, along with any information you feel relevant to replicating the bug. --> i tried this code: i expected to see this happen: and are auto traits, and thus do not (currently) take any extra space in the vtable for [^1], so the lint should not fire. (similarly if had one auto and one non-auto supertrait). instead, this happened: the lint fires, since has multiple supertraits ( and ) ([playground]( tracking issue for : this could be determined to not be a false positive, as the diagnostic is *technically* correct. however, the reason for the lint when it was introduced was: > to support upcasting with multiple supertraits, we need to store multiple vtables and this can result in extra space overhead, even if no code actually uses upcasting. this lint allows users to identify when such scenarios occur and to decide whether the additional overhead is justified under that reasoning, i think should not be linted on. there could also be an argument for not linting on empty non-auto supertraits, but that is less convincing imo. non-auto supertraits there could also be an argument for not linting on below, as and also do not take any extra vtable space. however, this is less convincing imo, since they could semver-compatibly get new items with default implementations that would give them vtable entries, whereas auto traits like / cannot (currently). ### meta <!-- if you're using the stable version of the compiler, you should also check if the bug also exists in the beta or nightly versions. --> : (no backtrace) [^1]: upcasting to just re-uses the vtable since it has a common prefix of",2.376,Medium,0.76,functional impact
containerd/containerd#12802,sporadic image pull errors/timeouts on 2.2.1,"### description we recently attempted to upgrade containerd from 2.1.3 to 2.2.1. following the upgrade we started noticing a small error rate where image pulls fail due to what seems to be a timeout. the error rate is fairly small. for a given day we saw 4,482 error out of 6,458,935 image pulls (~0.07% error rate). the main changes to our config from 2.1.3 to 2.2.1 was the configuration for parallel pulls: ### steps to reproduce the issue haven't found a way to reproduce yet. the error rate is minimal. from our initial investigation it seems like should have fixed it but potentially introduced another error path? ### describe the results you received and expected image pull failures where we don't expect to see them. ### what version of containerd are you using? 2.2.1 ### any other relevant information is the default cri base with overrides on and doesn't seem relevant to the bug being reported. w ### show configuration if it is related to cri plugin.",[],['BUG'],"['kind/bug', 'area/cri']",github,2026-01-21T02:48:50Z,,"sporadic image pull errors/timeouts on 2.2.1 ### description we recently attempted to upgrade containerd from 2.1.3 to 2.2.1. following the upgrade we started noticing a small error rate where image pulls fail due to what seems to be a timeout. the error rate is fairly small. for a given day we saw 4,482 error out of 6,458,935 image pulls (~0.07% error rate). the main changes to our config from 2.1.3 to 2.2.1 was the configuration for parallel pulls: ### steps to reproduce the issue haven't found a way to reproduce yet. the error rate is minimal. from our initial investigation it seems like should have fixed it but potentially introduced another error path? ### describe the results you received and expected image pull failures where we don't expect to see them. ### what version of containerd are you using? 2.2.1 ### any other relevant information is the default cri base with overrides on and doesn't seem relevant to the bug being reported. w ### show configuration if it is related to cri plugin.",4.6,Critical,1.0,crash-like behavior
rust-lang/rust#151440,how should try_as_dyn handle inductive cycles?,"an inductive cycle is when proving whether a type implements a trait or not relies on whether that same type implements that same trait. currently, under normal circumstances, trying to rely on an inductive cycle results in an ""overflow evaluating the requirement"" error, which is the same behavior as exceeding the recursion limit via other means. example code and error from a normal inductive cycle without inductive cycles, can cause post-mono errors by exceeding the recursion limit via non-inductive-cycle means, if the relevant function gets to codegen. this is intended. example code and error from try_as_dyn exceeding the recursion limit another example code and error from try_as_dyn exceeding the recursion limit however, try_as_dyn, for some reason, seems to treat inductive cycles as ""the type doesn't implement the trait"". example code and error with try_as_dyn and an inductive cycle that is, under normal circumstances, inductive cycles are treated like the recursion limit being exceeded normally. however, try_as_dyn treats a coinductive cycle as if the type just not implementing the trait. this seems inconsistent. furthermore, inductive cycles can sometimes be only discovered after monomorphization. (see .) as a result, it is possible to have a generic type that try_as_dyn reports as not implementing . this seems undesirable. example code and error with post-mono inductive cycle interacting with try_as_dyn note that specialization is also currently broken in the presence of inductive cycles. see . tracking issue for try_as_dyn: cc -obk ### meta reproducible on the playground with version",[],['BUG'],"['A-trait-system', 'T-libs-api', 'T-compiler', 'C-bug', 'T-types', 'A-coinduction']",github,2026-01-21T03:06:29Z,,"how should try_as_dyn handle inductive cycles? an inductive cycle is when proving whether a type implements a trait or not relies on whether that same type implements that same trait. currently, under normal circumstances, trying to rely on an inductive cycle results in an ""overflow evaluating the requirement"" error, which is the same behavior as exceeding the recursion limit via other means. example code and error from a normal inductive cycle without inductive cycles, can cause post-mono errors by exceeding the recursion limit via non-inductive-cycle means, if the relevant function gets to codegen. this is intended. example code and error from try_as_dyn exceeding the recursion limit another example code and error from try_as_dyn exceeding the recursion limit however, try_as_dyn, for some reason, seems to treat inductive cycles as ""the type doesn't implement the trait"". example code and error with try_as_dyn and an inductive cycle that is, under normal circumstances, inductive cycles are treated like the recursion limit being exceeded normally. however, try_as_dyn treats a coinductive cycle as if the type just not implementing the trait. this seems inconsistent. furthermore, inductive cycles can sometimes be only discovered after monomorphization. (see .) as a result, it is possible to have a generic type that try_as_dyn reports as not implementing . this seems undesirable. example code and error with post-mono inductive cycle interacting with try_as_dyn note that specialization is also currently broken in the presence of inductive cycles. see . tracking issue for try_as_dyn: cc -obk ### meta reproducible on the playground with version",4.6,Critical,1.0,crash-like behavior
cockroachdb/cockroach#161477,ccl/changefeedccl: testdatabaselevelchangefeedskipofflinetables failed,ccl/changefeedccl.testdatabaselevelchangefeedskipofflinetables [failed]( with [artifacts]( on master @ [5224f4df30db34f944e7c9e9badfbb8fed9f78dc]( help see also: [how to investigate a go test failure \(internal\)]( /cc /cdc [this test on roachdash]( | [improve this report!]( jira issue: crdb-58890,[],['TESTING'],"['C-test-failure', 'O-robot', 'A-cdc', 'branch-master', 'T-cdc', 'P-3']",github,2026-01-21T03:16:54Z,,ccl/changefeedccl: testdatabaselevelchangefeedskipofflinetables failed ccl/changefeedccl.testdatabaselevelchangefeedskipofflinetables [failed]( with [artifacts]( on master @ [5224f4df30db34f944e7c9e9badfbb8fed9f78dc]( help see also: [how to investigate a go test failure \(internal\)]( /cc /cdc [this test on roachdash]( | [improve this report!]( jira issue: crdb-58890,3.8,Critical,1.0,crash-like behavior
openssl/openssl#29700,no software update for my samsung. i am in beta program,"<!-- thank you for your feature request. if this is your first one, please take the time to read the following lines before posting it. note: if you're asking about how to use openssl, this isn't the right forum. please see our user support resources: please remember to put $ echo output output output output output output #include int main() { int foo = 1; printf(""%d\n"", foo); } ` -->",[],['FEATURE'],['issue: feature request'],github,2026-01-21T03:25:34Z,2026-01-21T03:38:09Z,"no software update for my samsung. i am in beta program <!-- thank you for your feature request. if this is your first one, please take the time to read the following lines before posting it. note: if you're asking about how to use openssl, this isn't the right forum. please see our user support resources: please remember to put $ echo output output output output output output #include int main() { int foo = 1; printf(""%d\n"", foo); } ` -->",3.433,High,1.0,system-wide impact
cockroachdb/cockroach#161479,pkg/sql/schemachanger/schemachanger_test: testpausemixedversion_add_column_serial_rowid failed,pkg/sql/schemachanger/schemachanger_test.testpausemixedversion_add_column_serial_rowid [failed]( with [artifacts]( on master @ [c3da6456a16400c5333dfe4a17555ed58ef7b745]( help see also: [how to investigate a go test failure \(internal\)]( /cc /sql-foundations [this test on roachdash]( | [improve this report!]( jira issue: crdb-58891,[],"['TESTING', 'BUG']","['C-bug', 'C-test-failure', 'O-robot', 'X-duplicate', 'branch-master', 'T-kv', 'A-kv-rangefeed', 'P-2']",github,2026-01-21T04:09:55Z,,pkg/sql/schemachanger/schemachanger_test: testpausemixedversion_add_column_serial_rowid failed pkg/sql/schemachanger/schemachanger_test.testpausemixedversion_add_column_serial_rowid [failed]( with [artifacts]( on master @ [c3da6456a16400c5333dfe4a17555ed58ef7b745]( help see also: [how to investigate a go test failure \(internal\)]( /cc /sql-foundations [this test on roachdash]( | [improve this report!]( jira issue: crdb-58891,3.123,High,0.93,crash-like behavior
cockroachdb/cockroach#161483,server: enable drpc for,**describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58892,[],['TESTING'],"['C-test-failure', 'T-db-server']",github,2026-01-21T06:07:50Z,,server: enable drpc for **describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58892,1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161484,storageapi: enable drpc for teststatuslogredaction,**describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58893,[],['TESTING'],"['C-test-failure', 'T-db-server']",github,2026-01-21T06:11:37Z,,storageapi: enable drpc for teststatuslogredaction **describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58893,1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161485,applicationapi: enable drpc for teststatusapistatements,**describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58894,[],['TESTING'],"['C-test-failure', 'T-db-server']",github,2026-01-21T06:12:59Z,,applicationapi: enable drpc for teststatusapistatements **describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58894,1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161486,sql/tests: testrandomsyntaxsqlsmith failed,sql/tests.testrandomsyntaxsqlsmith [failed]( with [artifacts]( on release-26.1 @ [6ac107383aa8b7246f0d6448366407b6f53e71e4]( help see also: [how to investigate a go test failure \(internal\)]( /cc /sql-foundations [this test on roachdash]( | [improve this report!]( jira issue: crdb-58895,[],['TESTING'],"['C-test-failure', 'O-robot', 'T-sql-queries', 'branch-release-26.1']",github,2026-01-21T07:00:19Z,,sql/tests: testrandomsyntaxsqlsmith failed sql/tests.testrandomsyntaxsqlsmith [failed]( with [artifacts]( on release-26.1 @ [6ac107383aa8b7246f0d6448366407b6f53e71e4]( help see also: [how to investigate a go test failure \(internal\)]( /cc /sql-foundations [this test on roachdash]( | [improve this report!]( jira issue: crdb-58895,1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161487,ccl/changefeedccl: testchangefeednemeses failed,ccl/changefeedccl.testchangefeednemeses [failed]( on master @ [c150609232aa6bf9d4e484fdcb58eeb55eba8bc4]( parameters: - attempt=1 - run=10 - shard=15 help see also: [how to investigate a go test failure \(internal\)]( /cc /cdc [this test on roachdash]( | [improve this report!]( jira issue: crdb-58896,[],['TESTING'],"['C-test-failure', 'O-robot', 'A-cdc', 'branch-master', 'T-cdc', 'P-2']",github,2026-01-21T07:08:43Z,,ccl/changefeedccl: testchangefeednemeses failed ccl/changefeedccl.testchangefeednemeses [failed]( on master @ [c150609232aa6bf9d4e484fdcb58eeb55eba8bc4]( parameters: - attempt=1 - run=10 - shard=15 help see also: [how to investigate a go test failure \(internal\)]( /cc /cdc [this test on roachdash]( | [improve this report!]( jira issue: crdb-58896,3.8,Critical,1.0,crash-like behavior
cockroachdb/cockroach#161488,roachtest: db-console/cypress-pages failed,roachtest.db-console/cypress-pages [failed]( with [artifacts]( on master @ [c150609232aa6bf9d4e484fdcb58eeb55eba8bc4]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21002858-1768978097-19-n4cpu4-0001 | 40.76.231.48 | 10.1.0.124 | | teamcity-21002858-1768978097-19-n4cpu4-0002 | 20.55.102.2 | 10.1.0.123 | | teamcity-21002858-1768978097-19-n4cpu4-0003 | 74.235.139.121 | 10.1.0.125 | | teamcity-21002858-1768978097-19-n4cpu4-0004 | 20.127.220.210 | 10.1.0.149 | parameters: - arch=amd64 - cloud=azure - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - metamorphicwritebuffering=true - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( _grafana is not yet available for azure clusters_ /cc /obs-prs [this test on roachdash]( | [improve this report!]( jira issue: crdb-58897,[],['TESTING'],"['C-test-failure', 'O-robot', 'O-roachtest', 'branch-master', 'T-observability', 'target-release-26.2.0']",github,2026-01-21T07:11:21Z,2026-01-21T16:46:55Z,roachtest: db-console/cypress-pages failed roachtest.db-console/cypress-pages [failed]( with [artifacts]( on master @ [c150609232aa6bf9d4e484fdcb58eeb55eba8bc4]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21002858-1768978097-19-n4cpu4-0001 | 40.76.231.48 | 10.1.0.124 | | teamcity-21002858-1768978097-19-n4cpu4-0002 | 20.55.102.2 | 10.1.0.123 | | teamcity-21002858-1768978097-19-n4cpu4-0003 | 74.235.139.121 | 10.1.0.125 | | teamcity-21002858-1768978097-19-n4cpu4-0004 | 20.127.220.210 | 10.1.0.149 | parameters: - arch=amd64 - cloud=azure - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - metamorphicwritebuffering=true - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( _grafana is not yet available for azure clusters_ /cc /obs-prs [this test on roachdash]( | [improve this report!]( jira issue: crdb-58897,2.882,Medium,0.875,functional impact
cockroachdb/cockroach#161489,roachtest: cluster_creation failed,roachtest.cluster_creation [failed]( with [artifacts]( on release-26.1.0-rc @ [6c9b2f778bd29895383d493a2fa4490323499ecf]( parameters: - metamorphicfilesystem=ext4 - metamorphicvolumetype=io2 - cloud=aws - coveragebuild=false - cpu=8 - diskcount=0 - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( same failure on other branches - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-24.3] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.2] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-24.3.25-rc] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.4.3-rc] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.2.11-rc] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.3.7-rc] - roachtest: cluster_creation failed [o-roachtest o-robot o-rsg t-testeng x-infra-flake branch-master] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-26.1] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.3] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.3] /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-58898,[],['TESTING'],"['O-robot', 'O-roachtest', 'T-testeng', 'X-infra-flake', 'branch-release-26.1.0-rc']",github,2026-01-21T07:21:46Z,,roachtest: cluster_creation failed roachtest.cluster_creation [failed]( with [artifacts]( on release-26.1.0-rc @ [6c9b2f778bd29895383d493a2fa4490323499ecf]( parameters: - metamorphicfilesystem=ext4 - metamorphicvolumetype=io2 - cloud=aws - coveragebuild=false - cpu=8 - diskcount=0 - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( same failure on other branches - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-24.3] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.2] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-24.3.25-rc] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.4.3-rc] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.2.11-rc] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.3.7-rc] - roachtest: cluster_creation failed [o-roachtest o-robot o-rsg t-testeng x-infra-flake branch-master] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-26.1] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.3] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.3] /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-58898,3.188,High,0.945,functional impact
python/cpython#144100,assertion failed: typeinfo->proto in pycpointertype_from_param_impl when using deprecated pointer(str) in argtypes,# bug report ### bug description: ### cpython versions tested on: cpython main branch ### operating systems tested on: linux ### linked prs * gh-144108 * gh-144244 * gh-144245,[],['BUG'],"['type-bug', 'topic-ctypes', 'extension-modules']",github,2026-01-21T07:49:08Z,2026-01-26T12:31:41Z,assertion failed: typeinfo->proto in pycpointertype_from_param_impl when using deprecated pointer(str) in argtypes # bug report ### bug description: ### cpython versions tested on: cpython main branch ### operating systems tested on: linux ### linked prs * gh-144108 * gh-144244 * gh-144245,2.22,Medium,0.724,functional impact
cockroachdb/cockroach#161490,importer: distributed import not properly checkpointing state,roachtest.import/nodeshutdown/coordinator/distmerge=true/nodes=4 [failed]( with [artifacts]( on master @ [c150609232aa6bf9d4e484fdcb58eeb55eba8bc4]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21002847-1768979735-01-n4cpu4-0001 | 35.196.42.36 | 10.142.3.59 | | teamcity-21002847-1768979735-01-n4cpu4-0002 | 34.139.90.33 | 10.142.3.64 | | teamcity-21002847-1768979735-01-n4cpu4-0003 | 34.148.128.123 | 10.142.3.58 | | teamcity-21002847-1768979735-01-n4cpu4-0004 | 34.23.26.204 | 10.142.3.66 | parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - metamorphicleases=expiration - metamorphicwritebuffering=true - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( [this test on roachdash]( | [improve this report!]( jira issue: crdb-58899,[],['TESTING'],"['C-test-failure', 'O-robot', 'O-roachtest', 'branch-master', 'release-blocker', 'T-sql-queries']",github,2026-01-21T07:49:41Z,,importer: distributed import not properly checkpointing state roachtest.import/nodeshutdown/coordinator/distmerge=true/nodes=4 [failed]( with [artifacts]( on master @ [c150609232aa6bf9d4e484fdcb58eeb55eba8bc4]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21002847-1768979735-01-n4cpu4-0001 | 35.196.42.36 | 10.142.3.59 | | teamcity-21002847-1768979735-01-n4cpu4-0002 | 34.139.90.33 | 10.142.3.64 | | teamcity-21002847-1768979735-01-n4cpu4-0003 | 34.148.128.123 | 10.142.3.58 | | teamcity-21002847-1768979735-01-n4cpu4-0004 | 34.23.26.204 | 10.142.3.66 | parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - metamorphicleases=expiration - metamorphicwritebuffering=true - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( [this test on roachdash]( | [improve this report!]( jira issue: crdb-58899,3.285,High,0.967,system-wide impact
cockroachdb/cockroach#161491,roachtest: import/nodeshutdown/worker/distmerge=true/nodes=4 failed,roachtest.import/nodeshutdown/worker/distmerge=true/nodes=4 [failed]( with [artifacts]( on master @ [c150609232aa6bf9d4e484fdcb58eeb55eba8bc4]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21002847-1768979735-07-n4cpu4-0001 | 34.26.90.152 | 10.142.3.52 | | teamcity-21002847-1768979735-07-n4cpu4-0002 | 34.148.178.87 | 10.142.3.68 | | teamcity-21002847-1768979735-07-n4cpu4-0003 | 35.196.157.51 | 10.142.3.45 | | teamcity-21002847-1768979735-07-n4cpu4-0004 | 34.26.69.8 | 10.142.3.54 | parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - metamorphicleases=default - metamorphicwritebuffering=true - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( [this test on roachdash]( | [improve this report!]( jira issue: crdb-58900,[],['TESTING'],"['C-test-failure', 'O-robot', 'O-roachtest', 'branch-master', 'release-blocker', 'T-sql-queries']",github,2026-01-21T07:51:05Z,2026-01-27T00:54:04Z,roachtest: import/nodeshutdown/worker/distmerge=true/nodes=4 failed roachtest.import/nodeshutdown/worker/distmerge=true/nodes=4 [failed]( with [artifacts]( on master @ [c150609232aa6bf9d4e484fdcb58eeb55eba8bc4]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21002847-1768979735-07-n4cpu4-0001 | 34.26.90.152 | 10.142.3.52 | | teamcity-21002847-1768979735-07-n4cpu4-0002 | 34.148.178.87 | 10.142.3.68 | | teamcity-21002847-1768979735-07-n4cpu4-0003 | 35.196.157.51 | 10.142.3.45 | | teamcity-21002847-1768979735-07-n4cpu4-0004 | 34.26.69.8 | 10.142.3.54 | parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - metamorphicleases=default - metamorphicwritebuffering=true - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( [this test on roachdash]( | [improve this report!]( jira issue: crdb-58900,2.806,Medium,0.858,functional impact
pandas-dev/pandas#63787,bug: inconsistent bom handling in pd.read_csv with encoding='utf-8',"### pandas version checks - [x] i have checked that this issue has not already been reported. - [x] i have confirmed this bug exists on the [latest version]( of pandas. - [ ] i have confirmed this bug exists on the [main branch]( of pandas. ### reproducible example ### issue description when reading a utf-8 encoded csv that contains a bom, specifying pandas strips the bom from the first column header and textwrapper leaves it as an invisible character. the outcome depends on the parsing, but this behaviour is undocumented and inconsistent. ### expected behavior i expected either to always keep the bom (requiring to remove it) or to always strip it, regardless of parser. at minimum, the documentation should clarify how boms are handled across engines when using . ### installed versions replace this line with the output of pd.show_versions()",[],['BUG'],"['Bug', 'IO CSV']",github,2026-01-21T07:51:36Z,,"bug: inconsistent bom handling in pd.read_csv with encoding='utf-8' ### pandas version checks - [x] i have checked that this issue has not already been reported. - [x] i have confirmed this bug exists on the [latest version]( of pandas. - [ ] i have confirmed this bug exists on the [main branch]( of pandas. ### reproducible example ### issue description when reading a utf-8 encoded csv that contains a bom, specifying pandas strips the bom from the first column header and textwrapper leaves it as an invisible character. the outcome depends on the parsing, but this behaviour is undocumented and inconsistent. ### expected behavior i expected either to always keep the bom (requiring to remove it) or to always strip it, regardless of parser. at minimum, the documentation should clarify how boms are handled across engines when using . ### installed versions replace this line with the output of pd.show_versions()",2.245,Medium,0.73,functional impact
electron/electron#49465,can not load bundled devtools in browserwindow,"### preflight checklist - [x] i have read the [contributing guidelines]( for this project. - [x] i agree to follow the [code of conduct]( that this project adheres to. - [x] i have searched the [issue tracker]( for a bug report that matches the one i want to file, without success. ### electron version 40.0.0 ### what operating system(s) are you using? macos ### operating system version tahoe (26.2) ### what arch are you using? arm64 (including apple silicon) ### last known working electron version 39.2.7 ### does the issue also appear in chromium / google chrome? no ### expected behavior loads devtools bundled with electron on browser window. i use this to inspect remote targets with devtools protocol. ### actual behavior window is blank. can not even open devtools on the blank window to see what happened. ### testcase gist url ### additional information _no response_",[],['BUG'],"['platform/macOS', 'component/devtools', 'bug :beetle:', 'has-repro-gist', '40-x-y']",github,2026-01-21T07:55:30Z,2026-01-22T01:05:28Z,"can not load bundled devtools in browserwindow ### preflight checklist - [x] i have read the [contributing guidelines]( for this project. - [x] i agree to follow the [code of conduct]( that this project adheres to. - [x] i have searched the [issue tracker]( for a bug report that matches the one i want to file, without success. ### electron version 40.0.0 ### what operating system(s) are you using? macos ### operating system version tahoe (26.2) ### what arch are you using? arm64 (including apple silicon) ### last known working electron version 39.2.7 ### does the issue also appear in chromium / google chrome? no ### expected behavior loads devtools bundled with electron on browser window. i use this to inspect remote targets with devtools protocol. ### actual behavior window is blank. can not even open devtools on the blank window to see what happened. ### testcase gist url ### additional information _no response_",2.51,Medium,0.79,functional impact
cockroachdb/cockroach#161492,pkg/sql/opt/exec/execbuilder/tests/5node/5node_test: testexecbuild_distsql_scan failed,pkg/sql/opt/exec/execbuilder/tests/5node/5node_test.testexecbuild_distsql_scan [failed]( on release-25.4.3-rc @ [71d853623f0d1f589fd5c727a1c4aec8a43e62e0]( parameters: - attempt=1 - run=4 - shard=15 help see also: [how to investigate a go test failure \(internal\)]( [this test on roachdash]( | [improve this report!]( jira issue: crdb-58901,[],['TESTING'],"['C-test-failure', 'O-robot', 'X-duplicate', 'release-blocker', 'T-sql-queries', 'branch-release-25.4.3-rc']",github,2026-01-21T08:05:37Z,2026-01-21T13:54:34Z,pkg/sql/opt/exec/execbuilder/tests/5node/5node_test: testexecbuild_distsql_scan failed pkg/sql/opt/exec/execbuilder/tests/5node/5node_test.testexecbuild_distsql_scan [failed]( on release-25.4.3-rc @ [71d853623f0d1f589fd5c727a1c4aec8a43e62e0]( parameters: - attempt=1 - run=4 - shard=15 help see also: [how to investigate a go test failure \(internal\)]( [this test on roachdash]( | [improve this report!]( jira issue: crdb-58901,1.6,Low,0.584,localized low-impact
envoyproxy/envoy#43090,newer release available : v41.0.0 (current: v24.0.4),package name: com_github_wasmtime .0.4 current version: v24.0.4 -07-18 available version: v41.0.0 -01-20 upstream releases:,[],['UI'],"['dependencies', 'no stalebot', 'area/build']",github,2026-01-21T08:11:11Z,2026-01-22T08:10:50Z,newer release available : v41.0.0 (current: v24.0.4) package name: com_github_wasmtime .0.4 current version: v24.0.4 -07-18 available version: v41.0.0 -01-20 upstream releases:,1.8,Low,0.629,user-visible issue
envoyproxy/envoy#43091,newer release available : v4.2.2 (current: v4.2.1),package name: fips_cmake_linux_aarch64 .2.1 current version: v4.2.1 -12-08 available version: v4.2.2 -01-20 upstream releases:,[],['UI'],"['dependencies', 'no stalebot', 'area/build']",github,2026-01-21T08:11:12Z,2026-01-28T08:11:02Z,newer release available : v4.2.2 (current: v4.2.1) package name: fips_cmake_linux_aarch64 .2.1 current version: v4.2.1 -12-08 available version: v4.2.2 -01-20 upstream releases:,1.8,Low,0.629,user-visible issue
envoyproxy/envoy#43092,newer release available : v4.2.2 (current: v4.2.1),package name: fips_cmake_linux_x86_64 .2.1 current version: v4.2.1 -12-08 available version: v4.2.2 -01-20 upstream releases:,[],['UI'],"['dependencies', 'no stalebot', 'area/build']",github,2026-01-21T08:11:12Z,2026-01-28T08:11:05Z,newer release available : v4.2.2 (current: v4.2.1) package name: fips_cmake_linux_x86_64 .2.1 current version: v4.2.1 -12-08 available version: v4.2.2 -01-20 upstream releases:,1.8,Low,0.629,user-visible issue
envoyproxy/envoy#43093,newer release available : 1.8.1 (current: 1.7.0),package name: rules_python .7.0 current version: 1.7.0 -11-14 available version: 1.8.1 -01-21 upstream releases:,[],['UI'],"['dependencies', 'no stalebot', 'area/build']",github,2026-01-21T08:11:13Z,2026-01-25T08:09:50Z,newer release available : 1.8.1 (current: 1.7.0) package name: rules_python .7.0 current version: 1.7.0 -11-14 available version: 1.8.1 -01-21 upstream releases:,1.8,Low,0.629,user-visible issue
cockroachdb/cockroach#161493,roachtest: db-console/mixed-version-endpoints failed,roachtest.db-console/mixed-version-endpoints [failed]( with [artifacts]( on release-25.4 @ [4319825e81ba935cd72604fd319210ca6d2e6b85]( parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - encrypted=false - mvtdeploymentmode=system-only - mvtversions=v24.3.19 ‚Üí v25.1.2 ‚Üí v25.2.11 ‚Üí v25.3.5 ‚Üí release-25.4 - runtimeassertionsbuild=false - ssd=0 help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( same failure on other branches - roachtest: db-console/mixed-version-endpoints failed [c-test-failure o-roachtest o-robot t-testeng branch-release-26.1 s390x-test-failure] - roachtest: db-console/mixed-version-endpoints failed [c-test-failure o-roachtest o-robot t-testeng branch-master s390x-test-failure] /cc /obs-prs [this test on roachdash]( | [improve this report!]( jira issue: crdb-58902,[],['TESTING'],"['C-test-failure', 'O-robot', 'O-roachtest', 'release-blocker', 'T-observability', 'branch-release-25.4']",github,2026-01-21T08:12:37Z,2026-01-27T18:22:42Z,roachtest: db-console/mixed-version-endpoints failed roachtest.db-console/mixed-version-endpoints [failed]( with [artifacts]( on release-25.4 @ [4319825e81ba935cd72604fd319210ca6d2e6b85]( parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - encrypted=false - mvtdeploymentmode=system-only - mvtversions=v24.3.19 ‚Üí v25.1.2 ‚Üí v25.2.11 ‚Üí v25.3.5 ‚Üí release-25.4 - runtimeassertionsbuild=false - ssd=0 help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( same failure on other branches - roachtest: db-console/mixed-version-endpoints failed [c-test-failure o-roachtest o-robot t-testeng branch-release-26.1 s390x-test-failure] - roachtest: db-console/mixed-version-endpoints failed [c-test-failure o-roachtest o-robot t-testeng branch-master s390x-test-failure] /cc /obs-prs [this test on roachdash]( | [improve this report!]( jira issue: crdb-58902,3.135,High,0.932,functional impact
cockroachdb/cockroach#161494,roachtest: c2c/disconnect failed,"**note:** this build has runtime assertions enabled. if the same failure was hit in a run without assertions enabled, there should be a similar failure without this message. if there isn& ;t one, then this failure is likely due to an assertion violation or (assertion) timeout. roachtest.c2c/disconnect [failed]( with [artifacts]( on release-24.1 @ [4446109113f9770eedffc18e0a8e0950a7002ec1]( parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - encrypted=false - fs=ext4 - localssd=true - runtimeassertionsbuild=true - ssd=0 help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( /cc /disaster-recovery [this test on roachdash]( | [improve this report!]( jira issue: crdb-58903",[],['TESTING'],"['C-test-failure', 'O-robot', 'A-disaster-recovery', 'O-roachtest', 'release-blocker', 'T-disaster-recovery', 'branch-release-24.1', 'B-runtime-assertions-enabled']",github,2026-01-21T08:15:33Z,2026-01-26T19:21:49Z,"roachtest: c2c/disconnect failed **note:** this build has runtime assertions enabled. if the same failure was hit in a run without assertions enabled, there should be a similar failure without this message. if there isn& ;t one, then this failure is likely due to an assertion violation or (assertion) timeout. roachtest.c2c/disconnect [failed]( with [artifacts]( on release-24.1 @ [4446109113f9770eedffc18e0a8e0950a7002ec1]( parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - encrypted=false - fs=ext4 - localssd=true - runtimeassertionsbuild=true - ssd=0 help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( /cc /disaster-recovery [this test on roachdash]( | [improve this report!]( jira issue: crdb-58903",2.882,Medium,0.875,functional impact
tikv/tikv#19303,expose tikv grpc-keepalive-permit-without-calls as a config item,"## feature request ### is your feature request related to a problem? please describe: tikv grpc connections leak when client shutdown without closing tcp connection ### describe the feature you'd like: tikv expose grpc-keepalive-permit-without-calls as a config item instead of keep it as false by default. ### describe alternatives you've considered: ### teachability, documentation, adoption, migration strategy:",[],['FEATURE'],"['contribution', 'type/feature-request', 'first-time-contributor']",github,2026-01-21T08:18:14Z,,"expose tikv grpc-keepalive-permit-without-calls as a config item ## feature request ### is your feature request related to a problem? please describe: tikv grpc connections leak when client shutdown without closing tcp connection ### describe the feature you'd like: tikv expose grpc-keepalive-permit-without-calls as a config item instead of keep it as false by default. ### describe alternatives you've considered: ### teachability, documentation, adoption, migration strategy:",1.4,Low,0.538,localized low-impact
electron/electron#49466,clipboard api support security options,"### preflight checklist - [x] i have read the [contributing guidelines]( for this project. - [x] i agree to follow the [code of conduct]( that this project adheres to. - [x] i have searched the [issue tracker]( for a feature request that matches the one i want to file, without success. ### problem description on macos, [when a user signs into icloud, the general pasteboard automatically transfers its contents to nearby devices that use the same icloud account]( and the api provide some options to disable this behavior, wonder to know if this feature can be supported in electron ? ### proposed solution hope clipboard write data api can support related options ### alternatives considered no ### additional information _no response_",[],['FEATURE'],['enhancement :sparkles:'],github,2026-01-21T08:27:05Z,,"clipboard api support security options ### preflight checklist - [x] i have read the [contributing guidelines]( for this project. - [x] i agree to follow the [code of conduct]( that this project adheres to. - [x] i have searched the [issue tracker]( for a feature request that matches the one i want to file, without success. ### problem description on macos, [when a user signs into icloud, the general pasteboard automatically transfers its contents to nearby devices that use the same icloud account]( and the api provide some options to disable this behavior, wonder to know if this feature can be supported in electron ? ### proposed solution hope clipboard write data api can support related options ### alternatives considered no ### additional information _no response_",1.4,Low,0.538,localized low-impact
etcd-io/etcd#21178,panic: tocommit(...) out of range [lastindex(0)] on fresh learner join after member remove/add,"### bug report criteria - [x] this bug report is not security related, security issues should be disclosed privately via security .io. - [x] this is not a support request or question, support requests or questions should be raised in the etcd [discussion forums]( - [x] you have read the etcd [bug reporting guidelines]( - [x] existing open issues along with etcd [frequently asked questions]( have been checked and this is not a duplicate. ### what happened? we are observing a consistent panic when a node that previously served as a **voting member** is re-added to the cluster as a **learner member** after a failure and a data directory reset. **the workflow:** 1. **initial state:** healthy cluster with 3 voting members (**a**, **b**, **c**) and 1 learner member (**d**). 2. **failure:** member **a** is forced offline (binary renamed to ). 3. **recovery phase 1:** our operator detects **a** is unhealthy and executes . 4. **promotion:** per etcd behavior, the learner (**d**) is automatically promoted to a voting member. the cluster now consists of **b**, **c**, and **d** as voters. 5. **recovery phase 2 (the backfill):** we restore the binary on the node where **a** resided. our workflow attempts to add this node back as a **learner** to maintain the desired cluster shape. 6. **the crash:** we wipe the data directory of the old member **a**, perform with a new name/id, and start the process. **result:** the new process immediately panics upon receiving its first heartbeat from the leader: . ### what did you expect to happen? the node that previously held the identity of member **a** should be able to join as a brand-new learner with a clean slate (index 0) and synchronize from the current leader without a range panic. ### how can we reproduce it (as minimally and precisely as possible)? 1. start a cluster with 3 voters and 1 learner. 2. kill one voter. 3. remove the dead voter from the member list. 4. verify the learner has been promoted to a voter. 5. on the node of the original dead voter: * wipe the data directory. * add it back to the cluster using . * start the etcd process with . 6. observe the panic on the new learner. ### anything else we need to know? * **strictly learner issue:** if we try to add this node back as a **voter** instead of a learner, the issue does not occur. * **id & name:** the re-added member has a fresh member id and a new unique name, but it reuses the **peer url** of the original voter **a**. * **mtls:** the cluster uses mtls for all peer and client communications. * **member list confirmation:** shows the cluster is healthy with 3 voters before we attempt to add the learner back. ### etcd version (please run commands below) ### etcd configuration (command line flags or environment variables) ### etcd debug information (please run commands below, feel free to obfuscate the ip address or fqdn in the output) *(note: is the new learner that is panicking).* ### relevant log output",[],['BUG'],['type/bug'],github,2026-01-21T08:38:35Z,,"panic: tocommit(...) out of range [lastindex(0)] on fresh learner join after member remove/add ### bug report criteria - [x] this bug report is not security related, security issues should be disclosed privately via security .io. - [x] this is not a support request or question, support requests or questions should be raised in the etcd [discussion forums]( - [x] you have read the etcd [bug reporting guidelines]( - [x] existing open issues along with etcd [frequently asked questions]( have been checked and this is not a duplicate. ### what happened? we are observing a consistent panic when a node that previously served as a **voting member** is re-added to the cluster as a **learner member** after a failure and a data directory reset. **the workflow:** 1. **initial state:** healthy cluster with 3 voting members (**a**, **b**, **c**) and 1 learner member (**d**). 2. **failure:** member **a** is forced offline (binary renamed to ). 3. **recovery phase 1:** our operator detects **a** is unhealthy and executes . 4. **promotion:** per etcd behavior, the learner (**d**) is automatically promoted to a voting member. the cluster now consists of **b**, **c**, and **d** as voters. 5. **recovery phase 2 (the backfill):** we restore the binary on the node where **a** resided. our workflow attempts to add this node back as a **learner** to maintain the desired cluster shape. 6. **the crash:** we wipe the data directory of the old member **a**, perform with a new name/id, and start the process. **result:** the new process immediately panics upon receiving its first heartbeat from the leader: . ### what did you expect to happen? the node that previously held the identity of member **a** should be able to join as a brand-new learner with a clean slate (index 0) and synchronize from the current leader without a range panic. ### how can we reproduce it (as minimally and precisely as possible)? 1. start a cluster with 3 voters and 1 learner. 2. kill one voter. 3. remove the dead voter from the member list. 4. verify the learner has been promoted to a voter. 5. on the node of the original dead voter: * wipe the data directory. * add it back to the cluster using . * start the etcd process with . 6. observe the panic on the new learner. ### anything else we need to know? * **strictly learner issue:** if we try to add this node back as a **voter** instead of a learner, the issue does not occur. * **id & name:** the re-added member has a fresh member id and a new unique name, but it reuses the **peer url** of the original voter **a**. * **mtls:** the cluster uses mtls for all peer and client communications. * **member list confirmation:** shows the cluster is healthy with 3 voters before we attempt to add the learner back. ### etcd version (please run commands below) ### etcd configuration (command line flags or environment variables) ### etcd debug information (please run commands below, feel free to obfuscate the ip address or fqdn in the output) *(note: is the new learner that is panicking).* ### relevant log output",6.4,Critical,1.0,crash-like behavior
pytorch/pytorch#172936,[doc] fix table formatting/line breaks in xpu get started docs,"### üìö the doc issue link: in the ""hardware prerequisite"" section of the xpu documentation, the table content is currently missing proper line breaks, leading to poor readability. since the 2.10 release tracker no longer accepts cherry-picks, could you please help apply this fix directly to the html files for release 2.10? i will created a pr to fix it in main branch. ### suggest a potential alternative/fix _no response_ cc",[],['DOCUMENTATION'],"['module: docs', 'triaged', 'module: xpu']",github,2026-01-21T08:46:27Z,2026-01-23T01:44:14Z,"[doc] fix table formatting/line breaks in xpu get started docs ### üìö the doc issue link: in the ""hardware prerequisite"" section of the xpu documentation, the table content is currently missing proper line breaks, leading to poor readability. since the 2.10 release tracker no longer accepts cherry-picks, could you please help apply this fix directly to the html files for release 2.10? i will created a pr to fix it in main branch. ### suggest a potential alternative/fix _no response_ cc",1.2,Low,0.493,localized low-impact
cockroachdb/cockroach#161495,roachtest: backup-restore/online-restore-recovery failed [failed generating operation: insertrow: descriptor is being dropped],"**note:** this build has runtime assertions enabled. if the same failure was hit in a run without assertions enabled, there should be a similar failure without this message. if there isn& ;t one, then this failure is likely due to an assertion violation or (assertion) timeout. roachtest.backup-restore/online-restore-recovery [failed]( with [artifacts]( on release-26.1 @ [6ac107383aa8b7246f0d6448366407b6f53e71e4]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21002997-1768979419-28-n4cpu4-0001 | 34.26.13.93 | 10.142.2.169 | | teamcity-21002997-1768979419-28-n4cpu4-0002 | 34.26.90.152 | 10.142.1.188 | | teamcity-21002997-1768979419-28-n4cpu4-0003 | 34.74.84.216 | 10.142.2.170 | | teamcity-21002997-1768979419-28-n4cpu4-0004 | 34.139.98.246 | 10.142.2.226 | parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=true - runtimeassertionsbuild=true help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( same failure on other branches - roachtest: backup-restore/online-restore-recovery failed [storage params in random schema workload] [c-test-failure o-roachtest o-robot p-2 t-sql-foundations branch-master] /cc /sql-foundations [this test on roachdash]( | [improve this report!]( jira issue: crdb-58904",[],['TESTING'],"['C-test-failure', 'O-robot', 'O-roachtest', 'T-sql-foundations', 'P-2', 'B-runtime-assertions-enabled', 'branch-release-26.1']",github,2026-01-21T08:50:51Z,,"roachtest: backup-restore/online-restore-recovery failed [failed generating operation: insertrow: descriptor is being dropped] **note:** this build has runtime assertions enabled. if the same failure was hit in a run without assertions enabled, there should be a similar failure without this message. if there isn& ;t one, then this failure is likely due to an assertion violation or (assertion) timeout. roachtest.backup-restore/online-restore-recovery [failed]( with [artifacts]( on release-26.1 @ [6ac107383aa8b7246f0d6448366407b6f53e71e4]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21002997-1768979419-28-n4cpu4-0001 | 34.26.13.93 | 10.142.2.169 | | teamcity-21002997-1768979419-28-n4cpu4-0002 | 34.26.90.152 | 10.142.1.188 | | teamcity-21002997-1768979419-28-n4cpu4-0003 | 34.74.84.216 | 10.142.2.170 | | teamcity-21002997-1768979419-28-n4cpu4-0004 | 34.139.98.246 | 10.142.2.226 | parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=true - runtimeassertionsbuild=true help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( same failure on other branches - roachtest: backup-restore/online-restore-recovery failed [storage params in random schema workload] [c-test-failure o-roachtest o-robot p-2 t-sql-foundations branch-master] /cc /sql-foundations [this test on roachdash]( | [improve this report!]( jira issue: crdb-58904",3.145,High,0.935,functional impact
pytorch/pytorch#172939,toch.compile api doc typo: torch_log,"### üìö the doc issue check [toch compile doc]( > there are other circumstances where cuda graphs are not applicable; use torch_log=perf_hints to debug. i think the environment variable should be . below is the related doc: [torch set logs]( as far as i know, the build-in api doc of torch.compile is also wrong for torch 2.9.0 ### suggest a potential alternative/fix _no response_ cc",[],['DOCUMENTATION'],"['module: docs', 'actionable', 'oncall: pt2']",github,2026-01-21T09:22:01Z,2026-01-21T15:58:37Z,"toch.compile api doc typo: torch_log ### üìö the doc issue check [toch compile doc]( > there are other circumstances where cuda graphs are not applicable; use torch_log=perf_hints to debug. i think the environment variable should be . below is the related doc: [torch set logs]( as far as i know, the build-in api doc of torch.compile is also wrong for torch 2.9.0 ### suggest a potential alternative/fix _no response_ cc",1.2,Low,0.493,localized low-impact
cockroachdb/cockroach#161496,sql: testdropwhilebackfill failed [slow locktable verify under deadlock],sql.testdropwhilebackfill [failed]( on release-26.1 @ [6ac107383aa8b7246f0d6448366407b6f53e71e4]( parameters: - attempt=1 - deadlock=true - run=1 - shard=7 help see also: [how to investigate a go test failure \(internal\)]( same failure on other branches - sql: testdropwhilebackfill failed [c-test-failure o-robot t-db-server branch-master] /cc /sql-foundations [this test on roachdash]( | [improve this report!]( jira issue: crdb-58905,[],"['TESTING', 'BUG']","['C-bug', 'C-test-failure', 'O-robot', 'A-testing', 'T-kv', 'branch-release-26.1', 'target-release-26.1.1']",github,2026-01-21T09:37:28Z,2026-01-26T15:40:18Z,sql: testdropwhilebackfill failed [slow locktable verify under deadlock] sql.testdropwhilebackfill [failed]( on release-26.1 @ [6ac107383aa8b7246f0d6448366407b6f53e71e4]( parameters: - attempt=1 - deadlock=true - run=1 - shard=7 help see also: [how to investigate a go test failure \(internal\)]( same failure on other branches - sql: testdropwhilebackfill failed [c-test-failure o-robot t-db-server branch-master] /cc /sql-foundations [this test on roachdash]( | [improve this report!]( jira issue: crdb-58905,3.8,Critical,1.0,crash-like behavior
openssl/openssl#29702,ignores documented per-key method overrides in openssl 4.0.0-dev (behavioral regression),"### description in openssl 4.0, no longer respects per-key method overrides attached to asymmetric key objects. this breaks integrations that rely on documented per-key method overrides attached to asymmetric key objects. when a key is wrapped in an , appears to bypass the key‚Äôs attached method and instead performs the operation using the default internal implementation. this is a behavioral regression relative to historical openssl behavior. ### expected behavior if an evp_pkey contains an asymmetric key object (rsa*, ec_key*, etc.) with a custom per-key method attached, then: - high-level signing apis such as ) should honor that method - the custom callbacks (e.g. priv_enc, sign) should be invoked. this behavior is required for backward compatibility of non-provider code. ### actual behavior - low-level apis ( , ) correctly invoke the custom per-key methods - does not invoke the custom per-key methods - instead, the default internal implementation is used. this indicates that no longer operates on the original key object, or otherwise discards its attached method. this strongly suggests that evp_signfinal() no longer performs signing operations via the underlying key object. ### minimal reproducer (rsa) the following minimal poc installs a custom that increments a counter when the private operation is invoked. observed output on fedora 43 with openssl 4.0.0-dev: this shows that bypasses the custom per-key method. no providers are loaded or used in this reproducer. ### proof of concept ### suspected cause while pr removed support for custom evp_pkey_methods, it does not document or justify bypassing per-key methods such as rsa_method or ec_key_method. ### documentation reference current [rsa_set_method]( documentation still states: > rsa_set_method() selects meth to perform all operations using the key rsa. this will replace the rsa_method used by the rsa key. the documented behavior directly contradicts the observed behavior in openssl 4.0.0-dev. ### note on ec / ecdsa the same behavior can be reproduced with ec keys by overriding sign callbacks: - invokes the custom method - does not. this demonstrates that the issue is not specific to rsa, but affects per-key methods across asymmetric algorithms. ### impact real-world applications use documented per-key method overrides to redirect, instrument, or constrain private key operations on a per-key basis. when bypasses these methods while low-level api functions still invoke them, application behavior becomes inconsistent and silently incorrect. ### regression note this behavior appears to be an unintentional regression caused by removal of legacy paths. there is no indication that ignoring per-key methods in was an intentional or documented change.",[],['BUG'],"['triaged: bug', 'severity: regression']",github,2026-01-21T09:47:13Z,,"ignores documented per-key method overrides in openssl 4.0.0-dev (behavioral regression) ### description in openssl 4.0, no longer respects per-key method overrides attached to asymmetric key objects. this breaks integrations that rely on documented per-key method overrides attached to asymmetric key objects. when a key is wrapped in an , appears to bypass the key‚Äôs attached method and instead performs the operation using the default internal implementation. this is a behavioral regression relative to historical openssl behavior. ### expected behavior if an evp_pkey contains an asymmetric key object (rsa*, ec_key*, etc.) with a custom per-key method attached, then: - high-level signing apis such as ) should honor that method - the custom callbacks (e.g. priv_enc, sign) should be invoked. this behavior is required for backward compatibility of non-provider code. ### actual behavior - low-level apis ( , ) correctly invoke the custom per-key methods - does not invoke the custom per-key methods - instead, the default internal implementation is used. this indicates that no longer operates on the original key object, or otherwise discards its attached method. this strongly suggests that evp_signfinal() no longer performs signing operations via the underlying key object. ### minimal reproducer (rsa) the following minimal poc installs a custom that increments a counter when the private operation is invoked. observed output on fedora 43 with openssl 4.0.0-dev: this shows that bypasses the custom per-key method. no providers are loaded or used in this reproducer. ### proof of concept ### suspected cause while pr removed support for custom evp_pkey_methods, it does not document or justify bypassing per-key methods such as rsa_method or ec_key_method. ### documentation reference current [rsa_set_method]( documentation still states: > rsa_set_method() selects meth to perform all operations using the key rsa. this will replace the rsa_method used by the rsa key. the documented behavior directly contradicts the observed behavior in openssl 4.0.0-dev. ### note on ec / ecdsa the same behavior can be reproduced with ec keys by overriding sign callbacks: - invokes the custom method - does not. this demonstrates that the issue is not specific to rsa, but affects per-key methods across asymmetric algorithms. ### impact real-world applications use documented per-key method overrides to redirect, instrument, or constrain private key operations on a per-key basis. when bypasses these methods while low-level api functions still invoke them, application behavior becomes inconsistent and silently incorrect. ### regression note this behavior appears to be an unintentional regression caused by removal of legacy paths. there is no indication that ignoring per-key methods in was an intentional or documented change.",4.6,Critical,1.0,crash-like behavior
kubernetes/kubernetes#136384,[failing test] [sig-scalability] clusterloaderv2.access-tokens overall (/home/prow/go/src/k8s.io/perf-tests/clusterloader2/testing/access-tokens/config.yaml),"### which jobs are failing? * sig-release-master-informing * ec2-master-scale-performance ### which tests are failing? * [clusterloaderv2.access-tokens overall (/home/prow/go/src/k8s.io/perf-tests/clusterloader2/testing/access-tokens/config.yaml)]( ### since when has it been failing? * first failure: wed, 07 jan 2026 07:03:18 utc * latest failure: wed, 21 jan 2026 07:02:53 utc ### testgrid link * [ * [ ### reason for failure (if possible) ### anything else we need to know? _no response_ ### relevant sig(s) /sig scalability /kind failing-test cc /release-team-release-signal",[],['TESTING'],"['sig/scalability', 'kind/failing-test', 'needs-triage']",github,2026-01-21T09:57:55Z,,"[failing test] [sig-scalability] clusterloaderv2.access-tokens overall (/home/prow/go/src/k8s.io/perf-tests/clusterloader2/testing/access-tokens/config.yaml) ### which jobs are failing? * sig-release-master-informing * ec2-master-scale-performance ### which tests are failing? * [clusterloaderv2.access-tokens overall (/home/prow/go/src/k8s.io/perf-tests/clusterloader2/testing/access-tokens/config.yaml)]( ### since when has it been failing? * first failure: wed, 07 jan 2026 07:03:18 utc * latest failure: wed, 21 jan 2026 07:02:53 utc ### testgrid link * [ * [ ### reason for failure (if possible) ### anything else we need to know? _no response_ ### relevant sig(s) /sig scalability /kind failing-test cc /release-team-release-signal",3.235,High,0.955,system-wide impact
pandas-dev/pandas#63790,bug: sort_values() does not retain name property of dataframe,"### pandas version checks - [x] i have checked that this issue has not already been reported. - [x] i have confirmed this bug exists on the [latest version]( of pandas. - [ ] i have confirmed this bug exists on the [main branch]( of pandas. ### reproducible example ### issue description when doing a group_by() followed by an apply(), when using sort_values() in the applied function, the name property of the dataframe passed to the applied function is not retained, i.e. the returned dataframe by sort_values() no longer has the name property. is this intended behaviour? if so, i would be curious what the reasoning is for the behaviour ### expected behavior i would expect name to be retained in the dataframe returned by sort_values(). especially given the deprication of operating on the grouping column in the apply() statement. ### installed versions replace this line with the output of pd.show_versions()",[],['BUG'],"['Bug', 'Groupby', 'Apply']",github,2026-01-21T10:26:09Z,2026-01-22T22:14:03Z,"bug: sort_values() does not retain name property of dataframe ### pandas version checks - [x] i have checked that this issue has not already been reported. - [x] i have confirmed this bug exists on the [latest version]( of pandas. - [ ] i have confirmed this bug exists on the [main branch]( of pandas. ### reproducible example ### issue description when doing a group_by() followed by an apply(), when using sort_values() in the applied function, the name property of the dataframe passed to the applied function is not retained, i.e. the returned dataframe by sort_values() no longer has the name property. is this intended behaviour? if so, i would be curious what the reasoning is for the behaviour ### expected behavior i would expect name to be retained in the dataframe returned by sort_values(). especially given the deprication of operating on the grouping column in the apply() statement. ### installed versions replace this line with the output of pd.show_versions()",2.691,Medium,0.832,functional impact
scikit-learn/scikit-learn#33119,support clusterers in,"### describe the workflow you want to enable so far, does not resolve correctly (list of options as for classifiers instead of ). the approach i implemented is: using pca, followed by dbscan and logistic regression as a final step. properly showing the relevant datapoints, categorised/colored by their identified clusters, would be great without an extra call to . ### describe your proposed solution as per discussion linked below, proper support for clusterers should be added to . ### describe alternatives you've considered, if relevant simply plotting a scatterplot on top does not necessarily yield the same class colors, because the clusters are not necessarily ordered in the same way as the values. ### additional context",[],['FEATURE'],['New Feature'],github,2026-01-21T10:27:57Z,,"support clusterers in ### describe the workflow you want to enable so far, does not resolve correctly (list of options as for classifiers instead of ). the approach i implemented is: using pca, followed by dbscan and logistic regression as a final step. properly showing the relevant datapoints, categorised/colored by their identified clusters, would be great without an extra call to . ### describe your proposed solution as per discussion linked below, proper support for clusterers should be added to . ### describe alternatives you've considered, if relevant simply plotting a scatterplot on top does not necessarily yield the same class colors, because the clusters are not necessarily ordered in the same way as the values. ### additional context",1.4,Low,0.538,localized low-impact
cockroachdb/cockroach#161498,roachtest: ssh_problem failed,roachtest.ssh_problem [failed]( with [artifacts]( on release-26.1.0-rc @ [6c9b2f778bd29895383d493a2fa4490323499ecf]( parameters: - metamorphicfilesystem=xfs - metamorphicvolumetype=io2 - cloud=aws - coveragebuild=false - cpu=32 - diskcount=0 - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( same failure on other branches - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.3.7-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.4.3-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.2.11-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-24.3.25-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-26.1] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.3.6-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-24.3.24-rc] - roachtest: ssh_problem failed [o-roachtest o-robot o-rsg t-testeng x-infra-flake branch-release-25.2.10-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.4.2-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.4.1-rc] /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-58906,[],['TESTING'],"['O-robot', 'O-roachtest', 'T-testeng', 'X-infra-flake', 'branch-release-26.1.0-rc']",github,2026-01-21T10:31:28Z,,roachtest: ssh_problem failed roachtest.ssh_problem [failed]( with [artifacts]( on release-26.1.0-rc @ [6c9b2f778bd29895383d493a2fa4490323499ecf]( parameters: - metamorphicfilesystem=xfs - metamorphicvolumetype=io2 - cloud=aws - coveragebuild=false - cpu=32 - diskcount=0 - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( same failure on other branches - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.3.7-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.4.3-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.2.11-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-24.3.25-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-26.1] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.3.6-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-24.3.24-rc] - roachtest: ssh_problem failed [o-roachtest o-robot o-rsg t-testeng x-infra-flake branch-release-25.2.10-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.4.2-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.4.1-rc] /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-58906,3.091,High,0.922,functional impact
pandas-dev/pandas#63791,bug: inferred_freq is infers the wrong freq 2bqs-oct instead of 2bqs-jan,"### pandas version checks - [x] i have checked that this issue has not already been reported. - [x] i have confirmed this bug exists on the [latest version]( of pandas. - [x] i have confirmed this bug exists on the [main branch]( of pandas. ### reproducible example ### issue description the code above, infers the wrong frequency, it should be and not ### expected behavior 2bqs-jan should be the correct freq ### installed versions installed versions ------------------ commit : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140 python : 3.12.4.final.0 python-bits : 64 os : windows os-release : 11 version : 10.0.26100 machine : amd64 processor : intel64 family 6 model 186 stepping 2, genuineintel byteorder : little lc_all : none lang : none locale : english_switzerland.1252 pandas : 2.2.2 numpy : 1.26.4 pytz : 2024.1 dateutil : 2.9.0 setuptools : 72.1.0 pip : 24.2 cython : 3.0.11 pytest : 8.3.2 hypothesis : none sphinx : none blosc : none feather : none xlsxwriter : 3.1.9 lxml.etree : 5.2.2 html5lib : 1.1 pymysql : none psycopg2 : 2.9.9 jinja2 : 3.1.4 ipython : 8.21.0 pandas_datareader : 0.10.0 adbc-driver-postgresql: none adbc-driver-sqlite : none bs4 : 4.12.3 bottleneck : none dataframe-api-compat : none fastparquet : none fsspec : 2024.6.1 gcsfs : none matplotlib : 3.9.1 numba : 0.60.0 numexpr : 2.10.0 odfpy : none openpyxl : 3.1.4 pandas_gbq : none pyarrow : 17.0.0 pyreadstat : none python-calamine : none pyxlsb : none s3fs : none scipy : 1.13.1 sqlalchemy : 2.0.31 tables : 3.9.2 tabulate : 0.9.0 xarray : none xlrd : 2.0.1 zstandard : 0.23.0 tzdata : 2024.1 qtpy : none pyqt5 : none",[],['BUG'],"['Bug', 'Frequency']",github,2026-01-21T10:35:01Z,,"bug: inferred_freq is infers the wrong freq 2bqs-oct instead of 2bqs-jan ### pandas version checks - [x] i have checked that this issue has not already been reported. - [x] i have confirmed this bug exists on the [latest version]( of pandas. - [x] i have confirmed this bug exists on the [main branch]( of pandas. ### reproducible example ### issue description the code above, infers the wrong frequency, it should be and not ### expected behavior 2bqs-jan should be the correct freq ### installed versions installed versions ------------------ commit : d9cdd2ee5a58015ef6f4d15c7226110c9aab8140 python : 3.12.4.final.0 python-bits : 64 os : windows os-release : 11 version : 10.0.26100 machine : amd64 processor : intel64 family 6 model 186 stepping 2, genuineintel byteorder : little lc_all : none lang : none locale : english_switzerland.1252 pandas : 2.2.2 numpy : 1.26.4 pytz : 2024.1 dateutil : 2.9.0 setuptools : 72.1.0 pip : 24.2 cython : 3.0.11 pytest : 8.3.2 hypothesis : none sphinx : none blosc : none feather : none xlsxwriter : 3.1.9 lxml.etree : 5.2.2 html5lib : 1.1 pymysql : none psycopg2 : 2.9.9 jinja2 : 3.1.4 ipython : 8.21.0 pandas_datareader : 0.10.0 adbc-driver-postgresql: none adbc-driver-sqlite : none bs4 : 4.12.3 bottleneck : none dataframe-api-compat : none fastparquet : none fsspec : 2024.6.1 gcsfs : none matplotlib : 3.9.1 numba : 0.60.0 numexpr : 2.10.0 odfpy : none openpyxl : 3.1.4 pandas_gbq : none pyarrow : 17.0.0 pyreadstat : none python-calamine : none pyxlsb : none s3fs : none scipy : 1.13.1 sqlalchemy : 2.0.31 tables : 3.9.2 tabulate : 0.9.0 xarray : none xlrd : 2.0.1 zstandard : 0.23.0 tzdata : 2024.1 qtpy : none pyqt5 : none",2.656,Medium,0.824,functional impact
containerd/containerd#12806,can conainerd api support to create/delete selinux label,"### what is the problem you're trying to solve ctr/nerdctl use to generate selinux label, but it can't make sure selinux label is unique. ctr/nerdctl use label.initlabels may generate same selinuxbel because it's not daemon process. ### describe the solution you'd like containerd can suupor an api to create and delete selinux label ### additional context _no response_",[],['FEATURE'],"['kind/feature', 'area/runtime']",github,2026-01-21T10:41:48Z,,"can conainerd api support to create/delete selinux label ### what is the problem you're trying to solve ctr/nerdctl use to generate selinux label, but it can't make sure selinux label is unique. ctr/nerdctl use label.initlabels may generate same selinuxbel because it's not daemon process. ### describe the solution you'd like containerd can suupor an api to create and delete selinux label ### additional context _no response_",1.4,Low,0.538,localized low-impact
cockroachdb/cockroach#161499,roachtest: typeorm failed,"**note:** this build has runtime assertions enabled. if the same failure was hit in a run without assertions enabled, there should be a similar failure without this message. if there isn& ;t one, then this failure is likely due to an assertion violation or (assertion) timeout. roachtest.typeorm [failed]( with [artifacts]( on release-26.1.0-rc @ [6c9b2f778bd29895383d493a2fa4490323499ecf]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21003012-1768979506-85-n1cpu4-0001 | 34.26.33.80 | 10.142.3.162 | parameters: - arch=fips - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - metamorphicleases=epoch - runtimeassertionsbuild=true help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( /cc /sql-foundations [this test on roachdash]( | [improve this report!]( jira issue: crdb-58909",[],['TESTING'],"['C-test-failure', 'O-robot', 'O-roachtest', 'T-sql-foundations', 'P-2', 'B-runtime-assertions-enabled', 'branch-release-26.1.0-rc']",github,2026-01-21T11:08:48Z,,"roachtest: typeorm failed **note:** this build has runtime assertions enabled. if the same failure was hit in a run without assertions enabled, there should be a similar failure without this message. if there isn& ;t one, then this failure is likely due to an assertion violation or (assertion) timeout. roachtest.typeorm [failed]( with [artifacts]( on release-26.1.0-rc @ [6c9b2f778bd29895383d493a2fa4490323499ecf]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21003012-1768979506-85-n1cpu4-0001 | 34.26.33.80 | 10.142.3.162 | parameters: - arch=fips - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - metamorphicleases=epoch - runtimeassertionsbuild=true help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( /cc /sql-foundations [this test on roachdash]( | [improve this report!]( jira issue: crdb-58909",3.213,High,0.95,system-wide impact
rust-lang/rust#151451,"ice: internal compiler error: invalidsimd { ty: simd , kind: zerolength }","<!-- thank you for finding an internal compiler error! üßä if possible, try to provide a minimal verifiable example. you can read ""rust bug minimization patterns"" for how to create smaller examples. --> i previously reported this issue as , which was marked as a duplicate of . recently, while reviewing this ice , i became confused as to why it was considered a duplicate. firstly, their stack traces appear to be significantly different. secondly, can only be reproduced with the dependencies of #[repr(simd)] and extern ""c"". could someone help clarify this? ### code ### meta <!-- if you're using the stable version of the compiler, you should also check if the bug also exists in the beta or nightly versions. --> : <!-- include a backtrace in the code block by setting in your environment. e.g. . --> program output ### code <!-- include a backtrace in the code block by setting in your environment. e.g. . --> program output",[],['BUG'],"['E-easy', 'A-diagnostics', 'I-ICE', 'T-compiler', 'A-SIMD', 'C-bug', 'A-ZST']",github,2026-01-21T11:09:44Z,2026-01-23T00:43:05Z,"ice: internal compiler error: invalidsimd { ty: simd , kind: zerolength } <!-- thank you for finding an internal compiler error! üßä if possible, try to provide a minimal verifiable example. you can read ""rust bug minimization patterns"" for how to create smaller examples. --> i previously reported this issue as , which was marked as a duplicate of . recently, while reviewing this ice , i became confused as to why it was considered a duplicate. firstly, their stack traces appear to be significantly different. secondly, can only be reproduced with the dependencies of #[repr(simd)] and extern ""c"". could someone help clarify this? ### code ### meta <!-- if you're using the stable version of the compiler, you should also check if the bug also exists in the beta or nightly versions. --> : <!-- include a backtrace in the code block by setting in your environment. e.g. . --> program output ### code <!-- include a backtrace in the code block by setting in your environment. e.g. . --> program output",2.366,Medium,0.758,functional impact
electron/electron#49469,"shell.writeshortcutlink throws ""insufficient number of arguments"" when omitting optional operation","### preflight checklist - [x] i have read the [contributing guidelines]( for this project. - [x] i agree to follow the [code of conduct]( that this project adheres to. - [x] i have searched the [issue tracker]( for a bug report that matches the one i want to file, without success. ### electron version 40.0.0 ### what operating system(s) are you using? windows ### operating system version windows 11 25h2 ### what arch are you using? x64 ### last known working electron version 39.2.7 ### does the issue also appear in chromium / google chrome? no ### expected behavior - the call should succeed. - operation should default to ""create"" as documented. ### actual behavior the call throws: ### testcase gist url ### additional information in my app, this error surfaces through an ipc handler when the optional operation parameter is omitted, despite being documented as optional.: this strongly suggests the error originates from shell.writeshortcutlink rather than application-level validation.",[],['BUG'],"['platform/windows', 'bug :beetle:', 'status/confirmed', 'has-repro-gist', 'component/shell', '40-x-y']",github,2026-01-21T11:18:44Z,2026-01-23T16:24:50Z,"shell.writeshortcutlink throws ""insufficient number of arguments"" when omitting optional operation ### preflight checklist - [x] i have read the [contributing guidelines]( for this project. - [x] i agree to follow the [code of conduct]( that this project adheres to. - [x] i have searched the [issue tracker]( for a bug report that matches the one i want to file, without success. ### electron version 40.0.0 ### what operating system(s) are you using? windows ### operating system version windows 11 25h2 ### what arch are you using? x64 ### last known working electron version 39.2.7 ### does the issue also appear in chromium / google chrome? no ### expected behavior - the call should succeed. - operation should default to ""create"" as documented. ### actual behavior the call throws: ### testcase gist url ### additional information in my app, this error surfaces through an ipc handler when the optional operation parameter is omitted, despite being documented as optional.: this strongly suggests the error originates from shell.writeshortcutlink rather than application-level validation.",2.421,Medium,0.77,functional impact
jaegertracing/jaeger#7911,[bug]: linting fails on main: time-naming violation in search_traces.go,"### what happened? running on the main branch fails, preventing local development and contributions. the linting process exits with an error about incorrect variable naming in search_traces.go, which blocks anyone trying to build or test the project locally. ### steps to reproduce 1. clone the repository 2. checkout the latest main branch (commit 3361fcc4 or later) 3. run 4. observe the linting failure with error about durationmin variable naming ### expected behavior should pass without errors on the main branch. ### relevant log output ### screenshot ### additional context _no response_ ### jaeger backend version _no response_ ### sdk _no response_ ### pipeline _no response_ ### stogage backend _no response_ ### operating system _no response_ ### deployment model _no response_ ### deployment configs",[],['BUG'],"['bug', 'go']",github,2026-01-21T11:23:55Z,,"[bug]: linting fails on main: time-naming violation in search_traces.go ### what happened? running on the main branch fails, preventing local development and contributions. the linting process exits with an error about incorrect variable naming in search_traces.go, which blocks anyone trying to build or test the project locally. ### steps to reproduce 1. clone the repository 2. checkout the latest main branch (commit 3361fcc4 or later) 3. run 4. observe the linting failure with error about durationmin variable naming ### expected behavior should pass without errors on the main branch. ### relevant log output ### screenshot ### additional context _no response_ ### jaeger backend version _no response_ ### sdk _no response_ ### pipeline _no response_ ### stogage backend _no response_ ### operating system _no response_ ### deployment model _no response_ ### deployment configs",2.607,Medium,0.813,functional impact
cockroachdb/cockroach#161500,obs: opt into execution traces for diagnostics bundles,"**is your feature request related to a problem? please describe.** field, support and engineering regularly spend extensive amounts of time diagnosing slow statement/transaction executions. some classes of root causes - numa, garbage collection overhead, locking issues - are difficult to glean from crdb traces alone. when standard analysis fails, we usually ask for ""statement diagnostics bundle with overlapping execution trace"", the latter being a recording of the go scheduler view of goroutine events which serves as a ""big hammer"" that, in combination with a specific slow execution, can usually point at deeper root causes. unfortunately, it is difficult for customers to reliably provide this data. the collection is fairly manual and as a result, error-prone. this adds redundant delays and round-trips between the customer and crl to the troubleshooting process, which is inefficient. the obs team already added which allows continuously capturing execution traces using cluster settings. this is helpful, but still requires manual setup and collection, and importantly, always requires ""already having exhausted the available venues"" before being a path that is taken. we could do ""one better"" and fully integrate execution trace collection with the user flow to collect stmt/txn diagnostics bundles: users can easily collect this information, perhaps even by default, and engineers don't have to spend time sorting through large amounts of execution traces to identify the correct ones to investigate. **describe the solution you'd like** a new option to collect an overlapping execution trace appears in the statement diagnostics modal (similar for transaction diagnostics, once we have it). when that option is selected, trace spans created in service of this diagnostics request are tagged as desiring an execution trace. on every node on which such a trace span is active, execution trace flight recording is enabled, and execution traces are retained (subject to a retention period and size limit), with each such file having its reference added to the trace span (as a structured payload). note that these execution traces are not necessarily persisted to disk at this point - only when the crdb trace actually matches the conditions for the diagnostics bundle (e.g. sufficiently slow) does the diagnostics code resolve the references to existing execution traces and collects them, to make them available as part of the corresponding bundle. the earlier prototype had a relatively sophisticated attempt at providing the flight recording infrastructure necessary to accommodate the flow above. **describe alternatives you've considered** **additional context** - is the original issue tracking this idea, and was marked as completed before the desired functionality was fully implemented. referenced in that issue are numerous support incidents that would've benefited from fully integrated execution trace integration. - saw much time spent obtaining execution traces. the customer wasn't even running with the change that allows using the cluster settings, but the incident still serves as an example of one in which we might save a lot of time had execution traces been available with lower friction early in the process. jira issue: crdb-58910",[],['FEATURE'],"['C-enhancement', 'O-support', 'A-kv-observability', 'A-sql-observability', 'A-observability-inf', 'A-cluster-observability', 'P-3', 'T-observability']",github,2026-01-21T11:32:30Z,,"obs: opt into execution traces for diagnostics bundles **is your feature request related to a problem? please describe.** field, support and engineering regularly spend extensive amounts of time diagnosing slow statement/transaction executions. some classes of root causes - numa, garbage collection overhead, locking issues - are difficult to glean from crdb traces alone. when standard analysis fails, we usually ask for ""statement diagnostics bundle with overlapping execution trace"", the latter being a recording of the go scheduler view of goroutine events which serves as a ""big hammer"" that, in combination with a specific slow execution, can usually point at deeper root causes. unfortunately, it is difficult for customers to reliably provide this data. the collection is fairly manual and as a result, error-prone. this adds redundant delays and round-trips between the customer and crl to the troubleshooting process, which is inefficient. the obs team already added which allows continuously capturing execution traces using cluster settings. this is helpful, but still requires manual setup and collection, and importantly, always requires ""already having exhausted the available venues"" before being a path that is taken. we could do ""one better"" and fully integrate execution trace collection with the user flow to collect stmt/txn diagnostics bundles: users can easily collect this information, perhaps even by default, and engineers don't have to spend time sorting through large amounts of execution traces to identify the correct ones to investigate. **describe the solution you'd like** a new option to collect an overlapping execution trace appears in the statement diagnostics modal (similar for transaction diagnostics, once we have it). when that option is selected, trace spans created in service of this diagnostics request are tagged as desiring an execution trace. on every node on which such a trace span is active, execution trace flight recording is enabled, and execution traces are retained (subject to a retention period and size limit), with each such file having its reference added to the trace span (as a structured payload). note that these execution traces are not necessarily persisted to disk at this point - only when the crdb trace actually matches the conditions for the diagnostics bundle (e.g. sufficiently slow) does the diagnostics code resolve the references to existing execution traces and collects them, to make them available as part of the corresponding bundle. the earlier prototype had a relatively sophisticated attempt at providing the flight recording infrastructure necessary to accommodate the flow above. **describe alternatives you've considered** **additional context** - is the original issue tracking this idea, and was marked as completed before the desired functionality was fully implemented. referenced in that issue are numerous support incidents that would've benefited from fully integrated execution trace integration. - saw much time spent obtaining execution traces. the customer wasn't even running with the change that allows using the cluster settings, but the incident still serves as an example of one in which we might save a lot of time had execution traces been available with lower friction early in the process. jira issue: crdb-58910",5.0,Critical,1.0,crash-like behavior
cockroachdb/cockroach#161501,sentry: version_set.go:620: log.fatal: manifest write failed: write √ó: insufficient system resources exist to complete the requested service. (1) attached stack trace -- stack trace: | github.com...,this issue was auto filed by sentry. it represents a crash or reported error on a live cluster with telemetry enabled. sentry link: [ panic message: stacktrace (expand for inline code snippets): [src/runtime/asm_amd64.s#l1694-l1696](src/runtime/asm_amd64.s#l1694-l1696) [external/com_github_cockroachdb_pebble/compaction.go#l1175-l1177](external/com_github_cockroachdb_pebble/compaction.go#l1175-l1177) [goroot/src/runtime/pprof/runtime.go#l50-l52](goroot/src/runtime/pprof/runtime.go#l50-l52) [external/com_github_cockroachdb_pebble/compaction.go#l1182-l1184](external/com_github_cockroachdb_pebble/compaction.go#l1182-l1184) [external/com_github_cockroachdb_pebble/compaction.go#l1534-l1536](external/com_github_cockroachdb_pebble/compaction.go#l1534-l1536) [external/com_github_cockroachdb_pebble/version_set.go#l619-l621](external/com_github_cockroachdb_pebble/version_set.go#l619-l621) ### tags | tag | value | | --- | --- | | command | server | | environment | v24.3.1 | | go version | go1.22.8 x:nocoverageredesign | | platform | windows amd64 | | distribution | ccl | | cockroach release | v24.3.1 | | cockroach sha | 950be288db97fe1d8f9c19918e37695d3c1f4bb0 | | # of cpus | 8 | | # of goroutines | 510 | jira issue: crdb-58912,[],['BUG'],"['C-bug', 'O-sentry', 'X-blathers-untriaged', 'branch-release-24.3']",github,2026-01-21T11:47:40Z,2026-01-24T01:10:10Z,sentry: version_set.go:620: log.fatal: manifest write failed: write √ó: insufficient system resources exist to complete the requested service. (1) attached stack trace -- stack trace: | github.com... this issue was auto filed by sentry. it represents a crash or reported error on a live cluster with telemetry enabled. sentry link: [ panic message: stacktrace (expand for inline code snippets): [src/runtime/asm_amd64.s#l1694-l1696](src/runtime/asm_amd64.s#l1694-l1696) [external/com_github_cockroachdb_pebble/compaction.go#l1175-l1177](external/com_github_cockroachdb_pebble/compaction.go#l1175-l1177) [goroot/src/runtime/pprof/runtime.go#l50-l52](goroot/src/runtime/pprof/runtime.go#l50-l52) [external/com_github_cockroachdb_pebble/compaction.go#l1182-l1184](external/com_github_cockroachdb_pebble/compaction.go#l1182-l1184) [external/com_github_cockroachdb_pebble/compaction.go#l1534-l1536](external/com_github_cockroachdb_pebble/compaction.go#l1534-l1536) [external/com_github_cockroachdb_pebble/version_set.go#l619-l621](external/com_github_cockroachdb_pebble/version_set.go#l619-l621) ### tags | tag | value | | --- | --- | | command | server | | environment | v24.3.1 | | go version | go1.22.8 x:nocoverageredesign | | platform | windows amd64 | | distribution | ccl | | cockroach release | v24.3.1 | | cockroach sha | 950be288db97fe1d8f9c19918e37695d3c1f4bb0 | | # of cpus | 8 | | # of goroutines | 510 | jira issue: crdb-58912,7.8,Critical,1.0,crash-like behavior
cockroachdb/cockroach#161502,cli: testlossofquorumrecovery failed,cli.testlossofquorumrecovery [failed]( with [artifacts]( on master @ [ab9a17f7cc0567df7f89d066421180e198b556ea]( help see also: [how to investigate a go test failure \(internal\)]( /cc /kv-triage /server [this test on roachdash]( | [improve this report!]( jira issue: crdb-58913,[],"['TESTING', 'BUG']","['C-bug', 'C-test-failure', 'O-robot', 'A-testing', 'branch-master', 'T-kv', 'X-nostale', 's390x-test-failure']",github,2026-01-21T12:04:00Z,,cli: testlossofquorumrecovery failed cli.testlossofquorumrecovery [failed]( with [artifacts]( on master @ [ab9a17f7cc0567df7f89d066421180e198b556ea]( help see also: [how to investigate a go test failure \(internal\)]( /cc /kv-triage /server [this test on roachdash]( | [improve this report!]( jira issue: crdb-58913,2.0,Medium,0.675,localized low-impact
facebook/react#35583,[devtools bug]:,### website or app githubrepo/code sanbox ### repro steps ![image]( [capstone-license.txt]( [license.txt]( [yarapython-license.txt]( [sourcecode.zip]( //github.com/user-attachments/files/24765783/pgbouncer-1.25.1.tar.gz) ### how often does this bug happen? every time ### devtools package (automated) _no response_ ### devtools version (automated) _no response_ ### error message (automated) _no response_ ### error call stack (automated) ### error component stack (automated) ### github query string (automated),[],['BUG'],"['Type: Bug', 'Status: Unconfirmed', 'Component: Developer Tools']",github,2026-01-21T12:15:22Z,2026-01-21T12:28:58Z,[devtools bug]: ### website or app githubrepo/code sanbox ### repro steps ![image]( [capstone-license.txt]( [license.txt]( [yarapython-license.txt]( [sourcecode.zip]( //github.com/user-attachments/files/24765783/pgbouncer-1.25.1.tar.gz) ### how often does this bug happen? every time ### devtools package (automated) _no response_ ### devtools version (automated) _no response_ ### error message (automated) _no response_ ### error call stack (automated) ### error component stack (automated) ### github query string (automated),4.2,Critical,1.0,system-wide impact
cockroachdb/cockroach#161503,"goexectrace,debug: embed metadata in go execution traces for debugging and correlation","**is your feature request related to a problem? please describe.** go execution traces captured by cockroachdb (via or ) contain no embedded metadata. this makes it difficult to correlate trace events with wall-clock time, identify which node produced a trace, or know the crdb version‚Äîespecially when traces are shared or uploaded without their original context. **describe the solution you'd like** embed metadata into traces using with a well-known category: this should be added to both capture mechanisms: - : log just before to ensure it's in the buffer - : wrap the handler (like ) and log at trace start **describe alternatives you've considered** - with metadata in the name (more visible in trace viewer, but more overhead) - relying on filenames (current state; metadata gets lost when traces are shared; when fetching trace via http filename is up to user) **additional context** - flight recorder: - pprof endpoint: - metadata can be extracted via or programmatically with epic: none release note: none jira issue: crdb-58914",[],['FEATURE'],"['C-enhancement', 'O-support', 'P-3', 'T-observability']",github,2026-01-21T12:34:22Z,,"goexectrace,debug: embed metadata in go execution traces for debugging and correlation **is your feature request related to a problem? please describe.** go execution traces captured by cockroachdb (via or ) contain no embedded metadata. this makes it difficult to correlate trace events with wall-clock time, identify which node produced a trace, or know the crdb version‚Äîespecially when traces are shared or uploaded without their original context. **describe the solution you'd like** embed metadata into traces using with a well-known category: this should be added to both capture mechanisms: - : log just before to ensure it's in the buffer - : wrap the handler (like ) and log at trace start **describe alternatives you've considered** - with metadata in the name (more visible in trace viewer, but more overhead) - relying on filenames (current state; metadata gets lost when traces are shared; when fetching trace via http filename is up to user) **additional context** - flight recorder: - pprof endpoint: - metadata can be extracted via or programmatically with epic: none release note: none jira issue: crdb-58914",1.4,Low,0.538,localized low-impact
python/cpython#144106,increase test coverage for tkinter.simpledialog,"the module currently has low test coverage (%46). test_simpledialog only tests function. i tried , it works well but throws an unrelated exception. ### linked prs * gh-144107",[],['TESTING'],"['tests', 'topic-tkinter']",github,2026-01-21T12:52:39Z,,"increase test coverage for tkinter.simpledialog the module currently has low test coverage (%46). test_simpledialog only tests function. i tried , it works well but throws an unrelated exception. ### linked prs * gh-144107",1.6,Low,0.584,localized low-impact
rust-lang/rust#151454,rustdoc creates links to doc(hidden) associated type,"this came up in if one takes just the first commit: the link checker started complaining about some rustdoc-generated files. is a associated type of the trait. it seems like the changes in [this commit]( cause rustdoc to render trait instances that refer to this associated type, with links to the type definition, but since the type is hidden those links are dangling.",[],['BUG'],"['T-rustdoc', 'C-bug', 'A-intra-doc-links']",github,2026-01-21T12:54:00Z,,"rustdoc creates links to doc(hidden) associated type this came up in if one takes just the first commit: the link checker started complaining about some rustdoc-generated files. is a associated type of the trait. it seems like the changes in [this commit]( cause rustdoc to render trait instances that refer to this associated type, with links to the type definition, but since the type is hidden those links are dangling.",4.6,Critical,1.0,crash-like behavior
istio/istio#58855,documentation of disable_shadow_host_suffix in 1.28.0 release notes is inverted,"### is this the right place to submit this? - [x] this is not a security vulnerability or a crashing bug - [x] this is not a question about how to use istio ### bug description the documentation of the new environment variable to control adding the suffix to the host/authority of mirrored requests (which ultimately controls [envoy's property]( [in the 1.28.0 release notes]( is inverted. the documentation says: > when set to true (default), shadow host suffixes are added to hostnames of mirrored requests. when set to false, shadow host suffixes are not added. but *actually* it's the other way around, because this property is a ""disable"" flag not an ""enable"" flag. this property is also (in the istio sourcecode) defined with a default of *true* (which means the suffix is **not** added to mirrored requests) whereas the envoy default is **false**. we saw a change in behaviour after upgrading to istio 1.28.0 that now mirrored request **don't have** the host suffix anymore, where as earlier with 1.27.x they **did have** this suffix (which was in line with envoy's default). ### version ### additional information _no response_",[],['DOCUMENTATION'],['kind/docs'],github,2026-01-21T13:32:27Z,,"documentation of disable_shadow_host_suffix in 1.28.0 release notes is inverted ### is this the right place to submit this? - [x] this is not a security vulnerability or a crashing bug - [x] this is not a question about how to use istio ### bug description the documentation of the new environment variable to control adding the suffix to the host/authority of mirrored requests (which ultimately controls [envoy's property]( [in the 1.28.0 release notes]( is inverted. the documentation says: > when set to true (default), shadow host suffixes are added to hostnames of mirrored requests. when set to false, shadow host suffixes are not added. but *actually* it's the other way around, because this property is a ""disable"" flag not an ""enable"" flag. this property is also (in the istio sourcecode) defined with a default of *true* (which means the suffix is **not** added to mirrored requests) whereas the envoy default is **false**. we saw a change in behaviour after upgrading to istio 1.28.0 that now mirrored request **don't have** the host suffix anymore, where as earlier with 1.27.x they **did have** this suffix (which was in line with envoy's default). ### version ### additional information _no response_",5.2,Critical,1.0,crash-like behavior
cilium/cilium#43901,"ci: different workfows - connectivity test - failing with panic ""runtime error: index out of range [3] with length 0""","there are more ci failures with the same reasons, personally i checked and verified logs from: the cilium agent panics with ""index out of range"" in pkg/datapath/linux/config/config.go during startup or configuration regeneration. example of the error: reason: when is set to true, unconditionally attempts to convert to a 32-bit host integer. if the internal ip (router ip) has not yet been initialized (e.g. waiting for ipam or k8s sync), accessing the nil byte slice causes the panic in byteorder.",[],['BUG'],"['kind/bug', 'area/loader', 'area/CI', 'area/datapath', 'ci/flake']",github,2026-01-21T13:34:12Z,2026-01-22T10:30:19Z,"ci: different workfows - connectivity test - failing with panic ""runtime error: index out of range [3] with length 0"" there are more ci failures with the same reasons, personally i checked and verified logs from: the cilium agent panics with ""index out of range"" in pkg/datapath/linux/config/config.go during startup or configuration regeneration. example of the error: reason: when is set to true, unconditionally attempts to convert to a 32-bit host integer. if the internal ip (router ip) has not yet been initialized (e.g. waiting for ipam or k8s sync), accessing the nil byte slice causes the panic in byteorder.",4.6,Critical,1.0,crash-like behavior
electron/electron#49475,cannot save/download pdf from asar file in pdf viewer,"### preflight checklist - [x] i have read the [contributing guidelines]( for this project. - [x] i agree to follow the [code of conduct]( that this project adheres to. - [x] i have searched the [issue tracker]( for a bug report that matches the one i want to file, without success. ### electron version 40.0.0 ### what operating system(s) are you using? macos ### operating system version macos 26.2, windows 11 ### what arch are you using? arm64 (including apple silicon) ### last known working electron version _no response_ ### does the issue also appear in chromium / google chrome? no ### expected behavior when viewing a pdf in the embedded pdf viewer it should be possible to save the file using the ""download"" button. ### actual behavior the save dialog appears, but no file is saved after confirming the destination and file name. ### testcase gist url (can't create an asar with a pdf in fiddle) ### additional information i have created a simple asar file that contains a pdf to avoid having to bundle the app. unfortunately, it is not possible to add asar files in electron fiddle.",[],['BUG'],"['platform/macOS', 'bug :beetle:', 'component/pdf-viewer', 'status/confirmed', 'has-repro-repo', '40-x-y']",github,2026-01-21T13:58:32Z,,"cannot save/download pdf from asar file in pdf viewer ### preflight checklist - [x] i have read the [contributing guidelines]( for this project. - [x] i agree to follow the [code of conduct]( that this project adheres to. - [x] i have searched the [issue tracker]( for a bug report that matches the one i want to file, without success. ### electron version 40.0.0 ### what operating system(s) are you using? macos ### operating system version macos 26.2, windows 11 ### what arch are you using? arm64 (including apple silicon) ### last known working electron version _no response_ ### does the issue also appear in chromium / google chrome? no ### expected behavior when viewing a pdf in the embedded pdf viewer it should be possible to save the file using the ""download"" button. ### actual behavior the save dialog appears, but no file is saved after confirming the destination and file name. ### testcase gist url (can't create an asar with a pdf in fiddle) ### additional information i have created a simple asar file that contains a pdf to avoid having to bundle the app. unfortunately, it is not possible to add asar files in electron fiddle.",2.364,Medium,0.757,functional impact
pandas-dev/pandas#63794,doc: typo in 3.0.0 release notes,"### pandas version checks - [x] i have checked that the issue still exists on the latest versions of the docs on [here]( ### location of the documentation ### documentation problem the first sentence of section ""datetime/timedelta resolution inference"" has the following typo: ""a sequence of of strings"" ### suggested fix for documentation i will create a pr for it.",[],['DOCUMENTATION'],"['Docs', 'Needs Triage']",github,2026-01-21T14:46:42Z,2026-01-21T15:45:03Z,"doc: typo in 3.0.0 release notes ### pandas version checks - [x] i have checked that the issue still exists on the latest versions of the docs on [here]( ### location of the documentation ### documentation problem the first sentence of section ""datetime/timedelta resolution inference"" has the following typo: ""a sequence of of strings"" ### suggested fix for documentation i will create a pr for it.",1.2,Low,0.493,localized low-impact
cockroachdb/cockroach#161506,kv/kvserver: testleaderlesswatchererrorrefreshedonunavailabilitytransition failed,kv/kvserver.testleaderlesswatchererrorrefreshedonunavailabilitytransition [failed]( on release-25.2.11-rc @ [0d9e4ae4e7e3c35adea65f020bfec48628482761]( parameters: - attempt=1 - race=true - run=1 - shard=16 help see also: [how to investigate a go test failure \(internal\)]( /cc /kv-triage [this test on roachdash]( | [improve this report!]( jira issue: crdb-58917,[],"['TESTING', 'BUG']","['C-bug', 'C-test-failure', 'O-robot', 'A-testing', 'X-unactionable', 'T-kv', 'branch-release-25.2.11-rc']",github,2026-01-21T14:48:30Z,2026-01-26T15:31:19Z,kv/kvserver: testleaderlesswatchererrorrefreshedonunavailabilitytransition failed kv/kvserver.testleaderlesswatchererrorrefreshedonunavailabilitytransition [failed]( on release-25.2.11-rc @ [0d9e4ae4e7e3c35adea65f020bfec48628482761]( parameters: - attempt=1 - race=true - run=1 - shard=16 help see also: [how to investigate a go test failure \(internal\)]( /cc /kv-triage [this test on roachdash]( | [improve this report!]( jira issue: crdb-58917,2.0,Medium,0.675,localized low-impact
pytorch/pytorch#172949,support for new eigen 5.0.x,"### üöÄ the feature, motivation and pitch eigen 5.0.1 was released about two months ago. this version provides new features, performance improvements, and bug fixes. one of the release notes says: ""the cmake build system has been modernized and older properties have been removed - projects relying on cmake may need to update their configurations "" for example, eigen3_* variables were removed which makes it not possible to build pytorch using system eigen 5.0.1. if possible, please consider adding support for the new eigen version. thank you in advance! ### alternatives _no response_ ### additional context _no response_ cc",[],"['FEATURE', 'UI']","['module: build', 'triaged', 'enhancement']",github,2026-01-21T14:52:07Z,,"support for new eigen 5.0.x ### üöÄ the feature, motivation and pitch eigen 5.0.1 was released about two months ago. this version provides new features, performance improvements, and bug fixes. one of the release notes says: ""the cmake build system has been modernized and older properties have been removed - projects relying on cmake may need to update their configurations "" for example, eigen3_* variables were removed which makes it not possible to build pytorch using system eigen 5.0.1. if possible, please consider adding support for the new eigen version. thank you in advance! ### alternatives _no response_ ### additional context _no response_ cc",2.253,Medium,0.732,user-visible issue
cockroachdb/cockroach#161507,sentry: vfs.go:262: rename √ó √ó: operation not permitted (1) attached stack trace -- stack trace: | runtime.gopanic | goroot/src/runtime/panic.go:770 | github.com/cockroachdb/pebble.(*db).ro...,this issue was auto filed by sentry. it represents a crash or reported error on a live cluster with telemetry enabled. sentry link: [ panic message: stacktrace (expand for inline code snippets): [src/runtime/asm_arm64.s#l1221-l1223](src/runtime/asm_arm64.s#l1221-l1223) [ [ [ [ [ [ [ [ [ [ [github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/batch.go#l1462-l1464](github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/batch.go#l1462-l1464) [github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l773-l775](github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l773-l775) [github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l844-l846](github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l844-l846) [github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/commit.go#l315-l317](github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/commit.go#l315-l317) [github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/commit.go#l466-l468](github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/commit.go#l466-l468) [github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l930-l932](github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l930-l932) [github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l2489-l2491](github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l2489-l2491) [github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l2613-l2615](github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l2613-l2615) [goroot/src/runtime/panic.go#l769-l771](goroot/src/runtime/panic.go#l769-l771) [src/runtime/asm_arm64.s#l1221-l1223](src/runtime/asm_arm64.s#l1221-l1223) [ [ [ [ [ [ [ [ [ [ [github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/batch.go#l1462-l1464](github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/batch.go#l1462-l1464) [github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l773-l775](github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l773-l775) [github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l844-l846](github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l844-l846) [github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/commit.go#l315-l317](github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/commit.go#l315-l317) [github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/commit.go#l466-l468](github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/commit.go#l466-l468) [github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l930-l932](github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l930-l932) [github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l2489-l2491](github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l2489-l2491) [github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l2611-l2613](github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l2611-l2613) [github.com/cockroachdb/pebble/wal/external/com_github_cockroachdb_pebble/wal/standalone_manager.go#l142-l144](github.com/cockroachdb/pebble/wal/external/com_github_cockroachdb_pebble/wal/standalone_manager.go#l142-l144) [ [github.com/cockroachdb/pebble/vfs/external/com_github_cockroachdb_pebble/vfs/disk_full.go#l291-l293](github.com/cockroachdb/pebble/vfs/external/com_github_cockroachdb_pebble/vfs/disk_full.go#l291-l293) [github.com/cockroachdb/pebble/vfs/external/com_github_cockroachdb_pebble/vfs/disk_health.go#l862-l864](github.com/cockroachdb/pebble/vfs/external/com_github_cockroachdb_pebble/vfs/disk_health.go#l862-l864) [github.com/cockroachdb/pebble/vfs/external/com_github_cockroachdb_pebble/vfs/disk_health.go#l637-l639](github.com/cockroachdb/pebble/vfs/external/com_github_cockroachdb_pebble/vfs/disk_health.go#l637-l639) [github.com/cockroachdb/pebble/vfs/external/com_github_cockroachdb_pebble/vfs/disk_health.go#l863-l865](github.com/cockroachdb/pebble/vfs/external/com_github_cockroachdb_pebble/vfs/disk_health.go#l863-l865) [github.com/cockroachdb/pebble/vfs/external/com_github_cockroachdb_pebble/vfs/vfs.go#l266-l268](github.com/cockroachdb/pebble/vfs/external/com_github_cockroachdb_pebble/vfs/vfs.go#l266-l268) [src/runtime/asm_arm64.s#l1221-l1223](src/runtime/asm_arm64.s#l1221-l1223) [ [ [ [ [ [ [ [ [ [ [github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/batch.go#l1462-l1464](github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/batch.go#l1462-l1464) [github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l773-l775](github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l773-l775) [github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l844-l846](github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l844-l846) [github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/commit.go#l315-l317](github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/commit.go#l315-l317) [github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/commit.go#l466-l468](github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/commit.go#l466-l468) [github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l930-l932](github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l930-l932) [github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l2489-l2491](github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l2489-l2491) [github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l2611-l2613](github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l2611-l2613) [github.com/cockroachdb/pebble/wal/external/com_github_cockroachdb_pebble/wal/standalone_manager.go#l142-l144](github.com/cockroachdb/pebble/wal/external/com_github_cockroachdb_pebble/wal/standalone_manager.go#l142-l144) [ [github.com/cockroachdb/pebble/vfs/external/com_github_cockroachdb_pebble/vfs/disk_full.go#l291-l293](github.com/cockroachdb/pebble/vfs/external/com_github_cockroachdb_pebble/vfs/disk_full.go#l291-l293) [github.com/cockroachdb/pebble/vfs/external/com_github_cockroachdb_pebble/vfs/disk_health.go#l862-l864](github.com/cockroachdb/pebble/vfs/external/com_github_cockroachdb_pebble/vfs/disk_health.go#l862-l864) [github.com/cockroachdb/pebble/vfs/external/com_github_cockroachdb_pebble/vfs/disk_health.go#l637-l639](github.com/cockroachdb/pebble/vfs/external/com_github_cockroachdb_pebble/vfs/disk_health.go#l637-l639) [github.com/cockroachdb/pebble/vfs/external/com_github_cockroachdb_pebble/vfs/disk_health.go#l863-l865](github.com/cockroachdb/pebble/vfs/external/com_github_cockroachdb_pebble/vfs/disk_health.go#l863-l865) [github.com/cockroachdb/pebble/vfs/external/com_github_cockroachdb_pebble/vfs/vfs.go#l265-l267](github.com/cockroachdb/pebble/vfs/external/com_github_cockroachdb_pebble/vfs/vfs.go#l265-l267) [github.com/cockroachdb/pebble/vfs/external/com_github_cockroachdb_pebble/vfs/vfs.go#l261-l263](github.com/cockroachdb/pebble/vfs/external/com_github_cockroachdb_pebble/vfs/vfs.go#l261-l263) ### tags | tag | value | | --- | --- | | command | start-single-node | | environment | v24.1.1 | | go version | go1.22.3 x:nocoverageredesign | | platform | linux arm64 | | distribution | ccl | | cockroach release | v24.1.1 | | cockroach sha | 484f8e2f4d3cb23582416944d79eab72ff42a6a1 | | # of cpus | 8 | | # of goroutines | 422 | jira issue: crdb-58918,[],['BUG'],"['C-bug', 'A-storage', 'O-sentry', 'X-blathers-triaged', 'T-storage', 'branch-release-24.1']",github,2026-01-21T14:57:19Z,2026-01-21T15:48:54Z,sentry: vfs.go:262: rename √ó √ó: operation not permitted (1) attached stack trace -- stack trace: | runtime.gopanic | goroot/src/runtime/panic.go:770 | github.com/cockroachdb/pebble.(*db).ro... this issue was auto filed by sentry. it represents a crash or reported error on a live cluster with telemetry enabled. sentry link: [ panic message: stacktrace (expand for inline code snippets): [src/runtime/asm_arm64.s#l1221-l1223](src/runtime/asm_arm64.s#l1221-l1223) [ [ [ [ [ [ [ [ [ [ [github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/batch.go#l1462-l1464](github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/batch.go#l1462-l1464) [github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l773-l775](github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l773-l775) [github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l844-l846](github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l844-l846) [github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/commit.go#l315-l317](github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/commit.go#l315-l317) [github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/commit.go#l466-l468](github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/commit.go#l466-l468) [github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l930-l932](github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l930-l932) [github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l2489-l2491](github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l2489-l2491) [github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l2613-l2615](github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l2613-l2615) [goroot/src/runtime/panic.go#l769-l771](goroot/src/runtime/panic.go#l769-l771) [src/runtime/asm_arm64.s#l1221-l1223](src/runtime/asm_arm64.s#l1221-l1223) [ [ [ [ [ [ [ [ [ [ [github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/batch.go#l1462-l1464](github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/batch.go#l1462-l1464) [github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l773-l775](github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l773-l775) [github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l844-l846](github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l844-l846) [github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/commit.go#l315-l317](github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/commit.go#l315-l317) [github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/commit.go#l466-l468](github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/commit.go#l466-l468) [github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l930-l932](github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l930-l932) [github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l2489-l2491](github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l2489-l2491) [github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l2611-l2613](github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l2611-l2613) [github.com/cockroachdb/pebble/wal/external/com_github_cockroachdb_pebble/wal/standalone_manager.go#l142-l144](github.com/cockroachdb/pebble/wal/external/com_github_cockroachdb_pebble/wal/standalone_manager.go#l142-l144) [ [github.com/cockroachdb/pebble/vfs/external/com_github_cockroachdb_pebble/vfs/disk_full.go#l291-l293](github.com/cockroachdb/pebble/vfs/external/com_github_cockroachdb_pebble/vfs/disk_full.go#l291-l293) [github.com/cockroachdb/pebble/vfs/external/com_github_cockroachdb_pebble/vfs/disk_health.go#l862-l864](github.com/cockroachdb/pebble/vfs/external/com_github_cockroachdb_pebble/vfs/disk_health.go#l862-l864) [github.com/cockroachdb/pebble/vfs/external/com_github_cockroachdb_pebble/vfs/disk_health.go#l637-l639](github.com/cockroachdb/pebble/vfs/external/com_github_cockroachdb_pebble/vfs/disk_health.go#l637-l639) [github.com/cockroachdb/pebble/vfs/external/com_github_cockroachdb_pebble/vfs/disk_health.go#l863-l865](github.com/cockroachdb/pebble/vfs/external/com_github_cockroachdb_pebble/vfs/disk_health.go#l863-l865) [github.com/cockroachdb/pebble/vfs/external/com_github_cockroachdb_pebble/vfs/vfs.go#l266-l268](github.com/cockroachdb/pebble/vfs/external/com_github_cockroachdb_pebble/vfs/vfs.go#l266-l268) [src/runtime/asm_arm64.s#l1221-l1223](src/runtime/asm_arm64.s#l1221-l1223) [ [ [ [ [ [ [ [ [ [ [github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/batch.go#l1462-l1464](github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/batch.go#l1462-l1464) [github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l773-l775](github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l773-l775) [github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l844-l846](github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l844-l846) [github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/commit.go#l315-l317](github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/commit.go#l315-l317) [github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/commit.go#l466-l468](github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/commit.go#l466-l468) [github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l930-l932](github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l930-l932) [github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l2489-l2491](github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l2489-l2491) [github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l2611-l2613](github.com/cockroachdb/pebble/external/com_github_cockroachdb_pebble/db.go#l2611-l2613) [github.com/cockroachdb/pebble/wal/external/com_github_cockroachdb_pebble/wal/standalone_manager.go#l142-l144](github.com/cockroachdb/pebble/wal/external/com_github_cockroachdb_pebble/wal/standalone_manager.go#l142-l144) [ [github.com/cockroachdb/pebble/vfs/external/com_github_cockroachdb_pebble/vfs/disk_full.go#l291-l293](github.com/cockroachdb/pebble/vfs/external/com_github_cockroachdb_pebble/vfs/disk_full.go#l291-l293) [github.com/cockroachdb/pebble/vfs/external/com_github_cockroachdb_pebble/vfs/disk_health.go#l862-l864](github.com/cockroachdb/pebble/vfs/external/com_github_cockroachdb_pebble/vfs/disk_health.go#l862-l864) [github.com/cockroachdb/pebble/vfs/external/com_github_cockroachdb_pebble/vfs/disk_health.go#l637-l639](github.com/cockroachdb/pebble/vfs/external/com_github_cockroachdb_pebble/vfs/disk_health.go#l637-l639) [github.com/cockroachdb/pebble/vfs/external/com_github_cockroachdb_pebble/vfs/disk_health.go#l863-l865](github.com/cockroachdb/pebble/vfs/external/com_github_cockroachdb_pebble/vfs/disk_health.go#l863-l865) [github.com/cockroachdb/pebble/vfs/external/com_github_cockroachdb_pebble/vfs/vfs.go#l265-l267](github.com/cockroachdb/pebble/vfs/external/com_github_cockroachdb_pebble/vfs/vfs.go#l265-l267) [github.com/cockroachdb/pebble/vfs/external/com_github_cockroachdb_pebble/vfs/vfs.go#l261-l263](github.com/cockroachdb/pebble/vfs/external/com_github_cockroachdb_pebble/vfs/vfs.go#l261-l263) ### tags | tag | value | | --- | --- | | command | start-single-node | | environment | v24.1.1 | | go version | go1.22.3 x:nocoverageredesign | | platform | linux arm64 | | distribution | ccl | | cockroach release | v24.1.1 | | cockroach sha | 484f8e2f4d3cb23582416944d79eab72ff42a6a1 | | # of cpus | 8 | | # of goroutines | 422 | jira issue: crdb-58918,6.0,Critical,1.0,crash-like behavior
cilium/cilium#43903,cilium node hangs and doesn't respond to any requests,"### is there an existing issue for this? - [x] i have searched the existing issues ### version equal or higher than v1.18.6 and lower than v1.19.0 ### what happened? this is a sporadic issue that doesn't happen often. we have dozens of eks clusters with 20+ nodes each and at some point of time some node stops responding at all. other cilium agents also can't reach it (icmp to stack: connection timed out, http to agent: context deadline exceeded (client.timeout exceeded while awaiting headers)). debugging this issue is not trivial because we have access to our nodes via amazon ssm and this faulty node doesn't respond to ssm. ### how can we reproduce the issue? this is an infrequent problem that happens to one node among hundreds others. but this problem already repeated multiple times on different clusters in the past months. ### cilium version client: 1.18.5 7d4d8932 2025-12-17t04:56:47+00:00 go version go1.24.11 linux/amd64 daemon: 1.18.5 7d4d8932 2025-12-17t04:56:47+00:00 go version go1.24.11 linux/amd64 i know it's not the freshest, but we can't update that fast ### kernel version 6.1.150-174.273.amzn2023.x86_64 smp preempt_dynamic tue sep 9 12:21:26 utc 2025 x86_64 x86_64 x86_64 gnu/linux ### kubernetes version server version: v1.32.9-eks-3025e55 ### regression _no response_ ### sysdump node is not reachable, can't run sysdump. ### relevant log output ### anything else? in the past we had similar issues where cilium agents stopped reaching one node, but that node was still accessible with ssm. at that time we determined it was related to wireguard encryption and updated cilium to the newest version (1.18.5). this issue is somewhat similar, but node is completely unresponsive. ### cilium users document - [ ] are you a user of cilium? please add yourself to the [users doc]( ### code of conduct - [x] i agree to follow this project's code of conduct",[],"['BUG', 'FEATURE']","['kind/bug', 'area/kernel', 'area/datapath', 'kind/community-report', 'area/encryption', 'info-completed', 'feature/wireguard']",github,2026-01-21T15:18:01Z,,"cilium node hangs and doesn't respond to any requests ### is there an existing issue for this? - [x] i have searched the existing issues ### version equal or higher than v1.18.6 and lower than v1.19.0 ### what happened? this is a sporadic issue that doesn't happen often. we have dozens of eks clusters with 20+ nodes each and at some point of time some node stops responding at all. other cilium agents also can't reach it (icmp to stack: connection timed out, http to agent: context deadline exceeded (client.timeout exceeded while awaiting headers)). debugging this issue is not trivial because we have access to our nodes via amazon ssm and this faulty node doesn't respond to ssm. ### how can we reproduce the issue? this is an infrequent problem that happens to one node among hundreds others. but this problem already repeated multiple times on different clusters in the past months. ### cilium version client: 1.18.5 7d4d8932 2025-12-17t04:56:47+00:00 go version go1.24.11 linux/amd64 daemon: 1.18.5 7d4d8932 2025-12-17t04:56:47+00:00 go version go1.24.11 linux/amd64 i know it's not the freshest, but we can't update that fast ### kernel version 6.1.150-174.273.amzn2023.x86_64 smp preempt_dynamic tue sep 9 12:21:26 utc 2025 x86_64 x86_64 x86_64 gnu/linux ### kubernetes version server version: v1.32.9-eks-3025e55 ### regression _no response_ ### sysdump node is not reachable, can't run sysdump. ### relevant log output ### anything else? in the past we had similar issues where cilium agents stopped reaching one node, but that node was still accessible with ssm. at that time we determined it was related to wireguard encryption and updated cilium to the newest version (1.18.5). this issue is somewhat similar, but node is completely unresponsive. ### cilium users document - [ ] are you a user of cilium? please add yourself to the [users doc]( ### code of conduct - [x] i agree to follow this project's code of conduct",2.948,Medium,0.89,crash-like behavior
rust-lang/rust#151458,doesn't simplify to when compiled with,"i tried compiling this code with : i expected to see this happen: this should simplify to because the documentation of says ""determines whether the current thread is unwinding because of panic"", but the current thread cannot be unwinding when we are in mode. instead, this happened: a dozen instructions are generated, referring to , etc. see can the [implementation]( be changed to something like this? due to constant folding, this could potentially automatically optimize away a lot of code referring to when compiled in mode. ### meta rust version 1.92.0 on",[],"['DOCUMENTATION', 'UI']","['E-easy', 'T-libs-api', 'A-docs', 'T-libs', 'C-discussion', 'A-panic', '-Zbuild-std']",github,2026-01-21T15:33:40Z,,"doesn't simplify to when compiled with i tried compiling this code with : i expected to see this happen: this should simplify to because the documentation of says ""determines whether the current thread is unwinding because of panic"", but the current thread cannot be unwinding when we are in mode. instead, this happened: a dozen instructions are generated, referring to , etc. see can the [implementation]( be changed to something like this? due to constant folding, this could potentially automatically optimize away a lot of code referring to when compiled in mode. ### meta rust version 1.92.0 on",2.568,Medium,0.804,"user-visible issue, crash-like behavior"
cockroachdb/cockroach#161512,spanconfigstore: cache tenant id in spanconfigpairinterned to optimize computesplitkey,"## summary the function in is a cpu hotspot in large schema benchmarks. profiling shows that repeated calls to and in the coalescing loop account for ~38% of the function's cpu time. ## profile data [cpu-prof.pb.gz]( from a 1m table benchmark ( ): | function/line | cumulative time | % of total | |---------------|-----------------|------------| | total | 102.91s | 96.82% | | | 29.18s | 27.45% | | | 11.36s | 10.69% | the hot loop (lines 199-208) iterates through span config entries looking for where coalescing ends, calling on every entry: ## proposed solution cache the tenant id as a in the struct, computing it once when entries are created in : in : this eliminates the per-iteration and calls, replacing them with a simple uint64 comparison. ## code references - [span_store.go:199-208]( - hot loop - [span_store.go:535-538]( - struct - [span_store.go:555-567]( - function ## expected impact ~38% reduction in cpu time, which dominates the profile in large schema scenarios. jira issue: crdb-58920 epic crdb-58737",[],['PERFORMANCE'],"['C-performance', 'T-sql-foundations', 'A-many-descriptors', 'target-release-26.2.0']",github,2026-01-21T15:58:24Z,2026-01-23T17:53:14Z,"spanconfigstore: cache tenant id in spanconfigpairinterned to optimize computesplitkey ## summary the function in is a cpu hotspot in large schema benchmarks. profiling shows that repeated calls to and in the coalescing loop account for ~38% of the function's cpu time. ## profile data [cpu-prof.pb.gz]( from a 1m table benchmark ( ): | function/line | cumulative time | % of total | |---------------|-----------------|------------| | total | 102.91s | 96.82% | | | 29.18s | 27.45% | | | 11.36s | 10.69% | the hot loop (lines 199-208) iterates through span config entries looking for where coalescing ends, calling on every entry: ## proposed solution cache the tenant id as a in the struct, computing it once when entries are created in : in : this eliminates the per-iteration and calls, replacing them with a simple uint64 comparison. ## code references - [span_store.go:199-208]( - hot loop - [span_store.go:535-538]( - struct - [span_store.go:555-567]( - function ## expected impact ~38% reduction in cpu time, which dominates the profile in large schema scenarios. jira issue: crdb-58920 epic crdb-58737",4.6,Critical,1.0,performance degradation
cilium/cilium#43904,panic in github.com/cilium/cilium/pkg/byteorder.netipv4tohost32,commit 855259bc3fbb07ae5d7f1dafafc4d71921831976,[],['BUG'],"['kind/bug', 'area/loader', 'area/datapath']",github,2026-01-21T16:05:38Z,2026-01-21T16:49:15Z,panic in github.com/cilium/cilium/pkg/byteorder.netipv4tohost32 commit 855259bc3fbb07ae5d7f1dafafc4d71921831976,4.6,Critical,1.0,crash-like behavior
rust-lang/rust#151461,[ice]:,"<!-- [31mice[0m: rustc ./d1cb805ab4649609b1d27535ee0f31402ccc238ca4241c73b31ebd5b3137acef.rs '-znext-solver=globally -ooutputfile -zdump-mir-dir=dir' 'error: internal compiler error: compiler/rustc_infer/src/infer/outlives/obligations.rs:220:17: unexpected overflowed when processing region obligations: [', 'error: internal compiler error: compiler/rustc_infer/src/infer/outlives/obligations.rs:220:17: unexpected overflowed when processing region obligations: [' file: /tmp/mi/d1cb805ab4649609b1d27535ee0f31402ccc238ca4241c73b31ebd5b3137acef.rs --> auto-reduced (treereduce-rust): ` original: ` version information ` possibly related line of code: command: program output <!-- query stack: [param_env] computing normalized predicates of [impl_self_is_guaranteed_unsized] computing whether has a guaranteed unsized self type [check_well_formed] checking that is well-formed [check_type_wf] checking that types are well-formed [analysis] running analysis passes on crate --> label +wg-trait-system-refactor",[],"['CLEANUP', 'BUG']","['I-ICE', 'T-compiler', 'C-bug', 'WG-trait-system-refactor', 'needs-triage']",github,2026-01-21T16:12:26Z,,"[ice]: <!-- [31mice[0m: rustc ./d1cb805ab4649609b1d27535ee0f31402ccc238ca4241c73b31ebd5b3137acef.rs '-znext-solver=globally -ooutputfile -zdump-mir-dir=dir' 'error: internal compiler error: compiler/rustc_infer/src/infer/outlives/obligations.rs:220:17: unexpected overflowed when processing region obligations: [', 'error: internal compiler error: compiler/rustc_infer/src/infer/outlives/obligations.rs:220:17: unexpected overflowed when processing region obligations: [' file: /tmp/mi/d1cb805ab4649609b1d27535ee0f31402ccc238ca4241c73b31ebd5b3137acef.rs --> auto-reduced (treereduce-rust): ` original: ` version information ` possibly related line of code: command: program output <!-- query stack: [param_env] computing normalized predicates of [impl_self_is_guaranteed_unsized] computing whether has a guaranteed unsized self type [check_well_formed] checking that is well-formed [check_type_wf] checking that types are well-formed [analysis] running analysis passes on crate --> label +wg-trait-system-refactor",2.988,Medium,0.899,crash-like behavior
rust-lang/rust#151462,[ice]:,"<!-- [31mice[0m: rustc ./79a2ede254192087e59eb19f5529819bd8d8e75c77566a75a6455944663e57b8.rs '-zcrate-attr=feature(lazy_type_alias) -znext-solver=globally -ooutputfile -zdump-mir-dir=dir' 'error: internal compiler error: compiler/rustc_trait_selection/src/error_reporting/traits/fulfillment_errors.rs:2917:32: inconsistent rustc_transmute::is_transmutable(...) result, got yes', 'error: internal compiler error: compiler/rustc_trait_selection/src/error_reporting/traits/fulfillment_errors.rs:2917:32: inconsistent rustc_transmute::is_transmutable(...) result, got yes' file: /tmp/mi/79a2ede254192087e59eb19f5529819bd8d8e75c77566a75a6455944663e57b8.rs --> auto-reduced (treereduce-rust): ` original: ` version information ` possibly related line of code: command: program output <!-- query stack: [typeck] type-checking [analysis] running analysis passes on crate --> label +f-transmutability +f-lazy_type_alias +wg-trait-system-refactor",[],"['CLEANUP', 'BUG']","['I-ICE', 'T-compiler', 'C-bug', 'WG-trait-system-refactor', 'needs-triage', 'F-lazy_type_alias', 'F-transmutability']",github,2026-01-21T16:15:31Z,,"[ice]: <!-- [31mice[0m: rustc ./79a2ede254192087e59eb19f5529819bd8d8e75c77566a75a6455944663e57b8.rs '-zcrate-attr=feature(lazy_type_alias) -znext-solver=globally -ooutputfile -zdump-mir-dir=dir' 'error: internal compiler error: compiler/rustc_trait_selection/src/error_reporting/traits/fulfillment_errors.rs:2917:32: inconsistent rustc_transmute::is_transmutable(...) result, got yes', 'error: internal compiler error: compiler/rustc_trait_selection/src/error_reporting/traits/fulfillment_errors.rs:2917:32: inconsistent rustc_transmute::is_transmutable(...) result, got yes' file: /tmp/mi/79a2ede254192087e59eb19f5529819bd8d8e75c77566a75a6455944663e57b8.rs --> auto-reduced (treereduce-rust): ` original: ` version information ` possibly related line of code: command: program output <!-- query stack: [typeck] type-checking [analysis] running analysis passes on crate --> label +f-transmutability +f-lazy_type_alias +wg-trait-system-refactor",1.8,Low,0.629,localized low-impact
istio/istio#58861,joinwithmerge under stress fails with an assertion,"### is this the right place to submit this? - [x] this is not a security vulnerability or a crashing bug - [x] this is not a question about how to use istio ### bug description given this test case the test will always pass without enableassertions. now that enableassertions is enabled, it will fail every so often with i am not convinced this is a real bug given its a timing issue between the mergejoin collection and what is happening above. ### version ### additional information _no response_",[],['TESTING'],['area/test and release'],github,2026-01-21T16:49:22Z,,"joinwithmerge under stress fails with an assertion ### is this the right place to submit this? - [x] this is not a security vulnerability or a crashing bug - [x] this is not a question about how to use istio ### bug description given this test case the test will always pass without enableassertions. now that enableassertions is enabled, it will fail every so often with i am not convinced this is a real bug given its a timing issue between the mergejoin collection and what is happening above. ### version ### additional information _no response_",5.6,Critical,1.0,crash-like behavior
cockroachdb/cockroach#161516,"sql,parser: storing clause for hash sharded indexes appears in the wrong place in pg_indexes","the storing clause for create index .. with hash is in the wrong place when shown in pg_indexes. the index statement is created by this function: to test it, we can add a test case in an appropriate part of the logictest: the test can look at the output from show create table, or if needed it can query pg_indexes directly. another place where we should add testing is: these are parser-only tests, which may not reproduce the bug, but i noticed that hash-sharded indexes are not tested at all here, so we should add them while working on this. you can easily regenerate the ""expected"" output by running with : , and verifying manually that the expected output is what it should be. jira issue: crdb-58921 epic crdb-58150",[],['BUG'],"['C-bug', 'E-starter', 'A-sql-syntax', 'T-sql-foundations', 'target-release-26.2.0']",github,2026-01-21T16:59:15Z,2026-01-27T22:32:40Z,"sql,parser: storing clause for hash sharded indexes appears in the wrong place in pg_indexes the storing clause for create index .. with hash is in the wrong place when shown in pg_indexes. the index statement is created by this function: to test it, we can add a test case in an appropriate part of the logictest: the test can look at the output from show create table, or if needed it can query pg_indexes directly. another place where we should add testing is: these are parser-only tests, which may not reproduce the bug, but i noticed that hash-sharded indexes are not tested at all here, so we should add them while working on this. you can easily regenerate the ""expected"" output by running with : , and verifying manually that the expected output is what it should be. jira issue: crdb-58921 epic crdb-58150",2.252,Medium,0.732,functional impact
cockroachdb/cockroach#161518,sql/tablemetadatacache: testtablemetadataupdatejobprogressandmetrics failed,sql/tablemetadatacache.testtablemetadataupdatejobprogressandmetrics [failed]( on master @ [a21482d70b2e4fe28738bdf047d187e5ed3cf44f]( parameters: - attempt=1 - run=1 - shard=1 help see also: [how to investigate a go test failure \(internal\)]( /cc /obs-prs [this test on roachdash]( | [improve this report!]( jira issue: crdb-58922,[],['TESTING'],"['C-test-failure', 'O-robot', 'branch-master', 'release-blocker', 'T-observability']",github,2026-01-21T17:21:53Z,,sql/tablemetadatacache: testtablemetadataupdatejobprogressandmetrics failed sql/tablemetadatacache.testtablemetadataupdatejobprogressandmetrics [failed]( on master @ [a21482d70b2e4fe28738bdf047d187e5ed3cf44f]( parameters: - attempt=1 - run=1 - shard=1 help see also: [how to investigate a go test failure \(internal\)]( /cc /obs-prs [this test on roachdash]( | [improve this report!]( jira issue: crdb-58922,1.6,Low,0.584,localized low-impact
python/cpython#144121,make compiler definition in getcompiler.c optionally static for reproducible builds,"# feature or enhancement ### proposal: suse (both for sle and opensuse) have been using for a long time (the name of the patchfile starts with ;)) similar patch to avoid having dynamically generated symbols in the build binaries (version of compilers in this case). i have discussed upstreaming of this patch on and it seems to me that in order to give this issue some momentum, i have to just file pr here. ### has this already been discussed elsewhere? no response given ### links to previous discussion of this feature: _no response_ ### linked prs * gh-144124",[],"['FEATURE', 'UI']","['type-feature', 'build']",github,2026-01-21T17:30:53Z,,"make compiler definition in getcompiler.c optionally static for reproducible builds # feature or enhancement ### proposal: suse (both for sle and opensuse) have been using for a long time (the name of the patchfile starts with ;)) similar patch to avoid having dynamically generated symbols in the build binaries (version of compilers in this case). i have discussed upstreaming of this patch on and it seems to me that in order to give this issue some momentum, i have to just file pr here. ### has this already been discussed elsewhere? no response given ### links to previous discussion of this feature: _no response_ ### linked prs * gh-144124",1.6,Low,0.584,user-visible issue
rust-lang/rust#151466,"[ice]: rustc panics on using the ""pattern_type"" macro","when try to use the patten_type , the rustc compiler will panic when that function will be called. rustc version: 1.94.0 os: fedora 49",[],"['BUG', 'FEATURE', 'UI']","['I-ICE', 'T-compiler', 'C-bug', 'requires-nightly', 'requires-internal-features', 'S-needs-repro', 'S-needs-info']",github,2026-01-21T17:35:10Z,2026-01-21T19:22:37Z,"[ice]: rustc panics on using the ""pattern_type"" macro when try to use the patten_type , the rustc compiler will panic when that function will be called. rustc version: 1.94.0 os: fedora 49",2.609,Medium,0.813,"user-visible issue, crash-like behavior"
cockroachdb/cockroach#161520,server: surface warning when gc has to run aggressively,"we currently set the [gc memory limit]( which makes gc become more and more aggressive as the heap gets close to this limit. this is very effective in fighting ooms, and also improves performance when there is a lot of memory headroom. the downside is that if we get close to the limit, the system can get into a sustained mode of degraded performance. this issue tracks adding code to detect this situation and surface a warning in the cluster event log. jira issue: crdb-58923",[],['FEATURE'],"['C-enhancement', 'O-support', 'T-db-server']",github,2026-01-21T17:43:57Z,,"server: surface warning when gc has to run aggressively we currently set the [gc memory limit]( which makes gc become more and more aggressive as the heap gets close to this limit. this is very effective in fighting ooms, and also improves performance when there is a lot of memory headroom. the downside is that if we get close to the limit, the system can get into a sustained mode of degraded performance. this issue tracks adding code to detect this situation and surface a warning in the cluster event log. jira issue: crdb-58923",2.946,Medium,0.89,functional impact
cockroachdb/cockroach#161522,roachtest: kv-rangefeed/write-rate=10000/sink-rate=12000/catchup=5m0s/splits=1000 failed,roachtest.kv-rangefeed/write-rate=10000/sink-rate=12000/catchup=5m0s/splits=1000 [failed]( with [artifacts]( on release-26.1 @ [6ac107383aa8b7246f0d6448366407b6f53e71e4]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21002997-1768979419-211-n4cpu16-0001 | 34.27.253.66 | 10.128.0.119 | | teamcity-21002997-1768979419-211-n4cpu16-0002 | 136.111.89.127 | 10.128.0.123 | | teamcity-21002997-1768979419-211-n4cpu16-0003 | 136.113.8.45 | 10.128.0.120 | | teamcity-21002997-1768979419-211-n4cpu16-0004 | 136.112.118.164 | 10.128.0.129 | parameters: - metamorphicfilesystem=xfs - metamorphicvolumetype=pd-ssd - arch=arm64 - cloud=gce - coveragebuild=false - cpu=16 - diskcount=0 - encrypted=false - fs=xfs - localssd=false - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( same failure on other branches - roachtest: kv-rangefeed/write-rate=10000/sink-rate=12000/catchup=5m0s/splits=1000 failed [c-bug c-test-failure o-roachtest o-robot p-2 t-kv branch-master release-blocker] /cc /kv-triage [this test on roachdash]( | [improve this report!]( jira issue: crdb-58924,[],"['TESTING', 'BUG']","['C-bug', 'C-test-failure', 'O-robot', 'X-duplicate', 'A-testing', 'O-roachtest', 'T-kv', 'branch-release-26.1']",github,2026-01-21T18:08:31Z,,roachtest: kv-rangefeed/write-rate=10000/sink-rate=12000/catchup=5m0s/splits=1000 failed roachtest.kv-rangefeed/write-rate=10000/sink-rate=12000/catchup=5m0s/splits=1000 [failed]( with [artifacts]( on release-26.1 @ [6ac107383aa8b7246f0d6448366407b6f53e71e4]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21002997-1768979419-211-n4cpu16-0001 | 34.27.253.66 | 10.128.0.119 | | teamcity-21002997-1768979419-211-n4cpu16-0002 | 136.111.89.127 | 10.128.0.123 | | teamcity-21002997-1768979419-211-n4cpu16-0003 | 136.113.8.45 | 10.128.0.120 | | teamcity-21002997-1768979419-211-n4cpu16-0004 | 136.112.118.164 | 10.128.0.129 | parameters: - metamorphicfilesystem=xfs - metamorphicvolumetype=pd-ssd - arch=arm64 - cloud=gce - coveragebuild=false - cpu=16 - diskcount=0 - encrypted=false - fs=xfs - localssd=false - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( same failure on other branches - roachtest: kv-rangefeed/write-rate=10000/sink-rate=12000/catchup=5m0s/splits=1000 failed [c-bug c-test-failure o-roachtest o-robot p-2 t-kv branch-master release-blocker] /cc /kv-triage [this test on roachdash]( | [improve this report!]( jira issue: crdb-58924,2.613,Medium,0.814,functional impact
python/cpython#144125,encode newlines in headers when using bytegenerator,related: * cve-2024-6923 * ### linked prs * gh-144126 * gh-144180 * gh-144181 * gh-144182 * gh-144188 * gh-144189,[],['SECURITY'],"['type-security', 'stdlib', 'topic-email']",github,2026-01-21T18:33:26Z,,encode newlines in headers when using bytegenerator related: * cve-2024-6923 * ### linked prs * gh-144126 * gh-144180 * gh-144181 * gh-144182 * gh-144188 * gh-144189,3.8,Critical,1.0,security risk
istio/istio#58864,gateway api proxies should use the same tlsdefaults of the environment,"(this is used to request new product features, please visit < for questions on using istio) **describe the feature request** as part of security policy, companies may want to ensure that proxies are always provisioned with a minimum protocol version and ciphersuite. today this is already supported on istio e/w and also when using istio gateway, but gatewayapi does not respect these configurations (meshconfig.tlsdefaults). while gateway api does not support this configuration, it would be nice to make istio and pilot configure the listener with the same defaults specified on tlsdefaults. **describe alternatives you've considered** using envoyfilter, which works but is per gateway **affected product area (please put an x in all that apply)** [ ] ambient [ ] docs [ ] dual stack [ ] installation [ ] networking [ ] performance and scalability [ ] extensions and telemetry [x] security [ ] test and release [ ] user experience [ ] developer infrastructure **affected features (please put an x in all that apply)** [ ] multi cluster [ ] virtual machine [ ] multi control plane **additional context**",[],"['SECURITY', 'FEATURE']","['kind/enhancement', 'area/security']",github,2026-01-21T19:00:28Z,,"gateway api proxies should use the same tlsdefaults of the environment (this is used to request new product features, please visit < for questions on using istio) **describe the feature request** as part of security policy, companies may want to ensure that proxies are always provisioned with a minimum protocol version and ciphersuite. today this is already supported on istio e/w and also when using istio gateway, but gatewayapi does not respect these configurations (meshconfig.tlsdefaults). while gateway api does not support this configuration, it would be nice to make istio and pilot configure the listener with the same defaults specified on tlsdefaults. **describe alternatives you've considered** using envoyfilter, which works but is per gateway **affected product area (please put an x in all that apply)** [ ] ambient [ ] docs [ ] dual stack [ ] installation [ ] networking [ ] performance and scalability [ ] extensions and telemetry [x] security [ ] test and release [ ] user experience [ ] developer infrastructure **affected features (please put an x in all that apply)** [ ] multi cluster [ ] virtual machine [ ] multi control plane **additional context**",3.542,High,1.0,security risk
containerd/containerd#12808,containerd config default generates unsupported colon-separated config_path for cri registry hosts,"### description i am not the most familiar with containerd, but could not find any duplicate issues. i used the default config.toml generated by 'containerd config default > /etc/containerd/config.toml' i have a local docker registry mirroring a few online registries. for each, i created /etc/containerd/certs.d/ /hosts.toml i tested pulling images with the command 'sudo crictl pull registry.k8s.io/pause:3.10' the default config.toml registry hosts config_path did not work for me. crictl attempted to reach the public registry, which is outside of my network. the config_path was as follows: config_path = '/etc/containerd/certs.d:/etc/docker/certs.d' removing the docker path allowed me to successfully execute the crictl pull command: config_path = '/etc/containerd/certs.d' i did not test whether: - the colon-separated multi-path syntax is unsupported, or - containerd fails when one of the paths (/etc/docker/certs.d) doesn't exist. it does not exist on my system. note: there was no error or warning in logs indicating the config_path was invalid or that a directory was missing. containerd simply ignored the registry hosts configuration. ### steps to reproduce the issue 1. set up an instance of ubuntu server 24.04.1 2. execute 3. execute 4. create and configure to point at a toy/dummy mirror registry. i don't think the registry needs to exist, as crictl will hit the public registry instead. example below: server = "" [host."" capabilities = [""pull"", ""resolve""] 5. execute 6. in another shell, execute 7. attempt in the initial shell. 8. ### describe the results you received and expected i saw the output of crictl pull and logs in journalctl -xeu containerd.service show hits on registry.k8s.io rather than my mirror. i instead expected a successful pull. the successful pull occurs when the /etc/docker/certs.d path is removed from config_path. ### what version of containerd are you using? containerd containerd.io v2.2.0 1c4457e00facac03ce1d75f7b6777a7a851e5c41 ### any other relevant information ubuntu 24.04.1 lts (noble numbat) ### show configuration if it is related to cri plugin. _no response_",[],['BUG'],"['kind/bug', 'area/cri']",github,2026-01-21T19:12:14Z,,"containerd config default generates unsupported colon-separated config_path for cri registry hosts ### description i am not the most familiar with containerd, but could not find any duplicate issues. i used the default config.toml generated by 'containerd config default > /etc/containerd/config.toml' i have a local docker registry mirroring a few online registries. for each, i created /etc/containerd/certs.d/ /hosts.toml i tested pulling images with the command 'sudo crictl pull registry.k8s.io/pause:3.10' the default config.toml registry hosts config_path did not work for me. crictl attempted to reach the public registry, which is outside of my network. the config_path was as follows: config_path = '/etc/containerd/certs.d:/etc/docker/certs.d' removing the docker path allowed me to successfully execute the crictl pull command: config_path = '/etc/containerd/certs.d' i did not test whether: - the colon-separated multi-path syntax is unsupported, or - containerd fails when one of the paths (/etc/docker/certs.d) doesn't exist. it does not exist on my system. note: there was no error or warning in logs indicating the config_path was invalid or that a directory was missing. containerd simply ignored the registry hosts configuration. ### steps to reproduce the issue 1. set up an instance of ubuntu server 24.04.1 2. execute 3. execute 4. create and configure to point at a toy/dummy mirror registry. i don't think the registry needs to exist, as crictl will hit the public registry instead. example below: server = "" [host."" capabilities = [""pull"", ""resolve""] 5. execute 6. in another shell, execute 7. attempt in the initial shell. 8. ### describe the results you received and expected i saw the output of crictl pull and logs in journalctl -xeu containerd.service show hits on registry.k8s.io rather than my mirror. i instead expected a successful pull. the successful pull occurs when the /etc/docker/certs.d path is removed from config_path. ### what version of containerd are you using? containerd containerd.io v2.2.0 1c4457e00facac03ce1d75f7b6777a7a851e5c41 ### any other relevant information ubuntu 24.04.1 lts (noble numbat) ### show configuration if it is related to cri plugin. _no response_",2.514,Medium,0.791,functional impact
tensorflow/tensorflow#108605,"tf.compat.dimension_value returns a tensor when given a tensor, despite documentation stating it returns only int or none.","### issue type bug ### have you reproduced the bug with tensorflow nightly? no ### source source ### tensorflow version tensorflow 2.20.0 ### custom code yes ### os platform and distribution _no response_ ### mobile device no ### python version 3.12 ### bazel version _no response_ ### gcc/compiler version _no response_ ### cuda/cudnn version _no response_ ### gpu model and memory _no response_ ### current behavior? according to the documentation, tf.compat.dimension_value has the following signature: dimension: union['dimension', int, none] returns: union[int, none] however, passing a tf.tensor is silently accepted and the tensor is returned unchanged, resulting in a return value outside the documented return type. ### standalone code to reproduce the issue ### relevant log output",[],['BUG'],['type:bug'],github,2026-01-21T19:13:41Z,2026-01-22T22:16:16Z,"tf.compat.dimension_value returns a tensor when given a tensor, despite documentation stating it returns only int or none. ### issue type bug ### have you reproduced the bug with tensorflow nightly? no ### source source ### tensorflow version tensorflow 2.20.0 ### custom code yes ### os platform and distribution _no response_ ### mobile device no ### python version 3.12 ### bazel version _no response_ ### gcc/compiler version _no response_ ### cuda/cudnn version _no response_ ### gpu model and memory _no response_ ### current behavior? according to the documentation, tf.compat.dimension_value has the following signature: dimension: union['dimension', int, none] returns: union[int, none] however, passing a tf.tensor is silently accepted and the tensor is returned unchanged, resulting in a return value outside the documented return type. ### standalone code to reproduce the issue ### relevant log output",6.4,Critical,1.0,crash-like behavior
python/cpython#144127,performance improvement for small,"# feature or enhancement ## overview currently has two execution paths: while this is correct and efficient in general, there are common cases (especially or very small ) where introduces avoidable overhead (heap allocation, tuple comparisons, python-level callbacks). this proposal is to investigate and potentially introduce small fast paths for common values, without changing semantics or public api behavior. --- is a very common usage pattern (e.g., ‚Äúfind the mode‚Äù), but it still uses the full capability of . for example: this could potentially be implemented more efficiently via a single-pass max selection: or an equivalent loop-based approach, avoiding heap construction entirely. similarly, for very small , a specialized strategy may outperform , especially for large counters. ### has this already been discussed elsewhere? this is a minor feature, which does not need previous discussion elsewhere ### links to previous discussion of this feature: _no response_ ### linked prs * gh-144129",[],"['PERFORMANCE', 'FEATURE']","['type-feature', 'performance', 'stdlib']",github,2026-01-21T19:32:18Z,2026-01-22T02:39:11Z,"performance improvement for small # feature or enhancement ## overview currently has two execution paths: while this is correct and efficient in general, there are common cases (especially or very small ) where introduces avoidable overhead (heap allocation, tuple comparisons, python-level callbacks). this proposal is to investigate and potentially introduce small fast paths for common values, without changing semantics or public api behavior. --- is a very common usage pattern (e.g., ‚Äúfind the mode‚Äù), but it still uses the full capability of . for example: this could potentially be implemented more efficiently via a single-pass max selection: or an equivalent loop-based approach, avoiding heap construction entirely. similarly, for very small , a specialized strategy may outperform , especially for large counters. ### has this already been discussed elsewhere? this is a minor feature, which does not need previous discussion elsewhere ### links to previous discussion of this feature: _no response_ ### linked prs * gh-144129",4.1,Critical,1.0,"performance degradation, crash-like behavior"
tensorflow/tensorflow#108608,tf.compat.forward_compatible does not validate year/month/day ranges as documented,"### issue type bug ### have you reproduced the bug with tensorflow nightly? no ### source source ### tensorflow version tensorflow 2.20.0 ### custom code yes ### os platform and distribution _no response_ ### mobile device no ### python version 3.12 ### bazel version _no response_ ### gcc/compiler version _no response_ ### cuda/cudnn version _no response_ ### gpu model and memory _no response_ ### current behavior? the documentation for tf.compat.forward_compatible(year, month, day) specifies that must be in 1‚Äì12 and must be in appropriate ranges for the given month. however, the implementation does not validate these ranges and is silently returning an output instead of raising an error ### standalone code to reproduce the issue ### relevant log output",[],['BUG'],"['type:bug', 'comp:core', 'TF 2.19']",github,2026-01-21T19:35:06Z,2026-01-22T22:14:55Z,"tf.compat.forward_compatible does not validate year/month/day ranges as documented ### issue type bug ### have you reproduced the bug with tensorflow nightly? no ### source source ### tensorflow version tensorflow 2.20.0 ### custom code yes ### os platform and distribution _no response_ ### mobile device no ### python version 3.12 ### bazel version _no response_ ### gcc/compiler version _no response_ ### cuda/cudnn version _no response_ ### gpu model and memory _no response_ ### current behavior? the documentation for tf.compat.forward_compatible(year, month, day) specifies that must be in 1‚Äì12 and must be in appropriate ranges for the given month. however, the implementation does not validate these ranges and is silently returning an output instead of raising an error ### standalone code to reproduce the issue ### relevant log output",4.2,Critical,1.0,system-wide impact
pytorch/pytorch#172976,add a support for fp8 in symm_mem,"### new feature for release add fp8 support in symm_memory, please see ### point(s) of contact _no response_ ### release mode (pytorch/pytorch features only) in-tree ### out-of-tree repo _no response_ ### description and value to the user using fp32/fp16 workarounds would temporarily inflate memory because all-gather happens over nvlink, defeating the purpose of fp8. ideally, you want fp8 allocation + random initialization directly in symmetric memory, so nvlink transfer stays fp8. ### link to design doc, github issues, past submissions, etc ### what feedback adopters have provided _no response_ ### plan for documentations / tutorials tutorial exists ### additional context for tutorials _no response_ ### marketing/blog coverage yes ### are you requesting other marketing assistance with this feature? _no response_ ### release version _no response_ ### os / platform / compute coverage _no response_ ### testing support (ci, test cases, etc..) _no response_ cc -huang",[],['FEATURE'],"['oncall: distributed', 'feature', 'release-feature-request']",github,2026-01-21T19:35:35Z,2026-01-25T05:28:43Z,"add a support for fp8 in symm_mem ### new feature for release add fp8 support in symm_memory, please see ### point(s) of contact _no response_ ### release mode (pytorch/pytorch features only) in-tree ### out-of-tree repo _no response_ ### description and value to the user using fp32/fp16 workarounds would temporarily inflate memory because all-gather happens over nvlink, defeating the purpose of fp8. ideally, you want fp8 allocation + random initialization directly in symmetric memory, so nvlink transfer stays fp8. ### link to design doc, github issues, past submissions, etc ### what feedback adopters have provided _no response_ ### plan for documentations / tutorials tutorial exists ### additional context for tutorials _no response_ ### marketing/blog coverage yes ### are you requesting other marketing assistance with this feature? _no response_ ### release version _no response_ ### os / platform / compute coverage _no response_ ### testing support (ci, test cases, etc..) _no response_ cc -huang",1.4,Low,0.538,localized low-impact
kubernetes/kubernetes#136406,dra: non-pod references in reservedfor prevents removal of pod references,"### what happened? the field in the resource allows for references to arbitrary objects, but only pod references are set by the scheduler and managed by the resourceclaim controller. this involves removing references to a pod when it terminates. when a non-pod reference is added to the reservedfor list, either before or after the scheduler adds a pod reference, the pod reference is no longer removed by the resourceclaim controller when the pod terminates. ### what did you expect to happen? i expected that the pod reference would be removed, but that the non-pod reference would remain. ### how can we reproduce it (as minimally and precisely as possible)? set up the dra example driver with kind as described in but don't apply any of the demo workloads. apply the following resourceclaim and pod: wait until the pod is running. edit the resourceclaim to add the non-pod reference in the list: add the following: delete the pod and wait for it to terminate. inspect the resourceclaim and see that the pod reference is still in the list. ### anything else we need to know? _no response_ ### kubernetes version server version: v1.34.0 on kind ### cloud provider ### os version ### install tools ### container runtime (cri) and version (if applicable) ### related plugins (cni, csi, ...) and versions (if applicable)",[],['BUG'],"['kind/bug', 'needs-triage', 'wg/device-management']",github,2026-01-21T19:35:56Z,,"dra: non-pod references in reservedfor prevents removal of pod references ### what happened? the field in the resource allows for references to arbitrary objects, but only pod references are set by the scheduler and managed by the resourceclaim controller. this involves removing references to a pod when it terminates. when a non-pod reference is added to the reservedfor list, either before or after the scheduler adds a pod reference, the pod reference is no longer removed by the resourceclaim controller when the pod terminates. ### what did you expect to happen? i expected that the pod reference would be removed, but that the non-pod reference would remain. ### how can we reproduce it (as minimally and precisely as possible)? set up the dra example driver with kind as described in but don't apply any of the demo workloads. apply the following resourceclaim and pod: wait until the pod is running. edit the resourceclaim to add the non-pod reference in the list: add the following: delete the pod and wait for it to terminate. inspect the resourceclaim and see that the pod reference is still in the list. ### anything else we need to know? _no response_ ### kubernetes version server version: v1.34.0 on kind ### cloud provider ### os version ### install tools ### container runtime (cri) and version (if applicable) ### related plugins (cni, csi, ...) and versions (if applicable)",4.2,Critical,1.0,system-wide impact
cockroachdb/cockroach#161528,pkg/ccl/logictestccl/tests/multiregion-9node-3region-3azs-tenant/multiregion-9node-3region-3azs-tenant_test: testccllogic_regional_by_row_foreign_key failed,pkg/ccl/logictestccl/tests/multiregion-9node-3region-3azs-tenant/multiregion-9node-3region-3azs-tenant_test.testccllogic_regional_by_row_foreign_key [failed]( with [artifacts]( on release-26.1.0-rc @ [6c9b2f778bd29895383d493a2fa4490323499ecf]( help see also: [how to investigate a go test failure \(internal\)]( [this test on roachdash]( | [improve this report!]( jira issue: crdb-58925,[],['TESTING'],"['C-test-failure', 'O-robot', 'T-sql-queries', 's390x-test-failure', 'branch-release-26.1.0-rc']",github,2026-01-21T19:54:37Z,,pkg/ccl/logictestccl/tests/multiregion-9node-3region-3azs-tenant/multiregion-9node-3region-3azs-tenant_test: testccllogic_regional_by_row_foreign_key failed pkg/ccl/logictestccl/tests/multiregion-9node-3region-3azs-tenant/multiregion-9node-3region-3azs-tenant_test.testccllogic_regional_by_row_foreign_key [failed]( with [artifacts]( on release-26.1.0-rc @ [6c9b2f778bd29895383d493a2fa4490323499ecf]( help see also: [how to investigate a go test failure \(internal\)]( [this test on roachdash]( | [improve this report!]( jira issue: crdb-58925,1.6,Low,0.584,localized low-impact
pytorch/pytorch#172987,leveraging unified memory for mps tensors on apple silicone,"### üöÄ the feature, motivation and pitch ### background in the current pytorch mps backend, tensor backing buffers are allocated in the owner device‚Äôs private memory. when a tensor is moved between cpu and mps, pytorch: 1. allocates a new backing buffer in the destination device‚Äôs private memory. 2. encodes a metal operation to copy data from source to destination. this behavior is consistent and correct, but may not fully leverage the unified physical memory architecture of apple silicon. specifically in cases where the source and destination tensors are not modified, this results in two sets of identical buffers representing the same tensor on source and destination device. our proposal to address this issue is comprised of three parts: 1. allocate all tensors in unified memory instead of device's private memory. 2. in case of moving read-only tensors: since the data cannot be mutated, avoid duplication and of the source backing buffer, and instead reference the same unified memory for the destination tensor. 3. in case of assigning the result of a tensor move to itself: since source tensor and its backing buffer are discarded after device move, avoid duplication and of the source backing buffer, and instead reference the same unified memory for the destination tensor. ### proposal part 1 - allocate tensors in unified memory part 1 of the proposal is implemented and currently going through testing and verification. should parts 2 & 3 be rejected, part 1 can still be merged, as it is independent of the subsequent parts and can still provide performance benefits. **summary** instead of allocating tensor backing buffers in device-private memory by default, we propose allocating _all_ tensors in the unified (shared) memory accessible by both cpu and mps. under this model: - tensor device transfers (cpu ‚Üî mps) would still create a destination tensor with new ownership. - however, data movement would use a on unified memory plus a memory barrier, rather than a metal cpu ‚Üî mps operation where applicable. - this part of the proposal on its own is **not** intended to eliminate destination allocations for calls rather, it focuses on reducing the cost of data movement, when both source and destination backing buffers reside in unified memory. it also paves the way for the next parts of the proposal to specifically reduce memory duplication. **expected impact** - memory allocation size: no significant reduction expected in the general case. - transfer cost: potential reduction by replacing cpu ‚Üî gpu operations with lighter-weight calls on unified memory. - semantics: no change to pytorch tensor ownership or device semantics. **code changes** - tensor allocation [[emptytensor.cpp]( tensor allocation is performed by fetching an allocator class through the call. if device supports unified memory, we propose to pass for allocator creation. as a result, all tensors allocated through this allocator will reside in unified memory. - tensor device move [[copy.mm]( currently, during an cpu ‚Üí mps copy, an explicit blit ( ) is encoded to move bytes from cpu-backed private memory into mps. this will no longer be needed as both source and destination will reside in shared memory. instead, we propose to perform a of the backing buffer on cpu-visible unified memory. since a mere does not carry tensor-specific information, we also manually perform cpu-side negation/conjugation if necessary, as well as to manually change ownership to . we also add a memory barrier ( ) to ensure writes to the tensor by source device are visible to the destination device. please note that at this stage memory duplication still occurs but it is on unified memory rather than device private memory. ### proposal part 2 - avoid duplication for read-only tensors **summary** with part 1 implemented, all tensors will reside in unified memory and be accessible from both cpu and mps. however, when a tensor is moved between devices, we cannot enforce source and destination tensors to reference the same backing buffer, as pytorch semantics require them to behave independently, i.e. write to either tensor should not modify the other. this problem does not however exist in case of read-only tensors, and we can reference the same memory with new device ownership instead of maintaining two copies of the same data. **code changes** still in progress and subject to approval of the proposal. **expected impact** - memory allocation size: reduction by 50% for read-only tensors, by maintaining a single copy of the backing buffer in unified memory for both source and destination tensors. - transfer cost: reduction by avoiding a on unified memory all together, and simply changing the tensor ownership. - semantics: mutable tensors will still maintain two copies for source and destination tensors. therefore, no change to pytorch tensor ownership or device semantics. ### proposal part 3 - avoid duplication when assigning a tensor device move to itself **summary** currently and with part 1 of the proposal in place, when the result of a tensor device move is assigned to itself ( ), a duplicate destination buffer is allocated and copied from source, and immediately after the source tensor is discarded and its backing buffer freed. this is equivalent to simply moving the backing buffer from one place of the unified memory to another. thus we propose to eliminate the unnecessary duplication, and simply reference the same source backing buffer with new ownership for the destination tensor. **code changes** still in progress and subject to approval of the proposal. we are also still investigating if it is possible to distinguish when a tensor device move is assigned to a new tensor vs. when it is assigned to itself. **expected impact** - memory allocation size: overall memory consumption should remain unchanged, but peak memory consumption has the potential to be reduced by reusing the source backing buffer. - transfer cost: reduction by avoiding a on unified memory all together, and simply changing the tensor ownership. - semantics: if both source and destination tensors remain alive, they will still maintain two copies for source and destination tensors. therefore, no change to pytorch tensor ownership or device semantics. ### alternatives instead of parts 2 & 3 of the proposal, a general copy-on-write implementation was also considered that would keep both source and destination tensors referencing the same backing buffer in unified memory, until either one was modified. however, as we understand, [the last copy-on-write attempt]( was ultimately rejected due to the design being too brittle. thus, we propose to reduce the optimization scope to read-only tensors and self-assigning device moves. ### additional context - part 1 of the proposal is implemented and currently at a 97% pass rate for . we are tracking down the remaining 3% failures. - parts 2 & 3 are still under investigation and not implemented. we hope to reach an understanding on whether or not these parts of the proposal in theory are acceptable to the maintainers before attempting to implement. cc",[],['FEATURE'],"['module: memory usage', 'triaged', 'enhancement', 'module: mps']",github,2026-01-21T20:57:45Z,,"leveraging unified memory for mps tensors on apple silicone ### üöÄ the feature, motivation and pitch ### background in the current pytorch mps backend, tensor backing buffers are allocated in the owner device‚Äôs private memory. when a tensor is moved between cpu and mps, pytorch: 1. allocates a new backing buffer in the destination device‚Äôs private memory. 2. encodes a metal operation to copy data from source to destination. this behavior is consistent and correct, but may not fully leverage the unified physical memory architecture of apple silicon. specifically in cases where the source and destination tensors are not modified, this results in two sets of identical buffers representing the same tensor on source and destination device. our proposal to address this issue is comprised of three parts: 1. allocate all tensors in unified memory instead of device's private memory. 2. in case of moving read-only tensors: since the data cannot be mutated, avoid duplication and of the source backing buffer, and instead reference the same unified memory for the destination tensor. 3. in case of assigning the result of a tensor move to itself: since source tensor and its backing buffer are discarded after device move, avoid duplication and of the source backing buffer, and instead reference the same unified memory for the destination tensor. ### proposal part 1 - allocate tensors in unified memory part 1 of the proposal is implemented and currently going through testing and verification. should parts 2 & 3 be rejected, part 1 can still be merged, as it is independent of the subsequent parts and can still provide performance benefits. **summary** instead of allocating tensor backing buffers in device-private memory by default, we propose allocating _all_ tensors in the unified (shared) memory accessible by both cpu and mps. under this model: - tensor device transfers (cpu ‚Üî mps) would still create a destination tensor with new ownership. - however, data movement would use a on unified memory plus a memory barrier, rather than a metal cpu ‚Üî mps operation where applicable. - this part of the proposal on its own is **not** intended to eliminate destination allocations for calls rather, it focuses on reducing the cost of data movement, when both source and destination backing buffers reside in unified memory. it also paves the way for the next parts of the proposal to specifically reduce memory duplication. **expected impact** - memory allocation size: no significant reduction expected in the general case. - transfer cost: potential reduction by replacing cpu ‚Üî gpu operations with lighter-weight calls on unified memory. - semantics: no change to pytorch tensor ownership or device semantics. **code changes** - tensor allocation [[emptytensor.cpp]( tensor allocation is performed by fetching an allocator class through the call. if device supports unified memory, we propose to pass for allocator creation. as a result, all tensors allocated through this allocator will reside in unified memory. - tensor device move [[copy.mm]( currently, during an cpu ‚Üí mps copy, an explicit blit ( ) is encoded to move bytes from cpu-backed private memory into mps. this will no longer be needed as both source and destination will reside in shared memory. instead, we propose to perform a of the backing buffer on cpu-visible unified memory. since a mere does not carry tensor-specific information, we also manually perform cpu-side negation/conjugation if necessary, as well as to manually change ownership to . we also add a memory barrier ( ) to ensure writes to the tensor by source device are visible to the destination device. please note that at this stage memory duplication still occurs but it is on unified memory rather than device private memory. ### proposal part 2 - avoid duplication for read-only tensors **summary** with part 1 implemented, all tensors will reside in unified memory and be accessible from both cpu and mps. however, when a tensor is moved between devices, we cannot enforce source and destination tensors to reference the same backing buffer, as pytorch semantics require them to behave independently, i.e. write to either tensor should not modify the other. this problem does not however exist in case of read-only tensors, and we can reference the same memory with new device ownership instead of maintaining two copies of the same data. **code changes** still in progress and subject to approval of the proposal. **expected impact** - memory allocation size: reduction by 50% for read-only tensors, by maintaining a single copy of the backing buffer in unified memory for both source and destination tensors. - transfer cost: reduction by avoiding a on unified memory all together, and simply changing the tensor ownership. - semantics: mutable tensors will still maintain two copies for source and destination tensors. therefore, no change to pytorch tensor ownership or device semantics. ### proposal part 3 - avoid duplication when assigning a tensor device move to itself **summary** currently and with part 1 of the proposal in place, when the result of a tensor device move is assigned to itself ( ), a duplicate destination buffer is allocated and copied from source, and immediately after the source tensor is discarded and its backing buffer freed. this is equivalent to simply moving the backing buffer from one place of the unified memory to another. thus we propose to eliminate the unnecessary duplication, and simply reference the same source backing buffer with new ownership for the destination tensor. **code changes** still in progress and subject to approval of the proposal. we are also still investigating if it is possible to distinguish when a tensor device move is assigned to a new tensor vs. when it is assigned to itself. **expected impact** - memory allocation size: overall memory consumption should remain unchanged, but peak memory consumption has the potential to be reduced by reusing the source backing buffer. - transfer cost: reduction by avoiding a on unified memory all together, and simply changing the tensor ownership. - semantics: if both source and destination tensors remain alive, they will still maintain two copies for source and destination tensors. therefore, no change to pytorch tensor ownership or device semantics. ### alternatives instead of parts 2 & 3 of the proposal, a general copy-on-write implementation was also considered that would keep both source and destination tensors referencing the same backing buffer in unified memory, until either one was modified. however, as we understand, [the last copy-on-write attempt]( was ultimately rejected due to the design being too brittle. thus, we propose to reduce the optimization scope to read-only tensors and self-assigning device moves. ### additional context - part 1 of the proposal is implemented and currently at a 97% pass rate for . we are tracking down the remaining 3% failures. - parts 2 & 3 are still under investigation and not implemented. we hope to reach an understanding on whether or not these parts of the proposal in theory are acceptable to the maintainers before attempting to implement. cc",6.8,Critical,1.0,crash-like behavior
flutter/flutter#181283,proposal: make it easier to write tests without material,"during the [decoupling framework tests]( project, several test utils were created that made it easier to write tests in widgets and cupertino without using the material library. they were placed in utils files in the widgets test directory, so they can't be used by flutter app devs, only by the widgets tests themselves. we should consider moving some or all of these utils from widgets to flutter_test in order to make them publicly available to app devs. ### list of utils to consider making public: * [ ] testtextfield ( * [ ] testwidgetsapp ( ### list of utils that we've decided should not be made public: none yet! related:",[],"['TESTING', 'FEATURE']","['a: tests', 'c: new feature', 'framework', 'f: material design', 'c: proposal', 'team-framework']",github,2026-01-21T20:58:26Z,,"proposal: make it easier to write tests without material during the [decoupling framework tests]( project, several test utils were created that made it easier to write tests in widgets and cupertino without using the material library. they were placed in utils files in the widgets test directory, so they can't be used by flutter app devs, only by the widgets tests themselves. we should consider moving some or all of these utils from widgets to flutter_test in order to make them publicly available to app devs. ### list of utils to consider making public: * [ ] testtextfield ( * [ ] testwidgetsapp ( ### list of utils that we've decided should not be made public: none yet! related:",1.5,Low,0.561,localized low-impact
flutter/flutter#181287,[impeller] asan failures in interactive tests on opengles,"## summary interactive tests using the impeller playground trigger asan failures at runtime ## expected behaviour the interactive impeller tests should run without errors on all supported backends ## actual behaviour opengles tests crash at runtime. asan (address sanitizer) failures are reported on an appropriately instrumented build - for example: ## steps to reproduce 1. configure a build with address sanitizer 2. run an interactive renderer test with opengles ## system info linux flutter master ( ) reproduced on * opengl es 3.2 nvidia 590.48.01 * opengl es 3.2 mesa 25.3.3-arch1.3 ## notes looks a reference to the 's backing store may be getting lost, resulting in a use after free when the underlying pointer is accessed. [attached log](",[],['TESTING'],"['a: tests', 'engine', 'P1', 'e: impeller', 'team-engine', 'triaged-engine']",github,2026-01-21T21:28:41Z,,"[impeller] asan failures in interactive tests on opengles ## summary interactive tests using the impeller playground trigger asan failures at runtime ## expected behaviour the interactive impeller tests should run without errors on all supported backends ## actual behaviour opengles tests crash at runtime. asan (address sanitizer) failures are reported on an appropriately instrumented build - for example: ## steps to reproduce 1. configure a build with address sanitizer 2. run an interactive renderer test with opengles ## system info linux flutter master ( ) reproduced on * opengl es 3.2 nvidia 590.48.01 * opengl es 3.2 mesa 25.3.3-arch1.3 ## notes looks a reference to the 's backing store may be getting lost, resulting in a use after free when the underlying pointer is accessed. [attached log](",3.8,Critical,1.0,crash-like behavior
cilium/cilium#43914,cfp: support regex and/or cidr matching for headers in l7 network policy,"**is your proposed feature related to a problem?** i want to restrict access to a service based on the source ip address when using the gateway api. currently, source ip is available as a header, but network policy allows only strict matching for headers. that means it's not possible to define a range of ips using a single policy. **describe the feature you'd like** i'd like to match the header against a regex or, ideally, against a cidr range. [relevant issue](",[],['FEATURE'],"['kind/feature', 'sig/policy', 'area/agent', 'kind/cfp']",github,2026-01-21T21:30:06Z,,"cfp: support regex and/or cidr matching for headers in l7 network policy **is your proposed feature related to a problem?** i want to restrict access to a service based on the source ip address when using the gateway api. currently, source ip is available as a header, but network policy allows only strict matching for headers. that means it's not possible to define a range of ips using a single policy. **describe the feature you'd like** i'd like to match the header against a regex or, ideally, against a cidr range. [relevant issue](",3.385,High,0.989,system-wide impact
flutter/flutter#181288,migrate cross-library docimports before decoupling,"dart docimports allow you to reference identifiers from outside of the current file using square brackets in dart doc. the framework contains many of these that reference material or cupertino and will need to be migrated before decoupling can be completed. ### what should we do about problematic docimports? docimports are problematic when they reference material or cupertino from outside of that referenced library. for example, references both material and cupertino so that it can refer to things like and in its docs. #### option 0: do nothing this is not viable in the long term because it will break dart doc after material and cupertino are removed from flutter/flutter. the references in the example above to and will not be found. in the short term immediately after decoupling, because we plan to keep the original material and cupertino code in flutter/flutter during a deprecation period, it will not break anything. however, users that click on these links in the framework docs will be taken to the deprecated versions of this code, which is misleading. #### option 1: remove affected docimports we could remove the problematic docimports and replace the linked identifiers with unlinked inline code. in the scrollbar.dart example above, we would remove the two docimports and replace with . this is an imperfect solution because users of our docs would lose a lot of useful links. #### options 2: migrate each problematic docimport to reference the new external package does dartdoc support referencing something from an external pub package? how? ### where are problematic docimports located? problematic docimports have been seen in the following places, but they may also exist elsewhere: - packages/flutter_test - packages/flutter/test - packages/flutter/lib/src/widgets - packages/flutter/lib/src/material - packages/flutter/lib/src/cupertino ### how can we prevent this problem in the future? we should ideally include a lint that can catch problematic docimports, similar to what was done for cross-library test imports in ### links this was discovered by in:",[],['TESTING'],"['a: tests', 'framework', 'f: material design', 'f: cupertino', 'c: proposal', 'P1', 'team-framework', 'triaged-framework']",github,2026-01-21T21:35:01Z,,"migrate cross-library docimports before decoupling dart docimports allow you to reference identifiers from outside of the current file using square brackets in dart doc. the framework contains many of these that reference material or cupertino and will need to be migrated before decoupling can be completed. ### what should we do about problematic docimports? docimports are problematic when they reference material or cupertino from outside of that referenced library. for example, references both material and cupertino so that it can refer to things like and in its docs. #### option 0: do nothing this is not viable in the long term because it will break dart doc after material and cupertino are removed from flutter/flutter. the references in the example above to and will not be found. in the short term immediately after decoupling, because we plan to keep the original material and cupertino code in flutter/flutter during a deprecation period, it will not break anything. however, users that click on these links in the framework docs will be taken to the deprecated versions of this code, which is misleading. #### option 1: remove affected docimports we could remove the problematic docimports and replace the linked identifiers with unlinked inline code. in the scrollbar.dart example above, we would remove the two docimports and replace with . this is an imperfect solution because users of our docs would lose a lot of useful links. #### options 2: migrate each problematic docimport to reference the new external package does dartdoc support referencing something from an external pub package? how? ### where are problematic docimports located? problematic docimports have been seen in the following places, but they may also exist elsewhere: - packages/flutter_test - packages/flutter/test - packages/flutter/lib/src/widgets - packages/flutter/lib/src/material - packages/flutter/lib/src/cupertino ### how can we prevent this problem in the future? we should ideally include a lint that can catch problematic docimports, similar to what was done for cross-library test imports in ### links this was discovered by in:",1.6,Low,0.584,localized low-impact
cilium/cilium#43916,pass verdict broken for non-consecutive tiers and with default-allow,"### is there an existing issue for this? - [x] i have searched the existing issues ### version equal or higher than v1.18.6 and lower than v1.19.0 ### what happened? there are two issues with the newly implemented policy rule priorities and pass verdict for policies: 1. when a pass and allow/deny rule exist for the same identity at non-consecutive tiers (e.g. tier 0 and tier 2), the policy engine throws an error ( ) 2. when a pass rule exists for a specific identity at a higher tier with a wildcard allow rule at a lower tier, the policy engine incorrectly retains the pass rule as an invalid rule, which throws off the policy evaluation. the wildcard allow is never added for the affected identity. ### how can we reproduce the issue? see the added test cases in the pr. ### cilium version 1.19 ### kernel version n/a ### kubernetes version n/a ### regression _no response_ ### sysdump _no response_ ### relevant log output ### anything else? _no response_ ### cilium users document - [ ] are you a user of cilium? please add yourself to the [users doc]( ### code of conduct - [x] i agree to follow this project's code of conduct",[],['BUG'],"['kind/bug', 'sig/policy', 'kind/community-report', 'area/agent']",github,2026-01-21T22:46:43Z,,"pass verdict broken for non-consecutive tiers and with default-allow ### is there an existing issue for this? - [x] i have searched the existing issues ### version equal or higher than v1.18.6 and lower than v1.19.0 ### what happened? there are two issues with the newly implemented policy rule priorities and pass verdict for policies: 1. when a pass and allow/deny rule exist for the same identity at non-consecutive tiers (e.g. tier 0 and tier 2), the policy engine throws an error ( ) 2. when a pass rule exists for a specific identity at a higher tier with a wildcard allow rule at a lower tier, the policy engine incorrectly retains the pass rule as an invalid rule, which throws off the policy evaluation. the wildcard allow is never added for the affected identity. ### how can we reproduce the issue? see the added test cases in the pr. ### cilium version 1.19 ### kernel version n/a ### kubernetes version n/a ### regression _no response_ ### sysdump _no response_ ### relevant log output ### anything else? _no response_ ### cilium users document - [ ] are you a user of cilium? please add yourself to the [users doc]( ### code of conduct - [x] i agree to follow this project's code of conduct",4.6,Critical,1.0,crash-like behavior
cockroachdb/cockroach#161544,pkg/ccl/benchccl/rttanalysisccl/rttanalysisccl_test: testbenchmarkexpectation failed,pkg/ccl/benchccl/rttanalysisccl/rttanalysisccl_test.testbenchmarkexpectation [failed]( with [artifacts]( on master @ [01d347bab42d9c4df13ef2d0e5eaf617198c7254]( help see also: [how to investigate a go test failure \(internal\)]( /cc /sql-foundations [this test on roachdash]( | [improve this report!]( jira issue: crdb-58927,[],['TESTING'],"['C-test-failure', 'O-robot', 'branch-master', 'T-sql-foundations', 'P-2']",github,2026-01-21T23:12:45Z,,pkg/ccl/benchccl/rttanalysisccl/rttanalysisccl_test: testbenchmarkexpectation failed pkg/ccl/benchccl/rttanalysisccl/rttanalysisccl_test.testbenchmarkexpectation [failed]( with [artifacts]( on master @ [01d347bab42d9c4df13ef2d0e5eaf617198c7254]( help see also: [how to investigate a go test failure \(internal\)]( /cc /sql-foundations [this test on roachdash]( | [improve this report!]( jira issue: crdb-58927,1.6,Low,0.584,localized low-impact
numpy/numpy#30706,bug: np.unique memory usage with floats,"### describe the issue: i'm seeing high memory usage with np.unique on float32 and float64 arrays with numpy 2.4. for example, repeatedly running np.unique 2000 times on one array of length 100000 used 1.5 gb of memory. ### reproduce the code example: ### error message: ### python and numpy versions: python 3.12, 3.14 numpy 2.4.0, 2.4.1 ### runtime environment: [{'numpy_version': '2.4.0', 'python': '3.12.0 | packaged by conda-forge | (main, oct 3 2023, 08:36:57) ' '[clang 15.0.7 ]', 'uname': uname_result(system='darwin', node='usfc-olw-053343', release='24.6.0', version='darwin kernel version 24.6.0: wed nov 5 21:33:58 pst 2025; root:xnu-11417.140.69.705.2~1/release_arm64_t6000', machine='arm64')}, {'simd_extensions': {'baseline': ['neon', 'neon_fp16', 'neon_vfpv4', 'asimd'], 'found': ['asimdhp', 'asimddp'], 'not_found': ['asimdfhm']}}, {'ignore_floating_point_errors_in_matmul': true}] ### how does this issue affect you or how did you find it: i found this when running large numbers of logistic regressions via sklearn, which calls np.unique during data validation. instances failed due to running out of memory.",[],['BUG'],['00 - Bug'],github,2026-01-21T23:28:39Z,2026-01-22T07:01:52Z,"bug: np.unique memory usage with floats ### describe the issue: i'm seeing high memory usage with np.unique on float32 and float64 arrays with numpy 2.4. for example, repeatedly running np.unique 2000 times on one array of length 100000 used 1.5 gb of memory. ### reproduce the code example: ### error message: ### python and numpy versions: python 3.12, 3.14 numpy 2.4.0, 2.4.1 ### runtime environment: [{'numpy_version': '2.4.0', 'python': '3.12.0 | packaged by conda-forge | (main, oct 3 2023, 08:36:57) ' '[clang 15.0.7 ]', 'uname': uname_result(system='darwin', node='usfc-olw-053343', release='24.6.0', version='darwin kernel version 24.6.0: wed nov 5 21:33:58 pst 2025; root:xnu-11417.140.69.705.2~1/release_arm64_t6000', machine='arm64')}, {'simd_extensions': {'baseline': ['neon', 'neon_fp16', 'neon_vfpv4', 'asimd'], 'found': ['asimdhp', 'asimddp'], 'not_found': ['asimdfhm']}}, {'ignore_floating_point_errors_in_matmul': true}] ### how does this issue affect you or how did you find it: i found this when running large numbers of logistic regressions via sklearn, which calls np.unique during data validation. instances failed due to running out of memory.",4.6,Critical,1.0,crash-like behavior
tensorflow/tensorflow#108621,with causes crash (aborted/segfault) on large instead of oom,"### issue type bug ### have you reproduced the bug with tensorflow nightly? no ### source source ### tensorflow version 2.20.0 ### custom code yes ### os platform and distribution linux ubuntu 24.04 ### mobile device _no response_ ### python version 3.12 ### bazel version _no response_ ### gcc/compiler version _no response_ ### cuda/cudnn version _no response_ ### gpu model and memory _no response_ ### current behavior? when creating an identity matrix using with an extremely large (e.g., ), tensorflow correctly raises a in the default execution path. however, if onednn optimizations are enabled ( ), the process crashes (receiving or ) instead of raising a python exception. this suggests the onednn kernel implementation (specifically in fill or eye operations) fails to check for integer overflow or memory exhaustion before attempting allocation/write. here is a [gist]( ### standalone code to reproduce the issue ### relevant log output",[],['BUG'],"['type:bug', 'comp:ops', 'TF 2.19']",github,2026-01-22T00:02:15Z,,"with causes crash (aborted/segfault) on large instead of oom ### issue type bug ### have you reproduced the bug with tensorflow nightly? no ### source source ### tensorflow version 2.20.0 ### custom code yes ### os platform and distribution linux ubuntu 24.04 ### mobile device _no response_ ### python version 3.12 ### bazel version _no response_ ### gcc/compiler version _no response_ ### cuda/cudnn version _no response_ ### gpu model and memory _no response_ ### current behavior? when creating an identity matrix using with an extremely large (e.g., ), tensorflow correctly raises a in the default execution path. however, if onednn optimizations are enabled ( ), the process crashes (receiving or ) instead of raising a python exception. this suggests the onednn kernel implementation (specifically in fill or eye operations) fails to check for integer overflow or memory exhaustion before attempting allocation/write. here is a [gist]( ### standalone code to reproduce the issue ### relevant log output",6.4,Critical,1.0,crash-like behavior
pandas-dev/pandas#63803,tst: remove pandas_future_infer_string=0 test workflow,"now that 3.0 is officially out, the pandas_future_infer_string=0 test pipeline should be retired. it is currently failing by default due to the release.",[],['TESTING'],"['Testing', 'Strings']",github,2026-01-22T00:08:30Z,,"tst: remove pandas_future_infer_string=0 test workflow now that 3.0 is officially out, the pandas_future_infer_string=0 test pipeline should be retired. it is currently failing by default due to the release.",1.6,Low,0.584,localized low-impact
python/cpython#144130,support getrandom syscall on freebsd,"# feature or enhancement ### proposal: at least freebsd and netbsd also implement getrandom using the same semantics as linux >= 3.17. the only thing preventing its use is linux specific headers being used in configure to test for its existence. ### has this already been discussed elsewhere? this is a minor feature, which does not need previous discussion elsewhere ### links to previous discussion of this feature: _no response_ ### linked prs * gh-144131",[],['FEATURE'],"['type-feature', 'extension-modules', 'OS-freebsd']",github,2026-01-22T00:51:12Z,,"support getrandom syscall on freebsd # feature or enhancement ### proposal: at least freebsd and netbsd also implement getrandom using the same semantics as linux >= 3.17. the only thing preventing its use is linux specific headers being used in configure to test for its existence. ### has this already been discussed elsewhere? this is a minor feature, which does not need previous discussion elsewhere ### links to previous discussion of this feature: _no response_ ### linked prs * gh-144131",1.4,Low,0.538,localized low-impact
python/cpython#144132,implement suffix automaton for difflib's longest common substring,"# feature or enhancement ### proposal: o(n) solution is not that complex - 100 of python lines. i hacked in initial implementation and ran on the - the change itself. ~~so 30% faster.~~ so 20% slower. also, this is a fairly rough plug-in - there is a chance that the gap can be bridged. solution should avoid some of the user dissatisfaction: ### has this already been discussed elsewhere? no response given ### links to previous discussion of this feature: made a couple of posts in unrelated thread. performance tables are there showing that this is faster even for sizes of 10. ### linked prs * gh-144164",[],"['PERFORMANCE', 'FEATURE']","['type-feature', 'performance', 'stdlib']",github,2026-01-22T00:57:39Z,,"implement suffix automaton for difflib's longest common substring # feature or enhancement ### proposal: o(n) solution is not that complex - 100 of python lines. i hacked in initial implementation and ran on the - the change itself. ~~so 30% faster.~~ so 20% slower. also, this is a fairly rough plug-in - there is a chance that the gap can be bridged. solution should avoid some of the user dissatisfaction: ### has this already been discussed elsewhere? no response given ### links to previous discussion of this feature: made a couple of posts in unrelated thread. performance tables are there showing that this is faster even for sizes of 10. ### linked prs * gh-144164",4.1,Critical,1.0,"performance degradation, crash-like behavior"
pytorch/pytorch#173027,clarify signature: and are keyword-only arguments,"### üìö the doc issue i encountered a when attempting to pass as a second positional argument to . **the code:** **the issue:** the error message indicates that must be passed as a keyword argument. however, the current documentation at [ lists the function signature as: in standard python semantics, this signature implies that and can be passed positionally. the documentation is currently misleading regarding the required usage of the function. ### suggest a potential alternative/fix to prevent confusion, the documentation signature should be updated to explicitly show that arguments following are keyword-only. this is typically done using the asterisk ( ) syntax in the signature: **current:** **proposed:** alternatively, a note could be added to the parameter description explicitly stating that and must be passed as keyword arguments. cc",[],['DOCUMENTATION'],"['module: docs', 'triaged', 'actionable']",github,2026-01-22T01:12:59Z,2026-01-22T15:41:57Z,"clarify signature: and are keyword-only arguments ### üìö the doc issue i encountered a when attempting to pass as a second positional argument to . **the code:** **the issue:** the error message indicates that must be passed as a keyword argument. however, the current documentation at [ lists the function signature as: in standard python semantics, this signature implies that and can be passed positionally. the documentation is currently misleading regarding the required usage of the function. ### suggest a potential alternative/fix to prevent confusion, the documentation signature should be updated to explicitly show that arguments following are keyword-only. this is typically done using the asterisk ( ) syntax in the signature: **current:** **proposed:** alternatively, a note could be added to the parameter description explicitly stating that and must be passed as keyword arguments. cc",1.2,Low,0.493,localized low-impact
python/cpython#144133,a series of potential dos risk points in the cpython standard library,"# bug report ### bug description: approximately six months ago, our automated scanning program detected several issues and an issue( ) was filed. recently, after improving the scanner and re-running it against cpython, additional locations were found that may be susceptible to algorithmic complexity attacks. although each finding has been validated as carefully as possible, some false positives may still exist. please note that these findings do not necessarily imply an exploitable vulnerability (and are unlikely to represent a straightforward remote attack vector). after discussion with psrc, these points are being disclosed in this issue in order to leverage community review and contributions to assess impact and, where appropriate, implement hardening or fixes. incremental updates will be posted over the coming weeks. we‚Äôve updated the items related to the module in [ 1. _c3_mroÔºàexpÔºâ 2.allmethodsÔºàexpÔºâ 3.fragmentbuilderns._getnsattrs (polyÔºâ 4. __dict_replaceÔºàexpÔºâ 5.punycode_decodeÔºàpoly) . ### cpython versions tested on: cpython main branch ### operating systems tested on: linux",[],['SECURITY'],"['type-security', 'stdlib', 'topic-XML', 'topic-email']",github,2026-01-22T02:28:01Z,,"a series of potential dos risk points in the cpython standard library # bug report ### bug description: approximately six months ago, our automated scanning program detected several issues and an issue( ) was filed. recently, after improving the scanner and re-running it against cpython, additional locations were found that may be susceptible to algorithmic complexity attacks. although each finding has been validated as carefully as possible, some false positives may still exist. please note that these findings do not necessarily imply an exploitable vulnerability (and are unlikely to represent a straightforward remote attack vector). after discussion with psrc, these points are being disclosed in this issue in order to leverage community review and contributions to assess impact and, where appropriate, implement hardening or fixes. incremental updates will be posted over the coming weeks. we‚Äôve updated the items related to the module in [ 1. _c3_mroÔºàexpÔºâ 2.allmethodsÔºàexpÔºâ 3.fragmentbuilderns._getnsattrs (polyÔºâ 4. __dict_replaceÔºàexpÔºâ 5.punycode_decodeÔºàpoly) . ### cpython versions tested on: cpython main branch ### operating systems tested on: linux",7.8,Critical,1.0,"security risk, crash-like behavior"
rust-lang/rust#151477,"ice: ""can't type-check body of defid"" when using #[type_const] with nested generics.","<!-- thank you for filing a bug report! üêõ please provide a short summary of the bug, along with any information you feel relevant to replicating the bug. --> i tried this code: i expected to see this happen: the compiler should emit error e0401 instead, this happened: panics with: error: internal compiler error: .../compiler/rustc_hir_typeck/src/lib.rs:124:9: can't type-check body of defid(...) | release channel | result | |-----------------------------------|--------| | current stable | error | | current nightly (default solver) | ice | | nightly + | ice | ### meta <!-- if you're using the stable version of the compiler, you should also check if the bug also exists in the beta or nightly versions. --> : <!-- include a backtrace in the code block by setting in your environment. e.g. . --> backtrace",[],['BUG'],"['I-ICE', 'T-compiler', 'C-bug', 'S-has-mcve', 'F-min_generic_const_args']",github,2026-01-22T03:24:33Z,,"ice: ""can't type-check body of defid"" when using #[type_const] with nested generics. <!-- thank you for filing a bug report! üêõ please provide a short summary of the bug, along with any information you feel relevant to replicating the bug. --> i tried this code: i expected to see this happen: the compiler should emit error e0401 instead, this happened: panics with: error: internal compiler error: .../compiler/rustc_hir_typeck/src/lib.rs:124:9: can't type-check body of defid(...) | release channel | result | |-----------------------------------|--------| | current stable | error | | current nightly (default solver) | ice | | nightly + | ice | ### meta <!-- if you're using the stable version of the compiler, you should also check if the bug also exists in the beta or nightly versions. --> : <!-- include a backtrace in the code block by setting in your environment. e.g. . --> backtrace",4.6,Critical,1.0,crash-like behavior
tensorflow/tensorflow#108628,when a single tensor and out of range axis is passed to tf.concat it silently succeeds,"### issue type bug ### have you reproduced the bug with tensorflow nightly? no ### source source ### tensorflow version tf 2.20.0 ### custom code yes ### os platform and distribution _no response_ ### mobile device no ### python version 3.12 ### bazel version _no response_ ### gcc/compiler version _no response_ ### cuda/cudnn version _no response_ ### gpu model and memory _no response_ ### current behavior? tf.concat raises an error when axis is out of range for a list of tensors, but silently succeeds when a single tensor is passed. even though no concatenation happens for a single tensor, it is surprising that axis validation is skipped, leading to inconsistent behavior. in pipelines with conditional or dynamic feature selection, the number of tensors passed to tf.concat may vary at runtime. if axis is invalid: the bug is silently masked when only one feature is enabled the same code crashes when two features are enabled ### standalone code to reproduce the issue ### relevant log output",[],['BUG'],"['type:bug', 'comp:ops', 'TF 2.19']",github,2026-01-22T03:46:36Z,2026-01-22T22:12:50Z,"when a single tensor and out of range axis is passed to tf.concat it silently succeeds ### issue type bug ### have you reproduced the bug with tensorflow nightly? no ### source source ### tensorflow version tf 2.20.0 ### custom code yes ### os platform and distribution _no response_ ### mobile device no ### python version 3.12 ### bazel version _no response_ ### gcc/compiler version _no response_ ### cuda/cudnn version _no response_ ### gpu model and memory _no response_ ### current behavior? tf.concat raises an error when axis is out of range for a list of tensors, but silently succeeds when a single tensor is passed. even though no concatenation happens for a single tensor, it is surprising that axis validation is skipped, leading to inconsistent behavior. in pipelines with conditional or dynamic feature selection, the number of tensors passed to tf.concat may vary at runtime. if axis is invalid: the bug is silently masked when only one feature is enabled the same code crashes when two features are enabled ### standalone code to reproduce the issue ### relevant log output",6.4,Critical,1.0,crash-like behavior
tensorflow/tensorflow#108629,cpu/gpu inconsistency when casting bfloat16 to int8 for the api tf.experimental.numpy.asarray,### issue type bug ### have you reproduced the bug with tensorflow nightly? no ### source source ### tensorflow version tf 2.20.0 ### custom code yes ### os platform and distribution _no response_ ### mobile device no ### python version _no response_ ### bazel version _no response_ ### gcc/compiler version _no response_ ### cuda/cudnn version _no response_ ### gpu model and memory _no response_ ### current behavior? tf.experimental.numpy.asarray produces different int8 results on cpu vs gpu for bfloat16 input ### standalone code to reproduce the issue ### relevant log output,[],['BUG'],"['type:bug', 'comp:ops', 'TF 2.19']",github,2026-01-22T04:11:12Z,2026-01-22T22:13:32Z,cpu/gpu inconsistency when casting bfloat16 to int8 for the api tf.experimental.numpy.asarray ### issue type bug ### have you reproduced the bug with tensorflow nightly? no ### source source ### tensorflow version tf 2.20.0 ### custom code yes ### os platform and distribution _no response_ ### mobile device no ### python version _no response_ ### bazel version _no response_ ### gcc/compiler version _no response_ ### cuda/cudnn version _no response_ ### gpu model and memory _no response_ ### current behavior? tf.experimental.numpy.asarray produces different int8 results on cpu vs gpu for bfloat16 input ### standalone code to reproduce the issue ### relevant log output,5.6,Critical,1.0,system-wide impact
rust-lang/rust#151479,"""missing optimized mir"" due to privacy violation, via associated type equality constraint in supertrait",this bug is a variation of . cc i tried this code: src/dep/lib.rs src/main.rs compiling the above code resulted in the following error: i assume that this error isn't supposed to happen for normal cargo usage. ### meta :,[],['BUG'],"['A-trait-system', 'A-visibility', 'A-associated-items', 'T-lang', 'T-compiler', 'C-bug', 'T-types']",github,2026-01-22T04:48:34Z,,"""missing optimized mir"" due to privacy violation, via associated type equality constraint in supertrait this bug is a variation of . cc i tried this code: src/dep/lib.rs src/main.rs compiling the above code resulted in the following error: i assume that this error isn't supposed to happen for normal cargo usage. ### meta :",2.333,Medium,0.75,functional impact
tensorflow/tensorflow#108640,bbm,### issue type bug ### have you reproduced the bug with tensorflow nightly? yes ### source source ### tensorflow version jhg ### custom code yes ### os platform and distribution _no response_ ### mobile device _no response_ ### python version _no response_ ### bazel version _no response_ ### gcc/compiler version _no response_ ### cuda/cudnn version _no response_ ### gpu model and memory _no response_ ### current behavior? nhgh ### standalone code to reproduce the issue ### relevant log output,[],['BUG'],['type:bug'],github,2026-01-22T04:59:12Z,2026-01-22T22:12:01Z,bbm ### issue type bug ### have you reproduced the bug with tensorflow nightly? yes ### source source ### tensorflow version jhg ### custom code yes ### os platform and distribution _no response_ ### mobile device _no response_ ### python version _no response_ ### bazel version _no response_ ### gcc/compiler version _no response_ ### cuda/cudnn version _no response_ ### gpu model and memory _no response_ ### current behavior? nhgh ### standalone code to reproduce the issue ### relevant log output,4.2,Critical,1.0,system-wide impact
pytorch/pytorch#173055,broken link to user guide for masked tensors,"### üìö the doc issue tried to navigate to new user guide on the [torch.masked]( page. link to ""[overview ‚Äì the place to start for new users, discusses how to use maskedtensors and why they‚Äôre useful]( was broken. ### suggest a potential alternative/fix either put in the correct link or remove the article (i would prefer you link a guide, though :) ) cc",[],['DOCUMENTATION'],"['module: sparse', 'module: docs', 'triaged', 'module: nestedtensor']",github,2026-01-22T05:15:08Z,2026-01-23T20:29:42Z,"broken link to user guide for masked tensors ### üìö the doc issue tried to navigate to new user guide on the [torch.masked]( page. link to ""[overview ‚Äì the place to start for new users, discusses how to use maskedtensors and why they‚Äôre useful]( was broken. ### suggest a potential alternative/fix either put in the correct link or remove the article (i would prefer you link a guide, though :) ) cc",1.2,Low,0.493,localized low-impact
pandas-dev/pandas#63804,build: docs dependency fails on python 3.14 due to outdated sphinx-theme-builder,"### installation check - [x] i have read the [installation guide]( ### platform linux-6.18.5-200.fc43.x86_64-x86_64-with-glibc2.42 ### installation method pip install ### pandas version latest/main branch ### python version 3.14.2 ### installation logs collecting downloading pydata-sphinx-theme v0.16.1+dismissable-announcement-banner (2.6 mb) ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2.6/2.6 mb 0:00:01 installing build dependencies ... done getting requirements to build wheel ... done preparing metadata (pyproject.toml) ... error error: subprocess-exited-with-error √ó preparing metadata (pyproject.toml) did not run successfully. ‚îÇ exit code: 1 ‚ï∞‚îÄ> [26 lines of output] traceback (most recent call last): file "".../site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 389, in main() file "".../site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 373, in main json_out[""return_val""] = hook(**hook_input[""kwargs""]) file "".../site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 175, in prepare_metadata_for_build_wheel return hook(metadata_directory, config_settings) file "".../site-packages/sphinx_theme_builder/__init__.py"", line 79, in prepare_metadata_for_build_wheel project = project.from_cwd() file "".../site-packages/sphinx_theme_builder/_internal/project.py"", line 233, in from_cwd retval = cls.from_path(path.cwd()) file "".../site-packages/sphinx_theme_builder/_internal/project.py"", line 247, in from_path version_s, version_comes_from = _determine_version( file "".../site-packages/sphinx_theme_builder/_internal/project.py"", line 149, in _determine_version declared_in_python = get_version_using_ast(package_init_file.read_bytes()) file "".../site-packages/sphinx_theme_builder/_internal/project.py"", line 43, in get_version_using_ast and isinstance(child.value, ast.str) attributeerror: module 'ast' has no attribute 'str' [end of output] note: this error originates from a subprocess, and is likely not a problem with pip. error: metadata-generation-failed √ó encountered error while generating package metadata. ‚ï∞‚îÄ> from pydata-sphinx-theme (git archive) note: this is an issue with the package mentioned above, not pip.",[],['UI'],['Build'],github,2026-01-22T05:35:09Z,2026-01-22T15:22:53Z,"build: docs dependency fails on python 3.14 due to outdated sphinx-theme-builder ### installation check - [x] i have read the [installation guide]( ### platform linux-6.18.5-200.fc43.x86_64-x86_64-with-glibc2.42 ### installation method pip install ### pandas version latest/main branch ### python version 3.14.2 ### installation logs collecting downloading pydata-sphinx-theme v0.16.1+dismissable-announcement-banner (2.6 mb) ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ 2.6/2.6 mb 0:00:01 installing build dependencies ... done getting requirements to build wheel ... done preparing metadata (pyproject.toml) ... error error: subprocess-exited-with-error √ó preparing metadata (pyproject.toml) did not run successfully. ‚îÇ exit code: 1 ‚ï∞‚îÄ> [26 lines of output] traceback (most recent call last): file "".../site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 389, in main() file "".../site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 373, in main json_out[""return_val""] = hook(**hook_input[""kwargs""]) file "".../site-packages/pip/_vendor/pyproject_hooks/_in_process/_in_process.py"", line 175, in prepare_metadata_for_build_wheel return hook(metadata_directory, config_settings) file "".../site-packages/sphinx_theme_builder/__init__.py"", line 79, in prepare_metadata_for_build_wheel project = project.from_cwd() file "".../site-packages/sphinx_theme_builder/_internal/project.py"", line 233, in from_cwd retval = cls.from_path(path.cwd()) file "".../site-packages/sphinx_theme_builder/_internal/project.py"", line 247, in from_path version_s, version_comes_from = _determine_version( file "".../site-packages/sphinx_theme_builder/_internal/project.py"", line 149, in _determine_version declared_in_python = get_version_using_ast(package_init_file.read_bytes()) file "".../site-packages/sphinx_theme_builder/_internal/project.py"", line 43, in get_version_using_ast and isinstance(child.value, ast.str) attributeerror: module 'ast' has no attribute 'str' [end of output] note: this error originates from a subprocess, and is likely not a problem with pip. error: metadata-generation-failed √ó encountered error while generating package metadata. ‚ï∞‚îÄ> from pydata-sphinx-theme (git archive) note: this is an issue with the package mentioned above, not pip.",1.8,Low,0.629,user-visible issue
tensorflow/tensorflow#108641,"tf.math.igamma returns 0 for (a < 0, x = 0) while pytorch and scipy return nan","### issue type bug ### have you reproduced the bug with tensorflow nightly? yes ### source source ### tensorflow version v2.20.0-rc0-4-g72fbba3d20f 2.20.0 ### custom code yes ### os platform and distribution _no response_ ### mobile device _no response_ ### python version _no response_ ### bazel version _no response_ ### gcc/compiler version _no response_ ### cuda/cudnn version _no response_ ### gpu model and memory _no response_ ### current behavior? ## description produces an inconsistent result when the parameter a is negative and x == 0. specifically, tensorflow returns 0 for this input, while both pytorch ( ) and scipy ( ) return nan, which is consistent with the mathematical definition of the regularized incomplete gamma function. the lower regularized incomplete gamma function is defined as: p(a, x) = gamma(a, x) / gamma(a) the function is **mathematically defined only for**: - a > 0 - x >= 0 when a <= 0, the gamma function has poles, and the expression is undefined. therefore, for inputs such as , the mathematically correct result is **undefined**, and returning is the expected behavior. this suggests a potential **incorrect edge-case handling or missing domain check** in tensorflow‚Äôs implementation. ### standalone code to reproduce the issue ### relevant log output",[],['BUG'],"['type:bug', 'comp:ops']",github,2026-01-22T05:45:27Z,,"tf.math.igamma returns 0 for (a < 0, x = 0) while pytorch and scipy return nan ### issue type bug ### have you reproduced the bug with tensorflow nightly? yes ### source source ### tensorflow version v2.20.0-rc0-4-g72fbba3d20f 2.20.0 ### custom code yes ### os platform and distribution _no response_ ### mobile device _no response_ ### python version _no response_ ### bazel version _no response_ ### gcc/compiler version _no response_ ### cuda/cudnn version _no response_ ### gpu model and memory _no response_ ### current behavior? ## description produces an inconsistent result when the parameter a is negative and x == 0. specifically, tensorflow returns 0 for this input, while both pytorch ( ) and scipy ( ) return nan, which is consistent with the mathematical definition of the regularized incomplete gamma function. the lower regularized incomplete gamma function is defined as: p(a, x) = gamma(a, x) / gamma(a) the function is **mathematically defined only for**: - a > 0 - x >= 0 when a <= 0, the gamma function has poles, and the expression is undefined. therefore, for inputs such as , the mathematically correct result is **undefined**, and returning is the expected behavior. this suggests a potential **incorrect edge-case handling or missing domain check** in tensorflow‚Äôs implementation. ### standalone code to reproduce the issue ### relevant log output",4.2,Critical,1.0,system-wide impact
cockroachdb/cockroach#161549,roachtest: vecindex/dbpedia-100k/nodes=3/prefix=0 failed,roachtest.vecindex/dbpedia-100k/nodes=3/prefix=0 [failed]( with [artifacts]( on release-26.1.0-rc @ [6c9b2f778bd29895383d493a2fa4490323499ecf]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21008550-1769063470-01-n3cpu4-0001 | 34.138.71.17 | 10.142.1.117 | | teamcity-21008550-1769063470-01-n3cpu4-0002 | 34.73.110.121 | 10.142.1.116 | | teamcity-21008550-1769063470-01-n3cpu4-0003 | 34.75.182.115 | 10.142.1.108 | parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - fs=ext4 - localssd=true - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( same failure on other branches - roachtest: vecindex/dbpedia-100k/nodes=3/prefix=0 failed [readfile unexpected eof] [c-test-failure o-roachtest o-robot p-2 t-sql-queries branch-master branch-release-26.1] [this test on roachdash]( | [improve this report!]( jira issue: crdb-58930,[],['TESTING'],"['C-test-failure', 'O-robot', 'O-roachtest', 'T-sql-queries', 'branch-release-26.1.0-rc']",github,2026-01-22T06:39:34Z,2026-01-24T02:40:18Z,roachtest: vecindex/dbpedia-100k/nodes=3/prefix=0 failed roachtest.vecindex/dbpedia-100k/nodes=3/prefix=0 [failed]( with [artifacts]( on release-26.1.0-rc @ [6c9b2f778bd29895383d493a2fa4490323499ecf]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21008550-1769063470-01-n3cpu4-0001 | 34.138.71.17 | 10.142.1.117 | | teamcity-21008550-1769063470-01-n3cpu4-0002 | 34.73.110.121 | 10.142.1.116 | | teamcity-21008550-1769063470-01-n3cpu4-0003 | 34.75.182.115 | 10.142.1.108 | parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - fs=ext4 - localssd=true - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( same failure on other branches - roachtest: vecindex/dbpedia-100k/nodes=3/prefix=0 failed [readfile unexpected eof] [c-test-failure o-roachtest o-robot p-2 t-sql-queries branch-master branch-release-26.1] [this test on roachdash]( | [improve this report!]( jira issue: crdb-58930,3.058,High,0.915,functional impact
cockroachdb/cockroach#161550,roachtest: vecindex/dbpedia-100k/nodes=3/prefix=3 failed,roachtest.vecindex/dbpedia-100k/nodes=3/prefix=3 [failed]( with [artifacts]( on release-26.1.0-rc @ [6c9b2f778bd29895383d493a2fa4490323499ecf]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21008550-1769063470-04-n3cpu4-0001 | 104.196.139.212 | 10.142.1.113 | | teamcity-21008550-1769063470-04-n3cpu4-0002 | 35.227.3.65 | 10.142.1.115 | | teamcity-21008550-1769063470-04-n3cpu4-0003 | 34.148.50.211 | 10.142.1.114 | parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - fs=ext4 - localssd=true - metamorphicwritebuffering=true - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( same failure on other branches - roachtest: vecindex/dbpedia-100k/nodes=3/prefix=3 failed [readfile unexpected eof] [c-test-failure o-roachtest o-robot p-2 t-sql-queries branch-master branch-release-26.1] [this test on roachdash]( | [improve this report!]( jira issue: crdb-58931,[],['TESTING'],"['C-test-failure', 'O-robot', 'O-roachtest', 'T-sql-queries', 'branch-release-26.1.0-rc']",github,2026-01-22T06:39:39Z,2026-01-24T02:40:39Z,roachtest: vecindex/dbpedia-100k/nodes=3/prefix=3 failed roachtest.vecindex/dbpedia-100k/nodes=3/prefix=3 [failed]( with [artifacts]( on release-26.1.0-rc @ [6c9b2f778bd29895383d493a2fa4490323499ecf]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21008550-1769063470-04-n3cpu4-0001 | 104.196.139.212 | 10.142.1.113 | | teamcity-21008550-1769063470-04-n3cpu4-0002 | 35.227.3.65 | 10.142.1.115 | | teamcity-21008550-1769063470-04-n3cpu4-0003 | 34.148.50.211 | 10.142.1.114 | parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - fs=ext4 - localssd=true - metamorphicwritebuffering=true - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( same failure on other branches - roachtest: vecindex/dbpedia-100k/nodes=3/prefix=3 failed [readfile unexpected eof] [c-test-failure o-roachtest o-robot p-2 t-sql-queries branch-master branch-release-26.1] [this test on roachdash]( | [improve this report!]( jira issue: crdb-58931,3.22,High,0.952,system-wide impact
cockroachdb/cockroach#161551,roachtest: cluster_creation failed,roachtest.cluster_creation [failed]( with [artifacts]( on release-25.3.8-rc @ [129f86631b285cfd2bd8740db920947b5a9c89ab]( parameters: - cloud=aws - coveragebuild=false - cpu=8 - runtimeassertionsbuild=false - ssd=0 help see: [roachtest readme]( see: [how to investigate \(internal\)]( same failure on other branches - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-26.1.0-rc] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-24.3] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.2] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-24.3.25-rc] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.4.3-rc] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.2.11-rc] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.3.7-rc] - roachtest: cluster_creation failed [o-roachtest o-robot o-rsg t-testeng x-infra-flake branch-master] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-26.1] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.3] /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-58932,[],['TESTING'],"['O-robot', 'O-roachtest', 'T-testeng', 'X-infra-flake', 'branch-release-25.3.8-rc']",github,2026-01-22T06:51:13Z,,roachtest: cluster_creation failed roachtest.cluster_creation [failed]( with [artifacts]( on release-25.3.8-rc @ [129f86631b285cfd2bd8740db920947b5a9c89ab]( parameters: - cloud=aws - coveragebuild=false - cpu=8 - runtimeassertionsbuild=false - ssd=0 help see: [roachtest readme]( see: [how to investigate \(internal\)]( same failure on other branches - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-26.1.0-rc] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-24.3] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.2] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-24.3.25-rc] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.4.3-rc] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.2.11-rc] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.3.7-rc] - roachtest: cluster_creation failed [o-roachtest o-robot o-rsg t-testeng x-infra-flake branch-master] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-26.1] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.3] /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-58932,3.189,High,0.945,functional impact
cockroachdb/cockroach#161552,roachtest: cluster_creation failed,roachtest.cluster_creation [failed]( with [artifacts]( on release-25.2.12-rc @ [7fc1e95228ad6610c2ea21d8ba55d0d6513c319f]( parameters: - cloud=aws - coveragebuild=false - cpu=16 - diskcount=0 - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( same failure on other branches - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.3.8-rc] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-26.1.0-rc] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-24.3] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.2] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-24.3.25-rc] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.4.3-rc] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.2.11-rc] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.3.7-rc] - roachtest: cluster_creation failed [o-roachtest o-robot o-rsg t-testeng x-infra-flake branch-master] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-26.1] /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-58933,[],['TESTING'],"['O-robot', 'O-roachtest', 'T-testeng', 'X-infra-flake', 'branch-release-25.2.12-rc']",github,2026-01-22T06:51:27Z,,roachtest: cluster_creation failed roachtest.cluster_creation [failed]( with [artifacts]( on release-25.2.12-rc @ [7fc1e95228ad6610c2ea21d8ba55d0d6513c319f]( parameters: - cloud=aws - coveragebuild=false - cpu=16 - diskcount=0 - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( same failure on other branches - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.3.8-rc] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-26.1.0-rc] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-24.3] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.2] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-24.3.25-rc] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.4.3-rc] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.2.11-rc] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.3.7-rc] - roachtest: cluster_creation failed [o-roachtest o-robot o-rsg t-testeng x-infra-flake branch-master] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-26.1] /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-58933,2.851,Medium,0.868,functional impact
cockroachdb/cockroach#161553,roachtest: backup-restore/online-restore-recovery failed,roachtest.backup-restore/online-restore-recovery [failed]( with [artifacts]( on release-25.3 @ [bf3dd04672f129ebf85364e323c3d7594072aab4]( parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - encrypted=true - metamorphicbufferedsender=true - metamorphicwritebuffering=true - runtimeassertionsbuild=false - ssd=0 help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( same failure on other branches - roachtest: backup-restore/online-restore-recovery failed [b-runtime-assertions-enabled c-test-failure o-roachtest o-robot t-sql-foundations branch-release-26.1 release-blocker] /cc /disaster-recovery [this test on roachdash]( | [improve this report!]( jira issue: crdb-58934,[],['TESTING'],"['C-test-failure', 'O-robot', 'A-disaster-recovery', 'O-roachtest', 'T-disaster-recovery', 'P-2', 'branch-release-25.3']",github,2026-01-22T06:53:24Z,,roachtest: backup-restore/online-restore-recovery failed roachtest.backup-restore/online-restore-recovery [failed]( with [artifacts]( on release-25.3 @ [bf3dd04672f129ebf85364e323c3d7594072aab4]( parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - encrypted=true - metamorphicbufferedsender=true - metamorphicwritebuffering=true - runtimeassertionsbuild=false - ssd=0 help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( same failure on other branches - roachtest: backup-restore/online-restore-recovery failed [b-runtime-assertions-enabled c-test-failure o-roachtest o-robot t-sql-foundations branch-release-26.1 release-blocker] /cc /disaster-recovery [this test on roachdash]( | [improve this report!]( jira issue: crdb-58934,3.148,High,0.935,functional impact
cockroachdb/cockroach#161554,roachtest: cluster_creation failed,roachtest.cluster_creation [failed]( with [artifacts]( on release-24.3.26-rc @ [77aa3fcad169b09166b9105d52c05ab0122988d1]( parameters: - cloud=aws - coveragebuild=false - cpu=96 - runtimeassertionsbuild=false - ssd=0 help see: [roachtest readme]( see: [how to investigate \(internal\)]( same failure on other branches - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.2.12-rc] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.3.8-rc] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-26.1.0-rc] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-24.3] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.2] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-24.3.25-rc] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.4.3-rc] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.2.11-rc] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.3.7-rc] - roachtest: cluster_creation failed [o-roachtest o-robot o-rsg t-testeng x-infra-flake branch-master] /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-58935,[],['TESTING'],"['O-robot', 'O-roachtest', 'T-testeng', 'X-infra-flake', 'branch-release-24.3.26-rc']",github,2026-01-22T06:54:49Z,,roachtest: cluster_creation failed roachtest.cluster_creation [failed]( with [artifacts]( on release-24.3.26-rc @ [77aa3fcad169b09166b9105d52c05ab0122988d1]( parameters: - cloud=aws - coveragebuild=false - cpu=96 - runtimeassertionsbuild=false - ssd=0 help see: [roachtest readme]( see: [how to investigate \(internal\)]( same failure on other branches - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.2.12-rc] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.3.8-rc] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-26.1.0-rc] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-24.3] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.2] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-24.3.25-rc] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.4.3-rc] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.2.11-rc] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.3.7-rc] - roachtest: cluster_creation failed [o-roachtest o-robot o-rsg t-testeng x-infra-flake branch-master] /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-58935,2.972,Medium,0.895,functional impact
cockroachdb/cockroach#161555,kvserver: enable drpc for testflowcontrolcrashednodev2,**describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58936,[],['TESTING'],"['C-test-failure', 'T-db-server']",github,2026-01-22T07:12:48Z,,kvserver: enable drpc for testflowcontrolcrashednodev2 **describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58936,3.8,Critical,1.0,crash-like behavior
cockroachdb/cockroach#161556,roachtest: cluster_creation failed,roachtest.cluster_creation [failed]( with [artifacts]( on release-25.4.4-rc @ [688cda9084f2924ec4e22044c861aa0a3f34e956]( parameters: - cloud=aws - coveragebuild=false - cpu=16 - runtimeassertionsbuild=false - ssd=0 help see: [roachtest readme]( see: [how to investigate \(internal\)]( same failure on other branches - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-24.3.26-rc] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.2.12-rc] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.3.8-rc] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-26.1.0-rc] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-24.3] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.2] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-24.3.25-rc] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.4.3-rc] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.2.11-rc] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.3.7-rc] /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-58937,[],['TESTING'],"['O-robot', 'O-roachtest', 'T-testeng', 'X-infra-flake', 'branch-release-25.4.4-rc']",github,2026-01-22T07:13:39Z,,roachtest: cluster_creation failed roachtest.cluster_creation [failed]( with [artifacts]( on release-25.4.4-rc @ [688cda9084f2924ec4e22044c861aa0a3f34e956]( parameters: - cloud=aws - coveragebuild=false - cpu=16 - runtimeassertionsbuild=false - ssd=0 help see: [roachtest readme]( see: [how to investigate \(internal\)]( same failure on other branches - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-24.3.26-rc] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.2.12-rc] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.3.8-rc] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-26.1.0-rc] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-24.3] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.2] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-24.3.25-rc] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.4.3-rc] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.2.11-rc] - roachtest: cluster_creation failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.3.7-rc] /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-58937,2.964,Medium,0.894,functional impact
cockroachdb/cockroach#161557,roachtest: db-console/cypress-pages failed,roachtest.db-console/cypress-pages [failed]( with [artifacts]( on master @ [c6ec5849dfea61702bc94c5b794a7659c0e914f4]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21008324-1769065069-19-n4cpu4-0001 | 40.76.99.235 | 10.1.0.126 | | teamcity-21008324-1769065069-19-n4cpu4-0002 | 20.102.88.88 | 10.1.0.125 | | teamcity-21008324-1769065069-19-n4cpu4-0003 | 20.127.147.189 | 10.1.0.127 | | teamcity-21008324-1769065069-19-n4cpu4-0004 | 20.127.148.207 | 10.1.0.15 | parameters: - arch=amd64 - cloud=azure - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - metamorphicwritebuffering=true - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( _grafana is not yet available for azure clusters_ /cc /obs-prs [this test on roachdash]( | [improve this report!]( jira issue: crdb-58938,[],['TESTING'],"['C-test-failure', 'O-robot', 'O-roachtest', 'branch-master', 'T-observability', 'target-release-26.2.0']",github,2026-01-22T07:15:41Z,2026-01-26T20:09:31Z,roachtest: db-console/cypress-pages failed roachtest.db-console/cypress-pages [failed]( with [artifacts]( on master @ [c6ec5849dfea61702bc94c5b794a7659c0e914f4]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21008324-1769065069-19-n4cpu4-0001 | 40.76.99.235 | 10.1.0.126 | | teamcity-21008324-1769065069-19-n4cpu4-0002 | 20.102.88.88 | 10.1.0.125 | | teamcity-21008324-1769065069-19-n4cpu4-0003 | 20.127.147.189 | 10.1.0.127 | | teamcity-21008324-1769065069-19-n4cpu4-0004 | 20.127.148.207 | 10.1.0.15 | parameters: - arch=amd64 - cloud=azure - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - metamorphicwritebuffering=true - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( _grafana is not yet available for azure clusters_ /cc /obs-prs [this test on roachdash]( | [improve this report!]( jira issue: crdb-58938,3.2,High,0.947,system-wide impact
cockroachdb/cockroach#161558,server: enable drpc for testcreatestatementdiagnosticsreportwithviewactivityoptions,**describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58939,[],['TESTING'],"['C-test-failure', 'T-db-server']",github,2026-01-22T07:16:08Z,,server: enable drpc for testcreatestatementdiagnosticsreportwithviewactivityoptions **describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58939,1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161559,server: enable drpc for testlistactivitysecurity,**describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58940,[],['TESTING'],"['C-test-failure', 'T-db-server']",github,2026-01-22T07:16:46Z,,server: enable drpc for testlistactivitysecurity **describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58940,1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161560,server: enable drpc for testlistsessionssecurity,**describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58941,[],['TESTING'],"['C-test-failure', 'T-db-server']",github,2026-01-22T07:22:49Z,,server: enable drpc for testlistsessionssecurity **describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58941,1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161561,server: enable drpc for testtransactioncontentionevents,**describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58942,[],['TESTING'],"['C-test-failure', 'T-db-server']",github,2026-01-22T07:23:49Z,,server: enable drpc for testtransactioncontentionevents **describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58942,1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161562,roachtest: rebalance/by-load/mode=leases/mixed-version failed,"**note:** this build has runtime assertions enabled. if the same failure was hit in a run without assertions enabled, there should be a similar failure without this message. if there isn& ;t one, then this failure is likely due to an assertion violation or (assertion) timeout. roachtest.rebalance/by-load/mode=leases/mixed-version [failed]( with [artifacts]( on master @ [c6ec5849dfea61702bc94c5b794a7659c0e914f4]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21008324-1769065069-11-n4cpu4-0001 | 20.55.83.135 | 10.1.0.111 | | teamcity-21008324-1769065069-11-n4cpu4-0002 | 20.121.139.75 | 10.1.0.114 | | teamcity-21008324-1769065069-11-n4cpu4-0003 | 172.203.243.146 | 10.1.0.112 | | teamcity-21008324-1769065069-11-n4cpu4-0004 | 52.186.182.102 | 10.1.0.113 | parameters: - arch=amd64 - cloud=azure - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - mvtdeploymentmode=shared-process - mvtversions=v25.4.3 ‚Üí v26.2.0-alpha.00000000-dev-c6ec5849dfea61702bc94c5b794a7659c0e914f4 (master) - runtimeassertionsbuild=true help see: [roachtest readme]( see: [how to investigate \(internal\)]( _grafana is not yet available for azure clusters_ /cc /kv-triage [this test on roachdash]( | [improve this report!]( jira issue: crdb-58943",[],"['TESTING', 'BUG']","['C-bug', 'C-test-failure', 'O-robot', 'X-duplicate', 'A-testing', 'O-roachtest', 'branch-master', 'T-kv', 'B-runtime-assertions-enabled']",github,2026-01-22T07:27:23Z,2026-01-23T17:27:58Z,"roachtest: rebalance/by-load/mode=leases/mixed-version failed **note:** this build has runtime assertions enabled. if the same failure was hit in a run without assertions enabled, there should be a similar failure without this message. if there isn& ;t one, then this failure is likely due to an assertion violation or (assertion) timeout. roachtest.rebalance/by-load/mode=leases/mixed-version [failed]( with [artifacts]( on master @ [c6ec5849dfea61702bc94c5b794a7659c0e914f4]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21008324-1769065069-11-n4cpu4-0001 | 20.55.83.135 | 10.1.0.111 | | teamcity-21008324-1769065069-11-n4cpu4-0002 | 20.121.139.75 | 10.1.0.114 | | teamcity-21008324-1769065069-11-n4cpu4-0003 | 172.203.243.146 | 10.1.0.112 | | teamcity-21008324-1769065069-11-n4cpu4-0004 | 52.186.182.102 | 10.1.0.113 | parameters: - arch=amd64 - cloud=azure - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - mvtdeploymentmode=shared-process - mvtversions=v25.4.3 ‚Üí v26.2.0-alpha.00000000-dev-c6ec5849dfea61702bc94c5b794a7659c0e914f4 (master) - runtimeassertionsbuild=true help see: [roachtest readme]( see: [how to investigate \(internal\)]( _grafana is not yet available for azure clusters_ /cc /kv-triage [this test on roachdash]( | [improve this report!]( jira issue: crdb-58943",2.984,Medium,0.898,functional impact
cockroachdb/cockroach#161563,jobs: enable drpc for testprofilerstoreplandiagram,**describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58944,[],['TESTING'],"['C-test-failure', 'T-db-server']",github,2026-01-22T07:29:01Z,,jobs: enable drpc for testprofilerstoreplandiagram **describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58944,1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161564,kvcoord: enable drpc for testproxytracing,**describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58945,[],['TESTING'],"['C-test-failure', 'T-db-server']",github,2026-01-22T07:30:31Z,,kvcoord: enable drpc for testproxytracing **describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58945,1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161565,kvcoord: enable drpc for testpartialpartitiondirectfivefail,**describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58946,[],['TESTING'],"['C-test-failure', 'T-db-server']",github,2026-01-22T07:31:13Z,,kvcoord: enable drpc for testpartialpartitiondirectfivefail **describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58946,1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161566,kvserver: enable drpc for testdefaultconnectiondisruptiondoesnotinterferewithsystemtraffic,**describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58947,[],['TESTING'],"['C-test-failure', 'T-db-server']",github,2026-01-22T07:33:02Z,,kvserver: enable drpc for testdefaultconnectiondisruptiondoesnotinterferewithsystemtraffic **describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58947,1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161567,kvserver: enable drpc for testreceivesnapshotlogging,**describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58948,[],['TESTING'],"['C-test-failure', 'T-db-server']",github,2026-01-22T07:33:48Z,,kvserver: enable drpc for testreceivesnapshotlogging **describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58948,1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161568,kvserver: enable drpc for testreportunreachableheartbeats,**describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58949,[],['TESTING'],"['C-test-failure', 'T-db-server']",github,2026-01-22T07:34:26Z,,kvserver: enable drpc for testreportunreachableheartbeats **describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58949,1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161569,roachtest: c2c/mixed-version failed,"**note:** this build has runtime assertions enabled. if the same failure was hit in a run without assertions enabled, there should be a similar failure without this message. if there isn& ;t one, then this failure is likely due to an assertion violation or (assertion) timeout. roachtest.c2c/mixed-version [failed]( with [artifacts]( on release-24.3.25-rc @ [eecd7c2c316ed213daab1c664e0d854fc937a746]( parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - encrypted=false - fs=ext4 - localssd=true - mvtdeploymentmode=system-only - mvtversions=v24.2.10 ‚Üí release-24.3.25-rc - runtimeassertionsbuild=true - ssd=0 help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( same failure on other branches - roachtest: c2c/mixed-version failed [a-disaster-recovery b-runtime-assertions-enabled c-test-failure o-roachtest o-robot p-3 t-disaster-recovery branch-release-25.2] /cc /disaster-recovery [this test on roachdash]( | [improve this report!]( jira issue: crdb-58950",[],['TESTING'],"['C-test-failure', 'O-robot', 'A-disaster-recovery', 'O-roachtest', 'T-disaster-recovery', 'P-3', 'B-runtime-assertions-enabled', 'branch-release-24.3.25-rc']",github,2026-01-22T07:35:26Z,,"roachtest: c2c/mixed-version failed **note:** this build has runtime assertions enabled. if the same failure was hit in a run without assertions enabled, there should be a similar failure without this message. if there isn& ;t one, then this failure is likely due to an assertion violation or (assertion) timeout. roachtest.c2c/mixed-version [failed]( with [artifacts]( on release-24.3.25-rc @ [eecd7c2c316ed213daab1c664e0d854fc937a746]( parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - encrypted=false - fs=ext4 - localssd=true - mvtdeploymentmode=system-only - mvtversions=v24.2.10 ‚Üí release-24.3.25-rc - runtimeassertionsbuild=true - ssd=0 help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( same failure on other branches - roachtest: c2c/mixed-version failed [a-disaster-recovery b-runtime-assertions-enabled c-test-failure o-roachtest o-robot p-3 t-disaster-recovery branch-release-25.2] /cc /disaster-recovery [this test on roachdash]( | [improve this report!]( jira issue: crdb-58950",2.81,Medium,0.859,functional impact
cockroachdb/cockroach#161570,pkg/sql/opt/exec/execbuilder/tests/5node/5node_test: testexecbuild_distsql_scan failed,pkg/sql/opt/exec/execbuilder/tests/5node/5node_test.testexecbuild_distsql_scan [failed]( on release-25.2.11-rc @ [0d9e4ae4e7e3c35adea65f020bfec48628482761]( parameters: - attempt=1 - run=9 - shard=15 help see also: [how to investigate a go test failure \(internal\)]( [this test on roachdash]( | [improve this report!]( jira issue: crdb-58951,[],['TESTING'],"['C-test-failure', 'O-robot', 'X-duplicate', 'release-blocker', 'T-sql-queries', 'branch-release-25.2.11-rc']",github,2026-01-22T07:41:39Z,2026-01-22T16:18:32Z,pkg/sql/opt/exec/execbuilder/tests/5node/5node_test: testexecbuild_distsql_scan failed pkg/sql/opt/exec/execbuilder/tests/5node/5node_test.testexecbuild_distsql_scan [failed]( on release-25.2.11-rc @ [0d9e4ae4e7e3c35adea65f020bfec48628482761]( parameters: - attempt=1 - run=9 - shard=15 help see also: [how to investigate a go test failure \(internal\)]( [this test on roachdash]( | [improve this report!]( jira issue: crdb-58951,1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161571,kvserver: enable drpc for testreportunreachableremoverace,**describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58952,[],['TESTING'],"['C-test-failure', 'T-db-server']",github,2026-01-22T07:42:59Z,,kvserver: enable drpc for testreportunreachableremoverace **describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58952,1.6,Low,0.584,localized low-impact
envoyproxy/envoy#43110,newer release available : v1.9.5 (current: v1.9.4),package name: com_github_google_benchmark .9.4 current version: v1.9.4 -05-19 available version: v1.9.5 -01-21 upstream releases:,[],['UI'],"['dependencies', 'no stalebot', 'area/build']",github,2026-01-22T08:10:49Z,,newer release available : v1.9.5 (current: v1.9.4) package name: com_github_google_benchmark .9.4 current version: v1.9.4 -05-19 available version: v1.9.5 -01-21 upstream releases:,1.8,Low,0.629,user-visible issue
envoyproxy/envoy#43111,newer release available : v0.3.4 (current: v0.1.1),package name: yq_bzl .1.1 current version: v0.1.1 -05-26 available version: v0.3.4 -12-20 upstream releases:,[],['UI'],"['dependencies', 'no stalebot', 'area/build']",github,2026-01-22T08:10:50Z,,newer release available : v0.3.4 (current: v0.1.1) package name: yq_bzl .1.1 current version: v0.1.1 -05-26 available version: v0.3.4 -12-20 upstream releases:,1.8,Low,0.629,user-visible issue
cockroachdb/cockroach#161572,roachtest: activerecord failed,roachtest.activerecord [failed]( with [artifacts]( on release-26.1.0-rc @ [6c9b2f778bd29895383d493a2fa4490323499ecf]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21008538-1769066290-08-n1cpu4-0001 | 34.42.238.17 | 10.128.15.200 | parameters: - arch=arm64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - fs=ext4 - localssd=false - metamorphicleases=default - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( /cc /sql-foundations [this test on roachdash]( | [improve this report!]( jira issue: crdb-58953,[],['TESTING'],"['C-test-failure', 'O-robot', 'O-roachtest', 'T-sql-foundations', 'branch-release-26.1.0-rc']",github,2026-01-22T08:18:46Z,2026-01-27T18:05:10Z,roachtest: activerecord failed roachtest.activerecord [failed]( with [artifacts]( on release-26.1.0-rc @ [6c9b2f778bd29895383d493a2fa4490323499ecf]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21008538-1769066290-08-n1cpu4-0001 | 34.42.238.17 | 10.128.15.200 | parameters: - arch=arm64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - fs=ext4 - localssd=false - metamorphicleases=default - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( /cc /sql-foundations [this test on roachdash]( | [improve this report!]( jira issue: crdb-58953,2.834,Medium,0.864,functional impact
cockroachdb/cockroach#161573,kvserver: enable drpc for testflowcontrolcrashednodev2,**describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58954,[],['TESTING'],"['C-test-failure', 'T-db-server']",github,2026-01-22T08:35:59Z,,kvserver: enable drpc for testflowcontrolcrashednodev2 **describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58954,3.8,Critical,1.0,crash-like behavior
cockroachdb/cockroach#161574,kvserver: enable drpc for testflowcontrolsendqueue,**describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58955,[],['TESTING'],"['C-test-failure', 'T-db-server']",github,2026-01-22T08:37:04Z,,kvserver: enable drpc for testflowcontrolsendqueue **describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58955,1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161575,kvserver: enable drpc for testflowcontrolsendqueuerangefeed,**describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58956,[],['TESTING'],"['C-test-failure', 'T-db-server']",github,2026-01-22T08:37:40Z,,kvserver: enable drpc for testflowcontrolsendqueuerangefeed **describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58956,1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161576,loqrecovery: enable drpc for testreplicacollection,**describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58957,[],['TESTING'],"['C-test-failure', 'T-db-server']",github,2026-01-22T08:38:41Z,,loqrecovery: enable drpc for testreplicacollection **describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58957,1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161577,kvserver: enable drpc for testsnapshotstodrainingnodes,**describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58958,[],['TESTING'],"['C-test-failure', 'T-db-server']",github,2026-01-22T08:40:12Z,,kvserver: enable drpc for testsnapshotstodrainingnodes **describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58958,1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161578,kvserver: enable drpc for testreplicatequeuerebalancemultistore,**describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58959,[],['TESTING'],"['C-test-failure', 'T-db-server']",github,2026-01-22T08:40:52Z,,kvserver: enable drpc for testreplicatequeuerebalancemultistore **describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58959,1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161579,security: enable drpc for testrotatecerts,**describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58960,[],['TESTING'],"['C-test-failure', 'T-db-server']",github,2026-01-22T08:42:13Z,,security: enable drpc for testrotatecerts **describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58960,1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161580,security: enable drpc for testusewrongsplitcacerts,**describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58961,[],['TESTING'],"['C-test-failure', 'T-db-server']",github,2026-01-22T08:42:46Z,,security: enable drpc for testusewrongsplitcacerts **describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58961,1.6,Low,0.584,localized low-impact
kubernetes/kubernetes#136417,post-quantum cryptography readiness observations & recommendations (research),"### what would you like to be added? this issue shares research-based observations on post-quantum cryptography (pqc) readiness in kubernetes, based solely on publicly available code and documentation. this is not a vulnerability report and does not include exploitation, attack paths, or sensitive configuration details. the intent is awareness, future-readiness, and community discussion aligned with nist pqc transition guidance. observations: ‚Ä¢ cryptographic usage is distributed across multiple layers ‚Ä¢ there is no centralized crypto inventory or ownership map ‚Ä¢ no defined crypto agility or pqc migration framework ‚Ä¢ no governance-level visibility into algorithm lifecycle recommendations: ‚Ä¢ consider adding a crypto inventory layer ‚Ä¢ define crypto ownership and lifecycle policy ‚Ä¢ track nist pqc migration readiness ‚Ä¢ introduce crypto agility abstractions ‚Ä¢ document pqc roadmap for operators why this matters: quantum-safe transitions are multi-year efforts. early visibility reduces long-term risk. happy to refine or contribute to documentation if helpful. ### why is this needed? post-quantum cryptography (pqc) transitions are multi-year efforts that require early visibility and planning. today, kubernetes uses strong cryptography, but cryptographic usage is distributed across many layers (control plane, components, sdks, dependencies), making long-term migration and governance difficult to track. this enhancement is needed to: - improve visibility into cryptographic usage and ownership - enable crypto agility and algorithm lifecycle management - align kubernetes with nist post-quantum transition guidance - reduce future migration risk by identifying readiness gaps early - support operators and maintainers with clear, documented pqc readiness signals this is not related to an exploitable vulnerability. it is a forward-looking readiness and governance improvement to help kubernetes remain secure over the next decade.",[],"['SECURITY', 'FEATURE']","['kind/feature', 'sig/security', 'needs-triage']",github,2026-01-22T08:43:05Z,,"post-quantum cryptography readiness observations & recommendations (research) ### what would you like to be added? this issue shares research-based observations on post-quantum cryptography (pqc) readiness in kubernetes, based solely on publicly available code and documentation. this is not a vulnerability report and does not include exploitation, attack paths, or sensitive configuration details. the intent is awareness, future-readiness, and community discussion aligned with nist pqc transition guidance. observations: ‚Ä¢ cryptographic usage is distributed across multiple layers ‚Ä¢ there is no centralized crypto inventory or ownership map ‚Ä¢ no defined crypto agility or pqc migration framework ‚Ä¢ no governance-level visibility into algorithm lifecycle recommendations: ‚Ä¢ consider adding a crypto inventory layer ‚Ä¢ define crypto ownership and lifecycle policy ‚Ä¢ track nist pqc migration readiness ‚Ä¢ introduce crypto agility abstractions ‚Ä¢ document pqc roadmap for operators why this matters: quantum-safe transitions are multi-year efforts. early visibility reduces long-term risk. happy to refine or contribute to documentation if helpful. ### why is this needed? post-quantum cryptography (pqc) transitions are multi-year efforts that require early visibility and planning. today, kubernetes uses strong cryptography, but cryptographic usage is distributed across many layers (control plane, components, sdks, dependencies), making long-term migration and governance difficult to track. this enhancement is needed to: - improve visibility into cryptographic usage and ownership - enable crypto agility and algorithm lifecycle management - align kubernetes with nist post-quantum transition guidance - reduce future migration risk by identifying readiness gaps early - support operators and maintainers with clear, documented pqc readiness signals this is not related to an exploitable vulnerability. it is a forward-looking readiness and governance improvement to help kubernetes remain secure over the next decade.",4.6,Critical,1.0,"security risk, crash-like behavior"
cockroachdb/cockroach#161581,security: enable drpc for testtlscipherrestrict,**describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58962,[],['TESTING'],"['C-test-failure', 'T-db-server']",github,2026-01-22T08:45:21Z,,security: enable drpc for testtlscipherrestrict **describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58962,1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161582,server: enable drpc for testtriggermetadataupdatejobtriggerfailed,**describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58963,[],['TESTING'],"['C-test-failure', 'T-db-server']",github,2026-01-22T08:46:17Z,,server: enable drpc for testtriggermetadataupdatejobtriggerfailed **describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58963,1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161583,server: enable drpc for testlistsessionsprivileges,**describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58964,[],['TESTING'],"['C-test-failure', 'T-db-server']",github,2026-01-22T08:47:35Z,,server: enable drpc for testlistsessionsprivileges **describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58964,1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161584,server: enable drpc for teststatusapicombinedstatements,**describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58965,[],['TESTING'],"['C-test-failure', 'T-db-server']",github,2026-01-22T08:48:22Z,,server: enable drpc for teststatusapicombinedstatements **describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58965,1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161585,server: enable drpc for teststatusapicombinedstatementstotallatency,**describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58966,[],['TESTING'],"['C-test-failure', 'T-db-server']",github,2026-01-22T08:48:54Z,,server: enable drpc for teststatusapicombinedstatementstotallatency **describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58966,3.145,High,0.935,functional impact
cockroachdb/cockroach#161586,server: enable drpc for teststatusapicombinedstatementswithfullscans,**describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58967,[],['TESTING'],"['C-test-failure', 'T-db-server']",github,2026-01-22T08:49:21Z,,server: enable drpc for teststatusapicombinedstatementswithfullscans **describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58967,1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161587,server: enable drpc for teststatusapistatementdetails,**describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58968,[],['TESTING'],"['C-test-failure', 'T-db-server']",github,2026-01-22T08:49:48Z,,server: enable drpc for teststatusapistatementdetails **describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58968,1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161588,server: enable drpc for testdecommissionednodecannotconnect,**describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58969,[],['TESTING'],"['C-test-failure', 'T-db-server']",github,2026-01-22T08:50:35Z,,server: enable drpc for testdecommissionednodecannotconnect **describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58969,1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161589,server: enable drpc for testensuresqlstatsareflushedduringdrain,**describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58970,[],['TESTING'],"['C-test-failure', 'T-db-server']",github,2026-01-22T08:51:14Z,,server: enable drpc for testensuresqlstatsareflushedduringdrain **describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58970,1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161590,server: enable drpc for testendpointtelemetrybasic,**describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58972,[],['TESTING'],"['C-test-failure', 'T-db-server']",github,2026-01-22T08:52:28Z,,server: enable drpc for testendpointtelemetrybasic **describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58972,1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161591,server: enable drpc for testrequestmetricregistered,**describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58973,[],['TESTING'],"['C-test-failure', 'T-db-server']",github,2026-01-22T08:52:52Z,,server: enable drpc for testrequestmetricregistered **describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58973,1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161592,server: enable drpc for testsqldecommissioned,**describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58974,[],['TESTING'],"['C-test-failure', 'T-db-server']",github,2026-01-22T08:54:17Z,,server: enable drpc for testsqldecommissioned **describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58974,1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161593,settingswatcher: enable drpc for testoverflowrestart,**describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58975,[],['TESTING'],"['C-test-failure', 'T-db-server']",github,2026-01-22T08:56:22Z,,settingswatcher: enable drpc for testoverflowrestart **describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58975,3.8,Critical,1.0,crash-like behavior
cockroachdb/cockroach#161594,settingswatcher: enable drpc for testsettingwatcherontenant,**describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58976,[],['TESTING'],"['C-test-failure', 'T-db-server']",github,2026-01-22T08:56:42Z,,settingswatcher: enable drpc for testsettingwatcherontenant **describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58976,1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161595,storageapi: enable drpc for testadmindecommissionedoperations,**describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58977,[],['TESTING'],"['C-test-failure', 'T-db-server']",github,2026-01-22T08:58:20Z,,storageapi: enable drpc for testadmindecommissionedoperations **describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58977,1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161596,storageapi: enable drpc for testnetworkconnectivity,**describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58978,[],['TESTING'],"['C-test-failure', 'T-db-server']",github,2026-01-22T08:58:48Z,,storageapi: enable drpc for testnetworkconnectivity **describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58978,1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161597,server: enable drpc for testsqlrolesapi,**describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58979,[],['TESTING'],"['C-test-failure', 'T-db-server']",github,2026-01-22T08:59:37Z,,server: enable drpc for testsqlrolesapi **describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58979,1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161598,ttljob: enable drpc for newrowlevelttltestjobtesthelper,**describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58980,[],['TESTING'],"['C-test-failure', 'T-db-server']",github,2026-01-22T09:01:30Z,,ttljob: enable drpc for newrowlevelttltestjobtesthelper **describe the problem** we currently have drpc disabled for since it fails when it is turned on. **to reproduce** **expected behavior** the test should pass with drpc enabled. epic: crdb-55382 jira issue: crdb-58980,1.6,Low,0.584,localized low-impact
flutter/flutter#181315,"impeller causes significant fps drop on powervr bxm gpu (moto g73, flutter 3.38.2)","### steps to reproduce **environment** flutter version: 3.38.2 (stable) platform: android device: moto g73 soc: dimensity 930 gpu: powervr bxm android version: 14 build mode: profile **description** on a moto g73 with powervr bxm gpu, running the same flutter app in profile mode shows a significant fps difference depending on impeller. - with impeller enabled: ~80 fps - with impeller disabled (skia): ~117 fps this is measured in profile mode using devtools. no vulkan fallback warnings are shown. the performance drop is visually noticeable as stutter/jank. ### expected results impeller performance should be comparable to or better than skia. ### actual results impeller shows ~30% lower frame rate than skia on this device. ### code sample code sample ### screenshots or video screenshots / video demonstration [upload media here] ### logs logs ### flutter doctor output doctor output",[],['PERFORMANCE'],"['e: device-specific', 'platform-android', 'engine', 'c: performance', 'perf: speed', 'a: production', 'P1', 'needs repro info', 'e: impeller', 'team-engine', 'triaged-engine']",github,2026-01-22T09:03:53Z,,"impeller causes significant fps drop on powervr bxm gpu (moto g73, flutter 3.38.2) ### steps to reproduce **environment** flutter version: 3.38.2 (stable) platform: android device: moto g73 soc: dimensity 930 gpu: powervr bxm android version: 14 build mode: profile **description** on a moto g73 with powervr bxm gpu, running the same flutter app in profile mode shows a significant fps difference depending on impeller. - with impeller enabled: ~80 fps - with impeller disabled (skia): ~117 fps this is measured in profile mode using devtools. no vulkan fallback warnings are shown. the performance drop is visually noticeable as stutter/jank. ### expected results impeller performance should be comparable to or better than skia. ### actual results impeller shows ~30% lower frame rate than skia on this device. ### code sample code sample ### screenshots or video screenshots / video demonstration [upload media here] ### logs logs ### flutter doctor output doctor output",4.6,Critical,1.0,performance degradation
cockroachdb/cockroach#161599,pkg/sql/logictest/tests/fakedist-vec-off/fakedist-vec-off_test: testlogic_cursor failed,pkg/sql/logictest/tests/fakedist-vec-off/fakedist-vec-off_test.testlogic_cursor [failed]( on release-26.1 @ [e39600cb891be2624d33bc7d171ada4629abdfdb]( parameters: - attempt=1 - run=5 - shard=18 help see also: [how to investigate a go test failure \(internal\)]( [this test on roachdash]( | [improve this report!]( jira issue: crdb-58981,[],['TESTING'],"['C-test-failure', 'O-robot', 'T-sql-queries', 'branch-release-25.2', 'branch-release-25.4', 'branch-release-26.1', 'target-release-26.2.0']",github,2026-01-22T09:28:14Z,2026-01-28T05:35:58Z,pkg/sql/logictest/tests/fakedist-vec-off/fakedist-vec-off_test: testlogic_cursor failed pkg/sql/logictest/tests/fakedist-vec-off/fakedist-vec-off_test.testlogic_cursor [failed]( on release-26.1 @ [e39600cb891be2624d33bc7d171ada4629abdfdb]( parameters: - attempt=1 - run=5 - shard=18 help see also: [how to investigate a go test failure \(internal\)]( [this test on roachdash]( | [improve this report!]( jira issue: crdb-58981,1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161600,kv/kvserver: testwriteloadstatsaccounting failed,kv/kvserver.testwriteloadstatsaccounting [failed]( on release-25.4 @ [dba21b616ff89f4718ce04d4b47a9baa07127a03]( parameters: - attempt=1 - deadlock=true - run=1 - shard=48 help see also: [how to investigate a go test failure \(internal\)]( /cc /kv-triage [this test on roachdash]( | [improve this report!]( jira issue: crdb-58982,[],"['TESTING', 'BUG']","['C-bug', 'C-test-failure', 'O-robot', 'A-testing', 'T-kv', 'X-nostale', 'branch-release-25.4']",github,2026-01-22T09:33:11Z,,kv/kvserver: testwriteloadstatsaccounting failed kv/kvserver.testwriteloadstatsaccounting [failed]( on release-25.4 @ [dba21b616ff89f4718ce04d4b47a9baa07127a03]( parameters: - attempt=1 - deadlock=true - run=1 - shard=48 help see also: [how to investigate a go test failure \(internal\)]( /cc /kv-triage [this test on roachdash]( | [improve this report!]( jira issue: crdb-58982,2.997,Medium,0.901,crash-like behavior
openssl/openssl#29713,regression of s390x os zoo ci runs in fips selftest ecdsa kat,"example of the failing run: the most likely commit that triggers this is: most likely, the computation when using the p-256 curve is being forwarded to the crypto accelerator which does not pull the randomness from the kat drbg but from an internal source, which of course makes the resulting signature different.",[],['BUG'],"['branch: master', 'triaged: bug', 'severity: regression']",github,2026-01-22T09:35:37Z,,"regression of s390x os zoo ci runs in fips selftest ecdsa kat example of the failing run: the most likely commit that triggers this is: most likely, the computation when using the p-256 curve is being forwarded to the crypto accelerator which does not pull the randomness from the kat drbg but from an internal source, which of course makes the resulting signature different.",4.2,Critical,1.0,system-wide impact
cockroachdb/cockroach#161601,roachtest: rebalance/by-load/mode=replicas/mixed-version failed,"**note:** this build has runtime assertions enabled. if the same failure was hit in a run without assertions enabled, there should be a similar failure without this message. if there isn& ;t one, then this failure is likely due to an assertion violation or (assertion) timeout. roachtest.rebalance/by-load/mode=replicas/mixed-version [failed]( with [artifacts]( on master @ [c6ec5849dfea61702bc94c5b794a7659c0e914f4]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21008324-1769065069-176-n7cpu4-0001 | 20.127.233.20 | 10.1.0.122 | | teamcity-21008324-1769065069-176-n7cpu4-0002 | 172.206.226.44 | 10.1.0.109 | | teamcity-21008324-1769065069-176-n7cpu4-0003 | 20.55.83.223 | 10.1.0.107 | | teamcity-21008324-1769065069-176-n7cpu4-0004 | 40.76.99.203 | 10.1.0.119 | | teamcity-21008324-1769065069-176-n7cpu4-0005 | 172.210.12.169 | 10.1.0.106 | | teamcity-21008324-1769065069-176-n7cpu4-0006 | 20.55.91.88 | 10.1.0.126 | | teamcity-21008324-1769065069-176-n7cpu4-0007 | 57.151.107.126 | 10.1.0.108 | parameters: - arch=amd64 - cloud=azure - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - fs=ext4 - localssd=true - mvtdeploymentmode=shared-process - mvtversions=v25.4.3 ‚Üí v26.2.0-alpha.00000000-dev-c6ec5849dfea61702bc94c5b794a7659c0e914f4 (master) - runtimeassertionsbuild=true help see: [roachtest readme]( see: [how to investigate \(internal\)]( _grafana is not yet available for azure clusters_ /cc /kv-triage [this test on roachdash]( | [improve this report!]( jira issue: crdb-58983",[],"['TESTING', 'BUG']","['C-bug', 'C-test-failure', 'O-robot', 'X-duplicate', 'A-testing', 'O-roachtest', 'branch-master', 'T-kv', 'B-runtime-assertions-enabled']",github,2026-01-22T09:56:51Z,2026-01-23T16:49:11Z,"roachtest: rebalance/by-load/mode=replicas/mixed-version failed **note:** this build has runtime assertions enabled. if the same failure was hit in a run without assertions enabled, there should be a similar failure without this message. if there isn& ;t one, then this failure is likely due to an assertion violation or (assertion) timeout. roachtest.rebalance/by-load/mode=replicas/mixed-version [failed]( with [artifacts]( on master @ [c6ec5849dfea61702bc94c5b794a7659c0e914f4]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21008324-1769065069-176-n7cpu4-0001 | 20.127.233.20 | 10.1.0.122 | | teamcity-21008324-1769065069-176-n7cpu4-0002 | 172.206.226.44 | 10.1.0.109 | | teamcity-21008324-1769065069-176-n7cpu4-0003 | 20.55.83.223 | 10.1.0.107 | | teamcity-21008324-1769065069-176-n7cpu4-0004 | 40.76.99.203 | 10.1.0.119 | | teamcity-21008324-1769065069-176-n7cpu4-0005 | 172.210.12.169 | 10.1.0.106 | | teamcity-21008324-1769065069-176-n7cpu4-0006 | 20.55.91.88 | 10.1.0.126 | | teamcity-21008324-1769065069-176-n7cpu4-0007 | 57.151.107.126 | 10.1.0.108 | parameters: - arch=amd64 - cloud=azure - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - fs=ext4 - localssd=true - mvtdeploymentmode=shared-process - mvtversions=v25.4.3 ‚Üí v26.2.0-alpha.00000000-dev-c6ec5849dfea61702bc94c5b794a7659c0e914f4 (master) - runtimeassertionsbuild=true help see: [roachtest readme]( see: [how to investigate \(internal\)]( _grafana is not yet available for azure clusters_ /cc /kv-triage [this test on roachdash]( | [improve this report!]( jira issue: crdb-58983",2.729,Medium,0.84,functional impact
cockroachdb/cockroach#161602,roachtest: ssh_problem failed,"**note:** this build has runtime assertions enabled. if the same failure was hit in a run without assertions enabled, there should be a similar failure without this message. if there isn& ;t one, then this failure is likely due to an assertion violation or (assertion) timeout. roachtest.ssh_problem [failed]( with [artifacts]( on release-25.4.4-rc @ [688cda9084f2924ec4e22044c861aa0a3f34e956]( parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - encrypted=false - fs=ext4 - localssd=true - runtimeassertionsbuild=true - ssd=0 help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( same failure on other branches - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-26.1.0-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.3.7-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.4.3-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.2.11-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-24.3.25-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-26.1] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.3.6-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-24.3.24-rc] - roachtest: ssh_problem failed [o-roachtest o-robot o-rsg t-testeng x-infra-flake branch-release-25.2.10-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.4.2-rc] /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-58984",[],['TESTING'],"['O-robot', 'O-roachtest', 'T-testeng', 'X-infra-flake', 'branch-release-25.4.4-rc']",github,2026-01-22T10:02:18Z,,"roachtest: ssh_problem failed **note:** this build has runtime assertions enabled. if the same failure was hit in a run without assertions enabled, there should be a similar failure without this message. if there isn& ;t one, then this failure is likely due to an assertion violation or (assertion) timeout. roachtest.ssh_problem [failed]( with [artifacts]( on release-25.4.4-rc @ [688cda9084f2924ec4e22044c861aa0a3f34e956]( parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - encrypted=false - fs=ext4 - localssd=true - runtimeassertionsbuild=true - ssd=0 help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( same failure on other branches - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-26.1.0-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.3.7-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.4.3-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.2.11-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-24.3.25-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-26.1] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.3.6-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-24.3.24-rc] - roachtest: ssh_problem failed [o-roachtest o-robot o-rsg t-testeng x-infra-flake branch-release-25.2.10-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.4.2-rc] /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-58984",2.975,Medium,0.896,functional impact
cockroachdb/cockroach#161603,roachtest: jepsen/monotonic/majority-ring-start-kill-2 failed,"**note:** this build has runtime assertions enabled. if the same failure was hit in a run without assertions enabled, there should be a similar failure without this message. if there isn& ;t one, then this failure is likely due to an assertion violation or (assertion) timeout. roachtest.jepsen/monotonic/majority-ring-start-kill-2 [failed]( with [artifacts]( on release-25.3.7-rc @ [67b1251ee27f325e85ddbaddb5c055624d12f57a]( parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - encrypted=false - metamorphicbufferedsender=true - metamorphicleases=default - runtimeassertionsbuild=true - ssd=0 help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-58985",[],['TESTING'],"['C-test-failure', 'O-robot', 'O-roachtest', 'T-testeng', 'X-infra-flake', 'B-runtime-assertions-enabled', 'branch-release-25.3.7-rc']",github,2026-01-22T10:03:52Z,2026-01-22T15:29:49Z,"roachtest: jepsen/monotonic/majority-ring-start-kill-2 failed **note:** this build has runtime assertions enabled. if the same failure was hit in a run without assertions enabled, there should be a similar failure without this message. if there isn& ;t one, then this failure is likely due to an assertion violation or (assertion) timeout. roachtest.jepsen/monotonic/majority-ring-start-kill-2 [failed]( with [artifacts]( on release-25.3.7-rc @ [67b1251ee27f325e85ddbaddb5c055624d12f57a]( parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - encrypted=false - metamorphicbufferedsender=true - metamorphicleases=default - runtimeassertionsbuild=true - ssd=0 help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-58985",3.09,High,0.922,functional impact
prometheus/prometheus#17914,native stateset in prometheus,"### proposal ### context openmetrics 1.0 defines the [stateset]( metric type for enums-like and bitset-like data, so currently it's stored as normal gauges in prometheus, which is prone to scrape inefficiencies, transactionality/atomicity problems on storage and long term storage ingestion. we are at the moment of releasing openmetrics 2.0, where we introduce composite types in the text format. we [could make stateset a composite type]( too e.g. however, it's hard to make this decision and agree on a good enough format. ideally text format is ""what you see is what you query"" but we never discussed how promql ux for potential stateset would look like. we rarely see stateset being used in practice. i see it only in kube state metrics project. **this ticket is to start the discussion about native stateset in tsdb** ### questions 1. is stateset a definitive answer to discussed a lot in the past? ( one thing that might be blocking some ideas there is that state can only be or whereas some use cases where mentioning any number (which is not allowed in the current om 1.0 stateset). allowing any number is a fun discussion because it would make it a value really which moves cardinality to chunks/sample side (not indexed labels essentially). 2. who uses statesets in prod? please add emoji üëçüèΩ or so to give us an idea and speak to us on slack and kubecons. we need user feedback on this! 3. who wants to use them but can't? what's the adoption blockers, if any? cardinality is one often mentioned item. 4. would you prefer if we optimize stateset in om 2.0 for the future native use? 5. if we ever add native stateset to prometheus, would that break opentelemetry compatibility (it's deprecated on otel side). cc big discussion [happened recently on slack too](",[],['FEATURE'],['kind/feature'],github,2026-01-22T10:11:12Z,,"native stateset in prometheus ### proposal ### context openmetrics 1.0 defines the [stateset]( metric type for enums-like and bitset-like data, so currently it's stored as normal gauges in prometheus, which is prone to scrape inefficiencies, transactionality/atomicity problems on storage and long term storage ingestion. we are at the moment of releasing openmetrics 2.0, where we introduce composite types in the text format. we [could make stateset a composite type]( too e.g. however, it's hard to make this decision and agree on a good enough format. ideally text format is ""what you see is what you query"" but we never discussed how promql ux for potential stateset would look like. we rarely see stateset being used in practice. i see it only in kube state metrics project. **this ticket is to start the discussion about native stateset in tsdb** ### questions 1. is stateset a definitive answer to discussed a lot in the past? ( one thing that might be blocking some ideas there is that state can only be or whereas some use cases where mentioning any number (which is not allowed in the current om 1.0 stateset). allowing any number is a fun discussion because it would make it a value really which moves cardinality to chunks/sample side (not indexed labels essentially). 2. who uses statesets in prod? please add emoji üëçüèΩ or so to give us an idea and speak to us on slack and kubecons. we need user feedback on this! 3. who wants to use them but can't? what's the adoption blockers, if any? cardinality is one often mentioned item. 4. would you prefer if we optimize stateset in om 2.0 for the future native use? 5. if we ever add native stateset to prometheus, would that break opentelemetry compatibility (it's deprecated on otel side). cc big discussion [happened recently on slack too](",1.4,Low,0.538,localized low-impact
kubernetes/kubernetes#136422,export as for external validation consistency,"**problem statement**: the function in package is currently _private_ (unexported), which forces external projects that need to validate affinity configurations to duplicate ~200 lines of validation logic. if we need to validate affinity, then we have to replicate the entire logic along with its helper functions ( , , , etc.) to pre-validate user-provided affinity configurations before deploying pods. this duplication: - creates maintenance burden (keeping in sync with upstream changes) - risks validation drift between client-side and server-side logic - duplicates code that already exists and is well-tested in kubernetes _similar functions are already exported_: the same package has validation methods exported for nodeselector( ), tolerations( ), but not for affinity. **proposed solution**: a simple, non-breaking wrapper following the existing pattern: _implementation impact_: - zero changes to internal logic : the private function remains unchanged - full backward compatibility : existing code paths unaffected - follows established pattern : mirrors how and are exported. **benefits**: - bug discovery: external projects using server-side validation logic help identify bugs earlier. the same validation code is used by kube-apiserver, so bugs found externally benefit the entire ecosystem. - validation accuracy: pre-validating configurations using the exact same logic as the api server ensures users get consistent error messages and behavior. **addressing the use of** while is not officially intended for use as an external library, the following points justify its use in this context: - high community reliance: per , over 1,900 projects import this validation package, demonstrating that many maintainers rely on it despite its ""internal"" designation. - functional availability: the package exports numerous validation functions, making it a viable‚Äîand often necessary‚Äîresource for maintaining consistency with kubernetes' internal validation logic. - community guidance: developer discussions frequently suggest leveraging these helpers to avoid diverging from core kubernetes behavior, as re-implementing these complex rules from scratch is error-prone. i‚Äôm happy to contribute the changes mentioned above. i wanted to align on the approach first to ensure it meets the project's standards‚Äîplease let me know if you‚Äôd like me to open a pr. /sig scheduling /kind feature /area api",[],['FEATURE'],"['area/api', 'sig/scheduling', 'kind/feature', 'needs-triage']",github,2026-01-22T10:30:42Z,,"export as for external validation consistency **problem statement**: the function in package is currently _private_ (unexported), which forces external projects that need to validate affinity configurations to duplicate ~200 lines of validation logic. if we need to validate affinity, then we have to replicate the entire logic along with its helper functions ( , , , etc.) to pre-validate user-provided affinity configurations before deploying pods. this duplication: - creates maintenance burden (keeping in sync with upstream changes) - risks validation drift between client-side and server-side logic - duplicates code that already exists and is well-tested in kubernetes _similar functions are already exported_: the same package has validation methods exported for nodeselector( ), tolerations( ), but not for affinity. **proposed solution**: a simple, non-breaking wrapper following the existing pattern: _implementation impact_: - zero changes to internal logic : the private function remains unchanged - full backward compatibility : existing code paths unaffected - follows established pattern : mirrors how and are exported. **benefits**: - bug discovery: external projects using server-side validation logic help identify bugs earlier. the same validation code is used by kube-apiserver, so bugs found externally benefit the entire ecosystem. - validation accuracy: pre-validating configurations using the exact same logic as the api server ensures users get consistent error messages and behavior. **addressing the use of** while is not officially intended for use as an external library, the following points justify its use in this context: - high community reliance: per , over 1,900 projects import this validation package, demonstrating that many maintainers rely on it despite its ""internal"" designation. - functional availability: the package exports numerous validation functions, making it a viable‚Äîand often necessary‚Äîresource for maintaining consistency with kubernetes' internal validation logic. - community guidance: developer discussions frequently suggest leveraging these helpers to avoid diverging from core kubernetes behavior, as re-implementing these complex rules from scratch is error-prone. i‚Äôm happy to contribute the changes mentioned above. i wanted to align on the approach first to ensure it meets the project's standards‚Äîplease let me know if you‚Äôd like me to open a pr. /sig scheduling /kind feature /area api",5.4,Critical,1.0,crash-like behavior
cockroachdb/cockroach#161604,roachtest: c2c/tpcc/warehouses=1000/duration=60/cutover=30 failed,roachtest.c2c/tpcc/warehouses=1000/duration=60/cutover=30 [failed]( with [artifacts]( on release-25.4 @ [4206b9fa19a55fc194880be412d77145a2c802ad]( parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=8 - encrypted=false - fs=ext4 - localssd=false - metamorphicleases=default - runtimeassertionsbuild=false - ssd=0 help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( same failure on other branches - roachtest: c2c/tpcc/warehouses=1000/duration=60/cutover=30 failed [a-disaster-recovery c-test-failure o-roachtest o-robot p-2 t-disaster-recovery branch-master release-blocker] /cc /disaster-recovery [this test on roachdash]( | [improve this report!]( jira issue: crdb-58987,[],['TESTING'],"['C-test-failure', 'O-robot', 'A-disaster-recovery', 'O-roachtest', 'release-blocker', 'T-disaster-recovery', 'branch-release-25.4']",github,2026-01-22T10:36:38Z,,roachtest: c2c/tpcc/warehouses=1000/duration=60/cutover=30 failed roachtest.c2c/tpcc/warehouses=1000/duration=60/cutover=30 [failed]( with [artifacts]( on release-25.4 @ [4206b9fa19a55fc194880be412d77145a2c802ad]( parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=8 - encrypted=false - fs=ext4 - localssd=false - metamorphicleases=default - runtimeassertionsbuild=false - ssd=0 help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( same failure on other branches - roachtest: c2c/tpcc/warehouses=1000/duration=60/cutover=30 failed [a-disaster-recovery c-test-failure o-roachtest o-robot p-2 t-disaster-recovery branch-master release-blocker] /cc /disaster-recovery [this test on roachdash]( | [improve this report!]( jira issue: crdb-58987,2.939,Medium,0.888,functional impact
python/cpython#144140,optimize len for string constants in optimizer,"# feature or enhancement ### proposal: hi team, i found an optimization opportunity: len() calls on string constants can be optimized at compile time, similar to how tuple constants are handled. quick testing with showed _call_len hits: - str: 23 (6 optimizable with this patch) - list: 28 - tuple: 4 (3 already optimized) this patch extends the existing constant folding for len(tuple) to also handle len(""string""), replacing the function call with _shuffle_3_load_const_inline_borrow when the result is an immortal integer. ### has this already been discussed elsewhere? no response given ### links to previous discussion of this feature: _no response_ ### linked prs * gh-144142",[],"['PERFORMANCE', 'FEATURE']","['type-feature', 'performance', 'interpreter-core', 'topic-JIT']",github,2026-01-22T10:45:26Z,2026-01-24T16:09:47Z,"optimize len for string constants in optimizer # feature or enhancement ### proposal: hi team, i found an optimization opportunity: len() calls on string constants can be optimized at compile time, similar to how tuple constants are handled. quick testing with showed _call_len hits: - str: 23 (6 optimizable with this patch) - list: 28 - tuple: 4 (3 already optimized) this patch extends the existing constant folding for len(tuple) to also handle len(""string""), replacing the function call with _shuffle_3_load_const_inline_borrow when the result is an immortal integer. ### has this already been discussed elsewhere? no response given ### links to previous discussion of this feature: _no response_ ### linked prs * gh-144142",2.511,Medium,0.791,performance degradation
python/cpython#144141,eliminate overhead of fetching and testing attributes in specializations for new objects,"consider this produces a trace looking something like this: each of those reads the old value out of memory and then conditionally decrefs it. but in this case we know that the old value was so we can just overwrite it. so we can replace this: with this: on aarch64, this reduces the number of machine instructions from 48 to 26. the same reasoning also applies to where it reduces the number of machine instructions from 32 to 14. see also we can probably remove some of those guards as well, but that's a separate issue.",[],['PERFORMANCE'],"['performance', 'interpreter-core']",github,2026-01-22T10:46:26Z,,"eliminate overhead of fetching and testing attributes in specializations for new objects consider this produces a trace looking something like this: each of those reads the old value out of memory and then conditionally decrefs it. but in this case we know that the old value was so we can just overwrite it. so we can replace this: with this: on aarch64, this reduces the number of machine instructions from 48 to 26. the same reasoning also applies to where it reduces the number of machine instructions from 32 to 14. see also we can probably remove some of those guards as well, but that's a separate issue.",3.423,High,0.998,performance degradation
python/cpython#144145,support track object and its property in the tier 2 optimizer,"# feature or enhancement ### proposal: currently, when the jit optimizer encounters _load_attr_slot, it creates a new unknown symbol ( ) even if the same slot was previously written in the same trace. this prevents the optimizer from: 1. knowing the type of values loaded from slots 2. eliminating redundant type guards 3. performing further type-based optimizations to achieve these optimizations, we need: 1. keep track of where code might escape 2. implement tracking and optimization for attributes of the object ### has this already been discussed elsewhere? no response given ### links to previous discussion of this feature: _no response_ ### linked prs * gh-144122",[],['FEATURE'],"['type-feature', 'interpreter-core', 'topic-JIT']",github,2026-01-22T10:53:08Z,,"support track object and its property in the tier 2 optimizer # feature or enhancement ### proposal: currently, when the jit optimizer encounters _load_attr_slot, it creates a new unknown symbol ( ) even if the same slot was previously written in the same trace. this prevents the optimizer from: 1. knowing the type of values loaded from slots 2. eliminating redundant type guards 3. performing further type-based optimizations to achieve these optimizations, we need: 1. keep track of where code might escape 2. implement tracking and optimization for attributes of the object ### has this already been discussed elsewhere? no response given ### links to previous discussion of this feature: _no response_ ### linked prs * gh-144122",1.4,Low,0.538,localized low-impact
tensorflow/tensorflow#108657,tf.experimental.numpy.isclose returns incorrect results for integer inputs,"### issue type bug ### have you reproduced the bug with tensorflow nightly? yes ### source source ### tensorflow version v2.20.0-rc0-4-g72fbba3d20f 2.20.0 ### custom code yes ### os platform and distribution _no response_ ### mobile device _no response_ ### python version _no response_ ### bazel version _no response_ ### gcc/compiler version _no response_ ### cuda/cudnn version _no response_ ### gpu model and memory _no response_ ### current behavior? ## description: is documented as a tensorflow variant of numpy's function. it is expected to behave identically to numpy, including support for integer inputs. however, when the input tensors are integers, return incorrect results. ## analysis: the expected behavior of the below reproduction code is that all elements return true, consistent with numpy. the discrepancy occurs only when the inputs are integer types (int32, int64). tensorflow does not correctly handle integer inputs internally in tf.experimental.numpy.isclose. even though integer arithmetic should in principle work with the formula: the internal implementation appears to mishandle the comparison, resulting in all false. **casting inputs to float32 or float64 returns the correct results.** ### standalone code to reproduce the issue ### relevant log output",[],['BUG'],"['type:bug', 'comp:apis', 'TF 2.19']",github,2026-01-22T11:03:22Z,,"tf.experimental.numpy.isclose returns incorrect results for integer inputs ### issue type bug ### have you reproduced the bug with tensorflow nightly? yes ### source source ### tensorflow version v2.20.0-rc0-4-g72fbba3d20f 2.20.0 ### custom code yes ### os platform and distribution _no response_ ### mobile device _no response_ ### python version _no response_ ### bazel version _no response_ ### gcc/compiler version _no response_ ### cuda/cudnn version _no response_ ### gpu model and memory _no response_ ### current behavior? ## description: is documented as a tensorflow variant of numpy's function. it is expected to behave identically to numpy, including support for integer inputs. however, when the input tensors are integers, return incorrect results. ## analysis: the expected behavior of the below reproduction code is that all elements return true, consistent with numpy. the discrepancy occurs only when the inputs are integer types (int32, int64). tensorflow does not correctly handle integer inputs internally in tf.experimental.numpy.isclose. even though integer arithmetic should in principle work with the formula: the internal implementation appears to mishandle the comparison, resulting in all false. **casting inputs to float32 or float64 returns the correct results.** ### standalone code to reproduce the issue ### relevant log output",4.2,Critical,1.0,system-wide impact
cockroachdb/cockroach#161605,schedulerlatency: incorrect data reported to datadog/prometheus,"i think the way this package reports metrics leads to incorrect (probably bogus) results. tldr: a histogram that is reported to datadog/prometheus should be cumulative (i.e. bucket counts only ever go up). however, the data is actually a delta. here's what an idle local single-node cluster exports to prometheus: you can see that this is not cumulative, which confirms an impression i had when looking at the code the other day. instead, the histogram is really the delta (since the last poll) of the go-internal cumulative histogram. i asked cursor about this as well and it similarly thought this was all incorrect. i _think_ that what this means is that whenever support/kv/anyone look at the runtime latency histograms, they are looking at what at worst might amount to random data. we recently fixed another data quality bug here ( but it looks like there's more to do. the [cursor transcript]( is compact and a good starting point for more breadcrumbs. i'm honestly not sure why the code is so bespoke, i would've expected the code to be structured loosely as follows: - we have a super duper vanilla crdb histogram (which wraps both a cumulative and windowed histogram) - we periodically poll the (cumulative) go metric and compute the delta since the last poll. - we re-bucket the delta (go histogram uses 720 buckets, our histogram has fewer buckets - this is [existing code]( - we add the delta to our crdb histogram. this will need a bit of work - we don't want to have to call 1000s of times (once for each observation, of which there can be large amounts). instead, we'll want to rely on the known fact that we already have the counts matching the bucket boundaries available. we can add a method to our histogram to efficiently accommodate this input (so we're basically merging the counts of the provided slice into the histogram's slice of counts, and ditto for the windowed histogram). modulo accommodating the at the same time, that should be it. **related:** - noticed the same problem, and there's a todo in the code. jira issue: crdb-58988",[],['FEATURE'],"['C-enhancement', 'A-admission-control', 'T-kv']",github,2026-01-22T11:15:34Z,,"schedulerlatency: incorrect data reported to datadog/prometheus i think the way this package reports metrics leads to incorrect (probably bogus) results. tldr: a histogram that is reported to datadog/prometheus should be cumulative (i.e. bucket counts only ever go up). however, the data is actually a delta. here's what an idle local single-node cluster exports to prometheus: you can see that this is not cumulative, which confirms an impression i had when looking at the code the other day. instead, the histogram is really the delta (since the last poll) of the go-internal cumulative histogram. i asked cursor about this as well and it similarly thought this was all incorrect. i _think_ that what this means is that whenever support/kv/anyone look at the runtime latency histograms, they are looking at what at worst might amount to random data. we recently fixed another data quality bug here ( but it looks like there's more to do. the [cursor transcript]( is compact and a good starting point for more breadcrumbs. i'm honestly not sure why the code is so bespoke, i would've expected the code to be structured loosely as follows: - we have a super duper vanilla crdb histogram (which wraps both a cumulative and windowed histogram) - we periodically poll the (cumulative) go metric and compute the delta since the last poll. - we re-bucket the delta (go histogram uses 720 buckets, our histogram has fewer buckets - this is [existing code]( - we add the delta to our crdb histogram. this will need a bit of work - we don't want to have to call 1000s of times (once for each observation, of which there can be large amounts). instead, we'll want to rely on the known fact that we already have the counts matching the bucket boundaries available. we can add a method to our histogram to efficiently accommodate this input (so we're basically merging the counts of the provided slice into the histogram's slice of counts, and ditto for the windowed histogram). modulo accommodating the at the same time, that should be it. **related:** - noticed the same problem, and there's a todo in the code. jira issue: crdb-58988",3.01,High,0.904,functional impact
rust-lang/rust#151486,chromium hitting sigill crashes on aarch64 after enabled outlined atomics by default,"continuing the discussion here as requested on after this change, we're getting reports from developers about hitting sigill in chromium's render process on some android devices. disabling outline-atomics seems to fix the crashes. we don't have a good understanding of the root cause yet, but for a start, we'd like to be able to disable the feature. passing works, but causes warnings: it would be good to have a knob for turning off the feature without warnings. alternatively it would be good to have a knob to turn off that kind of warning. (we'll hack it out for now:",[],"['BUG', 'FEATURE']","['T-compiler', 'regression-from-stable-to-beta', 'C-bug', 'I-prioritize', 'T-libs', 'O-AArch64', 'A-atomic', 'A-target-feature']",github,2026-01-22T11:39:08Z,,"chromium hitting sigill crashes on aarch64 after enabled outlined atomics by default continuing the discussion here as requested on after this change, we're getting reports from developers about hitting sigill in chromium's render process on some android devices. disabling outline-atomics seems to fix the crashes. we don't have a good understanding of the root cause yet, but for a start, we'd like to be able to disable the feature. passing works, but causes warnings: it would be good to have a knob for turning off the feature without warnings. alternatively it would be good to have a knob to turn off that kind of warning. (we'll hack it out for now:",3.264,High,0.962,crash-like behavior
rust-lang/rust#151487,macro ambiguity with #![no_implicit_prelude],"### code i tried this code: i expected to see this happen: the code should compile (no name-resolution ambiguity). instead, this happened: the compiler reports an ambiguity error for unreachable!() claiming a conflict between a name from a glob import and an outer scope (the prelude). ### version it worked on it most recently worked on: 1.93.0-beta.8 ### version with regression : ### notes / additional observations: rust playground: removing #![no_implicit_prelude] yields the same error. swapping order (define macro before mod foo) makes the code compile successfully.",[],['BUG'],"['A-resolve', 'A-macros', 'T-libs-api', 'regression-from-stable-to-beta', 'C-bug', 'I-prioritize', 'T-libs']",github,2026-01-22T11:50:46Z,,"macro ambiguity with #![no_implicit_prelude] ### code i tried this code: i expected to see this happen: the code should compile (no name-resolution ambiguity). instead, this happened: the compiler reports an ambiguity error for unreachable!() claiming a conflict between a name from a glob import and an outer scope (the prelude). ### version it worked on it most recently worked on: 1.93.0-beta.8 ### version with regression : ### notes / additional observations: rust playground: removing #![no_implicit_prelude] yields the same error. swapping order (define macro before mod foo) makes the code compile successfully.",2.247,Medium,0.731,functional impact
cockroachdb/cockroach#161606,roachtest: jepsen/sequential/majority-ring failed,"**note:** this build has runtime assertions enabled. if the same failure was hit in a run without assertions enabled, there should be a similar failure without this message. if there isn& ;t one, then this failure is likely due to an assertion violation or (assertion) timeout. roachtest.jepsen/sequential/majority-ring [failed]( with [artifacts]( on release-26.1.0-rc @ [6c9b2f778bd29895383d493a2fa4490323499ecf]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21008538-1769066290-102-n6cpu4-0001 | 34.74.53.55 | 10.142.2.151 | | teamcity-21008538-1769066290-102-n6cpu4-0002 | 34.26.122.42 | 10.142.2.132 | | teamcity-21008538-1769066290-102-n6cpu4-0003 | 104.196.172.213 | 10.142.2.76 | | teamcity-21008538-1769066290-102-n6cpu4-0004 | 34.26.72.182 | 10.142.2.91 | | teamcity-21008538-1769066290-102-n6cpu4-0005 | 104.196.36.14 | 10.142.2.152 | | teamcity-21008538-1769066290-102-n6cpu4-0006 | 34.138.207.69 | 10.142.2.170 | parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - fs=ext4 - localssd=true - metamorphicleases=epoch - metamorphicwritebuffering=true - runtimeassertionsbuild=true help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-58989",[],['TESTING'],"['C-test-failure', 'O-robot', 'O-roachtest', 'T-testeng', 'X-infra-flake', 'B-runtime-assertions-enabled', 'branch-release-26.1.0-rc']",github,2026-01-22T12:01:01Z,2026-01-22T15:44:37Z,"roachtest: jepsen/sequential/majority-ring failed **note:** this build has runtime assertions enabled. if the same failure was hit in a run without assertions enabled, there should be a similar failure without this message. if there isn& ;t one, then this failure is likely due to an assertion violation or (assertion) timeout. roachtest.jepsen/sequential/majority-ring [failed]( with [artifacts]( on release-26.1.0-rc @ [6c9b2f778bd29895383d493a2fa4490323499ecf]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21008538-1769066290-102-n6cpu4-0001 | 34.74.53.55 | 10.142.2.151 | | teamcity-21008538-1769066290-102-n6cpu4-0002 | 34.26.122.42 | 10.142.2.132 | | teamcity-21008538-1769066290-102-n6cpu4-0003 | 104.196.172.213 | 10.142.2.76 | | teamcity-21008538-1769066290-102-n6cpu4-0004 | 34.26.72.182 | 10.142.2.91 | | teamcity-21008538-1769066290-102-n6cpu4-0005 | 104.196.36.14 | 10.142.2.152 | | teamcity-21008538-1769066290-102-n6cpu4-0006 | 34.138.207.69 | 10.142.2.170 | parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - fs=ext4 - localssd=true - metamorphicleases=epoch - metamorphicwritebuffering=true - runtimeassertionsbuild=true help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-58989",3.3,High,0.97,system-wide impact
tensorflow/tensorflow#108659,tensorflow crash with multi-element step tensor in summary writer,"### issue type bug ### have you reproduced the bug with tensorflow nightly? yes ### source source ### tensorflow version tf 2.21.0 ### custom code yes ### os platform and distribution _no response_ ### mobile device _no response_ ### python version _no response_ ### bazel version _no response_ ### gcc/compiler version _no response_ ### cuda/cudnn version _no response_ ### gpu model and memory _no response_ ### current behavior? tensorflow crashes with a fatal assertion failure when using with a multi-element step tensor. the crash occurs in the internal tensor validation code with the error ""must have a one element tensor"". when creating a model that includes a summary file writer and calling it with a multi-element step tensor (e.g., ), tensorflow crashes with a fatal assertion failure. the crash occurs during tensor validation, indicating that a single-element tensor was expected but a multi-element tensor was provided. ### standalone code to reproduce the issue ### relevant log output",[],['BUG'],"['type:bug', 'comp:ops', 'TF 2.19']",github,2026-01-22T12:03:41Z,,"tensorflow crash with multi-element step tensor in summary writer ### issue type bug ### have you reproduced the bug with tensorflow nightly? yes ### source source ### tensorflow version tf 2.21.0 ### custom code yes ### os platform and distribution _no response_ ### mobile device _no response_ ### python version _no response_ ### bazel version _no response_ ### gcc/compiler version _no response_ ### cuda/cudnn version _no response_ ### gpu model and memory _no response_ ### current behavior? tensorflow crashes with a fatal assertion failure when using with a multi-element step tensor. the crash occurs in the internal tensor validation code with the error ""must have a one element tensor"". when creating a model that includes a summary file writer and calling it with a multi-element step tensor (e.g., ), tensorflow crashes with a fatal assertion failure. the crash occurs during tensor validation, indicating that a single-element tensor was expected but a multi-element tensor was provided. ### standalone code to reproduce the issue ### relevant log output",6.4,Critical,1.0,crash-like behavior
tensorflow/tensorflow#108660,xla jit compilation fails with using bicubic method,"### issue type bug ### have you reproduced the bug with tensorflow nightly? yes ### source source ### tensorflow version tf 2.21.0 ### custom code yes ### os platform and distribution _no response_ ### mobile device _no response_ ### python version _no response_ ### bazel version _no response_ ### gcc/compiler version _no response_ ### cuda/cudnn version _no response_ ### gpu model and memory _no response_ ### current behavior? when using to enable xla jit compilation, any function containing operations with the interpolation method fails to compile. the error occurs because xla does not support the operation on xla_cpu_jit devices. the code works correctly without , but fails when xla compilation is enabled. ### standalone code to reproduce the issue ### relevant log output",[],['BUG'],['type:bug'],github,2026-01-22T12:24:20Z,,"xla jit compilation fails with using bicubic method ### issue type bug ### have you reproduced the bug with tensorflow nightly? yes ### source source ### tensorflow version tf 2.21.0 ### custom code yes ### os platform and distribution _no response_ ### mobile device _no response_ ### python version _no response_ ### bazel version _no response_ ### gcc/compiler version _no response_ ### cuda/cudnn version _no response_ ### gpu model and memory _no response_ ### current behavior? when using to enable xla jit compilation, any function containing operations with the interpolation method fails to compile. the error occurs because xla does not support the operation on xla_cpu_jit devices. the code works correctly without , but fails when xla compilation is enabled. ### standalone code to reproduce the issue ### relevant log output",5.6,Critical,1.0,system-wide impact
cockroachdb/cockroach#161607,roachtest: jepsen/sets/parts-start-kill-2 failed,roachtest.jepsen/sets/parts-start-kill-2 [failed]( with [artifacts]( on release-25.4 @ [4206b9fa19a55fc194880be412d77145a2c802ad]( parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - encrypted=false - fs=ext4 - localssd=true - metamorphicleases=default - metamorphicwritebuffering=true - runtimeassertionsbuild=false - ssd=0 help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-58990,[],['TESTING'],"['C-test-failure', 'O-robot', 'O-roachtest', 'T-testeng', 'X-infra-flake', 'branch-release-25.4']",github,2026-01-22T12:27:13Z,2026-01-22T16:10:43Z,roachtest: jepsen/sets/parts-start-kill-2 failed roachtest.jepsen/sets/parts-start-kill-2 [failed]( with [artifacts]( on release-25.4 @ [4206b9fa19a55fc194880be412d77145a2c802ad]( parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - encrypted=false - fs=ext4 - localssd=true - metamorphicleases=default - metamorphicwritebuffering=true - runtimeassertionsbuild=false - ssd=0 help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-58990,3.009,High,0.904,functional impact
tensorflow/tensorflow#108661,tensorflow crash with integer overflow in sparse tensor concatenation,### issue type bug ### have you reproduced the bug with tensorflow nightly? yes ### source source ### tensorflow version tf 2.21.0 ### custom code yes ### os platform and distribution _no response_ ### mobile device _no response_ ### python version _no response_ ### bazel version _no response_ ### gcc/compiler version _no response_ ### cuda/cudnn version _no response_ ### gpu model and memory _no response_ ### current behavior? tensorflow crashes with a fatal assertion failure when using to concatenate sparse tensors containing extremely large dimension values in their shape specifications. the crash occurs during internal tensor validation when processing shape values that approach or exceed system limits. ### standalone code to reproduce the issue ### relevant log output,[],['BUG'],"['type:bug', 'TF 2.19']",github,2026-01-22T12:38:33Z,,tensorflow crash with integer overflow in sparse tensor concatenation ### issue type bug ### have you reproduced the bug with tensorflow nightly? yes ### source source ### tensorflow version tf 2.21.0 ### custom code yes ### os platform and distribution _no response_ ### mobile device _no response_ ### python version _no response_ ### bazel version _no response_ ### gcc/compiler version _no response_ ### cuda/cudnn version _no response_ ### gpu model and memory _no response_ ### current behavior? tensorflow crashes with a fatal assertion failure when using to concatenate sparse tensors containing extremely large dimension values in their shape specifications. the crash occurs during internal tensor validation when processing shape values that approach or exceed system limits. ### standalone code to reproduce the issue ### relevant log output,6.4,Critical,1.0,crash-like behavior
envoyproxy/envoy#43113,local_ratelimit: expose descriptor identification for overlapping descriptors,"### description when using the local rate limit filter with multiple overlapping descriptors, there's no way to identify which descriptor caused the rate limit or which descriptors consumed tokens. ### example configuration with overlapping path-based rate limits: a request to matches **all three descriptors** and consumes tokens from all buckets. when a 429 is returned, operators cannot identify which bucket was exhausted. ### proposal add the ability to assign a to each descriptor and expose it via: - response headers (similar to ) - dynamic metadata (for wasm plugins, access logs, custom metrics) example configuration: this would enable operators to: - debug which bucket caused rate limiting - create per-bucket metrics and alerts - implement bucket-specific retry strategies i'm happy to work on an implementation once the approach is agreed upon.",[],['FEATURE'],"['enhancement', 'area/local_ratelimit']",github,2026-01-22T13:04:45Z,,"local_ratelimit: expose descriptor identification for overlapping descriptors ### description when using the local rate limit filter with multiple overlapping descriptors, there's no way to identify which descriptor caused the rate limit or which descriptors consumed tokens. ### example configuration with overlapping path-based rate limits: a request to matches **all three descriptors** and consumes tokens from all buckets. when a 429 is returned, operators cannot identify which bucket was exhausted. ### proposal add the ability to assign a to each descriptor and expose it via: - response headers (similar to ) - dynamic metadata (for wasm plugins, access logs, custom metrics) example configuration: this would enable operators to: - debug which bucket caused rate limiting - create per-bucket metrics and alerts - implement bucket-specific retry strategies i'm happy to work on an implementation once the approach is agreed upon.",1.4,Low,0.538,localized low-impact
python/cpython#144148,soft deprecate urllib.parse.urlparse(),"is similar to , but additionally it splits on and using the "";"" separator. this is wrong. first, the corresponding rfcs ([rfc 3986]( [rfc 8820]( do not list ""params"" as a separate uri component. second, the "";"" separator is used to specify parameters for a path segment, not the whole path. and this is scheme-specific, most schemes don't use it or use "","" for similar purpose. it is mentioned in rfc 3986, but not in rfc 8820. so, it is better to use and then parse if needed. i afraid that most users of are not aware of this, and only use it because it have more attractive name than . we should clearly document as obsolete. cc",[],['DOCUMENTATION'],"['docs', '3.15']",github,2026-01-22T13:07:49Z,,"soft deprecate urllib.parse.urlparse() is similar to , but additionally it splits on and using the "";"" separator. this is wrong. first, the corresponding rfcs ([rfc 3986]( [rfc 8820]( do not list ""params"" as a separate uri component. second, the "";"" separator is used to specify parameters for a path segment, not the whole path. and this is scheme-specific, most schemes don't use it or use "","" for similar purpose. it is mentioned in rfc 3986, but not in rfc 8820. so, it is better to use and then parse if needed. i afraid that most users of are not aware of this, and only use it because it have more attractive name than . we should clearly document as obsolete. cc",1.2,Low,0.493,localized low-impact
cilium/cilium#43929,lrp backend is not selected when pod container ports are not specified,"when applying the following manifests, traffic to the service clusterip fails from nodes that have a local backend pod, even though a matching local endpoint exists and ciliumlocalredirectpolicy is configured. the issue occurs only when the pod does not specify . this manifest worked correctly in v1.17 and earlier, but stopped working in v1.18+",[],['BUG'],"['kind/bug', 'area/datapath', 'kind/regression', 'area/lrp', 'affects/v1.18', 'affects/v1.19']",github,2026-01-22T13:19:35Z,,"lrp backend is not selected when pod container ports are not specified when applying the following manifests, traffic to the service clusterip fails from nodes that have a local backend pod, even though a matching local endpoint exists and ciliumlocalredirectpolicy is configured. the issue occurs only when the pod does not specify . this manifest worked correctly in v1.17 and earlier, but stopped working in v1.18+",2.463,Medium,0.78,functional impact
docker/docker#51890,docker network inspect ipam config with empty iprange,"### description in newer versions of docker (i believe >= 29.0.0), will contain empty values in the ipam configuration, e.g. earlier versions would not have . looking at the definition, is specified: i think this was introduced with i presume the same bug exists for subnet and gateway. perhaps the issue is that these new types are not recognized as empty by the json library even if they are empty. it could also be that they [are always assigned]( i am not familiar with go, so this is just guesswork. this is affecting configuration management: ### reproduce docker network inspect ### expected behavior omitempty ### docker version ### docker info ### additional info _no response_",[],['BUG'],"['status/0-triage', 'kind/bug']",github,2026-01-22T13:22:46Z,2026-01-26T16:52:29Z,"docker network inspect ipam config with empty iprange ### description in newer versions of docker (i believe >= 29.0.0), will contain empty values in the ipam configuration, e.g. earlier versions would not have . looking at the definition, is specified: i think this was introduced with i presume the same bug exists for subnet and gateway. perhaps the issue is that these new types are not recognized as empty by the json library even if they are empty. it could also be that they [are always assigned]( i am not familiar with go, so this is just guesswork. this is affecting configuration management: ### reproduce docker network inspect ### expected behavior omitempty ### docker version ### docker info ### additional info _no response_",2.389,Medium,0.763,functional impact
pandas-dev/pandas#63810,doc: add py:type reference targets to each entry in pandas.api.typing.aliases,"### pandas version checks - [x] i have checked that the issue still exists on the latest versions of the docs on [here]( ### location of the documentation ### documentation problem trying to link to the type aliases with sphinx 9‚Äôs role doesn‚Äôt work. that‚Äôs because you don‚Äôt have them in your intersphinx inventory at all. in you added (failing) [*references*]( to them instead of using directives to define [*reference targets*]( for them: ### suggested fix for documentation this should instead be something like: or, to insert docstrings automatically: also that page uses instead of for similar effect.",[],['DOCUMENTATION'],"['Docs', 'good first issue']",github,2026-01-22T13:26:52Z,,"doc: add py:type reference targets to each entry in pandas.api.typing.aliases ### pandas version checks - [x] i have checked that the issue still exists on the latest versions of the docs on [here]( ### location of the documentation ### documentation problem trying to link to the type aliases with sphinx 9‚Äôs role doesn‚Äôt work. that‚Äôs because you don‚Äôt have them in your intersphinx inventory at all. in you added (failing) [*references*]( to them instead of using directives to define [*reference targets*]( for them: ### suggested fix for documentation this should instead be something like: or, to insert docstrings automatically: also that page uses instead of for similar effect.",1.2,Low,0.493,localized low-impact
istio/istio#58871,provide an option to disable tracing propagation only,"(this is used to request new product features, please visit < for questions on using istio) **describe the feature request** have a way to be able to disable context propagation, without affecting span reporting or other tracing instrumentation. in practice, for http traffic, this would mean ensuring that trace context headers (e.g. , for opentelemetry / w3c, or the headers for zipkin) would not be included when proxying requests through istio proxies. suggestion: have a field in [the tracing section of the telemetry config]( called , which is defaulted to . when set to the trace context is not propagated in forwarded requests (e.g. http headers are not included / removed from the request). **describe alternatives you've considered** currently the only way to prevent context propagation is to do the following: create a resource to select only the egress gateway proxies, and configure the tracing provider to , and configure a to perform [headeroperation]( to remove the relevant trace headers. the problem here is that the egress gateway now does not report spans - which is not good. note: when only using the to remove the trace headers (i.e. while still reporting spans from the egress gateway proxies), the trace context headers are still propagated. **affected product area (please put an x in all that apply)** [ ] ambient [x] docs (new features would need to be documented) [ ] dual stack [ ] installation [ ] networking [ ] performance and scalability [x] extensions and telemetry (it would be a tracing feature) [x] security (preventing leaking information to outside the mesh) [ ] test and release [x] user experience (improves the configurability of istio) [ ] developer infrastructure **affected features (please put an x in all that apply)** [ ] multi cluster [ ] virtual machine [ ] multi control plane **additional context** related config: - tracing providers (e.g. opentelemetry or zipkin) can be configured through [global mesh config]( - the [telemetry api]( is then used to configure telemetry for istio, allowing to select a provider, disable span reporting, etc. but the options available don't affect context propagation (even explicitly says it does not affect context propagation). example use case: - an istio service mesh exists, where all traffic routes through an egress gateway. - we don't want to leak information to 3rd parties by including trace context headers in the requests, but we want tracing configured for our service mesh. - therefore we need instrumentation & context propagation up to, and including, the egress gateway, but then context propagation to be removed before any egressing request is forwarded by the egress gateway proxy.",[],"['DOCUMENTATION', 'SECURITY', 'FEATURE']","['kind/enhancement', 'kind/docs', 'area/security', 'area/extensions and telemetry', 'area/user experience']",github,2026-01-22T13:42:39Z,,"provide an option to disable tracing propagation only (this is used to request new product features, please visit < for questions on using istio) **describe the feature request** have a way to be able to disable context propagation, without affecting span reporting or other tracing instrumentation. in practice, for http traffic, this would mean ensuring that trace context headers (e.g. , for opentelemetry / w3c, or the headers for zipkin) would not be included when proxying requests through istio proxies. suggestion: have a field in [the tracing section of the telemetry config]( called , which is defaulted to . when set to the trace context is not propagated in forwarded requests (e.g. http headers are not included / removed from the request). **describe alternatives you've considered** currently the only way to prevent context propagation is to do the following: create a resource to select only the egress gateway proxies, and configure the tracing provider to , and configure a to perform [headeroperation]( to remove the relevant trace headers. the problem here is that the egress gateway now does not report spans - which is not good. note: when only using the to remove the trace headers (i.e. while still reporting spans from the egress gateway proxies), the trace context headers are still propagated. **affected product area (please put an x in all that apply)** [ ] ambient [x] docs (new features would need to be documented) [ ] dual stack [ ] installation [ ] networking [ ] performance and scalability [x] extensions and telemetry (it would be a tracing feature) [x] security (preventing leaking information to outside the mesh) [ ] test and release [x] user experience (improves the configurability of istio) [ ] developer infrastructure **affected features (please put an x in all that apply)** [ ] multi cluster [ ] virtual machine [ ] multi control plane **additional context** related config: - tracing providers (e.g. opentelemetry or zipkin) can be configured through [global mesh config]( - the [telemetry api]( is then used to configure telemetry for istio, allowing to select a provider, disable span reporting, etc. but the options available don't affect context propagation (even explicitly says it does not affect context propagation). example use case: - an istio service mesh exists, where all traffic routes through an egress gateway. - we don't want to leak information to 3rd parties by including trace context headers in the requests, but we want tracing configured for our service mesh. - therefore we need instrumentation & context propagation up to, and including, the egress gateway, but then context propagation to be removed before any egressing request is forwarded by the egress gateway proxy.",3.289,High,0.967,security risk
tensorflow/tensorflow#108663,crashes ( ) due to integer overflow in shape calculation,"### issue type bug ### have you reproduced the bug with tensorflow nightly? no ### source source ### tensorflow version 2.20 ### custom code yes ### os platform and distribution linux ubuntu 24.04 ### mobile device _no response_ ### python version 3.12 ### bazel version _no response_ ### gcc/compiler version _no response_ ### cuda/cudnn version _no response_ ### gpu model and memory _no response_ ### current behavior? when calling with a argument containing large dimensions, the total element count calculation overflows the 64-bit integer limit. this triggers a assertion in , resulting in a process abort ( ) rather than raising a python or . here is a [gist]( ### standalone code to reproduce the issue ### relevant log output",[],['BUG'],['type:bug'],github,2026-01-22T13:58:03Z,,"crashes ( ) due to integer overflow in shape calculation ### issue type bug ### have you reproduced the bug with tensorflow nightly? no ### source source ### tensorflow version 2.20 ### custom code yes ### os platform and distribution linux ubuntu 24.04 ### mobile device _no response_ ### python version 3.12 ### bazel version _no response_ ### gcc/compiler version _no response_ ### cuda/cudnn version _no response_ ### gpu model and memory _no response_ ### current behavior? when calling with a argument containing large dimensions, the total element count calculation overflows the 64-bit integer limit. this triggers a assertion in , resulting in a process abort ( ) rather than raising a python or . here is a [gist]( ### standalone code to reproduce the issue ### relevant log output",6.4,Critical,1.0,crash-like behavior
cockroachdb/cockroach#161609,roachtest: jepsen/multi-register/split failed,"**note:** this build has runtime assertions enabled. if the same failure was hit in a run without assertions enabled, there should be a similar failure without this message. if there isn& ;t one, then this failure is likely due to an assertion violation or (assertion) timeout. roachtest.jepsen/multi-register/split [failed]( with [artifacts]( on master @ [c6ec5849dfea61702bc94c5b794a7659c0e914f4]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21008313-1769066750-118-n6cpu4-0001 | 35.237.246.5 | 10.142.1.228 | | teamcity-21008313-1769066750-118-n6cpu4-0002 | 34.74.178.212 | 10.142.0.73 | | teamcity-21008313-1769066750-118-n6cpu4-0003 | 35.185.14.107 | 10.142.1.199 | | teamcity-21008313-1769066750-118-n6cpu4-0004 | 35.237.31.100 | 10.142.0.9 | | teamcity-21008313-1769066750-118-n6cpu4-0005 | 35.185.8.142 | 10.142.1.180 | | teamcity-21008313-1769066750-118-n6cpu4-0006 | 34.23.44.70 | 10.142.1.211 | parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - metamorphicleases=leader - runtimeassertionsbuild=true help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-58991",[],['TESTING'],"['C-test-failure', 'O-robot', 'O-roachtest', 'branch-master', 'T-testeng', 'X-infra-flake', 'B-runtime-assertions-enabled']",github,2026-01-22T14:07:31Z,2026-01-22T15:32:43Z,"roachtest: jepsen/multi-register/split failed **note:** this build has runtime assertions enabled. if the same failure was hit in a run without assertions enabled, there should be a similar failure without this message. if there isn& ;t one, then this failure is likely due to an assertion violation or (assertion) timeout. roachtest.jepsen/multi-register/split [failed]( with [artifacts]( on master @ [c6ec5849dfea61702bc94c5b794a7659c0e914f4]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21008313-1769066750-118-n6cpu4-0001 | 35.237.246.5 | 10.142.1.228 | | teamcity-21008313-1769066750-118-n6cpu4-0002 | 34.74.178.212 | 10.142.0.73 | | teamcity-21008313-1769066750-118-n6cpu4-0003 | 35.185.14.107 | 10.142.1.199 | | teamcity-21008313-1769066750-118-n6cpu4-0004 | 35.237.31.100 | 10.142.0.9 | | teamcity-21008313-1769066750-118-n6cpu4-0005 | 35.185.8.142 | 10.142.1.180 | | teamcity-21008313-1769066750-118-n6cpu4-0006 | 34.23.44.70 | 10.142.1.211 | parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - metamorphicleases=leader - runtimeassertionsbuild=true help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-58991",3.132,High,0.932,functional impact
rust-lang/rust#151492,llvm-pass + cause error,"### tl;dr: llvm-pass + error: (demangled symbol name: ) caused by: ... ### reproduce build something with _also, that interesting, expectably same build using doesn't cause error._ ü§∑üèª‚Äç‚ôÇÔ∏è ### meta :",[],['UI'],"['A-LLVM', 'T-compiler', 'C-discussion', 'A-compiler-builtins']",github,2026-01-22T14:12:35Z,2026-01-22T15:52:53Z,"llvm-pass + cause error ### tl;dr: llvm-pass + error: (demangled symbol name: ) caused by: ... ### reproduce build something with _also, that interesting, expectably same build using doesn't cause error._ ü§∑üèª‚Äç‚ôÇÔ∏è ### meta :",1.8,Low,0.629,user-visible issue
cockroachdb/cockroach#161611,roachtest: jepsen/register/start-stop-2 failed,roachtest.jepsen/register/start-stop-2 [failed]( with [artifacts]( on master @ [c6ec5849dfea61702bc94c5b794a7659c0e914f4]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21008313-1769066750-119-n6cpu4-0001 | 35.231.208.243 | 10.142.3.16 | | teamcity-21008313-1769066750-119-n6cpu4-0002 | 35.231.23.151 | 10.142.3.40 | | teamcity-21008313-1769066750-119-n6cpu4-0003 | 34.139.126.30 | 10.142.2.170 | | teamcity-21008313-1769066750-119-n6cpu4-0004 | 34.74.73.0 | 10.142.2.189 | | teamcity-21008313-1769066750-119-n6cpu4-0005 | 35.237.13.216 | 10.142.2.188 | | teamcity-21008313-1769066750-119-n6cpu4-0006 | 34.73.71.25 | 10.142.2.231 | parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - metamorphicleases=default - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( /cc /test-eng [this test on roachdash]( | [improve this report!](,[],['TESTING'],"['C-test-failure', 'O-robot', 'O-roachtest', 'branch-master', 'T-testeng', 'X-infra-flake']",github,2026-01-22T14:13:35Z,2026-01-22T15:33:15Z,roachtest: jepsen/register/start-stop-2 failed roachtest.jepsen/register/start-stop-2 [failed]( with [artifacts]( on master @ [c6ec5849dfea61702bc94c5b794a7659c0e914f4]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21008313-1769066750-119-n6cpu4-0001 | 35.231.208.243 | 10.142.3.16 | | teamcity-21008313-1769066750-119-n6cpu4-0002 | 35.231.23.151 | 10.142.3.40 | | teamcity-21008313-1769066750-119-n6cpu4-0003 | 34.139.126.30 | 10.142.2.170 | | teamcity-21008313-1769066750-119-n6cpu4-0004 | 34.74.73.0 | 10.142.2.189 | | teamcity-21008313-1769066750-119-n6cpu4-0005 | 35.237.13.216 | 10.142.2.188 | | teamcity-21008313-1769066750-119-n6cpu4-0006 | 34.73.71.25 | 10.142.2.231 | parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - metamorphicleases=default - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( /cc /test-eng [this test on roachdash]( | [improve this report!](,3.216,High,0.951,system-wide impact
tensorflow/tensorflow#108664,segfaults with negative and,### issue type bug ### have you reproduced the bug with tensorflow nightly? no ### source source ### tensorflow version 2.20 ### custom code yes ### os platform and distribution linux ubuntu 24.04 ### mobile device _no response_ ### python version 3.12 ### bazel version _no response_ ### gcc/compiler version _no response_ ### cuda/cudnn version _no response_ ### gpu model and memory _no response_ ### current behavior? i discovered a segmentation fault in when passing negative integer values for the and arguments. the operation does not appear to validate that these dimensions are positive before passing them to the underlying c++ kernel. this results in a crash ( ) rather than raising a proper or . here is a [gist]( ### standalone code to reproduce the issue ### relevant log output,[],['BUG'],"['type:bug', 'comp:ops', 'TF 2.19']",github,2026-01-22T14:18:31Z,,segfaults with negative and ### issue type bug ### have you reproduced the bug with tensorflow nightly? no ### source source ### tensorflow version 2.20 ### custom code yes ### os platform and distribution linux ubuntu 24.04 ### mobile device _no response_ ### python version 3.12 ### bazel version _no response_ ### gcc/compiler version _no response_ ### cuda/cudnn version _no response_ ### gpu model and memory _no response_ ### current behavior? i discovered a segmentation fault in when passing negative integer values for the and arguments. the operation does not appear to validate that these dimensions are positive before passing them to the underlying c++ kernel. this results in a crash ( ) rather than raising a proper or . here is a [gist]( ### standalone code to reproduce the issue ### relevant log output,6.4,Critical,1.0,crash-like behavior
flutter/flutter#181321,[a11y][android] checkbox and switch state changes are not announced on android 16 (sdk 36),"### steps to reproduce 1. build an app that features checkboxes and/or switches 2. run it on a device running android 16 (sdk 36) 3. enable talkback 4. observe the behavior of toggling checkboxes and switches ### expected results checkboxes should announce ""checked"" or ""unchecked"" when toggled. switches should announce ""on"" or ""off"" when toggled. ### actual results nothing is announced. on android 15 (sdk 35) and older versions, announcements work correctly. ### code sample code sample ### screenshots or video screenshots / video demonstration * video on an older version of android: * video on android 36: ### logs n/a ### flutter doctor output n/a, google internal client",[],['ACCESSIBILITY'],"['c: regression', 'platform-android', 'framework', 'engine', 'f: material design', 'a: accessibility', 'customer: google', 'e: OS-version specific', 'customer: huggsy (g3)', 'team-accessibility']",github,2026-01-22T14:20:12Z,,"[a11y][android] checkbox and switch state changes are not announced on android 16 (sdk 36) ### steps to reproduce 1. build an app that features checkboxes and/or switches 2. run it on a device running android 16 (sdk 36) 3. enable talkback 4. observe the behavior of toggling checkboxes and switches ### expected results checkboxes should announce ""checked"" or ""unchecked"" when toggled. switches should announce ""on"" or ""off"" when toggled. ### actual results nothing is announced. on android 15 (sdk 35) and older versions, announcements work correctly. ### code sample code sample ### screenshots or video screenshots / video demonstration * video on an older version of android: * video on android 36: ### logs n/a ### flutter doctor output n/a, google internal client",5.0,Critical,1.0,crash-like behavior
cockroachdb/cockroach#161612,sql/sqlstats/sslocal: testsqlstatsdiscardstatsonfingerprintlimit failed,sql/sqlstats/sslocal.testsqlstatsdiscardstatsonfingerprintlimit [failed]( on release-25.4.3-rc @ [71d853623f0d1f589fd5c727a1c4aec8a43e62e0]( parameters: - attempt=1 - race=true - run=3 - shard=1 help see also: [how to investigate a go test failure \(internal\)]( same failure on other branches - sql/sqlstats/sslocal: testsqlstatsdiscardstatsonfingerprintlimit failed [c-test-failure o-robot t-observability branch-master release-blocker] /cc /obs-prs [this test on roachdash]( | [improve this report!]( jira issue: crdb-58993,[],['TESTING'],"['C-test-failure', 'O-robot', 'T-observability', 'branch-release-25.4.3-rc']",github,2026-01-22T14:26:02Z,2026-01-27T16:42:09Z,sql/sqlstats/sslocal: testsqlstatsdiscardstatsonfingerprintlimit failed sql/sqlstats/sslocal.testsqlstatsdiscardstatsonfingerprintlimit [failed]( on release-25.4.3-rc @ [71d853623f0d1f589fd5c727a1c4aec8a43e62e0]( parameters: - attempt=1 - race=true - run=3 - shard=1 help see also: [how to investigate a go test failure \(internal\)]( same failure on other branches - sql/sqlstats/sslocal: testsqlstatsdiscardstatsonfingerprintlimit failed [c-test-failure o-robot t-observability branch-master release-blocker] /cc /obs-prs [this test on roachdash]( | [improve this report!]( jira issue: crdb-58993,1.6,Low,0.584,localized low-impact
openssl/openssl#29719,missing z=1 optimization in nist prime curve point-to-affine conversion,"## summary the optimized nist prime curve implementations ( , , , and ) are missing a simple but important optimization check when converting points from jacobian to affine coordinates. specifically, they fail to check if the z coordinate is already 1 before performing expensive field inversion and multiplication operations. ## affected functions - in - in - in - in ## description ### current behavior the current implementations of for all nist prime curves (p-224, p-256/secp256r1, p-384, and p-521) unconditionally perform expensive operations when converting from jacobian coordinates (x, y, z) to affine coordinates (x, y). for example, in lines 1936-1975: ### expected behavior the reference implementation in (lines 539-558) correctly handles the case where z = 1: when z = 1, the point is already in affine form, so the affine coordinates are simply (x, y) = (x, y), requiring no inversions or multiplications. ## proposed fix add a check for z = 1 at the beginning of each function, similar to the simple implementation. for p-256 specifically: the same fix should be applied to , , and . best regards,",[],['FEATURE'],"['branch: master', 'help wanted', 'triaged: feature']",github,2026-01-22T14:26:04Z,,"missing z=1 optimization in nist prime curve point-to-affine conversion ## summary the optimized nist prime curve implementations ( , , , and ) are missing a simple but important optimization check when converting points from jacobian to affine coordinates. specifically, they fail to check if the z coordinate is already 1 before performing expensive field inversion and multiplication operations. ## affected functions - in - in - in - in ## description ### current behavior the current implementations of for all nist prime curves (p-224, p-256/secp256r1, p-384, and p-521) unconditionally perform expensive operations when converting from jacobian coordinates (x, y, z) to affine coordinates (x, y). for example, in lines 1936-1975: ### expected behavior the reference implementation in (lines 539-558) correctly handles the case where z = 1: when z = 1, the point is already in affine form, so the affine coordinates are simply (x, y) = (x, y), requiring no inversions or multiplications. ## proposed fix add a check for z = 1 at the beginning of each function, similar to the simple implementation. for p-256 specifically: the same fix should be applied to , , and . best regards,",1.4,Low,0.538,localized low-impact
tensorflow/tensorflow#108665,causes segfault ( ) with large shape dimensions,"### issue type bug ### have you reproduced the bug with tensorflow nightly? no ### source source ### tensorflow version 2.20.0 ### custom code yes ### os platform and distribution linux ubuntu 24.04 ### mobile device _no response_ ### python version 3.12 ### bazel version _no response_ ### gcc/compiler version _no response_ ### cuda/cudnn version _no response_ ### gpu model and memory _no response_ ### current behavior? a segmentation fault occurs in t when provided with large shape dimensions (approx ~2.1 billion). the operation appears to suffer from an integer overflow or missing bounds check when calculating the required memory size against the provided file region, causing the python process to crash immediately instead of raising an or . here is a [gist]( ### standalone code to reproduce the issue ### relevant log output",[],['BUG'],"['type:bug', 'comp:ops', 'awaiting PR merge', 'TF 2.19']",github,2026-01-22T14:36:56Z,,"causes segfault ( ) with large shape dimensions ### issue type bug ### have you reproduced the bug with tensorflow nightly? no ### source source ### tensorflow version 2.20.0 ### custom code yes ### os platform and distribution linux ubuntu 24.04 ### mobile device _no response_ ### python version 3.12 ### bazel version _no response_ ### gcc/compiler version _no response_ ### cuda/cudnn version _no response_ ### gpu model and memory _no response_ ### current behavior? a segmentation fault occurs in t when provided with large shape dimensions (approx ~2.1 billion). the operation appears to suffer from an integer overflow or missing bounds check when calculating the required memory size against the provided file region, causing the python process to crash immediately instead of raising an or . here is a [gist]( ### standalone code to reproduce the issue ### relevant log output",6.4,Critical,1.0,crash-like behavior
pandas-dev/pandas#63811,doc: 3.0 release notes has an odd contributor issue,### pandas version checks - [x] i have checked that the issue still exists on the latest versions of the docs on [here]( ### location of the documentation ### documentation problem the 3.0 release notes has this: ### suggested fix for documentation not sure - i guess the script that creates the list of contributors has a bug in it?? for visibility,[],['DOCUMENTATION'],"['Docs', 'Needs Triage']",github,2026-01-22T14:54:13Z,2026-01-23T18:33:17Z,doc: 3.0 release notes has an odd contributor issue ### pandas version checks - [x] i have checked that the issue still exists on the latest versions of the docs on [here]( ### location of the documentation ### documentation problem the 3.0 release notes has this: ### suggested fix for documentation not sure - i guess the script that creates the list of contributors has a bug in it?? for visibility,1.2,Low,0.493,localized low-impact
cockroachdb/cockroach#161614,pkg/sql/isession/isession_test: testinternalsession failed,pkg/sql/isession/isession_test.testinternalsession [failed]( on release-26.1 @ [e39600cb891be2624d33bc7d171ada4629abdfdb]( parameters: - attempt=1 - race=true - run=3 - shard=1 help see also: [how to investigate a go test failure \(internal\)]( [this test on roachdash]( | [improve this report!]( jira issue: crdb-58995,[],['TESTING'],"['C-test-failure', 'O-robot', 'T-sql-queries', 'branch-release-26.1']",github,2026-01-22T15:15:53Z,,pkg/sql/isession/isession_test: testinternalsession failed pkg/sql/isession/isession_test.testinternalsession [failed]( on release-26.1 @ [e39600cb891be2624d33bc7d171ada4629abdfdb]( parameters: - attempt=1 - race=true - run=3 - shard=1 help see also: [how to investigate a go test failure \(internal\)]( [this test on roachdash]( | [improve this report!]( jira issue: crdb-58995,1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161615,sql: generic query plan cannot use geography inverted index for filtering,"we implemented to generate the inverted expression representing an inverted filter condition over the given geospatial index, but this doesn't include the case where there're only placeholders in the filter expression, thus limit its usage in generic query plan. this is similar to . the reproduction steps: jira issue: crdb-58996",[],['PERFORMANCE'],"['C-performance', 'A-sql-optimizer', 'branch-master', 'T-sql-queries', 'A-generic-query-plans']",github,2026-01-22T15:30:00Z,,"sql: generic query plan cannot use geography inverted index for filtering we implemented to generate the inverted expression representing an inverted filter condition over the given geospatial index, but this doesn't include the case where there're only placeholders in the filter expression, thus limit its usage in generic query plan. this is similar to . the reproduction steps: jira issue: crdb-58996",3.314,High,0.973,performance degradation
python/cpython#144156,missing 'linear-white-space' between encoded-word in rfc2047 header,"# bug report ### bug description: result when sending an email with this subject, yahoo rejects it with a soft-bounce in the last line of the subject, two encoded-words are appended together not complying to this part of the [rfc2047]( >ordinary ascii text and 'encoded-word's may appear together in the same header field. however, an 'encoded-word' that appears in a header field defined as '*text' must be separated from any adjacent 'encoded-word' or 'text' by 'linear-white-space'. also, i'm not sure that it has to create an encoded-word for this , but i'm a bit lost in the specs. it is highly dependent to the number of chars and where the chars that need encoding are placed, as the same subject which does not start with ""re: "" is fine i could reproduce this behavior on 3.13.x, 3.14.2, 3.15-dev apologies if there already is an issue for this, i found several issues on the topic, but could not find a matching one. ### cpython versions tested on: cpython main branch, 3.15 ### operating systems tested on: linux",[],['BUG'],"['type-bug', 'stdlib', 'topic-email']",github,2026-01-22T16:00:16Z,,"missing 'linear-white-space' between encoded-word in rfc2047 header # bug report ### bug description: result when sending an email with this subject, yahoo rejects it with a soft-bounce in the last line of the subject, two encoded-words are appended together not complying to this part of the [rfc2047]( >ordinary ascii text and 'encoded-word's may appear together in the same header field. however, an 'encoded-word' that appears in a header field defined as '*text' must be separated from any adjacent 'encoded-word' or 'text' by 'linear-white-space'. also, i'm not sure that it has to create an encoded-word for this , but i'm a bit lost in the specs. it is highly dependent to the number of chars and where the chars that need encoding are placed, as the same subject which does not start with ""re: "" is fine i could reproduce this behavior on 3.13.x, 3.14.2, 3.15-dev apologies if there already is an issue for this, i found several issues on the topic, but could not find a matching one. ### cpython versions tested on: cpython main branch, 3.15 ### operating systems tested on: linux",2.394,Medium,0.764,functional impact
cockroachdb/cockroach#161618,"sql, parser: foreign key referencing a hash index primary key fails when not explicitly providing columns","**describe the problem** when defining a foreign key which references a hash index primary key and the parent table columns are not explicitly provided, we encounter an error which calls out the internal virtual column used to support the hash index. if the visible primary key columns are explicitly provided when defining the foreign key, the operation will succeed. **to reproduce** this issue is reproduced with both the legacy and declarative schema changer. **expected behavior** defining a foreign key with an implicit reference to the parent key should behave the same way as explicitly providing the columns. jira issue: crdb-58997 epic crdb-58150",[],['BUG'],"['C-bug', 'A-sql-syntax', 'T-sql-foundations']",github,2026-01-22T16:06:28Z,,"sql, parser: foreign key referencing a hash index primary key fails when not explicitly providing columns **describe the problem** when defining a foreign key which references a hash index primary key and the parent table columns are not explicitly provided, we encounter an error which calls out the internal virtual column used to support the hash index. if the visible primary key columns are explicitly provided when defining the foreign key, the operation will succeed. **to reproduce** this issue is reproduced with both the legacy and declarative schema changer. **expected behavior** defining a foreign key with an implicit reference to the parent key should behave the same way as explicitly providing the columns. jira issue: crdb-58997 epic crdb-58150",4.6,Critical,1.0,crash-like behavior
python/cpython#144157,optimize bytes.translate by letting the compiler unroll the loop more usefully,"# feature or enhancement ### proposal: this is a simple tracking issue for an easy optimization in . ### has this already been discussed elsewhere? this is a minor feature, which does not need previous discussion elsewhere ### links to previous discussion of this feature: _no response_ ### linked prs * gh-144158",[],"['PERFORMANCE', 'FEATURE']","['type-feature', 'performance', 'interpreter-core']",github,2026-01-22T16:17:02Z,2026-01-22T17:21:42Z,"optimize bytes.translate by letting the compiler unroll the loop more usefully # feature or enhancement ### proposal: this is a simple tracking issue for an easy optimization in . ### has this already been discussed elsewhere? this is a minor feature, which does not need previous discussion elsewhere ### links to previous discussion of this feature: _no response_ ### linked prs * gh-144158",2.257,Medium,0.733,performance degradation
rust-lang/rust#151496,local could make use of local pages for types,"posting this for and big thanks for having spotted this issue real quick :) ### location (url) the same problem persists in many different pages, just to pick an example on my machine: ### summary the external link for is std $rustup_home/toolchains/nightly-aarch64-apple-darwin/share/doc/rust/html/std/option/enum.option.html`? a few things to be noted for this issue: 1. this issue should be resolved in a way that it doesn't break the online docs. 2. this does imply an implicit dependency between and which is not quite ready to handle, but since is mostly for in-house usage, i believe we can focus on redirecting the links for the time being. cc",[],['DOCUMENTATION'],"['T-rustdoc', 'A-docs']",github,2026-01-22T16:22:11Z,,"local could make use of local pages for types posting this for and big thanks for having spotted this issue real quick :) ### location (url) the same problem persists in many different pages, just to pick an example on my machine: ### summary the external link for is std $rustup_home/toolchains/nightly-aarch64-apple-darwin/share/doc/rust/html/std/option/enum.option.html`? a few things to be noted for this issue: 1. this issue should be resolved in a way that it doesn't break the online docs. 2. this does imply an implicit dependency between and which is not quite ready to handle, but since is mostly for in-house usage, i believe we can focus on redirecting the links for the time being. cc",1.2,Low,0.493,localized low-impact
kubernetes/kubernetes#136431,best-effort topolopgy mgr numanodeaffinity hint doesn't consider amount of devices per numa node,"### what happened? ### very simply put: given a 2 numa node server with 3 devices split across each node, (1 on node 0, 2 on node 1), is it possible to configure the topology manager to select the numa node in which 2/3 gpus presides when requesting a pod with all 3 devices ? ### context: in our current setup we would like to schedule numa localized pods in a best-effort manner, meaning pods can span multiple numa nodes without failing. we have a server with 128 cores, dual socket, configured as two numa nodes. this server has 3 gpus. we have written our own device manager plugin (as we want to enable vfio passthrough) and this plugin advertises which node each gpu is on. (0x10de is nvidia vendor) this setup works exactly as desired when requesting resources which fit into a single numa node. - example, deploy pod 2 gpus & 8 cpus plugin logs check cpusets, mem, hugepages allocated from numa node 1 üëç - however, when scheduling a pod with 3 gpus and 8 cpus plugin logs check cpuset, mem sets, hugepage allocation result: allocating from numa node 0 when only 1/3 gpus resides in this node. üëé ### further notes seems the numanodeaffinity bitmask calculation does not seem to consider preferences in this context and just returns 3 ### kubelet configuration ### what did you expect to happen? ideally prefer numa node with most devices ### how can we reproduce it (as minimally and precisely as possible)? 1. create garaunteed pod with 3 nvidia gpus on 2 numa node system, likely where 1 is on node 0 and 2 are on node 1 2. observe where resources are allocated ### anything else we need to know? can provide device plugin code snippets upon request, however i think the logic driving this decision presides with the topology manager. ### kubernetes version ### cloud provider bare metal ### os version ### install tools kubeadm ### container runtime (cri) and version (if applicable) containerd://1.7.24 ### related plugins (cni, csi, ...) and versions (if applicable) n/a",[],['BUG'],"['kind/bug', 'sig/node', 'needs-triage']",github,2026-01-22T16:28:26Z,,"best-effort topolopgy mgr numanodeaffinity hint doesn't consider amount of devices per numa node ### what happened? ### very simply put: given a 2 numa node server with 3 devices split across each node, (1 on node 0, 2 on node 1), is it possible to configure the topology manager to select the numa node in which 2/3 gpus presides when requesting a pod with all 3 devices ? ### context: in our current setup we would like to schedule numa localized pods in a best-effort manner, meaning pods can span multiple numa nodes without failing. we have a server with 128 cores, dual socket, configured as two numa nodes. this server has 3 gpus. we have written our own device manager plugin (as we want to enable vfio passthrough) and this plugin advertises which node each gpu is on. (0x10de is nvidia vendor) this setup works exactly as desired when requesting resources which fit into a single numa node. - example, deploy pod 2 gpus & 8 cpus plugin logs check cpusets, mem, hugepages allocated from numa node 1 üëç - however, when scheduling a pod with 3 gpus and 8 cpus plugin logs check cpuset, mem sets, hugepage allocation result: allocating from numa node 0 when only 1/3 gpus resides in this node. üëé ### further notes seems the numanodeaffinity bitmask calculation does not seem to consider preferences in this context and just returns 3 ### kubelet configuration ### what did you expect to happen? ideally prefer numa node with most devices ### how can we reproduce it (as minimally and precisely as possible)? 1. create garaunteed pod with 3 nvidia gpus on 2 numa node system, likely where 1 is on node 0 and 2 are on node 1 2. observe where resources are allocated ### anything else we need to know? can provide device plugin code snippets upon request, however i think the logic driving this decision presides with the topology manager. ### kubernetes version ### cloud provider bare metal ### os version ### install tools kubeadm ### container runtime (cri) and version (if applicable) containerd://1.7.24 ### related plugins (cni, csi, ...) and versions (if applicable) n/a",5.6,Critical,1.0,system-wide impact
envoyproxy/envoy#43116,active health checks fail when using upstream tls with client certificate,"**description:** _prerequisites:_ - the cluster has a tls transport socket configured for upstream connections. - the tls transport socket references a client certificate via sds. - the cluster has active health checking enabled. all of these must be true for the bug to happen. if removing any of them, the problem goes away. control plane ([contour]( uses basic xds with sotw updates (separate grpc streams for each resource type). _the problem:_ the upstream service is unavailable for certain period of time: - **scenario 1:** the client gets a ""no healthy upstream"" error for 4 minutes after envoy restarts - this is counted from the moment envoy is back up and ready with all configuration from xds. it doesn't include the time envoy takes to be ready serve requests. - the downtime lasts for * (for example: 4 * 60s = 4min). - **scenario 2:** the client gets a ""no healthy upstream"" error for several seconds after rotating the envoy client certificate. - the downtime lasts for * seconds (for example: 4 * 5s = 20s). i don't know exactly which envoy versions are affected, but this isn't just a problem in the newest code. it's not specific to any recent version. i do not have a complete understanding of the root cause yet, but the issue appears to be related to the order in which xds resources are delivered and processed. envoy handles the request forwarding correctly regardless the delivery order. however, active health checks fail if the sds secret isn't ready when the cluster and endpoints are created. this happens even if the secrets arrive just a few milliseconds later. it seems there isn't any special retry logic for this. instead, it just acts like the upstream host is actually down. my current theory is as follows: **scenario 1:** 4 minutes of unavailability after envoy restart envoy is restarted and it connects to the xds server to fetch its configuration: 1. cluster is created via cds with: - active health checking enabled - tls transport socket with sds-referenced client certificate 2. cluster endpoints are added via eds 3. health checker is started - endpoints are marked unhealthy because the sds secret is not yet available 4. sds discovery request is sent by envoy and the secret is delivered by the control plane 5. cluster enters warming state and it starts sending health check probes - upstream respond with success and probes succeed - upstream remains unhealthy until * (for example: 4 * 60s = 4min) 6. cluster becomes healthy and client requests start to succeed during this time (about 4 minutes) traffic is rejected with ""no healthy upstream"". note: if triggering a cluster update during this period, then upstream becomes available again immediately. **scenario 2:** 20 seconds of unavailability after client certificate rotation the cluster is updated with a new sds secret reference due to certificate rotation: 1. cluster is updated via cds with - active health checking enabled - tls transport socket with new sds reference 2. old cluster is removed and new cluster is created 3. there is no eds update since endpoints remain the same 4. client requests still succeed for a while (~6 seconds but it varies) 5. health checker for new cluster is started - endpoints are marked unhealthy because the sds secret is not yet available - client requests start to fail with ""no healthy upstream"" 6. new sds secret is delivered 7. cluster re-enters warming state and it starts sending health check probes - upstream respond with success and probes succeed - upstream remains unhealthy until * (for example: 4 * 5s = 20s) 8. cluster becomes healthy and client requests start to succeed during this time (about 20 seconds) traffic is rejected with ""no healthy upstream"". **repro steps:** it's hard to provide steps to reproduce this, as it requires xds server code. you can find the steps for contour here: **admin and stats output:** stats are included in attached logs. **config:** see the xds grpc captures attached below. **logs:** i'm attaching grpc captures taken from control plane side of the xds connection (contour) using my own capture tool. they can be viewed just as plain json file or with the browser based [grpc-json-sniffer]( app. * [troubleshooting-grpc-capture-certificate-rotation.json]( xds message dump, taken from certificate rotation test (traffic disturbance for 20 seconds) * [troubleshooting-logs-certificate-rotation.txt]( logs, stats etc, taken from certificate rotation test (which caused traffic disturbance) * [troubleshooting-grpc-capture-envoy-restart.json]( xds message dump taken after envoy restart (traffic disturbance for 4 minutes after full configuration has been received by envoy) * [troubleshooting-grpc-capture-initial- xds message dump from initial/successful cluster creation (no disturbance) * [troubleshooting-envoy-restart.txt]( logs (no stats) from envoy restart (which caused traffic disturbance). these logs have some additional debug prints i've added (lines with prefix)",[],['BUG'],"['bug', 'area/tls', 'area/sds']",github,2026-01-22T16:30:02Z,,"active health checks fail when using upstream tls with client certificate **description:** _prerequisites:_ - the cluster has a tls transport socket configured for upstream connections. - the tls transport socket references a client certificate via sds. - the cluster has active health checking enabled. all of these must be true for the bug to happen. if removing any of them, the problem goes away. control plane ([contour]( uses basic xds with sotw updates (separate grpc streams for each resource type). _the problem:_ the upstream service is unavailable for certain period of time: - **scenario 1:** the client gets a ""no healthy upstream"" error for 4 minutes after envoy restarts - this is counted from the moment envoy is back up and ready with all configuration from xds. it doesn't include the time envoy takes to be ready serve requests. - the downtime lasts for * (for example: 4 * 60s = 4min). - **scenario 2:** the client gets a ""no healthy upstream"" error for several seconds after rotating the envoy client certificate. - the downtime lasts for * seconds (for example: 4 * 5s = 20s). i don't know exactly which envoy versions are affected, but this isn't just a problem in the newest code. it's not specific to any recent version. i do not have a complete understanding of the root cause yet, but the issue appears to be related to the order in which xds resources are delivered and processed. envoy handles the request forwarding correctly regardless the delivery order. however, active health checks fail if the sds secret isn't ready when the cluster and endpoints are created. this happens even if the secrets arrive just a few milliseconds later. it seems there isn't any special retry logic for this. instead, it just acts like the upstream host is actually down. my current theory is as follows: **scenario 1:** 4 minutes of unavailability after envoy restart envoy is restarted and it connects to the xds server to fetch its configuration: 1. cluster is created via cds with: - active health checking enabled - tls transport socket with sds-referenced client certificate 2. cluster endpoints are added via eds 3. health checker is started - endpoints are marked unhealthy because the sds secret is not yet available 4. sds discovery request is sent by envoy and the secret is delivered by the control plane 5. cluster enters warming state and it starts sending health check probes - upstream respond with success and probes succeed - upstream remains unhealthy until * (for example: 4 * 60s = 4min) 6. cluster becomes healthy and client requests start to succeed during this time (about 4 minutes) traffic is rejected with ""no healthy upstream"". note: if triggering a cluster update during this period, then upstream becomes available again immediately. **scenario 2:** 20 seconds of unavailability after client certificate rotation the cluster is updated with a new sds secret reference due to certificate rotation: 1. cluster is updated via cds with - active health checking enabled - tls transport socket with new sds reference 2. old cluster is removed and new cluster is created 3. there is no eds update since endpoints remain the same 4. client requests still succeed for a while (~6 seconds but it varies) 5. health checker for new cluster is started - endpoints are marked unhealthy because the sds secret is not yet available - client requests start to fail with ""no healthy upstream"" 6. new sds secret is delivered 7. cluster re-enters warming state and it starts sending health check probes - upstream respond with success and probes succeed - upstream remains unhealthy until * (for example: 4 * 5s = 20s) 8. cluster becomes healthy and client requests start to succeed during this time (about 20 seconds) traffic is rejected with ""no healthy upstream"". **repro steps:** it's hard to provide steps to reproduce this, as it requires xds server code. you can find the steps for contour here: **admin and stats output:** stats are included in attached logs. **config:** see the xds grpc captures attached below. **logs:** i'm attaching grpc captures taken from control plane side of the xds connection (contour) using my own capture tool. they can be viewed just as plain json file or with the browser based [grpc-json-sniffer]( app. * [troubleshooting-grpc-capture-certificate-rotation.json]( xds message dump, taken from certificate rotation test (traffic disturbance for 20 seconds) * [troubleshooting-logs-certificate-rotation.txt]( logs, stats etc, taken from certificate rotation test (which caused traffic disturbance) * [troubleshooting-grpc-capture-envoy-restart.json]( xds message dump taken after envoy restart (traffic disturbance for 4 minutes after full configuration has been received by envoy) * [troubleshooting-grpc-capture-initial- xds message dump from initial/successful cluster creation (no disturbance) * [troubleshooting-envoy-restart.txt]( logs (no stats) from envoy restart (which caused traffic disturbance). these logs have some additional debug prints i've added (lines with prefix)",4.2,Critical,1.0,system-wide impact
cockroachdb/cockroach#161624,sentry: ordering.go:507: no output column equivalent to 0 (1) assertion failure wraps: (2) attached stack trace -- stack trace: | github.com/cockroachdb/cockroach/pkg/sql/opt/ordering.finalizeprov...,this issue was auto filed by sentry. it represents a crash or reported error on a live cluster with telemetry enabled. sentry link: [ panic message: stacktrace (expand for inline code snippets): [src/runtime/asm_arm64.s#l1222-l1224](src/runtime/asm_arm64.s#l1222-l1224) [pkg/sql/pgwire/server.go#l1222-l1224](pkg/sql/pgwire/server.go#l1222-l1224) [pkg/sql/pgwire/conn.go#l252-l254](pkg/sql/pgwire/conn.go#l252-l254) [pkg/sql/conn_executor.go#l1055-l1057](pkg/sql/conn_executor.go#l1055-l1057) [pkg/sql/conn_executor.go#l2277-l2279](pkg/sql/conn_executor.go#l2277-l2279) [pkg/sql/conn_executor.go#l2501-l2503](pkg/sql/conn_executor.go#l2501-l2503) [pkg/sql/conn_executor_prepare.go#l76-l78](pkg/sql/conn_executor_prepare.go#l76-l78) [pkg/sql/conn_executor_prepare.go#l115-l117](pkg/sql/conn_executor_prepare.go#l115-l117) [pkg/sql/conn_executor_prepare.go#l257-l259](pkg/sql/conn_executor_prepare.go#l257-l259) [pkg/sql/conn_executor_prepare.go#l252-l254](pkg/sql/conn_executor_prepare.go#l252-l254) [pkg/sql/conn_executor_prepare.go#l312-l314](pkg/sql/conn_executor_prepare.go#l312-l314) [pkg/sql/plan_opt.go#l173-l175](pkg/sql/plan_opt.go#l173-l175) [pkg/sql/plan_opt.go#l529-l531](pkg/sql/plan_opt.go#l529-l531) [pkg/sql/opt/xform/optimizer.go#l284-l286](pkg/sql/opt/xform/optimizer.go#l284-l286) [pkg/sql/opt/xform/optimizer.go#l846-l848](pkg/sql/opt/xform/optimizer.go#l846-l848) [pkg/sql/opt/xform/optimizer.go#l846-l848](pkg/sql/opt/xform/optimizer.go#l846-l848) [pkg/sql/opt/xform/optimizer.go#l859-l861](pkg/sql/opt/xform/optimizer.go#l859-l861) [pkg/sql/opt/ordering/ordering.go#l124-l126](pkg/sql/opt/ordering/ordering.go#l124-l126) [pkg/sql/opt/ordering/ordering.go#l506-l508](pkg/sql/opt/ordering/ordering.go#l506-l508) ### tags | tag | value | | --- | --- | | command | start-single-node | | environment | v25.3.2 | | go version | go1.23.12 x:nocoverageredesign | | platform | linux arm64 | | distribution | ccl | | cockroach release | v25.3.2 | | cockroach sha | a5e060ee11ad11e1649e7569486b579e9da8c8d7 | | # of cpus | 16 | | # of goroutines | 592 | jira issue: crdb-58998,[],['BUG'],"['C-bug', 'O-sentry', 'X-blathers-triaged', 'T-sql-queries', 'branch-release-25.3']",github,2026-01-22T16:51:03Z,2026-01-23T04:17:51Z,sentry: ordering.go:507: no output column equivalent to 0 (1) assertion failure wraps: (2) attached stack trace -- stack trace: | github.com/cockroachdb/cockroach/pkg/sql/opt/ordering.finalizeprov... this issue was auto filed by sentry. it represents a crash or reported error on a live cluster with telemetry enabled. sentry link: [ panic message: stacktrace (expand for inline code snippets): [src/runtime/asm_arm64.s#l1222-l1224](src/runtime/asm_arm64.s#l1222-l1224) [pkg/sql/pgwire/server.go#l1222-l1224](pkg/sql/pgwire/server.go#l1222-l1224) [pkg/sql/pgwire/conn.go#l252-l254](pkg/sql/pgwire/conn.go#l252-l254) [pkg/sql/conn_executor.go#l1055-l1057](pkg/sql/conn_executor.go#l1055-l1057) [pkg/sql/conn_executor.go#l2277-l2279](pkg/sql/conn_executor.go#l2277-l2279) [pkg/sql/conn_executor.go#l2501-l2503](pkg/sql/conn_executor.go#l2501-l2503) [pkg/sql/conn_executor_prepare.go#l76-l78](pkg/sql/conn_executor_prepare.go#l76-l78) [pkg/sql/conn_executor_prepare.go#l115-l117](pkg/sql/conn_executor_prepare.go#l115-l117) [pkg/sql/conn_executor_prepare.go#l257-l259](pkg/sql/conn_executor_prepare.go#l257-l259) [pkg/sql/conn_executor_prepare.go#l252-l254](pkg/sql/conn_executor_prepare.go#l252-l254) [pkg/sql/conn_executor_prepare.go#l312-l314](pkg/sql/conn_executor_prepare.go#l312-l314) [pkg/sql/plan_opt.go#l173-l175](pkg/sql/plan_opt.go#l173-l175) [pkg/sql/plan_opt.go#l529-l531](pkg/sql/plan_opt.go#l529-l531) [pkg/sql/opt/xform/optimizer.go#l284-l286](pkg/sql/opt/xform/optimizer.go#l284-l286) [pkg/sql/opt/xform/optimizer.go#l846-l848](pkg/sql/opt/xform/optimizer.go#l846-l848) [pkg/sql/opt/xform/optimizer.go#l846-l848](pkg/sql/opt/xform/optimizer.go#l846-l848) [pkg/sql/opt/xform/optimizer.go#l859-l861](pkg/sql/opt/xform/optimizer.go#l859-l861) [pkg/sql/opt/ordering/ordering.go#l124-l126](pkg/sql/opt/ordering/ordering.go#l124-l126) [pkg/sql/opt/ordering/ordering.go#l506-l508](pkg/sql/opt/ordering/ordering.go#l506-l508) ### tags | tag | value | | --- | --- | | command | start-single-node | | environment | v25.3.2 | | go version | go1.23.12 x:nocoverageredesign | | platform | linux arm64 | | distribution | ccl | | cockroach release | v25.3.2 | | cockroach sha | a5e060ee11ad11e1649e7569486b579e9da8c8d7 | | # of cpus | 16 | | # of goroutines | 592 | jira issue: crdb-58998,6.0,Critical,1.0,crash-like behavior
cockroachdb/cockroach#161625,sentry: catalog_query.go:76: unexpected catalog key √ó (1) wraps: (2) wraps: (3) assertion failure wraps: (4) attached stack trace -- stack trace: | github.com/cockroachdb/cockroach/pkg/sql/catalo...,this issue was auto filed by sentry. it represents a crash or reported error on a live cluster with telemetry enabled. sentry link: [ panic message: stacktrace (expand for inline code snippets): [src/runtime/asm_amd64.s#l1694-l1696](src/runtime/asm_amd64.s#l1694-l1696) [pkg/util/stop/stopper.go#l497-l499](pkg/util/stop/stopper.go#l497-l499) [pkg/sql/virtual_table.go#l138-l140](pkg/sql/virtual_table.go#l138-l140) [pkg/sql/virtual_table.go#l137-l139](pkg/sql/virtual_table.go#l137-l139) [pkg/sql/virtual_schema.go#l656-l658](pkg/sql/virtual_schema.go#l656-l658) [pkg/sql/pg_catalog.go#l1178-l1180](pkg/sql/pg_catalog.go#l1178-l1180) [pkg/sql/information_schema.go#l2648-l2650](pkg/sql/information_schema.go#l2648-l2650) [pkg/sql/catalog/descs/collection.go#l1069-l1071](pkg/sql/catalog/descs/collection.go#l1069-l1071) [pkg/sql/catalog/descs/collection.go#l885-l887](pkg/sql/catalog/descs/collection.go#l885-l887) [pkg/sql/catalog/descs/collection.go#l1042-l1044](pkg/sql/catalog/descs/collection.go#l1042-l1044) [pkg/sql/catalog/descs/descriptor.go#l201-l203](pkg/sql/catalog/descs/descriptor.go#l201-l203) [pkg/sql/catalog/internal/catkv/catalog_reader_cached.go#l356-l358](pkg/sql/catalog/internal/catkv/catalog_reader_cached.go#l356-l358) [pkg/sql/catalog/internal/catkv/catalog_reader.go#l355-l357](pkg/sql/catalog/internal/catkv/catalog_reader.go#l355-l357) [pkg/sql/catalog/internal/catkv/catalog_query.go#l75-l77](pkg/sql/catalog/internal/catkv/catalog_query.go#l75-l77) ### tags | tag | value | | --- | --- | | command | start-single-node | | environment | v24.3.3 | | go version | go1.22.8 x:nocoverageredesign | | platform | linux amd64 | | distribution | ccl | | cockroach release | v24.3.3 | | cockroach sha | 343b4202e553d58a76a36c60018588615bd4c30a | | # of cpus | 24 | | # of goroutines | 1226 | jira issue: crdb-58999,[],['BUG'],"['C-bug', 'O-sentry', 'X-blathers-triaged', 'T-sql-foundations', 'branch-release-24.3']",github,2026-01-22T17:16:53Z,2026-01-27T15:07:05Z,sentry: catalog_query.go:76: unexpected catalog key √ó (1) wraps: (2) wraps: (3) assertion failure wraps: (4) attached stack trace -- stack trace: | github.com/cockroachdb/cockroach/pkg/sql/catalo... this issue was auto filed by sentry. it represents a crash or reported error on a live cluster with telemetry enabled. sentry link: [ panic message: stacktrace (expand for inline code snippets): [src/runtime/asm_amd64.s#l1694-l1696](src/runtime/asm_amd64.s#l1694-l1696) [pkg/util/stop/stopper.go#l497-l499](pkg/util/stop/stopper.go#l497-l499) [pkg/sql/virtual_table.go#l138-l140](pkg/sql/virtual_table.go#l138-l140) [pkg/sql/virtual_table.go#l137-l139](pkg/sql/virtual_table.go#l137-l139) [pkg/sql/virtual_schema.go#l656-l658](pkg/sql/virtual_schema.go#l656-l658) [pkg/sql/pg_catalog.go#l1178-l1180](pkg/sql/pg_catalog.go#l1178-l1180) [pkg/sql/information_schema.go#l2648-l2650](pkg/sql/information_schema.go#l2648-l2650) [pkg/sql/catalog/descs/collection.go#l1069-l1071](pkg/sql/catalog/descs/collection.go#l1069-l1071) [pkg/sql/catalog/descs/collection.go#l885-l887](pkg/sql/catalog/descs/collection.go#l885-l887) [pkg/sql/catalog/descs/collection.go#l1042-l1044](pkg/sql/catalog/descs/collection.go#l1042-l1044) [pkg/sql/catalog/descs/descriptor.go#l201-l203](pkg/sql/catalog/descs/descriptor.go#l201-l203) [pkg/sql/catalog/internal/catkv/catalog_reader_cached.go#l356-l358](pkg/sql/catalog/internal/catkv/catalog_reader_cached.go#l356-l358) [pkg/sql/catalog/internal/catkv/catalog_reader.go#l355-l357](pkg/sql/catalog/internal/catkv/catalog_reader.go#l355-l357) [pkg/sql/catalog/internal/catkv/catalog_query.go#l75-l77](pkg/sql/catalog/internal/catkv/catalog_query.go#l75-l77) ### tags | tag | value | | --- | --- | | command | start-single-node | | environment | v24.3.3 | | go version | go1.22.8 x:nocoverageredesign | | platform | linux amd64 | | distribution | ccl | | cockroach release | v24.3.3 | | cockroach sha | 343b4202e553d58a76a36c60018588615bd4c30a | | # of cpus | 24 | | # of goroutines | 1226 | jira issue: crdb-58999,6.0,Critical,1.0,crash-like behavior
cockroachdb/cockroach#161626,sentry: catch.go:24: runtime error: invalid memory address or nil pointer dereference (1) wraps: (2) wraps: (3) assertion failure wraps: (4) attached stack trace -- stack trace: | github.com/cockr...,this issue was auto filed by sentry. it represents a crash or reported error on a live cluster with telemetry enabled. sentry link: [ panic message: stacktrace (expand for inline code snippets): [src/runtime/asm_amd64.s#l1694-l1696](src/runtime/asm_amd64.s#l1694-l1696) [pkg/util/stop/stopper.go#l497-l499](pkg/util/stop/stopper.go#l497-l499) [pkg/sql/virtual_table.go#l138-l140](pkg/sql/virtual_table.go#l138-l140) [pkg/sql/virtual_table.go#l137-l139](pkg/sql/virtual_table.go#l137-l139) [pkg/sql/virtual_schema.go#l656-l658](pkg/sql/virtual_schema.go#l656-l658) [pkg/sql/pg_catalog.go#l1178-l1180](pkg/sql/pg_catalog.go#l1178-l1180) [pkg/sql/information_schema.go#l2648-l2650](pkg/sql/information_schema.go#l2648-l2650) [pkg/sql/catalog/descs/collection.go#l1069-l1071](pkg/sql/catalog/descs/collection.go#l1069-l1071) [pkg/sql/catalog/descs/collection.go#l885-l887](pkg/sql/catalog/descs/collection.go#l885-l887) [pkg/sql/catalog/descs/collection.go#l1042-l1044](pkg/sql/catalog/descs/collection.go#l1042-l1044) [pkg/sql/catalog/descs/descriptor.go#l214-l216](pkg/sql/catalog/descs/descriptor.go#l214-l216) [pkg/sql/catalog/descs/descriptor.go#l660-l662](pkg/sql/catalog/descs/descriptor.go#l660-l662) [pkg/sql/catalog/descs/validate.go#l34-l36](pkg/sql/catalog/descs/validate.go#l34-l36) [pkg/sql/catalog/internal/validate/validate.go#l70-l72](pkg/sql/catalog/internal/validate/validate.go#l70-l72) [pkg/sql/catalog/internal/validate/validate.go#l192-l194](pkg/sql/catalog/internal/validate/validate.go#l192-l194) [pkg/sql/catalog/internal/validate/validate.go#l74-l76](pkg/sql/catalog/internal/validate/validate.go#l74-l76) [pkg/sql/catalog/tabledesc/validate.go#l758-l760](pkg/sql/catalog/tabledesc/validate.go#l758-l760) [pkg/sql/catalog/catprivilege/validate.go#l21-l23](pkg/sql/catalog/catprivilege/validate.go#l21-l23) [pkg/sql/catalog/catpb/privilege.go#l395-l397](pkg/sql/catalog/catpb/privilege.go#l395-l397) [pkg/sql/catalog/catpb/privilege.go#l366-l368](pkg/sql/catalog/catpb/privilege.go#l366-l368) [pkg/sql/catalog/catpb/privilege.go#l69-l71](pkg/sql/catalog/catpb/privilege.go#l69-l71) [pkg/sql/catalog/catpb/privilege.go#l57-l59](pkg/sql/catalog/catpb/privilege.go#l57-l59) [goroot/src/sort/search.go#l64-l66](goroot/src/sort/search.go#l64-l66) [pkg/sql/catalog/catpb/privilege.go#l58-l60](pkg/sql/catalog/catpb/privilege.go#l58-l60) [pkg/security/username/username.go#l286-l288](pkg/security/username/username.go#l286-l288) [src/internal/bytealg/compare_amd64.s#l123-l125](src/internal/bytealg/compare_amd64.s#l123-l125) [goroot/src/runtime/signal_unix.go#l880-l882](goroot/src/runtime/signal_unix.go#l880-l882) [goroot/src/runtime/panic.go#l260-l262](goroot/src/runtime/panic.go#l260-l262) [goroot/src/runtime/panic.go#l769-l771](goroot/src/runtime/panic.go#l769-l771) [pkg/sql/virtual_table.go#l129-l131](pkg/sql/virtual_table.go#l129-l131) [pkg/util/errorutil/catch.go#l23-l25](pkg/util/errorutil/catch.go#l23-l25) ### tags | tag | value | | --- | --- | | command | start-single-node | | environment | v24.3.3 | | go version | go1.22.8 x:nocoverageredesign | | platform | linux amd64 | | distribution | ccl | | cockroach release | v24.3.3 | | cockroach sha | 343b4202e553d58a76a36c60018588615bd4c30a | | # of cpus | 24 | | # of goroutines | 1259 | jira issue: crdb-59000,[],['BUG'],"['C-bug', 'O-sentry', 'X-blathers-triaged', 'T-sql-foundations', 'branch-release-24.3']",github,2026-01-22T17:29:28Z,2026-01-27T18:28:31Z,sentry: catch.go:24: runtime error: invalid memory address or nil pointer dereference (1) wraps: (2) wraps: (3) assertion failure wraps: (4) attached stack trace -- stack trace: | github.com/cockr... this issue was auto filed by sentry. it represents a crash or reported error on a live cluster with telemetry enabled. sentry link: [ panic message: stacktrace (expand for inline code snippets): [src/runtime/asm_amd64.s#l1694-l1696](src/runtime/asm_amd64.s#l1694-l1696) [pkg/util/stop/stopper.go#l497-l499](pkg/util/stop/stopper.go#l497-l499) [pkg/sql/virtual_table.go#l138-l140](pkg/sql/virtual_table.go#l138-l140) [pkg/sql/virtual_table.go#l137-l139](pkg/sql/virtual_table.go#l137-l139) [pkg/sql/virtual_schema.go#l656-l658](pkg/sql/virtual_schema.go#l656-l658) [pkg/sql/pg_catalog.go#l1178-l1180](pkg/sql/pg_catalog.go#l1178-l1180) [pkg/sql/information_schema.go#l2648-l2650](pkg/sql/information_schema.go#l2648-l2650) [pkg/sql/catalog/descs/collection.go#l1069-l1071](pkg/sql/catalog/descs/collection.go#l1069-l1071) [pkg/sql/catalog/descs/collection.go#l885-l887](pkg/sql/catalog/descs/collection.go#l885-l887) [pkg/sql/catalog/descs/collection.go#l1042-l1044](pkg/sql/catalog/descs/collection.go#l1042-l1044) [pkg/sql/catalog/descs/descriptor.go#l214-l216](pkg/sql/catalog/descs/descriptor.go#l214-l216) [pkg/sql/catalog/descs/descriptor.go#l660-l662](pkg/sql/catalog/descs/descriptor.go#l660-l662) [pkg/sql/catalog/descs/validate.go#l34-l36](pkg/sql/catalog/descs/validate.go#l34-l36) [pkg/sql/catalog/internal/validate/validate.go#l70-l72](pkg/sql/catalog/internal/validate/validate.go#l70-l72) [pkg/sql/catalog/internal/validate/validate.go#l192-l194](pkg/sql/catalog/internal/validate/validate.go#l192-l194) [pkg/sql/catalog/internal/validate/validate.go#l74-l76](pkg/sql/catalog/internal/validate/validate.go#l74-l76) [pkg/sql/catalog/tabledesc/validate.go#l758-l760](pkg/sql/catalog/tabledesc/validate.go#l758-l760) [pkg/sql/catalog/catprivilege/validate.go#l21-l23](pkg/sql/catalog/catprivilege/validate.go#l21-l23) [pkg/sql/catalog/catpb/privilege.go#l395-l397](pkg/sql/catalog/catpb/privilege.go#l395-l397) [pkg/sql/catalog/catpb/privilege.go#l366-l368](pkg/sql/catalog/catpb/privilege.go#l366-l368) [pkg/sql/catalog/catpb/privilege.go#l69-l71](pkg/sql/catalog/catpb/privilege.go#l69-l71) [pkg/sql/catalog/catpb/privilege.go#l57-l59](pkg/sql/catalog/catpb/privilege.go#l57-l59) [goroot/src/sort/search.go#l64-l66](goroot/src/sort/search.go#l64-l66) [pkg/sql/catalog/catpb/privilege.go#l58-l60](pkg/sql/catalog/catpb/privilege.go#l58-l60) [pkg/security/username/username.go#l286-l288](pkg/security/username/username.go#l286-l288) [src/internal/bytealg/compare_amd64.s#l123-l125](src/internal/bytealg/compare_amd64.s#l123-l125) [goroot/src/runtime/signal_unix.go#l880-l882](goroot/src/runtime/signal_unix.go#l880-l882) [goroot/src/runtime/panic.go#l260-l262](goroot/src/runtime/panic.go#l260-l262) [goroot/src/runtime/panic.go#l769-l771](goroot/src/runtime/panic.go#l769-l771) [pkg/sql/virtual_table.go#l129-l131](pkg/sql/virtual_table.go#l129-l131) [pkg/util/errorutil/catch.go#l23-l25](pkg/util/errorutil/catch.go#l23-l25) ### tags | tag | value | | --- | --- | | command | start-single-node | | environment | v24.3.3 | | go version | go1.22.8 x:nocoverageredesign | | platform | linux amd64 | | distribution | ccl | | cockroach release | v24.3.3 | | cockroach sha | 343b4202e553d58a76a36c60018588615bd4c30a | | # of cpus | 24 | | # of goroutines | 1259 | jira issue: crdb-59000,6.0,Critical,1.0,crash-like behavior
pytorch/pytorch#173092,feedback about torch.arange,there is the following issue on this page: very informative got to the point and explained it properly couldnt ask for anything else cc,[],['DOCUMENTATION'],"['module: docs', 'triaged']",github,2026-01-22T17:36:13Z,2026-01-22T22:46:37Z,feedback about torch.arange there is the following issue on this page: very informative got to the point and explained it properly couldnt ask for anything else cc,1.2,Low,0.493,localized low-impact
python/cpython#144161,inconsistent documentation: are interned strings immortal or not?,"# documentation from [howto about python free-threading]( > as of the 3.14 release, immortalization is limited to: > * code constants: numeric literals, string literals, and tuple literals composed of other constants. > * strings interned by [sys.intern()]( from [sys.intern()]( > interned strings are not [immortal]( you must keep a reference to the return value of [intern()]( around to benefit from it. so, are the interned strings immortal or not? ### linked prs * gh-144176",[],['DOCUMENTATION'],['docs'],github,2026-01-22T18:05:28Z,,"inconsistent documentation: are interned strings immortal or not? # documentation from [howto about python free-threading]( > as of the 3.14 release, immortalization is limited to: > * code constants: numeric literals, string literals, and tuple literals composed of other constants. > * strings interned by [sys.intern()]( from [sys.intern()]( > interned strings are not [immortal]( you must keep a reference to the return value of [intern()]( around to benefit from it. so, are the interned strings immortal or not? ### linked prs * gh-144176",1.2,Low,0.493,localized low-impact
rust-lang/rust#151503,hang: compiler hang in resolving matrix with gats [next-solver],"<!-- thank you for filing a bug report! üêõ please provide a short summary of the bug, along with any information you feel relevant to replicating the bug. --> i tried this code:Ôºàauto-reduceÔºâ the compiler hangs even with | release channel | result | |-----------------------------------|--------| | current stable | error | | current nightly (default solver) | hang | | nightly + | hang | ### meta <!-- if you're using the stable version of the compiler, you should also check if the bug also exists in the beta or nightly versions. --> : <!-- include a backtrace in the code block by setting in your environment. e.g. . --> backtrace",[],"['CLEANUP', 'BUG']","['T-compiler', 'C-bug', 'I-hang', 'WG-trait-system-refactor', 'needs-triage', 'A-GATs']",github,2026-01-22T18:33:20Z,,"hang: compiler hang in resolving matrix with gats [next-solver] <!-- thank you for filing a bug report! üêõ please provide a short summary of the bug, along with any information you feel relevant to replicating the bug. --> i tried this code:Ôºàauto-reduceÔºâ the compiler hangs even with | release channel | result | |-----------------------------------|--------| | current stable | error | | current nightly (default solver) | hang | | nightly + | hang | ### meta <!-- if you're using the stable version of the compiler, you should also check if the bug also exists in the beta or nightly versions. --> : <!-- include a backtrace in the code block by setting in your environment. e.g. . --> backtrace",2.917,Medium,0.883,crash-like behavior
grpc/grpc#41464,[python] add typing to the methods,"_(copied from [ ]( issue was initially reported to protobuf)_ what language does this apply to? it's about the python api describe the problem you are trying to solve. i'm using the protobuf api for a grpc program, and every time i use a default api method, the linter doesn't recognise additional methods because there are no types assigned to the method outputs describe the solution you'd like i would like for the methods to add the standard context for python to return values: def method_a(input_a, input_b) -> type: especially as the api documentation does state the types of the method, it makes absolutely no sense those aren't actually defined in the method itself. describe alternatives you've considered as of now, i'm having to do decorator methods to be able to ensure my ide does lint the services, by replicating all the api items i need. additional context example with methoddescriptor against servicedescriptor.findmethodbyname(str):",[],['FEATURE'],"['priority/P2', 'lang/Python', 'kind/enhancement']",github,2026-01-22T18:40:27Z,,"[python] add typing to the methods _(copied from [ ]( issue was initially reported to protobuf)_ what language does this apply to? it's about the python api describe the problem you are trying to solve. i'm using the protobuf api for a grpc program, and every time i use a default api method, the linter doesn't recognise additional methods because there are no types assigned to the method outputs describe the solution you'd like i would like for the methods to add the standard context for python to return values: def method_a(input_a, input_b) -> type: especially as the api documentation does state the types of the method, it makes absolutely no sense those aren't actually defined in the method itself. describe alternatives you've considered as of now, i'm having to do decorator methods to be able to ensure my ide does lint the services, by replicating all the api items i need. additional context example with methoddescriptor against servicedescriptor.findmethodbyname(str):",1.4,Low,0.538,localized low-impact
pytorch/pytorch#173097,typo in torch_compiler export.md,"### üìö the doc issue this should be ""different inputs"" instead of ""different outputs"", to match ### suggest a potential alternative/fix _no response_ cc",[],['DOCUMENTATION'],"['module: docs', 'triaged', 'actionable']",github,2026-01-22T18:41:53Z,2026-01-27T17:48:36Z,"typo in torch_compiler export.md ### üìö the doc issue this should be ""different inputs"" instead of ""different outputs"", to match ### suggest a potential alternative/fix _no response_ cc",1.2,Low,0.493,localized low-impact
facebook/react#35604,[compiler bug]: optional chaining not working with async try/catch function inside useeffect,"### what kind of issue is this? - [x] react compiler core (the js output is incorrect, or your app works incorrectly after optimization) - [x] babel-plugin-react-compiler (build issue installing or using the babel plugin) - [ ] eslint-plugin-react-hooks (build issue installing or using the eslint plugin) - [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script) ### link to repro ### repro steps using the next code reproduce the issue: ### how often does this bug happen? every time ### what version of react are you using? latest version ### what version of react compiler are you using? latest version",[],['BUG'],"['Type: Bug', 'Status: Unconfirmed']",github,2026-01-22T19:06:42Z,,"[compiler bug]: optional chaining not working with async try/catch function inside useeffect ### what kind of issue is this? - [x] react compiler core (the js output is incorrect, or your app works incorrectly after optimization) - [x] babel-plugin-react-compiler (build issue installing or using the babel plugin) - [ ] eslint-plugin-react-hooks (build issue installing or using the eslint plugin) - [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script) ### link to repro ### repro steps using the next code reproduce the issue: ### how often does this bug happen? every time ### what version of react are you using? latest version ### what version of react compiler are you using? latest version",2.408,Medium,0.767,functional impact
cockroachdb/cockroach#161629,sentry: conn_executor.go:961: runtime error: invalid memory address or nil pointer dereference (1) attached stack trace -- stack trace: | github.com/cockroachdb/cockroach/pkg/sql.(*server).serveco...,this issue was auto filed by sentry. it represents a crash or reported error on a live cluster with telemetry enabled. sentry link: [ panic message: stacktrace (expand for inline code snippets): [pkg/sql/catalog/internal/validate/validate.go#l410-l412](pkg/sql/catalog/internal/validate/validate.go#l410-l412) [pkg/sql/catalog/descs/validate.go#l125-l127](pkg/sql/catalog/descs/validate.go#l125-l127) [pkg/sql/catalog/internal/catkv/validate.go#l56-l58](pkg/sql/catalog/internal/catkv/validate.go#l56-l58) [pkg/sql/catalog/internal/catkv/catalog_reader_cached.go#l356-l358](pkg/sql/catalog/internal/catkv/catalog_reader_cached.go#l356-l358) [pkg/sql/catalog/internal/catkv/catalog_reader.go#l355-l357](pkg/sql/catalog/internal/catkv/catalog_reader.go#l355-l357) [pkg/sql/catalog/internal/catkv/catalog_query.go#l53-l55](pkg/sql/catalog/internal/catkv/catalog_query.go#l53-l55) [pkg/kv/txn.go#l804-l806](pkg/kv/txn.go#l804-l806) [pkg/kv/db.go#l964-l966](pkg/kv/db.go#l964-l966) [pkg/kv/txn.go#l1294-l1296](pkg/kv/txn.go#l1294-l1296) [pkg/kv/db.go#l1132-l1134](pkg/kv/db.go#l1132-l1134) [pkg/kv/kvclient/kvcoord/txn_coord_sender.go#l533-l535](pkg/kv/kvclient/kvcoord/txn_coord_sender.go#l533-l535) [pkg/kv/kvclient/kvcoord/txn_interceptor_heartbeater.go#l264-l266](pkg/kv/kvclient/kvcoord/txn_interceptor_heartbeater.go#l264-l266) [pkg/kv/kvclient/kvcoord/txn_interceptor_seq_num_allocator.go#l111-l113](pkg/kv/kvclient/kvcoord/txn_interceptor_seq_num_allocator.go#l111-l113) [pkg/kv/kvclient/kvcoord/txn_interceptor_pipeliner.go#l333-l335](pkg/kv/kvclient/kvcoord/txn_interceptor_pipeliner.go#l333-l335) [pkg/kv/kvclient/kvcoord/txn_interceptor_committer.go#l143-l145](pkg/kv/kvclient/kvcoord/txn_interceptor_committer.go#l143-l145) [pkg/kv/kvclient/kvcoord/txn_interceptor_span_refresher.go#l161-l163](pkg/kv/kvclient/kvcoord/txn_interceptor_span_refresher.go#l161-l163) [pkg/kv/kvclient/kvcoord/txn_interceptor_span_refresher.go#l233-l235](pkg/kv/kvclient/kvcoord/txn_interceptor_span_refresher.go#l233-l235) [pkg/kv/kvclient/kvcoord/txn_interceptor_metric_recorder.go#l40-l42](pkg/kv/kvclient/kvcoord/txn_interceptor_metric_recorder.go#l40-l42) [pkg/kv/kvclient/kvcoord/txn_lock_gatekeeper.go#l76-l78](pkg/kv/kvclient/kvcoord/txn_lock_gatekeeper.go#l76-l78) [pkg/kv/kvclient/kvcoord/dist_sender.go#l1269-l1271](pkg/kv/kvclient/kvcoord/dist_sender.go#l1269-l1271) [pkg/kv/kvclient/kvcoord/dist_sender.go#l2021-l2023](pkg/kv/kvclient/kvcoord/dist_sender.go#l2021-l2023) [pkg/kv/kvclient/kvcoord/dist_sender.go#l1861-l1863](pkg/kv/kvclient/kvcoord/dist_sender.go#l1861-l1863) [pkg/kv/kvpb/batch.go#l713-l715](pkg/kv/kvpb/batch.go#l713-l715) [bazel-out/k8-opt/bin/pkg/kv/kvpb/batch_generated.go#l119-l121](bazel-out/k8-opt/bin/pkg/kv/kvpb/batch_generated.go#l119-l121) [goroot/src/runtime/signal_unix.go#l880-l882](goroot/src/runtime/signal_unix.go#l880-l882) [goroot/src/runtime/panic.go#l260-l262](goroot/src/runtime/panic.go#l260-l262) [goroot/src/runtime/panic.go#l769-l771](goroot/src/runtime/panic.go#l769-l771) [pkg/sql/colexecerror/error.go#l136-l138](pkg/sql/colexecerror/error.go#l136-l138) [goroot/src/runtime/panic.go#l769-l771](goroot/src/runtime/panic.go#l769-l771) [pkg/sql/colexecerror/error.go#l136-l138](pkg/sql/colexecerror/error.go#l136-l138) [goroot/src/runtime/panic.go#l769-l771](goroot/src/runtime/panic.go#l769-l771) [pkg/sql/conn_executor.go#l960-l962](pkg/sql/conn_executor.go#l960-l962) ### tags | tag | value | | --- | --- | | command | start-single-node | | environment | v24.3.3 | | go version | go1.22.8 x:nocoverageredesign | | platform | linux amd64 | | distribution | ccl | | cockroach release | v24.3.3 | | cockroach sha | 343b4202e553d58a76a36c60018588615bd4c30a | | # of cpus | 24 | | # of goroutines | 1187 | jira issue: crdb-59001,[],['BUG'],"['C-bug', 'O-sentry', 'X-blathers-triaged', 'T-kv', 'branch-release-24.3']",github,2026-01-22T19:16:46Z,,sentry: conn_executor.go:961: runtime error: invalid memory address or nil pointer dereference (1) attached stack trace -- stack trace: | github.com/cockroachdb/cockroach/pkg/sql.(*server).serveco... this issue was auto filed by sentry. it represents a crash or reported error on a live cluster with telemetry enabled. sentry link: [ panic message: stacktrace (expand for inline code snippets): [pkg/sql/catalog/internal/validate/validate.go#l410-l412](pkg/sql/catalog/internal/validate/validate.go#l410-l412) [pkg/sql/catalog/descs/validate.go#l125-l127](pkg/sql/catalog/descs/validate.go#l125-l127) [pkg/sql/catalog/internal/catkv/validate.go#l56-l58](pkg/sql/catalog/internal/catkv/validate.go#l56-l58) [pkg/sql/catalog/internal/catkv/catalog_reader_cached.go#l356-l358](pkg/sql/catalog/internal/catkv/catalog_reader_cached.go#l356-l358) [pkg/sql/catalog/internal/catkv/catalog_reader.go#l355-l357](pkg/sql/catalog/internal/catkv/catalog_reader.go#l355-l357) [pkg/sql/catalog/internal/catkv/catalog_query.go#l53-l55](pkg/sql/catalog/internal/catkv/catalog_query.go#l53-l55) [pkg/kv/txn.go#l804-l806](pkg/kv/txn.go#l804-l806) [pkg/kv/db.go#l964-l966](pkg/kv/db.go#l964-l966) [pkg/kv/txn.go#l1294-l1296](pkg/kv/txn.go#l1294-l1296) [pkg/kv/db.go#l1132-l1134](pkg/kv/db.go#l1132-l1134) [pkg/kv/kvclient/kvcoord/txn_coord_sender.go#l533-l535](pkg/kv/kvclient/kvcoord/txn_coord_sender.go#l533-l535) [pkg/kv/kvclient/kvcoord/txn_interceptor_heartbeater.go#l264-l266](pkg/kv/kvclient/kvcoord/txn_interceptor_heartbeater.go#l264-l266) [pkg/kv/kvclient/kvcoord/txn_interceptor_seq_num_allocator.go#l111-l113](pkg/kv/kvclient/kvcoord/txn_interceptor_seq_num_allocator.go#l111-l113) [pkg/kv/kvclient/kvcoord/txn_interceptor_pipeliner.go#l333-l335](pkg/kv/kvclient/kvcoord/txn_interceptor_pipeliner.go#l333-l335) [pkg/kv/kvclient/kvcoord/txn_interceptor_committer.go#l143-l145](pkg/kv/kvclient/kvcoord/txn_interceptor_committer.go#l143-l145) [pkg/kv/kvclient/kvcoord/txn_interceptor_span_refresher.go#l161-l163](pkg/kv/kvclient/kvcoord/txn_interceptor_span_refresher.go#l161-l163) [pkg/kv/kvclient/kvcoord/txn_interceptor_span_refresher.go#l233-l235](pkg/kv/kvclient/kvcoord/txn_interceptor_span_refresher.go#l233-l235) [pkg/kv/kvclient/kvcoord/txn_interceptor_metric_recorder.go#l40-l42](pkg/kv/kvclient/kvcoord/txn_interceptor_metric_recorder.go#l40-l42) [pkg/kv/kvclient/kvcoord/txn_lock_gatekeeper.go#l76-l78](pkg/kv/kvclient/kvcoord/txn_lock_gatekeeper.go#l76-l78) [pkg/kv/kvclient/kvcoord/dist_sender.go#l1269-l1271](pkg/kv/kvclient/kvcoord/dist_sender.go#l1269-l1271) [pkg/kv/kvclient/kvcoord/dist_sender.go#l2021-l2023](pkg/kv/kvclient/kvcoord/dist_sender.go#l2021-l2023) [pkg/kv/kvclient/kvcoord/dist_sender.go#l1861-l1863](pkg/kv/kvclient/kvcoord/dist_sender.go#l1861-l1863) [pkg/kv/kvpb/batch.go#l713-l715](pkg/kv/kvpb/batch.go#l713-l715) [bazel-out/k8-opt/bin/pkg/kv/kvpb/batch_generated.go#l119-l121](bazel-out/k8-opt/bin/pkg/kv/kvpb/batch_generated.go#l119-l121) [goroot/src/runtime/signal_unix.go#l880-l882](goroot/src/runtime/signal_unix.go#l880-l882) [goroot/src/runtime/panic.go#l260-l262](goroot/src/runtime/panic.go#l260-l262) [goroot/src/runtime/panic.go#l769-l771](goroot/src/runtime/panic.go#l769-l771) [pkg/sql/colexecerror/error.go#l136-l138](pkg/sql/colexecerror/error.go#l136-l138) [goroot/src/runtime/panic.go#l769-l771](goroot/src/runtime/panic.go#l769-l771) [pkg/sql/colexecerror/error.go#l136-l138](pkg/sql/colexecerror/error.go#l136-l138) [goroot/src/runtime/panic.go#l769-l771](goroot/src/runtime/panic.go#l769-l771) [pkg/sql/conn_executor.go#l960-l962](pkg/sql/conn_executor.go#l960-l962) ### tags | tag | value | | --- | --- | | command | start-single-node | | environment | v24.3.3 | | go version | go1.22.8 x:nocoverageredesign | | platform | linux amd64 | | distribution | ccl | | cockroach release | v24.3.3 | | cockroach sha | 343b4202e553d58a76a36c60018588615bd4c30a | | # of cpus | 24 | | # of goroutines | 1187 | jira issue: crdb-59001,7.8,Critical,1.0,crash-like behavior
cockroachdb/cockroach#161631,pkg/sql/copy/copy_test: testlargecopy failed,pkg/sql/copy/copy_test.testlargecopy [failed]( on master @ [a16b5f8d396ccc67dd3f69ebacc54644317b1f0c]( parameters: - attempt=1 - run=1 - shard=3 help see also: [how to investigate a go test failure \(internal\)]( [this test on roachdash]( | [improve this report!]( jira issue: crdb-59002,[],['TESTING'],"['C-test-failure', 'O-robot', 'branch-master', 'T-sql-queries']",github,2026-01-22T19:26:52Z,,pkg/sql/copy/copy_test: testlargecopy failed pkg/sql/copy/copy_test.testlargecopy [failed]( on master @ [a16b5f8d396ccc67dd3f69ebacc54644317b1f0c]( parameters: - attempt=1 - run=1 - shard=3 help see also: [how to investigate a go test failure \(internal\)]( [this test on roachdash]( | [improve this report!]( jira issue: crdb-59002,1.6,Low,0.584,localized low-impact
openssl/openssl#29722,unable to build openssl 3.0.18 using embarcadero c++ builder,"hi, the issue is consistent using c++ builder 10.3.2 rio and c++ builder 11.3 patch 1(same unit tests fail). installled cmake 3.31.10, perl 5.42 (strawberry), ninja 1.12.0 and followed instructions: copied c:\program files (x86)\embarcadero\studio\22.0\cmake\modules\platform\windows-embarcadero.cmake to c:\program files\cmake\share\cmake-3.31\modules\platform\ and to c:\strawberryperl\c\share\cmake-3.29\modules\platform\ for c++ builder 11 (for c++ builder 10.3.2 copied from c:\program files (x86)\embarcadero\studio\20.0\cmake) open the rad studio command prompt (rsvars.bat) cd .....\openssl-3.0.18 perl configure bc-32 --prefix=%cd%` make -n make -n test then i get errors: ... ... ... ...",[],['BUG'],['issue: bug report'],github,2026-01-22T19:39:45Z,,"unable to build openssl 3.0.18 using embarcadero c++ builder hi, the issue is consistent using c++ builder 10.3.2 rio and c++ builder 11.3 patch 1(same unit tests fail). installled cmake 3.31.10, perl 5.42 (strawberry), ninja 1.12.0 and followed instructions: copied c:\program files (x86)\embarcadero\studio\22.0\cmake\modules\platform\windows-embarcadero.cmake to c:\program files\cmake\share\cmake-3.31\modules\platform\ and to c:\strawberryperl\c\share\cmake-3.29\modules\platform\ for c++ builder 11 (for c++ builder 10.3.2 copied from c:\program files (x86)\embarcadero\studio\20.0\cmake) open the rad studio command prompt (rsvars.bat) cd .....\openssl-3.0.18 perl configure bc-32 --prefix=%cd%` make -n make -n test then i get errors: ... ... ... ...",2.558,Medium,0.801,functional impact
python/cpython#144163,segfault in _testinternalcapi.assemble_code_object with load_closure,# bug report ### bug description: could be a low-priority bug. ### cpython versions tested on: cpython main branch ### operating systems tested on: linux,[],"['TESTING', 'BUG']","['type-bug', 'tests']",github,2026-01-22T19:40:47Z,,segfault in _testinternalcapi.assemble_code_object with load_closure # bug report ### bug description: could be a low-priority bug. ### cpython versions tested on: cpython main branch ### operating systems tested on: linux,3.11,High,0.927,crash-like behavior
cilium/cilium#43938,enabling big tcp with dsrdispatch=geneve in native routing mode fails to create the geneve tunnel,"### is there an existing issue for this? - [x] i have searched the existing issues ### version equal or higher than v1.18.6 and lower than v1.19.0 ### what happened? initially reported in a comment under another bug: in native routing mode, with dsr dispatch set to geneve, big tcp initializes successfully, but creating the geneve tunnel fails, because it attempts to set gso_max_size > 64k on the geneve netdev on a non-supported kernel. big tcp initialization code should sanity-check the config and forbid this configuration, unless enable-tunnel-big-tcp is set. ### how can we reproduce the issue? install cilium with the following config: ### cilium version cilium-cli: v1.19.0-pre.4-134-g5ea8503803 compiled with go1.25.6 x:nodwarf5 on linux/amd64 cilium image (default): v1.18.5 cilium image (stable): v1.18.6 cilium image (running): 1.20.0-dev ### kernel version 6.8.0-90-generic -ubuntu smp preempt_dynamic tue nov 18 14:14:30 utc 2025 x86_64 x86_64 x86_64 gnu/linux ### kubernetes version client version: v1.35.0 kustomize version: v5.7.1 server version: v1.32.2 warning: version difference between client (1.35) and server (1.32) exceeds the supported minor version skew of +/-1 ### regression _no response_ ### sysdump _no response_ ### relevant log output ### anything else? _no response_ ### cilium users document - [ ] are you a user of cilium? please add yourself to the [users doc]( ### code of conduct - [x] i agree to follow this project's code of conduct",[],"['BUG', 'FEATURE']","['kind/bug', 'area/datapath', 'kind/community-report', 'feature/dsr']",github,2026-01-22T19:58:33Z,2026-01-26T23:56:29Z,"enabling big tcp with dsrdispatch=geneve in native routing mode fails to create the geneve tunnel ### is there an existing issue for this? - [x] i have searched the existing issues ### version equal or higher than v1.18.6 and lower than v1.19.0 ### what happened? initially reported in a comment under another bug: in native routing mode, with dsr dispatch set to geneve, big tcp initializes successfully, but creating the geneve tunnel fails, because it attempts to set gso_max_size > 64k on the geneve netdev on a non-supported kernel. big tcp initialization code should sanity-check the config and forbid this configuration, unless enable-tunnel-big-tcp is set. ### how can we reproduce the issue? install cilium with the following config: ### cilium version cilium-cli: v1.19.0-pre.4-134-g5ea8503803 compiled with go1.25.6 x:nodwarf5 on linux/amd64 cilium image (default): v1.18.5 cilium image (stable): v1.18.6 cilium image (running): 1.20.0-dev ### kernel version 6.8.0-90-generic -ubuntu smp preempt_dynamic tue nov 18 14:14:30 utc 2025 x86_64 x86_64 x86_64 gnu/linux ### kubernetes version client version: v1.35.0 kustomize version: v5.7.1 server version: v1.32.2 warning: version difference between client (1.35) and server (1.32) exceeds the supported minor version skew of +/-1 ### regression _no response_ ### sysdump _no response_ ### relevant log output ### anything else? _no response_ ### cilium users document - [ ] are you a user of cilium? please add yourself to the [users doc]( ### code of conduct - [x] i agree to follow this project's code of conduct",2.821,Medium,0.861,crash-like behavior
cockroachdb/cockroach#161636,sql/schemachanger: type schema changes should install a pts or work on chunks,"currently, when dropping an enum value in a table, we end up running a query to ensure that the value is no longer in use across all references. this may involve doing full table scans in fairly large tables to confirm that an enum value is no longer in use. to optimize this logic, we should consider computing the table spans and check for values incrementally in case the tables are large. as a short term solution installing a pts, will at least guarantee this scan will eventually succeed for gigantic tables. as part of fixing this, we should determine if we should focus only on getting this implemented in declarative schema changer, or updating the legacy behavior as well. jira issue: crdb-59003 epic crdb-31325",[],['BUG'],"['C-bug', 'O-support', 'branch-master', 'T-sql-foundations', 'branch-release-24.1', 'branch-release-24.3', 'branch-release-25.2', 'branch-release-25.1', 'branch-release-25.3', 'branch-release-25.4', 'branch-release-26.1']",github,2026-01-22T20:00:32Z,,"sql/schemachanger: type schema changes should install a pts or work on chunks currently, when dropping an enum value in a table, we end up running a query to ensure that the value is no longer in use across all references. this may involve doing full table scans in fairly large tables to confirm that an enum value is no longer in use. to optimize this logic, we should consider computing the table spans and check for values incrementally in case the tables are large. as a short term solution installing a pts, will at least guarantee this scan will eventually succeed for gigantic tables. as part of fixing this, we should determine if we should focus only on getting this implemented in declarative schema changer, or updating the legacy behavior as well. jira issue: crdb-59003 epic crdb-31325",4.6,Critical,1.0,crash-like behavior
istio/istio#58877,grpcroute requires spec.hostnames while httproute allows host header matching without it,"### is this the right place to submit this? - [x] this is not a security vulnerability or a crashing bug - [x] this is not a question about how to use istio ### bug description ## what happened grpcroute fails to merge into virtual hosts that already have httproutes attached. the route is accepted ( , ) but is silently not programmed into envoy. ## expected behavior grpcroute should merge into the same virtual host as httproutes when they share the same . both route types should coexist on the same hostname. ## actual behavior - when grpcroute uses a hostname with **no existing httproutes** ‚Üí works correctly (creates its own virtual host) - when grpcroute uses a hostname that **httproutes already use** ‚Üí silently not programmed into envoy the grpcroute status shows and , giving no indication of the failure. ## steps to reproduce 1. create a gateway: 2. create an httproute with a specific hostname: 3. create a grpcroute with the **same hostname**: 4. observe: - shows - shows the httproute but not the grpcroute 5. now change the grpcroute to use a **different hostname** (one without httproutes): 6. observe: - the grpcroute now appears in with its own virtual host ## workaround use httproute instead of grpcroute for grpc routing. grpc uses http/2 with paths following format: ## additional context this creates a confusing user experience because: 1. the grpcroute status shows accepted with no errors 2. no warning is logged in istiod 3. the route simply doesn't exist in envoy config 4. behavior differs based on whether httproutes exist for the same hostname, which is not documented afaik ### version ### additional information _no response_",[],['NETWORK'],"['area/networking', 'area/user experience']",github,2026-01-22T20:04:48Z,2026-01-23T17:52:51Z,"grpcroute requires spec.hostnames while httproute allows host header matching without it ### is this the right place to submit this? - [x] this is not a security vulnerability or a crashing bug - [x] this is not a question about how to use istio ### bug description ## what happened grpcroute fails to merge into virtual hosts that already have httproutes attached. the route is accepted ( , ) but is silently not programmed into envoy. ## expected behavior grpcroute should merge into the same virtual host as httproutes when they share the same . both route types should coexist on the same hostname. ## actual behavior - when grpcroute uses a hostname with **no existing httproutes** ‚Üí works correctly (creates its own virtual host) - when grpcroute uses a hostname that **httproutes already use** ‚Üí silently not programmed into envoy the grpcroute status shows and , giving no indication of the failure. ## steps to reproduce 1. create a gateway: 2. create an httproute with a specific hostname: 3. create a grpcroute with the **same hostname**: 4. observe: - shows - shows the httproute but not the grpcroute 5. now change the grpcroute to use a **different hostname** (one without httproutes): 6. observe: - the grpcroute now appears in with its own virtual host ## workaround use httproute instead of grpcroute for grpc routing. grpc uses http/2 with paths following format: ## additional context this creates a confusing user experience because: 1. the grpcroute status shows accepted with no errors 2. no warning is logged in istiod 3. the route simply doesn't exist in envoy config 4. behavior differs based on whether httproutes exist for the same hostname, which is not documented afaik ### version ### additional information _no response_",7.0,Critical,1.0,"affects communication layer, crash-like behavior"
python/cpython#144165,function args sometimes not decref:ed after return,"# bug report ### bug description: i have an application written in c and python, which implements threading using pthreads for performance-critical c code, and occasionally runs python code from these threads, protected by the gil. i have somewhat complex reference counting logic which i unit test using weakref.ref. these tests work flawlessly in 3.13 and earlier, but started to break in 3.14 (specifically 3.14.2, compiled with free threading disabled, linux/sles12). my test looks something like: by inspecting reference counts it seems that data sometimes gets an extra reference while returning from f: always returns 4 in the end of , but right after returning into , returns either 3 or 4. with 3.14 the test fails maybe 75% of the time, and with 3.10..3.13 it has never failed over thousands of runs. unfortunately, i have not yet been able to reproduce it in a toy environment. playing around a bit, it seems to only happen when the variable is referenced from different pthreads, and only when a value is passed as an argument to a function implemented in c. i also dug a bit with gdb, seems like the difference between the failing and succeeding path is this code path in , which seems to defer deallocation of the tuple, thus inhibiting arg decref: it seems that these objects are leaked forever; that tuple object seems to remain even after i do things like . any ideas? in particular, do you have any ideas on how to create a reproducer i can share? (the full application is in part proprietary, so cannot share that) ### cpython versions tested on: 3.14 ### operating systems tested on: linux",[],['BUG'],"['type-bug', 'topic-C-API', 'pending']",github,2026-01-22T20:09:09Z,2026-01-27T21:38:19Z,"function args sometimes not decref:ed after return # bug report ### bug description: i have an application written in c and python, which implements threading using pthreads for performance-critical c code, and occasionally runs python code from these threads, protected by the gil. i have somewhat complex reference counting logic which i unit test using weakref.ref. these tests work flawlessly in 3.13 and earlier, but started to break in 3.14 (specifically 3.14.2, compiled with free threading disabled, linux/sles12). my test looks something like: by inspecting reference counts it seems that data sometimes gets an extra reference while returning from f: always returns 4 in the end of , but right after returning into , returns either 3 or 4. with 3.14 the test fails maybe 75% of the time, and with 3.10..3.13 it has never failed over thousands of runs. unfortunately, i have not yet been able to reproduce it in a toy environment. playing around a bit, it seems to only happen when the variable is referenced from different pthreads, and only when a value is passed as an argument to a function implemented in c. i also dug a bit with gdb, seems like the difference between the failing and succeeding path is this code path in , which seems to defer deallocation of the tuple, thus inhibiting arg decref: it seems that these objects are leaked forever; that tuple object seems to remain even after i do things like . any ideas? in particular, do you have any ideas on how to create a reproducer i can share? (the full application is in part proprietary, so cannot share that) ### cpython versions tested on: 3.14 ### operating systems tested on: linux",3.8,Critical,1.0,system-wide impact
rust-lang/rust#151511,[ice]: rustdoc in src/librustdoc/clean/mod.rs,"<!-- thank you for finding an internal compiler error! üßä if possible, try to provide a minimal verifiable example. you can read ""rust bug minimization patterns"" for how to create smaller examples. --> ### code ### meta <!-- if you're using the stable version of the compiler, you should also check if the bug also exists in the beta or nightly versions. --> : ### error output <!-- include a backtrace in the code block by setting in your environment. e.g. . --> backtrace",[],['BUG'],"['T-rustdoc', 'I-ICE', 'T-compiler', 'C-bug', 'F-adt_const_params', 'F-generic_const_parameter_types', 'F-unsized_const_params', 'F-min_generic_const_args']",github,2026-01-22T21:01:14Z,,"[ice]: rustdoc in src/librustdoc/clean/mod.rs <!-- thank you for finding an internal compiler error! üßä if possible, try to provide a minimal verifiable example. you can read ""rust bug minimization patterns"" for how to create smaller examples. --> ### code ### meta <!-- if you're using the stable version of the compiler, you should also check if the bug also exists in the beta or nightly versions. --> : ### error output <!-- include a backtrace in the code block by setting in your environment. e.g. . --> backtrace",2.452,Medium,0.777,functional impact
rust-lang/rust#151513,connect_timeout incorrectly succeeds on illumos and probably other systems,"## summary i tried this code: i expected to see this happen: when i provide this program with an ip address and remote port on which there is *no* server listening, this should print ""failed to connect to [address]: connection refused"". instead, it sometimes prints ""successfully connected to [address]"". the result depends on whether the os reports the error synchronously with the first attempted to call to (the syscall). if so, you get the correct behavior. if not, the call always succeeds, even if the underlying connect attempt fails. to be more specific, on my network, my machine is 172.20.2.70 and i have another machine at 172.20.2.80. neither is running a server on port 12345. if i try the local system, i get the expected error: but if i try the other machine, it incorrectly succeeds: ## details the [illumos manual page for explains]( > when a socket is set to be non-blocking, a > call to connect initiates an asynchronous connection. if the connection > cannot be completed without blocking, such as when making a tcp > connection to a remote server, then the connection attempt is made in > the background and connect returns -1 and errno is set to einprogress. > > applications can obtain the state of this connection attempt by polling > the socket's file descriptor for pollout. ... > when an asynchronous connection has completed, the application must call > getsockopt(3socket) using the macro sol_socket as the level argument and > the macro so_error as the value of the option argument. if the value of > the so_error socket option is zero, then the connect was successfully > established. otherwise, the connection could not be established and the > value is the corresponding error code that would be commonly found in > errno. the exact problem i'm reporting here was previously reported for vxworks under and fixed for that platform under . it looks to me like the fix for that platform exactly matches what the illumos manual pages say to do, which makes me think this is probably a common pattern on various unix-like systems. it was [discussed under 127300]( whether to apply the fix more generally and the decision was not to do this. at the time, mentioned: > the [linux] manpage doesn't say anything about setting an error to so_error. i would assume this is always true, but it does seem safer to have the fallback in place. this behavior does appear documented on linux under [ ]( > einprogress > the socket is nonblocking and the connection cannot be > completed immediately. (unix domain sockets failed with > eagain instead.) it is possible to select(2) or poll(2) > for completion by selecting the socket for writing. after > select(2) indicates writability, use getsockopt(2) to read > the so_error option at level sol_socket to determine > whether connect() completed successfully (so_error is zero) > or unsuccessfully (so_error is one of the usual error codes > listed here, explaining the reason for the failure). it seems to me that the fix for vxworks (which postdates a different fix for linux) ought to work for linux and illumos too and maybe should just be the behavior for all unix-like systems? but i haven't done more digging than this. also, i imagine there's no test for this behavior or the test isn't working correctly.",[],['BUG'],"['C-bug', 'A-io', 'needs-triage', 'O-illumos']",github,2026-01-22T21:31:17Z,,"connect_timeout incorrectly succeeds on illumos and probably other systems ## summary i tried this code: i expected to see this happen: when i provide this program with an ip address and remote port on which there is *no* server listening, this should print ""failed to connect to [address]: connection refused"". instead, it sometimes prints ""successfully connected to [address]"". the result depends on whether the os reports the error synchronously with the first attempted to call to (the syscall). if so, you get the correct behavior. if not, the call always succeeds, even if the underlying connect attempt fails. to be more specific, on my network, my machine is 172.20.2.70 and i have another machine at 172.20.2.80. neither is running a server on port 12345. if i try the local system, i get the expected error: but if i try the other machine, it incorrectly succeeds: ## details the [illumos manual page for explains]( > when a socket is set to be non-blocking, a > call to connect initiates an asynchronous connection. if the connection > cannot be completed without blocking, such as when making a tcp > connection to a remote server, then the connection attempt is made in > the background and connect returns -1 and errno is set to einprogress. > > applications can obtain the state of this connection attempt by polling > the socket's file descriptor for pollout. ... > when an asynchronous connection has completed, the application must call > getsockopt(3socket) using the macro sol_socket as the level argument and > the macro so_error as the value of the option argument. if the value of > the so_error socket option is zero, then the connect was successfully > established. otherwise, the connection could not be established and the > value is the corresponding error code that would be commonly found in > errno. the exact problem i'm reporting here was previously reported for vxworks under and fixed for that platform under . it looks to me like the fix for that platform exactly matches what the illumos manual pages say to do, which makes me think this is probably a common pattern on various unix-like systems. it was [discussed under 127300]( whether to apply the fix more generally and the decision was not to do this. at the time, mentioned: > the [linux] manpage doesn't say anything about setting an error to so_error. i would assume this is always true, but it does seem safer to have the fallback in place. this behavior does appear documented on linux under [ ]( > einprogress > the socket is nonblocking and the connection cannot be > completed immediately. (unix domain sockets failed with > eagain instead.) it is possible to select(2) or poll(2) > for completion by selecting the socket for writing. after > select(2) indicates writability, use getsockopt(2) to read > the so_error option at level sol_socket to determine > whether connect() completed successfully (so_error is zero) > or unsuccessfully (so_error is one of the usual error codes > listed here, explaining the reason for the failure). it seems to me that the fix for vxworks (which postdates a different fix for linux) ought to work for linux and illumos too and maybe should just be the behavior for all unix-like systems? but i haven't done more digging than this. also, i imagine there's no test for this behavior or the test isn't working correctly.",2.572,Medium,0.805,functional impact
cockroachdb/cockroach#161647,pkg/sql/schemachanger/schemachanger_test: testpausemixedversion_add_column_default_unique failed,pkg/sql/schemachanger/schemachanger_test.testpausemixedversion_add_column_default_unique [failed]( with [artifacts]( on master @ [07558a6168cac3e90cd949b2e2a9b8c0db2a7285]( help see also: [how to investigate a go test failure \(internal\)]( /cc /sql-foundations [this test on roachdash]( | [improve this report!]( jira issue: crdb-59004,[],"['TESTING', 'BUG']","['C-bug', 'C-test-failure', 'O-robot', 'X-duplicate', 'branch-master', 'T-kv', 'A-kv-rangefeed', 'P-2']",github,2026-01-22T21:48:11Z,,pkg/sql/schemachanger/schemachanger_test: testpausemixedversion_add_column_default_unique failed pkg/sql/schemachanger/schemachanger_test.testpausemixedversion_add_column_default_unique [failed]( with [artifacts]( on master @ [07558a6168cac3e90cd949b2e2a9b8c0db2a7285]( help see also: [how to investigate a go test failure \(internal\)]( /cc /sql-foundations [this test on roachdash]( | [improve this report!]( jira issue: crdb-59004,3.172,High,0.941,crash-like behavior
pytorch/pytorch#173112,"variable ""small"" in c10/cuda /cudacachingallocator.h collides with windows.h defining ""small"" as a macro","### üêõ describe the bug unfortunately ""small"" is a macro in . is included indirectly via so it's not possible to not include this on windows. can we rename ""small"" to something else at to avoid the collision? this problem was discovered when i updated libtorch from 2.9.1 to 2.10.0 for my codebase. ### versions libtorch 2.10.0 cc",[],['UI'],"['module: build', 'module: windows', 'triaged', 'module: regression', 'needs research']",github,2026-01-22T21:49:55Z,,"variable ""small"" in c10/cuda /cudacachingallocator.h collides with windows.h defining ""small"" as a macro ### üêõ describe the bug unfortunately ""small"" is a macro in . is included indirectly via so it's not possible to not include this on windows. can we rename ""small"" to something else at to avoid the collision? this problem was discovered when i updated libtorch from 2.9.1 to 2.10.0 for my codebase. ### versions libtorch 2.10.0 cc",1.8,Low,0.629,user-visible issue
rust-lang/rust#151514,unexpected warning about unused assignment in rust 1.92 and newer,"<!-- thank you for filing a regression report! üêõ a regression is something that changed between versions of rust but was not supposed to. please provide a short summary of the regression, along with any information you feel is relevant to replicate it. --> ### code i tried this code, which is using the [docsplay]( crate: i expected to see this happen: this code should compile, and no warning should be shown. instead, this happened: i get a warning about an unused assignment: this is unexpected, because there is no visible assignment anywhere. the expanded code for the macro looks like this: there is , which means that the unused field b does not trigger that warning, but it is unexpected that the warning is triggered. ### version it worked on <!-- provide the most recent version this worked on, for example: it most recently worked on: rust 1.47 --> it most recently worked on: 1.91 ### version with regression <!-- provide the version you are using that has the regression. --> : also seems to happen on the latest nighly,, . <!-- did the compiler crash? if so, please provide a backtrace. --> ### backtrace <!-- include a backtrace in the code block by setting in your environment. e.g. . --> backtrace <!-- if you know when this regression occurred, please add a line like below, replacing with one of stable, beta, or nightly. modify labels: +regression-from-stable-to-stable -regression-untriaged -->",[],['BUG'],"['P-high', 'regression-from-stable-to-stable', 'C-bug', 'L-unused_assignments']",github,2026-01-22T22:06:36Z,2026-01-25T16:30:30Z,"unexpected warning about unused assignment in rust 1.92 and newer <!-- thank you for filing a regression report! üêõ a regression is something that changed between versions of rust but was not supposed to. please provide a short summary of the regression, along with any information you feel is relevant to replicate it. --> ### code i tried this code, which is using the [docsplay]( crate: i expected to see this happen: this code should compile, and no warning should be shown. instead, this happened: i get a warning about an unused assignment: this is unexpected, because there is no visible assignment anywhere. the expanded code for the macro looks like this: there is , which means that the unused field b does not trigger that warning, but it is unexpected that the warning is triggered. ### version it worked on <!-- provide the most recent version this worked on, for example: it most recently worked on: rust 1.47 --> it most recently worked on: 1.91 ### version with regression <!-- provide the version you are using that has the regression. --> : also seems to happen on the latest nighly,, . <!-- did the compiler crash? if so, please provide a backtrace. --> ### backtrace <!-- include a backtrace in the code block by setting in your environment. e.g. . --> backtrace <!-- if you know when this regression occurred, please add a line like below, replacing with one of stable, beta, or nightly. modify labels: +regression-from-stable-to-stable -regression-untriaged -->",4.6,Critical,1.0,crash-like behavior
facebook/react#35605,[compiler bug]: (buildhir::lowerstatement) support throwstatement inside of try/catch,"### what kind of issue is this? - [ ] react compiler core (the js output is incorrect, or your app works incorrectly after optimization) - [x] babel-plugin-react-compiler (build issue installing or using the babel plugin) - [ ] eslint-plugin-react-hooks (build issue installing or using the eslint plugin) - [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script) ### link to repro ### repro steps 1. go to the playground 2. use the next code: you will see the error ### how often does this bug happen? every time ### what version of react are you using? latest ### what version of react compiler are you using? latest",[],['BUG'],"['Type: Bug', 'Status: Unconfirmed']",github,2026-01-22T22:35:53Z,,"[compiler bug]: (buildhir::lowerstatement) support throwstatement inside of try/catch ### what kind of issue is this? - [ ] react compiler core (the js output is incorrect, or your app works incorrectly after optimization) - [x] babel-plugin-react-compiler (build issue installing or using the babel plugin) - [ ] eslint-plugin-react-hooks (build issue installing or using the eslint plugin) - [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script) ### link to repro ### repro steps 1. go to the playground 2. use the next code: you will see the error ### how often does this bug happen? every time ### what version of react are you using? latest ### what version of react compiler are you using? latest",2.232,Medium,0.727,functional impact
llvm/llvm-project#177484,[libc++] are we missing a benchmark to showcase std::find_if autovectorization?,does not seem to have added one.,[],['PERFORMANCE'],"['question', 'libc++', 'performance']",github,2026-01-22T22:41:19Z,2026-01-23T08:59:43Z,[libc++] are we missing a benchmark to showcase std::find_if autovectorization? does not seem to have added one.,3.4,High,0.993,performance degradation
cockroachdb/cockroach#161655,observability: review use of gauges for gc metrics,"**describe the problem** some of the runtime gc metrics use gauges, but a very quick reading of the code implies they should be counters: notably: - gcstopns - nongcpausens - nongcstopns look suspicious to me. jira issue: crdb-59005",[],['BUG'],"['C-bug', 'branch-master', 'T-db-server']",github,2026-01-22T22:54:17Z,,"observability: review use of gauges for gc metrics **describe the problem** some of the runtime gc metrics use gauges, but a very quick reading of the code implies they should be counters: notably: - gcstopns - nongcpausens - nongcstopns look suspicious to me. jira issue: crdb-59005",2.269,Medium,0.736,functional impact
pandas-dev/pandas#63818,doc: should be updated for 3.0,"### pandas version checks - [x] i have checked that the issue still exists on the latest versions of the docs on [here]( ### location of the documentation ### documentation problem the docs say that in 3.0 the api will change, and since it has changed, we should update the docs! ### suggested fix for documentation remove , and , since they have no effect any more??",[],['DOCUMENTATION'],"['Docs', 'Reshaping']",github,2026-01-22T23:10:10Z,,"doc: should be updated for 3.0 ### pandas version checks - [x] i have checked that the issue still exists on the latest versions of the docs on [here]( ### location of the documentation ### documentation problem the docs say that in 3.0 the api will change, and since it has changed, we should update the docs! ### suggested fix for documentation remove , and , since they have no effect any more??",3.4,High,0.993,crash-like behavior
cockroachdb/cockroach#161657,sql: allow inlining of scalar expressions with function calls and placeholders,"the optimizer currently only considers inlining scalar expressions that are [very simple]( in order to prevent an expensive scalar expression from being evaluated too many times. but inlining can enable other optimizations. here's an example where inlining scalar expressions (xy.x and xy.y) allows us to pick a better index: if the scalar expressions use placeholders or function calls, we can no longer find the plan with the better index, because the optimizer doesn't consider inlining the expressions: we should allow inlining of placeholders, and some function calls, in order to discover better query plans. jira issue: crdb-59006",[],['PERFORMANCE'],"['C-performance', 'O-support', 'A-sql-optimizer', 'T-sql-queries', 'A-generic-query-plans']",github,2026-01-23T00:28:13Z,,"sql: allow inlining of scalar expressions with function calls and placeholders the optimizer currently only considers inlining scalar expressions that are [very simple]( in order to prevent an expensive scalar expression from being evaluated too many times. but inlining can enable other optimizations. here's an example where inlining scalar expressions (xy.x and xy.y) allows us to pick a better index: if the scalar expressions use placeholders or function calls, we can no longer find the plan with the better index, because the optimizer doesn't consider inlining the expressions: we should allow inlining of placeholders, and some function calls, in order to discover better query plans. jira issue: crdb-59006",3.055,High,0.914,performance degradation
cockroachdb/cockroach#161658,backup: testcloudbackuprestoreazure failed,backup.testcloudbackuprestoreazure [failed]( with [artifacts]( on master @ [bd57614e1f04a1ebfc1515d2fee80bf628eb4331]( help see also: [how to investigate a go test failure \(internal\)]( /cc /disaster-recovery [this test on roachdash]( | [improve this report!]( jira issue: crdb-59007,[],['TESTING'],"['C-test-failure', 'O-robot', 'A-disaster-recovery', 'branch-master', 'T-disaster-recovery', 'P-3']",github,2026-01-23T00:33:16Z,,backup: testcloudbackuprestoreazure failed backup.testcloudbackuprestoreazure [failed]( with [artifacts]( on master @ [bd57614e1f04a1ebfc1515d2fee80bf628eb4331]( help see also: [how to investigate a go test failure \(internal\)]( /cc /disaster-recovery [this test on roachdash]( | [improve this report!]( jira issue: crdb-59007,1.6,Low,0.584,localized low-impact
flutter/flutter#181357,[web integration test] appconnectionexception / socketexception - chrome debug fails on flutter 3.38.7,"### steps to reproduce flutter crash report. please report a bug at ## command flutter drive --driver=test_driver/integration_test.dart --target=integration_test/test.dart --driver-port=5555 --web-port=4444 --verbose -d chrome ## exception socketexception: socketexception: connection refused (os error: connection refused, errno = 61), address = localhost, port = 62183 ## flutter doctor ### expected results web tests run successfully, as they did in last used version (3.35.6) the process is non-deterministic, as it works in some devices but fails in others. we've also experienced multiple failed and successful attempts in a unique device (e.g. macbook pro m2 - 16 gb ram - 256gb storage) when creating an empty project with a basic sum functionality and creating a simple test, the same command indicated works perfectly... ### actual results app is started, but tests never start running and the process crashes after a couple minutes with the given error. ### code sample code sample ### screenshots or video screenshots / video demonstration [upload media here] ### logs logs ### flutter doctor output doctor output",[],"['TESTING', 'BUG']","['c: crash', 'tool', 'a: debugging', 'platform-web', 'f: integration_test', 'team-tool']",github,2026-01-23T01:34:14Z,,"[web integration test] appconnectionexception / socketexception - chrome debug fails on flutter 3.38.7 ### steps to reproduce flutter crash report. please report a bug at ## command flutter drive --driver=test_driver/integration_test.dart --target=integration_test/test.dart --driver-port=5555 --web-port=4444 --verbose -d chrome ## exception socketexception: socketexception: connection refused (os error: connection refused, errno = 61), address = localhost, port = 62183 ## flutter doctor ### expected results web tests run successfully, as they did in last used version (3.35.6) the process is non-deterministic, as it works in some devices but fails in others. we've also experienced multiple failed and successful attempts in a unique device (e.g. macbook pro m2 - 16 gb ram - 256gb storage) when creating an empty project with a basic sum functionality and creating a simple test, the same command indicated works perfectly... ### actual results app is started, but tests never start running and the process crashes after a couple minutes with the given error. ### code sample code sample ### screenshots or video screenshots / video demonstration [upload media here] ### logs logs ### flutter doctor output doctor output",3.068,High,0.917,crash-like behavior
numpy/numpy#30715,"bug: ""illegal instruction (core dumped)"" directly after importing","### describe the issue: version 2.4.1 on linux crashes immediately upon import. in searching around for the error message, i found issues mentioning cpu flagging, such as , but the discussion in there is frankly beyond me and doesn't look related. if there is any further information i can provide, such as specs or the core dump, i'm happy to. i tried installing an earlier version that worked on windows and it does import properly. ### reproduce the code example: ### error message: ### python and numpy versions: the interpreter crashes immediately on import, but this is the relevant logging from pip install: the python interpreter says: ### runtime environment: unable to comply; crashes on import. ### how does this issue affect you or how did you find it: this was discoved while migrating code that worked on windows to a linux box. i can probably work around it by finding a version of numpy/pandas that doesn't immediately crash on import.",[],['BUG'],['00 - Bug'],github,2026-01-23T01:54:18Z,2026-01-23T03:46:11Z,"bug: ""illegal instruction (core dumped)"" directly after importing ### describe the issue: version 2.4.1 on linux crashes immediately upon import. in searching around for the error message, i found issues mentioning cpu flagging, such as , but the discussion in there is frankly beyond me and doesn't look related. if there is any further information i can provide, such as specs or the core dump, i'm happy to. i tried installing an earlier version that worked on windows and it does import properly. ### reproduce the code example: ### error message: ### python and numpy versions: the interpreter crashes immediately on import, but this is the relevant logging from pip install: the python interpreter says: ### runtime environment: unable to comply; crashes on import. ### how does this issue affect you or how did you find it: this was discoved while migrating code that worked on windows to a linux box. i can probably work around it by finding a version of numpy/pandas that doesn't immediately crash on import.",6.0,Critical,1.0,crash-like behavior
openssl/openssl#29725,evp_cipher_ctx_get_params return value not documented,"<!-- thank you for taking the time to report a documentation issue. please remember to tell us which openssl version you are using and then briefly describe the documentation error and where you encountered it (e.g., in which manual page). if you are missing the documentation for a certain command or api function, please tell us its name. --> hello! i'm looking at and i don't see any explanation of the return value of this call. when i control-f for the last entry is in the section but the entry prior is just before the section, skipping the section entirely. i see this function being used in a demo here: which seems to indicate 0 is an error but anything else is a good return value. could someone confirm and possibly update the documentation?",[],['DOCUMENTATION'],['triaged: documentation'],github,2026-01-23T01:58:07Z,,"evp_cipher_ctx_get_params return value not documented <!-- thank you for taking the time to report a documentation issue. please remember to tell us which openssl version you are using and then briefly describe the documentation error and where you encountered it (e.g., in which manual page). if you are missing the documentation for a certain command or api function, please tell us its name. --> hello! i'm looking at and i don't see any explanation of the return value of this call. when i control-f for the last entry is in the section but the entry prior is just before the section, skipping the section entirely. i see this function being used in a demo here: which seems to indicate 0 is an error but anything else is a good return value. could someone confirm and possibly update the documentation?",1.2,Low,0.493,localized low-impact
cilium/cilium#43944,lrp shows invalid ip for nodelocal dns when applying example manifest,"when applying the nodelocal dns manifest under the directory and listing lrps, cilium lrp list shows invalid ip as the frontend address. this issue only occurs on v1.18 and later. the same manifest works correctly on v1.17 and earlier, where a valid clusterip is shown. v1.17",[],['BUG'],"['kind/bug', 'area/datapath', 'kind/regression', 'area/lrp', 'affects/v1.18', 'affects/v1.19']",github,2026-01-23T02:00:29Z,,"lrp shows invalid ip for nodelocal dns when applying example manifest when applying the nodelocal dns manifest under the directory and listing lrps, cilium lrp list shows invalid ip as the frontend address. this issue only occurs on v1.18 and later. the same manifest works correctly on v1.17 and earlier, where a valid clusterip is shown. v1.17",2.608,Medium,0.813,functional impact
openssl/openssl#29726,ossl_cipher_param_padding doesn't give 0 or 1 like documentation says it should,"<!-- thank you for your bug report. if this is your first one, please take the time to read the following lines before posting it. note: if you're asking about how to use openssl, this isn't the right forum. please see our user support resources: please remember to tell us in what openssl version you found the issue. for build issues: if this is a build issue, please include the configuration output as well as a log of all errors. don't forget to include the exact commands you typed. with openssl before 1.1.1, the configuration output comes from the configuration command. with openssl 1.1.1 and on, it's the output of for other issues: if it isn't a build issue, example code or commands to reproduce the issue is highly appreciated. also, please remember to tell us if you worked with your own openssl build or if it is system provided. please remember to put $ echo output output output output output output #include int main() { int foo = 1; printf(""%d\n"", foo); } c int main(void) { evp_cipher_ctx* cipher_ctx = evp_cipher_ctx_new(); evp_cipher* cipher = evp_cipher_fetch(null, ""aes-256-gcm"", ""provider=default""); evp_encryptinit_ex2(cipher_ctx, cipher, key, null, null); unsigned int value = 99; ossl_param paramlist[2] = {ossl_param_construct_uint(ossl_cipher_param_padding, &value), ossl_param_end}; evp_cipher_ctx_get_params(cipher_ctx, paramlist); printf(""%u\n"", value); fflush(stdout); } ` (note, error checking explicitly removed to shorten sample code.) i see still is . i'm not sure why is returned unmodified. says the value should be 0 or 1. is there an expectation of zero initialization that i am missing? neither or indicate the passed should be initially set to .",[],['BUG'],['issue: bug report'],github,2026-01-23T02:48:43Z,2026-01-23T03:17:17Z,"ossl_cipher_param_padding doesn't give 0 or 1 like documentation says it should <!-- thank you for your bug report. if this is your first one, please take the time to read the following lines before posting it. note: if you're asking about how to use openssl, this isn't the right forum. please see our user support resources: please remember to tell us in what openssl version you found the issue. for build issues: if this is a build issue, please include the configuration output as well as a log of all errors. don't forget to include the exact commands you typed. with openssl before 1.1.1, the configuration output comes from the configuration command. with openssl 1.1.1 and on, it's the output of for other issues: if it isn't a build issue, example code or commands to reproduce the issue is highly appreciated. also, please remember to tell us if you worked with your own openssl build or if it is system provided. please remember to put $ echo output output output output output output #include int main() { int foo = 1; printf(""%d\n"", foo); } c int main(void) { evp_cipher_ctx* cipher_ctx = evp_cipher_ctx_new(); evp_cipher* cipher = evp_cipher_fetch(null, ""aes-256-gcm"", ""provider=default""); evp_encryptinit_ex2(cipher_ctx, cipher, key, null, null); unsigned int value = 99; ossl_param paramlist[2] = {ossl_param_construct_uint(ossl_cipher_param_padding, &value), ossl_param_end}; evp_cipher_ctx_get_params(cipher_ctx, paramlist); printf(""%u\n"", value); fflush(stdout); } ` (note, error checking explicitly removed to shorten sample code.) i see still is . i'm not sure why is returned unmodified. says the value should be 0 or 1. is there an expectation of zero initialization that i am missing? neither or indicate the passed should be initially set to .",4.2,Critical,1.0,system-wide impact
python/cpython#144167,sequencematcher.find_longest_match match expansion doesn't capture maximum range,"# feature or enhancement ### proposal: after match is found it is expanded both ways. first for popular (autojunk), then for . however, the order of thee results in the situation where it does not capture what is possible if both were done in a single loop. i.e. maybe after , we can expand for again. i think this should be done for both in one go. simply: this results in slightly wider match ranges for 2 test cases. ### has this already been discussed elsewhere? no response given ### links to previous discussion of this feature: _no response_",[],['FEATURE'],['type-feature'],github,2026-01-23T04:10:45Z,2026-01-23T06:04:44Z,"sequencematcher.find_longest_match match expansion doesn't capture maximum range # feature or enhancement ### proposal: after match is found it is expanded both ways. first for popular (autojunk), then for . however, the order of thee results in the situation where it does not capture what is possible if both were done in a single loop. i.e. maybe after , we can expand for again. i think this should be done for both in one go. simply: this results in slightly wider match ranges for 2 test cases. ### has this already been discussed elsewhere? no response given ### links to previous discussion of this feature: _no response_",1.4,Low,0.538,localized low-impact
kubernetes/kubernetes#136448,"[bug] kube-apiserver: does not disable rotation as documented, but defaults to 100mb via lumberjack**","### what happened? when configuring with audit logging enabled, setting results in audit logs being rotated when they reach **100mb**, instead of growing indefinitely (or not rotating based on size) as implied by the documentation. since default values for and are also (meaning ""retain all"" and ""retain forever""), the result is not a single large file, but an accumulation of many 100mb log files until disk exhaustion. ### what did you expect to happen? according to the help text and documentation: > : the maximum size in megabytes of the audit log file before it gets rotated. **if this is 0, the log file is not rotated.** i expected the file to grow indefinitely without rotation when is set to . ### how can we reproduce it (as minimally and precisely as possible)? 1. start with the following flags: 2. generate audit events sufficient to exceed 100mb (e.g., using a load testing tool or simply appending if testing the library directly, or heavy api usage). 3. observe that the log file is rotated (renamed with a timestamp) once it hits approximately 100mb. ### anything else we need to know? the issue lies in the interaction between and the underlying logging library . 1. **kubernetes implementation**: in , the backend is initialized with the user-provided : 2. **lumberjack implementation**: the library treats not as ""unlimited"", but as ""use default"". reference: in : consequently, passing effectively sets the rotation threshold to 100mb. ### kubernetes version ### cloud provider none ### os version ### install tools ### container runtime (cri) and version (if applicable) ### related plugins (cni, csi, ...) and versions (if applicable)",[],['BUG'],"['kind/bug', 'sig/api-machinery', 'needs-triage']",github,2026-01-23T04:21:19Z,,"[bug] kube-apiserver: does not disable rotation as documented, but defaults to 100mb via lumberjack** ### what happened? when configuring with audit logging enabled, setting results in audit logs being rotated when they reach **100mb**, instead of growing indefinitely (or not rotating based on size) as implied by the documentation. since default values for and are also (meaning ""retain all"" and ""retain forever""), the result is not a single large file, but an accumulation of many 100mb log files until disk exhaustion. ### what did you expect to happen? according to the help text and documentation: > : the maximum size in megabytes of the audit log file before it gets rotated. **if this is 0, the log file is not rotated.** i expected the file to grow indefinitely without rotation when is set to . ### how can we reproduce it (as minimally and precisely as possible)? 1. start with the following flags: 2. generate audit events sufficient to exceed 100mb (e.g., using a load testing tool or simply appending if testing the library directly, or heavy api usage). 3. observe that the log file is rotated (renamed with a timestamp) once it hits approximately 100mb. ### anything else we need to know? the issue lies in the interaction between and the underlying logging library . 1. **kubernetes implementation**: in , the backend is initialized with the user-provided : 2. **lumberjack implementation**: the library treats not as ""unlimited"", but as ""use default"". reference: in : consequently, passing effectively sets the rotation threshold to 100mb. ### kubernetes version ### cloud provider none ### os version ### install tools ### container runtime (cri) and version (if applicable) ### related plugins (cni, csi, ...) and versions (if applicable)",2.308,Medium,0.744,functional impact
tikv/tikv#19314,"log_backup: consume too many memory when exists huge data from raft observer, such as ddl reorganize partition",## bug report ### what version of tikv are you using? v8.5 and v8.1 ### what operating system and cpu are you using? x86-64 ### steps to reproduce trigger a huge data from raft observer in different regions at the same time by ddl reorganize partition. ### what did you expect? no oom ### what did happened? oom,[],['BUG'],"['type/bug', 'component/backup-restore', 'severity/major', 'affects-7.5', 'affects-8.1', 'affects-8.5']",github,2026-01-23T06:01:48Z,,"log_backup: consume too many memory when exists huge data from raft observer, such as ddl reorganize partition ## bug report ### what version of tikv are you using? v8.5 and v8.1 ### what operating system and cpu are you using? x86-64 ### steps to reproduce trigger a huge data from raft observer in different regions at the same time by ddl reorganize partition. ### what did you expect? no oom ### what did happened? oom",3.8,Critical,1.0,system-wide impact
docker/docker#51905,vulnerability in moby project,"### description while working on moby project, i identified a security vulnerability in the sigstore package related to its legacy tuf client implementation. the issue exists in the file caching mechanism used to store tuf target files on disk. the vulnerability occurs because the legacy tuf client constructs file paths by combining a cache base directory with a target name obtained from signed metadata without validating whether the final path stays within the intended cache directory. [cve link]( [cve report]( ### reproduce i scanned the application dependencies using vulert. vulert analyzed the dependency list and identified a vulnerability in the github.com/sigstore/rekor package. ### expected behavior _no response_ ### docker version ### docker info ### additional info _no response_",[],['BUG'],"['status/0-triage', 'kind/bug']",github,2026-01-23T06:43:17Z,2026-01-24T00:09:21Z,"vulnerability in moby project ### description while working on moby project, i identified a security vulnerability in the sigstore package related to its legacy tuf client implementation. the issue exists in the file caching mechanism used to store tuf target files on disk. the vulnerability occurs because the legacy tuf client constructs file paths by combining a cache base directory with a target name obtained from signed metadata without validating whether the final path stays within the intended cache directory. [cve link]( [cve report]( ### reproduce i scanned the application dependencies using vulert. vulert analyzed the dependency list and identified a vulnerability in the github.com/sigstore/rekor package. ### expected behavior _no response_ ### docker version ### docker info ### additional info _no response_",4.2,Critical,1.0,system-wide impact
nodejs/node#61487,add an option not to trap sigint (ctrl + c) to,"### what is the problem this feature will solve? here is a typical code to accepts multiple lines from stdin: however, if you press ctrl + c, it is treated as ctrl + d‚Äîi.e. ""done"" is displayed and you cannot cancel the multiline input. it is a ridiculous behavior. throws and messes up your terminal: it is not sophisticated. ### what is the feature you are proposing to solve the problem? add an option to to prevent it from trapping by default. with this, you will be able to press ctrl + c to terminate the program immediately with the proper exit code. ### what alternatives have you considered? why do i have to add such a code? who the hell can remember such numbers for both platforms?",[],['FEATURE'],['feature request'],github,2026-01-23T06:53:01Z,,"add an option not to trap sigint (ctrl + c) to ### what is the problem this feature will solve? here is a typical code to accepts multiple lines from stdin: however, if you press ctrl + c, it is treated as ctrl + d‚Äîi.e. ""done"" is displayed and you cannot cancel the multiline input. it is a ridiculous behavior. throws and messes up your terminal: it is not sophisticated. ### what is the feature you are proposing to solve the problem? add an option to to prevent it from trapping by default. with this, you will be able to press ctrl + c to terminate the program immediately with the proper exit code. ### what alternatives have you considered? why do i have to add such a code? who the hell can remember such numbers for both platforms?",1.4,Low,0.538,localized low-impact
cockroachdb/cockroach#161661,sql/tests: testrandomsyntaxsqlsmith failed,sql/tests.testrandomsyntaxsqlsmith [failed]( with [artifacts]( on release-26.1.0-rc @ [6c9b2f778bd29895383d493a2fa4490323499ecf]( help see also: [how to investigate a go test failure \(internal\)]( same failure on other branches - sql/tests: testrandomsyntaxsqlsmith failed [c-test-failure o-robot t-sql-queries branch-release-26.1] /cc /sql-foundations [this test on roachdash]( | [improve this report!]( jira issue: crdb-59010,[],['TESTING'],"['C-test-failure', 'O-robot', 'X-duplicate', 'T-sql-queries', 'branch-release-26.1.0-rc']",github,2026-01-23T06:57:16Z,2026-01-26T03:28:01Z,sql/tests: testrandomsyntaxsqlsmith failed sql/tests.testrandomsyntaxsqlsmith [failed]( with [artifacts]( on release-26.1.0-rc @ [6c9b2f778bd29895383d493a2fa4490323499ecf]( help see also: [how to investigate a go test failure \(internal\)]( same failure on other branches - sql/tests: testrandomsyntaxsqlsmith failed [c-test-failure o-robot t-sql-queries branch-release-26.1] /cc /sql-foundations [this test on roachdash]( | [improve this report!]( jira issue: crdb-59010,1.6,Low,0.584,localized low-impact
flutter/flutter#181372,dropdownmenuformfield is missing parameters from dropdownmenu,### use case the parameters of dropdownmenu are needed in order to better control the behavior of the dropdownmenuformfield. see the individual params for further detail: - showtrailingicon - selectonly - cursorheight - trailingiconfocusnode - menucontroller ### proposal add the missing params to dropdownmenuformfield.,[],['FEATURE'],"['c: new feature', 'framework', 'f: material design', 'c: proposal', 'team-design']",github,2026-01-23T07:07:54Z,,dropdownmenuformfield is missing parameters from dropdownmenu ### use case the parameters of dropdownmenu are needed in order to better control the behavior of the dropdownmenuformfield. see the individual params for further detail: - showtrailingicon - selectonly - cursorheight - trailingiconfocusnode - menucontroller ### proposal add the missing params to dropdownmenuformfield.,1.4,Low,0.538,localized low-impact
tikv/tikv#19318,enhance the resource control mechanism,## development task the current priority based scheduling mechanism is too weak that it can resolve the problem that hotspot workloads (either read or write) that significantly influence other workload's latency even if the provisioned ru_per_sec is low and the priority is set to medium or low. we need to enhance it which something like an auto-tuned resource limiter that can limit the cpu usage of all resource groups to certain level to control the impact of a single resource group on hotspot tikv instances.,[],['FEATURE'],['type/enhancement'],github,2026-01-23T07:11:35Z,,enhance the resource control mechanism ## development task the current priority based scheduling mechanism is too weak that it can resolve the problem that hotspot workloads (either read or write) that significantly influence other workload's latency even if the provisioned ru_per_sec is low and the priority is set to medium or low. we need to enhance it which something like an auto-tuned resource limiter that can limit the cpu usage of all resource groups to certain level to control the impact of a single resource group on hotspot tikv instances.,4.6,Critical,1.0,system-wide impact
pytorch/pytorch#173160,feature request: load_state_dict should optionally support partial tensor filling in non-strict mode,"### üöÄ the feature, motivation and pitch the current with only loads weights into keys where shapes match. a recent work [(poseadapt, wacv'26)]( uses partial weight initialization in such cases. if a weight matrix has grown (because of model shape changes), pretrained weights are copied into a subset of the matrix, and remaining rows/columns are filled with a normal initialization method. a similar method was also used in a [cvpr'25 paper]( in the original paper, the use case is pose estimation where we have a pretrained weights from a model that predicts k keypoints, which we are loading into a model which must predict k+n keypoints, and showed significantly improved results in knowledge transfer from the pretrained weights. i believe that this can be generalized beyond pose estimation. my suggestion would be to add an additional parameter in . when and , the proposed method can be used. poseadapt's implementation is available [here]( which can be used as a starting point for this feature. ### additional context disclaimer: i am the author of both linked papers and designer of this method. cc",[],['FEATURE'],"['feature', 'module: nn', 'triaged']",github,2026-01-23T07:37:08Z,,"feature request: load_state_dict should optionally support partial tensor filling in non-strict mode ### üöÄ the feature, motivation and pitch the current with only loads weights into keys where shapes match. a recent work [(poseadapt, wacv'26)]( uses partial weight initialization in such cases. if a weight matrix has grown (because of model shape changes), pretrained weights are copied into a subset of the matrix, and remaining rows/columns are filled with a normal initialization method. a similar method was also used in a [cvpr'25 paper]( in the original paper, the use case is pose estimation where we have a pretrained weights from a model that predicts k keypoints, which we are loading into a model which must predict k+n keypoints, and showed significantly improved results in knowledge transfer from the pretrained weights. i believe that this can be generalized beyond pose estimation. my suggestion would be to add an additional parameter in . when and , the proposed method can be used. poseadapt's implementation is available [here]( which can be used as a starting point for this feature. ### additional context disclaimer: i am the author of both linked papers and designer of this method. cc",3.6,Critical,1.0,crash-like behavior
cockroachdb/cockroach#161663,kv/kvserver: testmergequeuewithslownonvotersnaps failed,kv/kvserver.testmergequeuewithslownonvotersnaps [failed]( on release-26.1.0-rc @ [6c9b2f778bd29895383d493a2fa4490323499ecf]( fatal error: stack: log preceding fatal error parameters: - attempt=1 - run=9 - shard=25 help see also: [how to investigate a go test failure \(internal\)]( same failure on other branches - kv/kvserver: testmergequeuewithslownonvotersnaps failed [c-bug c-test-failure o-robot p-2 t-kv branch-release-26.1] /cc /kv-triage [this test on roachdash]( | [improve this report!]( jira issue: crdb-59012,[],"['TESTING', 'BUG']","['C-bug', 'C-test-failure', 'O-robot', 'T-kv', 'A-kv-rangefeed', 'P-2', 'branch-release-26.1.0-rc']",github,2026-01-23T07:41:10Z,,kv/kvserver: testmergequeuewithslownonvotersnaps failed kv/kvserver.testmergequeuewithslownonvotersnaps [failed]( on release-26.1.0-rc @ [6c9b2f778bd29895383d493a2fa4490323499ecf]( fatal error: stack: log preceding fatal error parameters: - attempt=1 - run=9 - shard=25 help see also: [how to investigate a go test failure \(internal\)]( same failure on other branches - kv/kvserver: testmergequeuewithslownonvotersnaps failed [c-bug c-test-failure o-robot p-2 t-kv branch-release-26.1] /cc /kv-triage [this test on roachdash]( | [improve this report!]( jira issue: crdb-59012,3.8,Critical,1.0,crash-like behavior
cockroachdb/cockroach#161664,roachtest: ssh_problem failed,roachtest.ssh_problem [failed]( with [artifacts]( on release-25.2.12-rc @ [7fc1e95228ad6610c2ea21d8ba55d0d6513c319f]( parameters: - cloud=gce - coveragebuild=false - cpu=8 - diskcount=0 - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( same failure on other branches - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.4.4-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-26.1.0-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.3.7-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.4.3-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.2.11-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-24.3.25-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-26.1] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.3.6-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-24.3.24-rc] - roachtest: ssh_problem failed [o-roachtest o-robot o-rsg t-testeng x-infra-flake branch-release-25.2.10-rc] /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-59013,[],['TESTING'],"['O-robot', 'O-roachtest', 'T-testeng', 'X-infra-flake', 'branch-release-25.2.12-rc']",github,2026-01-23T07:42:26Z,,roachtest: ssh_problem failed roachtest.ssh_problem [failed]( with [artifacts]( on release-25.2.12-rc @ [7fc1e95228ad6610c2ea21d8ba55d0d6513c319f]( parameters: - cloud=gce - coveragebuild=false - cpu=8 - diskcount=0 - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( same failure on other branches - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.4.4-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-26.1.0-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.3.7-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.4.3-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.2.11-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-24.3.25-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-26.1] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.3.6-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-24.3.24-rc] - roachtest: ssh_problem failed [o-roachtest o-robot o-rsg t-testeng x-infra-flake branch-release-25.2.10-rc] /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-59013,2.889,Medium,0.876,functional impact
cockroachdb/cockroach#161665,pkg/ccl/testccl/workload/schemachange/schemachange_test: testworkload failed,pkg/ccl/testccl/workload/schemachange/schemachange_test.testworkload [failed]( on release-25.2 @ [a80059a42d03d5240f6efdad0324376dca17aa59]( parameters: - attempt=1 - run=10 - shard=1 help see also: [how to investigate a go test failure \(internal\)]( same failure on other branches - pkg/ccl/testccl/workload/schemachange/schemachange_test: testworkload failed [c-test-failure o-robot p-3 t-sql-foundations branch-release-24.3] - pkg/ccl/testccl/workload/schemachange/schemachange_test: testworkload failed [cannot specify a foreign key update action and an on update expression on the same column] [c-test-failure o-robot p-2 t-sql-foundations branch-master] /cc /sql-foundations [this test on roachdash]( | [improve this report!]( jira issue: crdb-59014,[],['TESTING'],"['C-test-failure', 'O-robot', 'T-sql-foundations', 'P-2', 'branch-release-25.2']",github,2026-01-23T07:54:28Z,,pkg/ccl/testccl/workload/schemachange/schemachange_test: testworkload failed pkg/ccl/testccl/workload/schemachange/schemachange_test.testworkload [failed]( on release-25.2 @ [a80059a42d03d5240f6efdad0324376dca17aa59]( parameters: - attempt=1 - run=10 - shard=1 help see also: [how to investigate a go test failure \(internal\)]( same failure on other branches - pkg/ccl/testccl/workload/schemachange/schemachange_test: testworkload failed [c-test-failure o-robot p-3 t-sql-foundations branch-release-24.3] - pkg/ccl/testccl/workload/schemachange/schemachange_test: testworkload failed [cannot specify a foreign key update action and an on update expression on the same column] [c-test-failure o-robot p-2 t-sql-foundations branch-master] /cc /sql-foundations [this test on roachdash]( | [improve this report!]( jira issue: crdb-59014,3.8,Critical,1.0,crash-like behavior
cockroachdb/cockroach#161666,roachtest: cdc/multi-table-pts-benchmark/per-table-pts=false/num-tables=50000/num-ranges=1 failed,roachtest.cdc/multi-table-pts-benchmark/per-table-pts=false/num-tables=50000/num-ranges=1 [failed]( with [artifacts]( on master @ [a66b5adfec1f99a27dd1c4dbbbe85ef9383591c4]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21013638-1769143262-87-n4cpu16-0001 | 13.116.85.120 | 10.250.0.196 | | teamcity-21013638-1769143262-87-n4cpu16-0002 | 13.116.82.67 | 10.250.0.195 | | teamcity-21013638-1769143262-87-n4cpu16-0003 | 13.116.84.43 | 10.250.0.194 | | teamcity-21013638-1769143262-87-n4cpu16-0004 | 13.116.94.37 | 10.250.0.197 | parameters: - arch=s390x - cloud=ibm - coveragebuild=false - cpu=16 - diskcount=0 - encrypted=false - fs=ext4 - localssd=false - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( _grafana is not yet available for ibm clusters_ same failure on other branches - roachtest: cdc/multi-table-pts-benchmark/per-table-pts=false/num-tables=50000/num-ranges=1 failed [a-cdc c-test-failure o-roachtest o-robot p-3 t-cdc branch-release-26.1] /cc /cdc [this test on roachdash]( | [improve this report!]( jira issue: crdb-59015,[],['TESTING'],"['C-test-failure', 'O-robot', 'A-cdc', 'O-roachtest', 'branch-master', 'T-cdc', 's390x-test-failure']",github,2026-01-23T07:54:51Z,,roachtest: cdc/multi-table-pts-benchmark/per-table-pts=false/num-tables=50000/num-ranges=1 failed roachtest.cdc/multi-table-pts-benchmark/per-table-pts=false/num-tables=50000/num-ranges=1 [failed]( with [artifacts]( on master @ [a66b5adfec1f99a27dd1c4dbbbe85ef9383591c4]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21013638-1769143262-87-n4cpu16-0001 | 13.116.85.120 | 10.250.0.196 | | teamcity-21013638-1769143262-87-n4cpu16-0002 | 13.116.82.67 | 10.250.0.195 | | teamcity-21013638-1769143262-87-n4cpu16-0003 | 13.116.84.43 | 10.250.0.194 | | teamcity-21013638-1769143262-87-n4cpu16-0004 | 13.116.94.37 | 10.250.0.197 | parameters: - arch=s390x - cloud=ibm - coveragebuild=false - cpu=16 - diskcount=0 - encrypted=false - fs=ext4 - localssd=false - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( _grafana is not yet available for ibm clusters_ same failure on other branches - roachtest: cdc/multi-table-pts-benchmark/per-table-pts=false/num-tables=50000/num-ranges=1 failed [a-cdc c-test-failure o-roachtest o-robot p-3 t-cdc branch-release-26.1] /cc /cdc [this test on roachdash]( | [improve this report!]( jira issue: crdb-59015,3.295,High,0.969,system-wide impact
envoyproxy/envoy#43130,newer release available : v1.67.0 (current: v1.66.2),package name: aws_lc .66.2 current version: v1.66.2 -01-06 available version: v1.67.0 -01-22 upstream releases:,[],['UI'],"['dependencies', 'no stalebot', 'area/build']",github,2026-01-23T08:10:46Z,,newer release available : v1.67.0 (current: v1.66.2) package name: aws_lc .66.2 current version: v1.66.2 -01-06 available version: v1.67.0 -01-22 upstream releases:,1.8,Low,0.629,user-visible issue
flutter/flutter#181377,[native assets] bundle code assets debug symbols in separate dsym file,"currently the the dynamic libraries from build hooks are bundled as is. on android and ios we should run some compiler commands to strip them from debug symbols to reduce code size and provide the symbol files on the side. * android: + + . * ios: + for ios we have code signing logic on copying. that would probably the right place to strip the symbols as well: for android, we can add the stripping also just before the copying: thanks for the suggestion ! (note: since the debug symbols can be stripped by the sdk invoking the build hooks, we don't provide anything the build hooks itself to allow for reporting debug symbols in a separate file.)",[],['UI'],"['tool', 'c: proposal', 'a: build', 'team-tool']",github,2026-01-23T08:49:47Z,,"[native assets] bundle code assets debug symbols in separate dsym file currently the the dynamic libraries from build hooks are bundled as is. on android and ios we should run some compiler commands to strip them from debug symbols to reduce code size and provide the symbol files on the side. * android: + + . * ios: + for ios we have code signing logic on copying. that would probably the right place to strip the symbols as well: for android, we can add the stripping also just before the copying: thanks for the suggestion ! (note: since the debug symbols can be stripped by the sdk invoking the build hooks, we don't provide anything the build hooks itself to allow for reporting debug symbols in a separate file.)",1.8,Low,0.629,user-visible issue
pandas-dev/pandas#63830,bug: read_excel with pyarrow backend does not infer correct data type with mixed values inside column,"### pandas version checks - [x] i have checked that this issue has not already been reported. - [x] i have confirmed this bug exists on the [latest version]( of pandas. - [x] i have confirmed this bug exists on the [main branch]( of pandas. ### reproducible example ### issue description hello, i have updated to the latest version of pandas 3.0.0 and i got some processes which fail. the issue seems to be linked to the function when i specify the . indeed, it seems to infer only from the first row the type of the column whereas with the , i find myself with a ""object"" type. i get the following stacktrace: kind regards ### expected behavior multiple behaviours can be expected: - crash as of today - coerce to string type - add an additional parameter for type conversion in case of error ### installed versions installed versions ------------------ commit : 366ccdfcd8ed1e5543bfb6d4ee0c9bc519898670 python : 3.13.11 python-bits : 64 os : linux os-release : 6.6.87.2-microsoft-standard-wsl2 version : smp preempt_dynamic thu jun 5 18:30:46 utc 2025 machine : x86_64 processor : byteorder : little lc_all : none lang : none locale : c.utf-8 pandas : 3.0.0 numpy : 2.4.1 dateutil : 2.9.0.post0 pip : 25.3 cython : none sphinx : none ipython : none adbc-driver-postgresql: none adbc-driver-sqlite : none bs4 : none bottleneck : none fastparquet : none fsspec : 2026.1.0 html5lib : none hypothesis : none gcsfs : none jinja2 : 3.1.6 lxml.etree : 6.0.2 matplotlib : none numba : none numexpr : none odfpy : none openpyxl : 3.1.5 psycopg2 : 2.9.11 pymysql : 1.4.6 pyarrow : 23.0.0 pyiceberg : none pyreadstat : none pytest : 9.0.2 python-calamine : none pytz : 2025.2 pyxlsb : none s3fs : none scipy : none sqlalchemy : 2.0.46 tables : n/a tabulate : none xarray : none xlrd : 2.0.2 xlsxwriter : none zstandard : none qtpy : none pyqt5 : none",[],['BUG'],"['Bug', 'IO Excel', 'Arrow']",github,2026-01-23T09:05:26Z,,"bug: read_excel with pyarrow backend does not infer correct data type with mixed values inside column ### pandas version checks - [x] i have checked that this issue has not already been reported. - [x] i have confirmed this bug exists on the [latest version]( of pandas. - [x] i have confirmed this bug exists on the [main branch]( of pandas. ### reproducible example ### issue description hello, i have updated to the latest version of pandas 3.0.0 and i got some processes which fail. the issue seems to be linked to the function when i specify the . indeed, it seems to infer only from the first row the type of the column whereas with the , i find myself with a ""object"" type. i get the following stacktrace: kind regards ### expected behavior multiple behaviours can be expected: - crash as of today - coerce to string type - add an additional parameter for type conversion in case of error ### installed versions installed versions ------------------ commit : 366ccdfcd8ed1e5543bfb6d4ee0c9bc519898670 python : 3.13.11 python-bits : 64 os : linux os-release : 6.6.87.2-microsoft-standard-wsl2 version : smp preempt_dynamic thu jun 5 18:30:46 utc 2025 machine : x86_64 processor : byteorder : little lc_all : none lang : none locale : c.utf-8 pandas : 3.0.0 numpy : 2.4.1 dateutil : 2.9.0.post0 pip : 25.3 cython : none sphinx : none ipython : none adbc-driver-postgresql: none adbc-driver-sqlite : none bs4 : none bottleneck : none fastparquet : none fsspec : 2026.1.0 html5lib : none hypothesis : none gcsfs : none jinja2 : 3.1.6 lxml.etree : 6.0.2 matplotlib : none numba : none numexpr : none odfpy : none openpyxl : 3.1.5 psycopg2 : 2.9.11 pymysql : 1.4.6 pyarrow : 23.0.0 pyiceberg : none pyreadstat : none pytest : 9.0.2 python-calamine : none pytz : 2025.2 pyxlsb : none s3fs : none scipy : none sqlalchemy : 2.0.46 tables : n/a tabulate : none xarray : none xlrd : 2.0.2 xlsxwriter : none zstandard : none qtpy : none pyqt5 : none",6.4,Critical,1.0,crash-like behavior
cockroachdb/cockroach#161667,roachtest: ssh_problem failed,roachtest.ssh_problem [failed]( with [artifacts]( on release-25.3.8-rc @ [129f86631b285cfd2bd8740db920947b5a9c89ab]( parameters: - cloud=gce - coveragebuild=false - cpu=2 - runtimeassertionsbuild=false - ssd=0 help see: [roachtest readme]( see: [how to investigate \(internal\)]( same failure on other branches - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.2.12-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.4.4-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-26.1.0-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.3.7-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.4.3-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.2.11-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-24.3.25-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-26.1] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.3.6-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-24.3.24-rc] /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-59018,[],['TESTING'],"['O-robot', 'O-roachtest', 'T-testeng', 'X-infra-flake', 'branch-release-25.3.8-rc']",github,2026-01-23T10:00:08Z,,roachtest: ssh_problem failed roachtest.ssh_problem [failed]( with [artifacts]( on release-25.3.8-rc @ [129f86631b285cfd2bd8740db920947b5a9c89ab]( parameters: - cloud=gce - coveragebuild=false - cpu=2 - runtimeassertionsbuild=false - ssd=0 help see: [roachtest readme]( see: [how to investigate \(internal\)]( same failure on other branches - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.2.12-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.4.4-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-26.1.0-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.3.7-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.4.3-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.2.11-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-24.3.25-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-26.1] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.3.6-rc] - roachtest: ssh_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-24.3.24-rc] /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-59018,3.162,High,0.939,functional impact
nodejs/node#61489,socket.settos,### what is the problem this feature will solve? allow users to set the type of service (tos) field on a tcp socket for e.g. priority hints for kernel and switches. ### what is the feature you are proposing to solve the problem? add a 2 new methods and . ### what alternatives have you considered? _no response_,[],['FEATURE'],['feature request'],github,2026-01-23T10:18:45Z,,socket.settos ### what is the problem this feature will solve? allow users to set the type of service (tos) field on a tcp socket for e.g. priority hints for kernel and switches. ### what is the feature you are proposing to solve the problem? add a 2 new methods and . ### what alternatives have you considered? _no response_,3.6,Critical,1.0,crash-like behavior
cilium/cilium#43947,ci: privileged unit tests - testprivilegedegressgatewaymanager,## ci failure hit on link: ` ‚õëÔ∏è the following owners are responsible for reliability of the failing test(s): github.com/cilium/cilium/pkg/egressgateway /egress-gateway,[],['FEATURE'],"['area/CI', 'ci/flake', 'feature/egress-gateway']",github,2026-01-23T10:21:41Z,,ci: privileged unit tests - testprivilegedegressgatewaymanager ## ci failure hit on link: ` ‚õëÔ∏è the following owners are responsible for reliability of the failing test(s): github.com/cilium/cilium/pkg/egressgateway /egress-gateway,1.4,Low,0.538,localized low-impact
containerd/containerd#12811,containerd multipart fetch cancelled by progress timeout,"### description we experienced a multipart fetch bug in production on containerd v2.2.1. **symptom:** image pulls failing with followed by 16 ""aborted"" errors after 5-minute progress timeout. **environment:** - containerd v2.2.1 - (8mb chunks) - - large image layer: ~15gb **logs:** i think that one request gets ""exhausted"" by the other ones, but, we are still downloading other chunks, and the progress timeout hits. ### steps to reproduce the issue 1. conf multipart fetch 2. pull a large image 3. ( this happens sporadically ) ### describe the results you received and expected i expected the pull to pull ### what version of containerd are you using? v2.2.1 ### any other relevant information _no response_ ### show configuration if it is related to cri plugin. _no response_",[],['BUG'],"['kind/bug', 'area/distribution']",github,2026-01-23T10:31:48Z,,"containerd multipart fetch cancelled by progress timeout ### description we experienced a multipart fetch bug in production on containerd v2.2.1. **symptom:** image pulls failing with followed by 16 ""aborted"" errors after 5-minute progress timeout. **environment:** - containerd v2.2.1 - (8mb chunks) - - large image layer: ~15gb **logs:** i think that one request gets ""exhausted"" by the other ones, but, we are still downloading other chunks, and the progress timeout hits. ### steps to reproduce the issue 1. conf multipart fetch 2. pull a large image 3. ( this happens sporadically ) ### describe the results you received and expected i expected the pull to pull ### what version of containerd are you using? v2.2.1 ### any other relevant information _no response_ ### show configuration if it is related to cri plugin. _no response_",2.469,Medium,0.781,functional impact
cockroachdb/cockroach#161668,roachtest: cluster_creation failed,roachtest.cluster_creation [failed]( with [artifacts]( on master @ [a66b5adfec1f99a27dd1c4dbbbe85ef9383591c4]( parameters: - cloud=ibm - coveragebuild=false - cpu=32 - diskcount=0 - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-59020,[],['TESTING'],"['O-robot', 'O-roachtest', 'branch-master', 'T-testeng', 'X-infra-flake']",github,2026-01-23T10:49:41Z,,roachtest: cluster_creation failed roachtest.cluster_creation [failed]( with [artifacts]( on master @ [a66b5adfec1f99a27dd1c4dbbbe85ef9383591c4]( parameters: - cloud=ibm - coveragebuild=false - cpu=32 - diskcount=0 - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-59020,2.82,Medium,0.861,functional impact
cockroachdb/cockroach#161669,roachtest: cluster_creation failed,roachtest.cluster_creation [failed]( with [artifacts]( on master @ [a66b5adfec1f99a27dd1c4dbbbe85ef9383591c4]( parameters: - cloud=ibm - coveragebuild=false - cpu=16 - diskcount=0 - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-59022,[],['TESTING'],"['O-robot', 'O-roachtest', 'branch-master', 'T-testeng', 'X-infra-flake']",github,2026-01-23T10:49:51Z,,roachtest: cluster_creation failed roachtest.cluster_creation [failed]( with [artifacts]( on master @ [a66b5adfec1f99a27dd1c4dbbbe85ef9383591c4]( parameters: - cloud=ibm - coveragebuild=false - cpu=16 - diskcount=0 - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-59022,3.045,High,0.912,functional impact
cockroachdb/cockroach#161670,roachtest: cluster_creation failed,roachtest.cluster_creation [failed]( with [artifacts]( on master @ [a66b5adfec1f99a27dd1c4dbbbe85ef9383591c4]( parameters: - cloud=ibm - coveragebuild=false - cpu=32 - diskcount=0 - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-59021,[],['TESTING'],"['O-robot', 'O-roachtest', 'branch-master', 'T-testeng', 'X-infra-flake']",github,2026-01-23T10:49:52Z,,roachtest: cluster_creation failed roachtest.cluster_creation [failed]( with [artifacts]( on master @ [a66b5adfec1f99a27dd1c4dbbbe85ef9383591c4]( parameters: - cloud=ibm - coveragebuild=false - cpu=32 - diskcount=0 - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-59021,3.092,High,0.923,functional impact
cockroachdb/cockroach#161671,roachtest: cluster_creation failed,roachtest.cluster_creation [failed]( with [artifacts]( on master @ [a66b5adfec1f99a27dd1c4dbbbe85ef9383591c4]( parameters: - cloud=ibm - coveragebuild=false - cpu=32 - diskcount=0 - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-59023,[],['TESTING'],"['O-robot', 'O-roachtest', 'branch-master', 'T-testeng', 'X-infra-flake']",github,2026-01-23T10:49:52Z,,roachtest: cluster_creation failed roachtest.cluster_creation [failed]( with [artifacts]( on master @ [a66b5adfec1f99a27dd1c4dbbbe85ef9383591c4]( parameters: - cloud=ibm - coveragebuild=false - cpu=32 - diskcount=0 - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-59023,2.979,Medium,0.897,functional impact
cockroachdb/cockroach#161672,roachtest: cluster_creation failed,roachtest.cluster_creation [failed]( with [artifacts]( on master @ [a66b5adfec1f99a27dd1c4dbbbe85ef9383591c4]( parameters: - cloud=ibm - coveragebuild=false - cpu=32 - diskcount=0 - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-59024,[],['TESTING'],"['O-robot', 'O-roachtest', 'branch-master', 'T-testeng', 'X-infra-flake']",github,2026-01-23T10:49:53Z,,roachtest: cluster_creation failed roachtest.cluster_creation [failed]( with [artifacts]( on master @ [a66b5adfec1f99a27dd1c4dbbbe85ef9383591c4]( parameters: - cloud=ibm - coveragebuild=false - cpu=32 - diskcount=0 - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-59024,2.977,Medium,0.897,functional impact
cockroachdb/cockroach#161673,roachtest: cluster_creation failed,roachtest.cluster_creation [failed]( with [artifacts]( on master @ [a66b5adfec1f99a27dd1c4dbbbe85ef9383591c4]( parameters: - cloud=ibm - coveragebuild=false - cpu=32 - diskcount=0 - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-59025,[],['TESTING'],"['O-robot', 'O-roachtest', 'branch-master', 'T-testeng', 'X-infra-flake']",github,2026-01-23T10:49:53Z,,roachtest: cluster_creation failed roachtest.cluster_creation [failed]( with [artifacts]( on master @ [a66b5adfec1f99a27dd1c4dbbbe85ef9383591c4]( parameters: - cloud=ibm - coveragebuild=false - cpu=32 - diskcount=0 - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-59025,2.843,Medium,0.866,functional impact
openssl/openssl#29737,fips provider sigsev when used into a shared lib,"openssl 3.5.4 and openssl 3.0.18 i have a program that performs cryptographic operations through a pkcs interface, dynamically loaded as a shared library (dlopen). this pkcs uses openssl to access a nethsm box (tls1.2 or tls1.3). by configuration, we can optionally use the fips provider. in this case, the program crash with sigsev when exiting. additionnal files: main_snippet.c extract from the main program p11_snippet.c extract from the pkcs startup openssl.cnf the used openssl configuration configdata.txt the configuration data used to build openssl gdb-bt.txt the back-trace see by gdb and finally, a patch that corrects this malfunction: fips.patch [p11_snippet.c]( [main_snippet.c]( [fips.patch]( [configdata.txt]( [gdb-bt.txt](",[],['BUG'],['issue: bug report'],github,2026-01-23T11:29:11Z,,"fips provider sigsev when used into a shared lib openssl 3.5.4 and openssl 3.0.18 i have a program that performs cryptographic operations through a pkcs interface, dynamically loaded as a shared library (dlopen). this pkcs uses openssl to access a nethsm box (tls1.2 or tls1.3). by configuration, we can optionally use the fips provider. in this case, the program crash with sigsev when exiting. additionnal files: main_snippet.c extract from the main program p11_snippet.c extract from the pkcs startup openssl.cnf the used openssl configuration configdata.txt the configuration data used to build openssl gdb-bt.txt the back-trace see by gdb and finally, a patch that corrects this malfunction: fips.patch [p11_snippet.c]( [main_snippet.c]( [fips.patch]( [configdata.txt]( [gdb-bt.txt](",4.6,Critical,1.0,crash-like behavior
kubernetes/kubernetes#136460,add missing tests for kubectl describe commands,"### what happened? several kubectl resource describers don't have corresponding tests in . ### what did you expect to happen? these describers should have basic test to verify their output formatting and prevent regressions. the following describers have implementations but no tests: - - - - - - - ### how can we reproduce it (as minimally and precisely as possible)? check . you won't find test functions like , , etc. for the listed resources. ### anything else we need to know? i checked describers registered in the describer and found these are missing tests. i excluded deprecated resources ( , which don't have any test in the same way) and resources already covered under different api groups ( , under ). ### kubernetes version ### cloud provider ### os version ### install tools ### container runtime (cri) and version (if applicable) ### related plugins (cni, csi, ...) and versions (if applicable)",[],['BUG'],"['kind/bug', 'sig/cli', 'triage/accepted']",github,2026-01-23T11:30:14Z,2026-01-28T11:07:54Z,"add missing tests for kubectl describe commands ### what happened? several kubectl resource describers don't have corresponding tests in . ### what did you expect to happen? these describers should have basic test to verify their output formatting and prevent regressions. the following describers have implementations but no tests: - - - - - - - ### how can we reproduce it (as minimally and precisely as possible)? check . you won't find test functions like , , etc. for the listed resources. ### anything else we need to know? i checked describers registered in the describer and found these are missing tests. i excluded deprecated resources ( , which don't have any test in the same way) and resources already covered under different api groups ( , under ). ### kubernetes version ### cloud provider ### os version ### install tools ### container runtime (cri) and version (if applicable) ### related plugins (cni, csi, ...) and versions (if applicable)",4.2,Critical,1.0,system-wide impact
python/cpython#144173,test_complex.test_truediv can fail,"# bug report ### bug description: has [the following check]( where checks that is [""close""]( to . this recently [failed on ]( with: it fails with for the first random number and for the rest. looks like a test bug but it might be better if a float/complex expert takes a look around here. , is this interesting for you? ### cpython versions tested on: cpython main branch ### operating systems tested on: linux",[],"['TESTING', 'BUG']","['type-bug', 'tests', 'interpreter-core']",github,2026-01-23T11:38:55Z,,"test_complex.test_truediv can fail # bug report ### bug description: has [the following check]( where checks that is [""close""]( to . this recently [failed on ]( with: it fails with for the first random number and for the rest. looks like a test bug but it might be better if a float/complex expert takes a look around here. , is this interesting for you? ### cpython versions tested on: cpython main branch ### operating systems tested on: linux",2.0,Medium,0.675,localized low-impact
pytorch/pytorch#173173,"torch.randn slow on cpu, bfloat16 significantly slower than float32","### üêõ describe the bug seems to be very slow, especially for dtype on cpu example code: **results** here is a comparison of a few cpu architectures | | aarch64 ( c7g ) | aarch64 ( c8g ) | x86_64 ( c7i )| | -------- | -------- | -------- | ------- | | float32 | 17988ms | 23009ms | 8382ms | | bfloat16 | 32551ms | 42559ms | 42155ms | for comparison, the same code with ( uniform distribution ) | | aarch64 ( c8g ) | x86_64 ( c7i )| | -------- | -------- | -------- | | float32 | 8074ms | 8257ms | | bfloat16 | 9233ms | 9245ms | so there are a few takeaways from this - there seems to be an issue that affects all cpu platforms - bfloat16 is signifantly slower than float32 for randn ( ~3-4x slower ). - aarch64 float32 randn performance seems to be slower than rand, where as x86 seems to maintain same performance characteristics in float32. ### versions collecting environment information... pytorch version: 2.11.0.dev20260120+cpu is debug build: false cuda used to build pytorch: none rocm used to build pytorch: n/a os: ubuntu 22.04.2 lts (aarch64) gcc version: (ubuntu 11.4.0-1ubuntu1~22.04.2) 11.4.0 clang version: could not collect cmake version: version 4.1.0 libc version: glibc-2.35 python version: 3.10.12 (main, jan 8 2026, 06:52:19) [gcc 11.4.0] (64-bit runtime) python platform: linux-6.8.0-1044-aws-aarch64-with-glibc2.35 is cuda available: false cuda runtime version: no cuda cuda_module_loading set to: n/a gpu models and configuration: no cuda nvidia driver version: no cuda cudnn version: no cuda is xpu available: false hip runtime version: n/a miopen runtime version: n/a is xnnpack available: true caching allocator config: n/a cpu: architecture: aarch64 cpu op-mode(s): 64-bit byte order: little endian cpu(s): 16 on-line cpu(s) list: 0-15 vendor id: arm model name: neoverse-v2 model: 1 thread(s) per core: 1 core(s) per socket: 16 socket(s): 1 stepping: r0p1 bogomips: 2000.00 flags: fp asimd evtstrm aes pmull sha1 sha2 crc32 atomics fphp asimdhp cpuid asimdrdm jscvt fcma lrcpc dcpop sha3 asimddp sha512 sve asimdfhm dit uscat ilrcpc flagm ssbs sb paca pacg dcpodp sve2 sveaes svepmull svebitperm svesha3 flagm2 frint svei8mm svebf16 i8mm bf16 dgh rng bti l1d cache: 1 mib (16 instances) l1i cache: 1 mib (16 instances) l2 cache: 32 mib (16 instances) l3 cache: 36 mib (1 instance) numa node(s): 1 numa node0 cpu(s): 0-15 vulnerability gather data sampling: not affected vulnerability itlb multihit: not affected vulnerability l1tf: not affected vulnerability mds: not affected vulnerability meltdown: not affected vulnerability mmio stale data: not affected vulnerability reg file data sampling: not affected vulnerability retbleed: not affected vulnerability spec rstack overflow: not affected vulnerability spec store bypass: mitigation; speculative store bypass disabled via prctl vulnerability spectre v1: mitigation; __user pointer sanitization vulnerability spectre v2: mitigation; csv2, bhb vulnerability srbds: not affected vulnerability tsx async abort: not affected vulnerability vmscape: not affected versions of relevant libraries: [pip3] numpy==2.2.6 [pip3] torch==2.11.0.dev20260120+cpu [pip3] torch-tb-profiler==0.4.3 [pip3] torchvision==0.25.0.dev20260120+cpu [conda] could not collect cc -arm",[],['PERFORMANCE'],"['module: performance', 'module: cpu', 'triaged', 'module: random', 'module: arm']",github,2026-01-23T11:59:03Z,,"torch.randn slow on cpu, bfloat16 significantly slower than float32 ### üêõ describe the bug seems to be very slow, especially for dtype on cpu example code: **results** here is a comparison of a few cpu architectures | | aarch64 ( c7g ) | aarch64 ( c8g ) | x86_64 ( c7i )| | -------- | -------- | -------- | ------- | | float32 | 17988ms | 23009ms | 8382ms | | bfloat16 | 32551ms | 42559ms | 42155ms | for comparison, the same code with ( uniform distribution ) | | aarch64 ( c8g ) | x86_64 ( c7i )| | -------- | -------- | -------- | | float32 | 8074ms | 8257ms | | bfloat16 | 9233ms | 9245ms | so there are a few takeaways from this - there seems to be an issue that affects all cpu platforms - bfloat16 is signifantly slower than float32 for randn ( ~3-4x slower ). - aarch64 float32 randn performance seems to be slower than rand, where as x86 seems to maintain same performance characteristics in float32. ### versions collecting environment information... pytorch version: 2.11.0.dev20260120+cpu is debug build: false cuda used to build pytorch: none rocm used to build pytorch: n/a os: ubuntu 22.04.2 lts (aarch64) gcc version: (ubuntu 11.4.0-1ubuntu1~22.04.2) 11.4.0 clang version: could not collect cmake version: version 4.1.0 libc version: glibc-2.35 python version: 3.10.12 (main, jan 8 2026, 06:52:19) [gcc 11.4.0] (64-bit runtime) python platform: linux-6.8.0-1044-aws-aarch64-with-glibc2.35 is cuda available: false cuda runtime version: no cuda cuda_module_loading set to: n/a gpu models and configuration: no cuda nvidia driver version: no cuda cudnn version: no cuda is xpu available: false hip runtime version: n/a miopen runtime version: n/a is xnnpack available: true caching allocator config: n/a cpu: architecture: aarch64 cpu op-mode(s): 64-bit byte order: little endian cpu(s): 16 on-line cpu(s) list: 0-15 vendor id: arm model name: neoverse-v2 model: 1 thread(s) per core: 1 core(s) per socket: 16 socket(s): 1 stepping: r0p1 bogomips: 2000.00 flags: fp asimd evtstrm aes pmull sha1 sha2 crc32 atomics fphp asimdhp cpuid asimdrdm jscvt fcma lrcpc dcpop sha3 asimddp sha512 sve asimdfhm dit uscat ilrcpc flagm ssbs sb paca pacg dcpodp sve2 sveaes svepmull svebitperm svesha3 flagm2 frint svei8mm svebf16 i8mm bf16 dgh rng bti l1d cache: 1 mib (16 instances) l1i cache: 1 mib (16 instances) l2 cache: 32 mib (16 instances) l3 cache: 36 mib (1 instance) numa node(s): 1 numa node0 cpu(s): 0-15 vulnerability gather data sampling: not affected vulnerability itlb multihit: not affected vulnerability l1tf: not affected vulnerability mds: not affected vulnerability meltdown: not affected vulnerability mmio stale data: not affected vulnerability reg file data sampling: not affected vulnerability retbleed: not affected vulnerability spec rstack overflow: not affected vulnerability spec store bypass: mitigation; speculative store bypass disabled via prctl vulnerability spectre v1: mitigation; __user pointer sanitization vulnerability spectre v2: mitigation; csv2, bhb vulnerability srbds: not affected vulnerability tsx async abort: not affected vulnerability vmscape: not affected versions of relevant libraries: [pip3] numpy==2.2.6 [pip3] torch==2.11.0.dev20260120+cpu [pip3] torch-tb-profiler==0.4.3 [pip3] torchvision==0.25.0.dev20260120+cpu [conda] could not collect cc -arm",8.6,Critical,1.0,"performance degradation, crash-like behavior"
rust-lang/rust#151537,[ice] in const eval when when storing simd,### code ### meta : ### error output <!-- include a backtrace in the code block by setting in your environment. e.g. . --> backtrace,[],['BUG'],"['I-ICE', 'T-compiler', 'A-SIMD', 'C-bug', 'A-const-eval']",github,2026-01-23T12:19:41Z,,[ice] in const eval when when storing simd ### code ### meta : ### error output <!-- include a backtrace in the code block by setting in your environment. e.g. . --> backtrace,2.459,Medium,0.779,functional impact
cockroachdb/cockroach#161674,sql: v25.4.1: nil pointer when cleaning up the pausable portal,this issue was auto filed by sentry. it represents a crash or reported error on a live cluster with telemetry enabled. sentry link: [ panic message: stacktrace (expand for inline code snippets): [src/runtime/asm_amd64.s#l1699-l1701](src/runtime/asm_amd64.s#l1699-l1701) [pkg/sql/pgwire/server.go#l1227-l1229](pkg/sql/pgwire/server.go#l1227-l1229) [pkg/sql/pgwire/conn.go#l252-l254](pkg/sql/pgwire/conn.go#l252-l254) [pkg/sql/conn_executor.go#l1066-l1068](pkg/sql/conn_executor.go#l1066-l1068) [pkg/sql/conn_executor.go#l2290-l2292](pkg/sql/conn_executor.go#l2290-l2292) [pkg/sql/conn_executor.go#l2521-l2523](pkg/sql/conn_executor.go#l2521-l2523) [pkg/sql/conn_executor_prepare.go#l382-l384](pkg/sql/conn_executor_prepare.go#l382-l384) [pkg/sql/conn_executor_prepare.go#l573-l575](pkg/sql/conn_executor_prepare.go#l573-l575) [pkg/sql/prepared_stmt.go#l163-l165](pkg/sql/prepared_stmt.go#l163-l165) [pkg/sql/prepared_stmt.go#l317-l319](pkg/sql/prepared_stmt.go#l317-l319) [pkg/sql/prepared_stmt.go#l191-l193](pkg/sql/prepared_stmt.go#l191-l193) [pkg/sql/conn_executor_exec.go#l1183-l1185](pkg/sql/conn_executor_exec.go#l1183-l1185) [pkg/sql/conn_executor_exec.go#l1662-l1664](pkg/sql/conn_executor_exec.go#l1662-l1664) [pkg/sql/exec_log.go#l141-l143](pkg/sql/exec_log.go#l141-l143) [pkg/sql/exec_log.go#l356-l358](pkg/sql/exec_log.go#l356-l358) [pkg/sql/event_log.go#l172-l174](pkg/sql/event_log.go#l172-l174) [goroot/src/runtime/signal_unix.go#l916-l918](goroot/src/runtime/signal_unix.go#l916-l918) [goroot/src/runtime/panic.go#l261-l263](goroot/src/runtime/panic.go#l261-l263) [goroot/src/runtime/panic.go#l790-l792](goroot/src/runtime/panic.go#l790-l792) [pkg/sql/conn_executor.go#l1064-l1066](pkg/sql/conn_executor.go#l1064-l1066) [pkg/sql/conn_executor.go#l1352-l1354](pkg/sql/conn_executor.go#l1352-l1354) [pkg/sql/conn_executor.go#l1376-l1378](pkg/sql/conn_executor.go#l1376-l1378) [pkg/sql/conn_executor.go#l1983-l1985](pkg/sql/conn_executor.go#l1983-l1985) [pkg/sql/prepared_stmt.go#l160-l162](pkg/sql/prepared_stmt.go#l160-l162) [pkg/util/mon/bytes_usage.go#l1174-l1176](pkg/util/mon/bytes_usage.go#l1174-l1176) [pkg/util/log/logcrash/crash_reporting.go#l431-l433](pkg/util/log/logcrash/crash_reporting.go#l431-l433) ### tags | tag | value | | --- | --- | | command | mt start-sql | | environment | v25.4.1 | | go version | go1.23.12 x:nocoverageredesign | | platform | linux amd64 | | distribution | ccl | | cockroach release | v25.4.1-1-g55e9e9d822f | | cockroach sha | 55e9e9d822f71cfe280f4772358f13859488aca6 | | # of cpus | 32 | | # of goroutines | 394 | jira issue: crdb-59026,[],['BUG'],"['C-bug', 'O-sentry', 'X-blathers-triaged', 'T-sql-queries', 'A-pausable-portals', 'branch-release-25.4', 'target-release-26.2.0']",github,2026-01-23T12:31:30Z,2026-01-28T04:28:50Z,sql: v25.4.1: nil pointer when cleaning up the pausable portal this issue was auto filed by sentry. it represents a crash or reported error on a live cluster with telemetry enabled. sentry link: [ panic message: stacktrace (expand for inline code snippets): [src/runtime/asm_amd64.s#l1699-l1701](src/runtime/asm_amd64.s#l1699-l1701) [pkg/sql/pgwire/server.go#l1227-l1229](pkg/sql/pgwire/server.go#l1227-l1229) [pkg/sql/pgwire/conn.go#l252-l254](pkg/sql/pgwire/conn.go#l252-l254) [pkg/sql/conn_executor.go#l1066-l1068](pkg/sql/conn_executor.go#l1066-l1068) [pkg/sql/conn_executor.go#l2290-l2292](pkg/sql/conn_executor.go#l2290-l2292) [pkg/sql/conn_executor.go#l2521-l2523](pkg/sql/conn_executor.go#l2521-l2523) [pkg/sql/conn_executor_prepare.go#l382-l384](pkg/sql/conn_executor_prepare.go#l382-l384) [pkg/sql/conn_executor_prepare.go#l573-l575](pkg/sql/conn_executor_prepare.go#l573-l575) [pkg/sql/prepared_stmt.go#l163-l165](pkg/sql/prepared_stmt.go#l163-l165) [pkg/sql/prepared_stmt.go#l317-l319](pkg/sql/prepared_stmt.go#l317-l319) [pkg/sql/prepared_stmt.go#l191-l193](pkg/sql/prepared_stmt.go#l191-l193) [pkg/sql/conn_executor_exec.go#l1183-l1185](pkg/sql/conn_executor_exec.go#l1183-l1185) [pkg/sql/conn_executor_exec.go#l1662-l1664](pkg/sql/conn_executor_exec.go#l1662-l1664) [pkg/sql/exec_log.go#l141-l143](pkg/sql/exec_log.go#l141-l143) [pkg/sql/exec_log.go#l356-l358](pkg/sql/exec_log.go#l356-l358) [pkg/sql/event_log.go#l172-l174](pkg/sql/event_log.go#l172-l174) [goroot/src/runtime/signal_unix.go#l916-l918](goroot/src/runtime/signal_unix.go#l916-l918) [goroot/src/runtime/panic.go#l261-l263](goroot/src/runtime/panic.go#l261-l263) [goroot/src/runtime/panic.go#l790-l792](goroot/src/runtime/panic.go#l790-l792) [pkg/sql/conn_executor.go#l1064-l1066](pkg/sql/conn_executor.go#l1064-l1066) [pkg/sql/conn_executor.go#l1352-l1354](pkg/sql/conn_executor.go#l1352-l1354) [pkg/sql/conn_executor.go#l1376-l1378](pkg/sql/conn_executor.go#l1376-l1378) [pkg/sql/conn_executor.go#l1983-l1985](pkg/sql/conn_executor.go#l1983-l1985) [pkg/sql/prepared_stmt.go#l160-l162](pkg/sql/prepared_stmt.go#l160-l162) [pkg/util/mon/bytes_usage.go#l1174-l1176](pkg/util/mon/bytes_usage.go#l1174-l1176) [pkg/util/log/logcrash/crash_reporting.go#l431-l433](pkg/util/log/logcrash/crash_reporting.go#l431-l433) ### tags | tag | value | | --- | --- | | command | mt start-sql | | environment | v25.4.1 | | go version | go1.23.12 x:nocoverageredesign | | platform | linux amd64 | | distribution | ccl | | cockroach release | v25.4.1-1-g55e9e9d822f | | cockroach sha | 55e9e9d822f71cfe280f4772358f13859488aca6 | | # of cpus | 32 | | # of goroutines | 394 | jira issue: crdb-59026,6.0,Critical,1.0,crash-like behavior
python/cpython#144175,add public pyarg_parsetupleandkeywords api equivalent for meth_fastcall,"# feature or enhancement ### proposal: opening an issue for this as requested by in . meth_fastcall is part of the stable abi as of python 3.10. however, using it in extension module is quite difficult, because there is no equivalent api for pyarg_parsetupleandkeywords (which is the usual way to parse complex signatures with keywords in meth_varargs-style functions), and parsing vectorcall-style keywords is not a trivial problem to begin with. there used to be an undocumented public api (_pyarg_parsestackandkeywords) that solves this problem that is used by argument clinic, but it was removed from the public api as part of a wider effort to unexpose undocumented apis in . so this issue requests a new public, stable api similar to pyarg_parsetuple and pyarg_parsetupleandkeywords but accepting vectorcall-style parameters instead. ### has this already been discussed elsewhere? this is a minor feature, which does not need previous discussion elsewhere ### links to previous discussion of this feature: _no response_ ### linked prs * gh-144283",[],['FEATURE'],"['type-feature', 'interpreter-core', 'topic-C-API']",github,2026-01-23T12:39:46Z,,"add public pyarg_parsetupleandkeywords api equivalent for meth_fastcall # feature or enhancement ### proposal: opening an issue for this as requested by in . meth_fastcall is part of the stable abi as of python 3.10. however, using it in extension module is quite difficult, because there is no equivalent api for pyarg_parsetupleandkeywords (which is the usual way to parse complex signatures with keywords in meth_varargs-style functions), and parsing vectorcall-style keywords is not a trivial problem to begin with. there used to be an undocumented public api (_pyarg_parsestackandkeywords) that solves this problem that is used by argument clinic, but it was removed from the public api as part of a wider effort to unexpose undocumented apis in . so this issue requests a new public, stable api similar to pyarg_parsetuple and pyarg_parsetupleandkeywords but accepting vectorcall-style parameters instead. ### has this already been discussed elsewhere? this is a minor feature, which does not need previous discussion elsewhere ### links to previous discussion of this feature: _no response_ ### linked prs * gh-144283",1.4,Low,0.538,localized low-impact
cilium/cilium#43952,cfp: ciliumnetworkpolicy entity (alias) for gateway api,"## cilium feature proposal **is your proposed feature related to a problem?** not a technical one, but maybe a ux one: there is a [ciliumnetworkpolicy ""entity"" for ""ingress""]( the term ""ingress"" is overloaded, but considering that gateway api is the successor to kubernetes ingress, it can be misleading that you have to use ""ingress"" in the entity while you use gateway api instead of ingress for exposing services outside kubernetes. **describe the feature you'd like** users can use an entity which is not coupled to ""ingress"" (when they don't use ""kubernetes ingress"" to expose services, but rather gateway api). **notify relevant community channels** <!-- - /example --> <!-- if you don't know which team to include or you haven't gotten feedback, consider raising this topic on [slack] in the #development channel or during a [community meeting] for awareness. --> **(optional) describe your proposed solution** i'm suggesting an alias for the ingress entity such as ""gateway-api"" or the like, just to remove confusion from the term ""ingress"" for users who don't use ingress in their clusters. <!-- please complete this section if you have ideas / suggestions on how to implement the feature. we strongly recommend discussing your approach with cilium committers before spending lots of time implementing a change. for longer proposals, you are welcome to link to an external doc (e.g. a google doc). we have a [cilium feature proposal template]( to help you structure your proposal - if you would like to use it, please make a copy and ensure it's publicly visible, and then add the link here. once the cfp is close to being finalized, please add it as a pr to the [design-cfps]( repo for final approval. [community meeting]: [slack]: [teams]: -->",[],['FEATURE'],"['kind/feature', 'area/proxy', 'kind/cfp', 'area/servicemesh', 'feature/k8s-gateway-api']",github,2026-01-23T12:43:23Z,,"cfp: ciliumnetworkpolicy entity (alias) for gateway api ## cilium feature proposal **is your proposed feature related to a problem?** not a technical one, but maybe a ux one: there is a [ciliumnetworkpolicy ""entity"" for ""ingress""]( the term ""ingress"" is overloaded, but considering that gateway api is the successor to kubernetes ingress, it can be misleading that you have to use ""ingress"" in the entity while you use gateway api instead of ingress for exposing services outside kubernetes. **describe the feature you'd like** users can use an entity which is not coupled to ""ingress"" (when they don't use ""kubernetes ingress"" to expose services, but rather gateway api). **notify relevant community channels** <!-- - /example --> <!-- if you don't know which team to include or you haven't gotten feedback, consider raising this topic on [slack] in the #development channel or during a [community meeting] for awareness. --> **(optional) describe your proposed solution** i'm suggesting an alias for the ingress entity such as ""gateway-api"" or the like, just to remove confusion from the term ""ingress"" for users who don't use ingress in their clusters. <!-- please complete this section if you have ideas / suggestions on how to implement the feature. we strongly recommend discussing your approach with cilium committers before spending lots of time implementing a change. for longer proposals, you are welcome to link to an external doc (e.g. a google doc). we have a [cilium feature proposal template]( to help you structure your proposal - if you would like to use it, please make a copy and ensure it's publicly visible, and then add the link here. once the cfp is close to being finalized, please add it as a pr to the [design-cfps]( repo for final approval. [community meeting]: [slack]: [teams]: -->",3.6,Critical,1.0,crash-like behavior
electron/electron#49500,add support for experimental-inspector-network-resource,"### preflight checklist - [x] i have read the [contributing guidelines]( for this project. - [x] i agree to follow the [code of conduct]( that this project adheres to. - [x] i have searched the [issue tracker]( for a feature request that matches the one i want to file, without success. ### problem description we use remote source maps (e.g. `//# sourcemappingurl= in electron's main and utility processes. historically, remote source maps for node.js applications could not be resolved by chrome devtools. now the situation has improved - in node.js 22.19 they added flag, which allows applications to manually add [support for remote source map resolution]( e.g. imagine i have the following script: if i compile the code and generate source maps, then run , i can use chromes 'dedicated devtools for node', and see that it successfully resolves source maps, allowing me to debug the original ts code. however, the same doesn't work in electron, because it only supports a subset of node.js flags. ### proposed solution add support for cli flags for both main and utility processes. ### alternatives considered the only 'real' alternative is not to have source map resolution for main/utility processes in chrome inspector (which is the current situation). it's not ideal, because there are developers who prefer using chrome's inspector over vs code ### additional information long term, consider implementing directly in electron (to provide remote source map resolution for all applications ""out of the box"") - i know it's a long shot, but i do think it might be useful for for other electron applications as well",[],['FEATURE'],"['enhancement :sparkles:', 'component/devtools']",github,2026-01-23T13:02:22Z,,"add support for experimental-inspector-network-resource ### preflight checklist - [x] i have read the [contributing guidelines]( for this project. - [x] i agree to follow the [code of conduct]( that this project adheres to. - [x] i have searched the [issue tracker]( for a feature request that matches the one i want to file, without success. ### problem description we use remote source maps (e.g. `//# sourcemappingurl= in electron's main and utility processes. historically, remote source maps for node.js applications could not be resolved by chrome devtools. now the situation has improved - in node.js 22.19 they added flag, which allows applications to manually add [support for remote source map resolution]( e.g. imagine i have the following script: if i compile the code and generate source maps, then run , i can use chromes 'dedicated devtools for node', and see that it successfully resolves source maps, allowing me to debug the original ts code. however, the same doesn't work in electron, because it only supports a subset of node.js flags. ### proposed solution add support for cli flags for both main and utility processes. ### alternatives considered the only 'real' alternative is not to have source map resolution for main/utility processes in chrome inspector (which is the current situation). it's not ideal, because there are developers who prefer using chrome's inspector over vs code ### additional information long term, consider implementing directly in electron (to provide remote source map resolution for all applications ""out of the box"") - i know it's a long shot, but i do think it might be useful for for other electron applications as well",3.437,High,1.0,system-wide impact
cockroachdb/cockroach#161676,sentry: version.go:165: invalid version √ó (1) attached stack trace -- stack trace: | github.com/cockroachdb/cockroach/pkg/cli.init.makezipdircommand.func59 | doctor.go:99 | [...repeated from...,this issue was auto filed by sentry. it represents a crash or reported error on a live cluster with telemetry enabled. sentry link: [ panic message: stacktrace (expand for inline code snippets): [src/runtime/asm_arm64.s#l1222-l1224](src/runtime/asm_arm64.s#l1222-l1224) [goroot/src/runtime/proc.go#l271-l273](goroot/src/runtime/proc.go#l271-l273) [pkg/cmd/cockroach/main.go#l20-l22](pkg/cmd/cockroach/main.go#l20-l22) [cli.go#l63-l65](cli.go#l63-l65) [cli.go#l138-l140](cli.go#l138-l140) [cli.go#l299-l301](cli.go#l299-l301) [external/com_github_spf13_cobra/command.go#l967-l969](external/com_github_spf13_cobra/command.go#l967-l969) [external/com_github_spf13_cobra/command.go#l1043-l1045](external/com_github_spf13_cobra/command.go#l1043-l1045) [external/com_github_spf13_cobra/command.go#l915-l917](external/com_github_spf13_cobra/command.go#l915-l917) [doctor.go#l98-l100](doctor.go#l98-l100) [src/runtime/asm_arm64.s#l1222-l1224](src/runtime/asm_arm64.s#l1222-l1224) [goroot/src/runtime/proc.go#l271-l273](goroot/src/runtime/proc.go#l271-l273) [pkg/cmd/cockroach/main.go#l20-l22](pkg/cmd/cockroach/main.go#l20-l22) [cli.go#l63-l65](cli.go#l63-l65) [cli.go#l138-l140](cli.go#l138-l140) [cli.go#l299-l301](cli.go#l299-l301) [external/com_github_spf13_cobra/command.go#l967-l969](external/com_github_spf13_cobra/command.go#l967-l969) [external/com_github_spf13_cobra/command.go#l1043-l1045](external/com_github_spf13_cobra/command.go#l1043-l1045) [external/com_github_spf13_cobra/command.go#l915-l917](external/com_github_spf13_cobra/command.go#l915-l917) [doctor.go#l98-l100](doctor.go#l98-l100) [pkg/roachpb/version.go#l194-l196](pkg/roachpb/version.go#l194-l196) [pkg/roachpb/version.go#l164-l166](pkg/roachpb/version.go#l164-l166) ### tags | tag | value | | --- | --- | | command | debug doctor recreate zipdir | | environment | v25.3.0 | | go version | go1.23.7 x:nocoverageredesign | | platform | darwin arm64 | | distribution | ccl | | cockroach release | v25.3.0 | | cockroach sha | cb540b101146345c1df9f265f65ea321c62b10b3 | | # of cpus | 14 | | # of goroutines | 9 | jira issue: crdb-59027,[],['BUG'],"['C-bug', 'O-sentry', 'X-blathers-untriaged', 'branch-release-25.3']",github,2026-01-23T13:26:38Z,2026-01-24T01:10:23Z,sentry: version.go:165: invalid version √ó (1) attached stack trace -- stack trace: | github.com/cockroachdb/cockroach/pkg/cli.init.makezipdircommand.func59 | doctor.go:99 | [...repeated from... this issue was auto filed by sentry. it represents a crash or reported error on a live cluster with telemetry enabled. sentry link: [ panic message: stacktrace (expand for inline code snippets): [src/runtime/asm_arm64.s#l1222-l1224](src/runtime/asm_arm64.s#l1222-l1224) [goroot/src/runtime/proc.go#l271-l273](goroot/src/runtime/proc.go#l271-l273) [pkg/cmd/cockroach/main.go#l20-l22](pkg/cmd/cockroach/main.go#l20-l22) [cli.go#l63-l65](cli.go#l63-l65) [cli.go#l138-l140](cli.go#l138-l140) [cli.go#l299-l301](cli.go#l299-l301) [external/com_github_spf13_cobra/command.go#l967-l969](external/com_github_spf13_cobra/command.go#l967-l969) [external/com_github_spf13_cobra/command.go#l1043-l1045](external/com_github_spf13_cobra/command.go#l1043-l1045) [external/com_github_spf13_cobra/command.go#l915-l917](external/com_github_spf13_cobra/command.go#l915-l917) [doctor.go#l98-l100](doctor.go#l98-l100) [src/runtime/asm_arm64.s#l1222-l1224](src/runtime/asm_arm64.s#l1222-l1224) [goroot/src/runtime/proc.go#l271-l273](goroot/src/runtime/proc.go#l271-l273) [pkg/cmd/cockroach/main.go#l20-l22](pkg/cmd/cockroach/main.go#l20-l22) [cli.go#l63-l65](cli.go#l63-l65) [cli.go#l138-l140](cli.go#l138-l140) [cli.go#l299-l301](cli.go#l299-l301) [external/com_github_spf13_cobra/command.go#l967-l969](external/com_github_spf13_cobra/command.go#l967-l969) [external/com_github_spf13_cobra/command.go#l1043-l1045](external/com_github_spf13_cobra/command.go#l1043-l1045) [external/com_github_spf13_cobra/command.go#l915-l917](external/com_github_spf13_cobra/command.go#l915-l917) [doctor.go#l98-l100](doctor.go#l98-l100) [pkg/roachpb/version.go#l194-l196](pkg/roachpb/version.go#l194-l196) [pkg/roachpb/version.go#l164-l166](pkg/roachpb/version.go#l164-l166) ### tags | tag | value | | --- | --- | | command | debug doctor recreate zipdir | | environment | v25.3.0 | | go version | go1.23.7 x:nocoverageredesign | | platform | darwin arm64 | | distribution | ccl | | cockroach release | v25.3.0 | | cockroach sha | cb540b101146345c1df9f265f65ea321c62b10b3 | | # of cpus | 14 | | # of goroutines | 9 | jira issue: crdb-59027,6.0,Critical,1.0,crash-like behavior
cockroachdb/cockroach#161677,upgrade/upgrades: testfirstupgraderepairbatchsize failed,upgrade/upgrades.testfirstupgraderepairbatchsize [failed]( with [artifacts]( on release-26.1 @ [6fa7405f7bf2b301b0878b3b420e00248b450725]( help see also: [how to investigate a go test failure \(internal\)]( /cc /release-eng /release-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-59028,[],['TESTING'],"['C-test-failure', 'O-robot', 'T-release', 's390x-test-failure', 'branch-release-26.1']",github,2026-01-23T13:28:59Z,,upgrade/upgrades: testfirstupgraderepairbatchsize failed upgrade/upgrades.testfirstupgraderepairbatchsize [failed]( with [artifacts]( on release-26.1 @ [6fa7405f7bf2b301b0878b3b420e00248b450725]( help see also: [how to investigate a go test failure \(internal\)]( /cc /release-eng /release-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-59028,1.6,Low,0.584,localized low-impact
flutter/flutter#181382,[path_provider_foundation] [path_provider] path_provider_foundation ffi implementation crashes flutter framework used in native ios app (add-to-app),"### steps to reproduce 1. create a flutter module intended to be used as a framework inside a native ios app: 2. build flutter ios frameworks (not a full flutter app): 3. integrate the generated flutter frameworks into a native uikit ios application using cocoapods (flutter add-to-app / frameworks approach). 4. in the flutter module, update pubspec.yaml: dependencies: flutter: sdk: flutter path_provider: ^2.1.5 5. run: 6. launch the native ios app that embeds the flutter framework. 7. call any api that uses path_provider, for example: ### expected results - the native ios app launches successfully. - the embedded flutter framework initializes correctly. - getapplicationdocumentsdirectory() returns a valid directory. - flutter framework behavior is identical to pre-ffi versions of path_provider_foundation. ### actual results the native ios app crashes at runtime when the embedded flutter framework initializes: this crash happens inside the flutter framework before any dart main() code executes, making it impossible to work around in application code. ### code sample code sample ### screenshots or video screenshots / video demonstration [upload media here] ### logs logs ### flutter doctor output doctor output ### regression information works when pinning: crashes with: recent related releases path_provider_foundation 2.6.0 (released ~8 hours ago): > re-release: replaces flutter-plugin-based implementation with direct ffi calls to foundation. objective_c 9.2.4 (released ~8 hours ago): > fix build hook path issue that could pass percent-encoded cache paths to clang. this suggests the new direct ffi implementation is incompatible with flutter frameworks embedded in native ios apps.",[],['UI'],"['platform-ios', 'tool', 'p: path_provider', 'package', 'a: build', 'team-tool']",github,2026-01-23T13:32:07Z,,"[path_provider_foundation] [path_provider] path_provider_foundation ffi implementation crashes flutter framework used in native ios app (add-to-app) ### steps to reproduce 1. create a flutter module intended to be used as a framework inside a native ios app: 2. build flutter ios frameworks (not a full flutter app): 3. integrate the generated flutter frameworks into a native uikit ios application using cocoapods (flutter add-to-app / frameworks approach). 4. in the flutter module, update pubspec.yaml: dependencies: flutter: sdk: flutter path_provider: ^2.1.5 5. run: 6. launch the native ios app that embeds the flutter framework. 7. call any api that uses path_provider, for example: ### expected results - the native ios app launches successfully. - the embedded flutter framework initializes correctly. - getapplicationdocumentsdirectory() returns a valid directory. - flutter framework behavior is identical to pre-ffi versions of path_provider_foundation. ### actual results the native ios app crashes at runtime when the embedded flutter framework initializes: this crash happens inside the flutter framework before any dart main() code executes, making it impossible to work around in application code. ### code sample code sample ### screenshots or video screenshots / video demonstration [upload media here] ### logs logs ### flutter doctor output doctor output ### regression information works when pinning: crashes with: recent related releases path_provider_foundation 2.6.0 (released ~8 hours ago): > re-release: replaces flutter-plugin-based implementation with direct ffi calls to foundation. objective_c 9.2.4 (released ~8 hours ago): > fix build hook path issue that could pass percent-encoded cache paths to clang. this suggests the new direct ffi implementation is incompatible with flutter frameworks embedded in native ios apps.",5.8,Critical,1.0,"user-visible issue, crash-like behavior"
pandas-dev/pandas#63832,bug: bug in pandas 3.0 with posixpath inside series,### pandas version checks - [x] i have checked that this issue has not already been reported. - [x] i have confirmed this bug exists on the [latest version]( of pandas. - [x] i have confirmed this bug exists on the [main branch]( of pandas. ### reproducible example ### issue description while and works correctly ### expected behavior expected behaviour is the same as for the following code ### installed versions installed versions ------------------ commit : 366ccdfcd8ed1e5543bfb6d4ee0c9bc519898670 python : 3.12.9 python-bits : 64 os : linux os-release : 5.15.0-133-generic version : -ubuntu smp fri feb 7 20:47:38 utc 2025 machine : x86_64 processor : x86_64 byteorder : little lc_all : none lang : c.utf-8 locale : c.utf-8 pandas : 3.0.0 numpy : 2.1.3 dateutil : 2.9.0.post0 pip : 25.0.1 cython : 3.1.1 sphinx : none ipython : 9.0.2 adbc-driver-postgresql: none adbc-driver-sqlite : none bs4 : 4.13.3 bottleneck : none fastparquet : none fsspec : 2025.3.0 html5lib : none hypothesis : none gcsfs : none jinja2 : 3.1.6 lxml.etree : 5.3.1 matplotlib : 3.10.1 numba : 0.61.0 numexpr : 2.11.0 odfpy : none openpyxl : none psycopg2 : none pymysql : none pyarrow : 19.0.1 pyiceberg : none pyreadstat : none pytest : none python-calamine : none pytz : 2025.2 pyxlsb : none s3fs : 2025.3.0 scipy : 1.15.2 sqlalchemy : none tables : none tabulate : 0.9.0 xarray : none xlrd : none xlsxwriter : none zstandard : 0.23.0 qtpy : none pyqt5 : none,[],['BUG'],"['Bug', 'Strings', 'Arrow']",github,2026-01-23T13:46:20Z,,bug: bug in pandas 3.0 with posixpath inside series ### pandas version checks - [x] i have checked that this issue has not already been reported. - [x] i have confirmed this bug exists on the [latest version]( of pandas. - [x] i have confirmed this bug exists on the [main branch]( of pandas. ### reproducible example ### issue description while and works correctly ### expected behavior expected behaviour is the same as for the following code ### installed versions installed versions ------------------ commit : 366ccdfcd8ed1e5543bfb6d4ee0c9bc519898670 python : 3.12.9 python-bits : 64 os : linux os-release : 5.15.0-133-generic version : -ubuntu smp fri feb 7 20:47:38 utc 2025 machine : x86_64 processor : x86_64 byteorder : little lc_all : none lang : c.utf-8 locale : c.utf-8 pandas : 3.0.0 numpy : 2.1.3 dateutil : 2.9.0.post0 pip : 25.0.1 cython : 3.1.1 sphinx : none ipython : 9.0.2 adbc-driver-postgresql: none adbc-driver-sqlite : none bs4 : 4.13.3 bottleneck : none fastparquet : none fsspec : 2025.3.0 html5lib : none hypothesis : none gcsfs : none jinja2 : 3.1.6 lxml.etree : 5.3.1 matplotlib : 3.10.1 numba : 0.61.0 numexpr : 2.11.0 odfpy : none openpyxl : none psycopg2 : none pymysql : none pyarrow : 19.0.1 pyiceberg : none pyreadstat : none pytest : none python-calamine : none pytz : 2025.2 pyxlsb : none s3fs : 2025.3.0 scipy : 1.15.2 sqlalchemy : none tables : none tabulate : 0.9.0 xarray : none xlrd : none xlsxwriter : none zstandard : 0.23.0 qtpy : none pyqt5 : none,2.336,Medium,0.751,functional impact
cockroachdb/cockroach#161679,"sentry: desc_builder.go:148: expected mutated √ó, instead got a √ó (1) assertion failure wraps: (2) attached stack trace -- stack trace: | github.com/cockroachdb/cockroach/pkg/sql/catalog/descbuil...",this issue was auto filed by sentry. it represents a crash or reported error on a live cluster with telemetry enabled. sentry link: [ panic message: stacktrace (expand for inline code snippets): [src/runtime/asm_arm64.s#l1222-l1224](src/runtime/asm_arm64.s#l1222-l1224) [pkg/sql/pgwire/server.go#l1213-l1215](pkg/sql/pgwire/server.go#l1213-l1215) [pkg/sql/pgwire/conn.go#l252-l254](pkg/sql/pgwire/conn.go#l252-l254) [pkg/sql/conn_executor.go#l1055-l1057](pkg/sql/conn_executor.go#l1055-l1057) [pkg/sql/conn_executor.go#l2277-l2279](pkg/sql/conn_executor.go#l2277-l2279) [pkg/sql/conn_executor.go#l2369-l2371](pkg/sql/conn_executor.go#l2369-l2371) [pkg/sql/conn_executor.go#l2364-l2366](pkg/sql/conn_executor.go#l2364-l2366) [pkg/sql/conn_executor_exec.go#l172-l174](pkg/sql/conn_executor_exec.go#l172-l174) [pkg/sql/conn_executor_exec.go#l4580-l4582](pkg/sql/conn_executor_exec.go#l4580-l4582) [pkg/sql/conn_executor_exec.go#l173-l175](pkg/sql/conn_executor_exec.go#l173-l175) [pkg/sql/conn_executor_exec.go#l1105-l1107](pkg/sql/conn_executor_exec.go#l1105-l1107) [pkg/sql/conn_executor_exec.go#l3080-l3082](pkg/sql/conn_executor_exec.go#l3080-l3082) [pkg/sql/conn_executor_exec.go#l3566-l3568](pkg/sql/conn_executor_exec.go#l3566-l3568) [pkg/sql/distsql_running.go#l1807-l1809](pkg/sql/distsql_running.go#l1807-l1809) [pkg/sql/distsql_running.go#l1804-l1806](pkg/sql/distsql_running.go#l1804-l1806) [pkg/sql/distsql_running.go#l2100-l2102](pkg/sql/distsql_running.go#l2100-l2102) [pkg/sql/distsql_running.go#l997-l999](pkg/sql/distsql_running.go#l997-l999) [pkg/sql/colflow/vectorized_flow.go#l300-l302](pkg/sql/colflow/vectorized_flow.go#l300-l302) [pkg/sql/flowinfra/flow.go#l573-l575](pkg/sql/flowinfra/flow.go#l573-l575) [pkg/sql/execinfra/processorsbase.go#l743-l745](pkg/sql/execinfra/processorsbase.go#l743-l745) [pkg/sql/colflow/flow_coordinator.go#l109-l111](pkg/sql/colflow/flow_coordinator.go#l109-l111) [pkg/sql/colexecerror/error.go#l161-l163](pkg/sql/colexecerror/error.go#l161-l163) [pkg/sql/colflow/flow_coordinator.go#l110-l112](pkg/sql/colflow/flow_coordinator.go#l110-l112) [pkg/sql/plan_node_to_row_source.go#l180-l182](pkg/sql/plan_node_to_row_source.go#l180-l182) [pkg/sql/plan.go#l579-l581](pkg/sql/plan.go#l579-l581) [pkg/sql/values.go#l85-l87](pkg/sql/values.go#l85-l87) [pkg/sql/sem/eval/expr.go#l21-l23](pkg/sql/sem/eval/expr.go#l21-l23) [bazel-out/k8-opt/bin/pkg/sql/sem/tree/eval_expr_generated.go#l286-l288](bazel-out/k8-opt/bin/pkg/sql/sem/tree/eval_expr_generated.go#l286-l288) [pkg/sql/sem/eval/expr.go#l503-l505](pkg/sql/sem/eval/expr.go#l503-l505) [pkg/sql/sem/builtins/builtins.go#l6997-l6999](pkg/sql/sem/builtins/builtins.go#l6997-l6999) [pkg/sql/repair.go#l114-l116](pkg/sql/repair.go#l114-l116) [pkg/sql/catalog/descbuilder/desc_builder.go#l147-l149](pkg/sql/catalog/descbuilder/desc_builder.go#l147-l149) ### tags | tag | value | | --- | --- | | command | demo | | environment | v25.3.0 | | go version | go1.23.7 x:nocoverageredesign | | platform | darwin arm64 | | distribution | ccl | | cockroach release | v25.3.0 | | cockroach sha | cb540b101146345c1df9f265f65ea321c62b10b3 | | # of cpus | 14 | | # of goroutines | 531 | jira issue: crdb-59029,[],['BUG'],"['C-bug', 'O-sentry', 'X-blathers-triaged', 'T-sql-foundations', 'branch-release-25.3']",github,2026-01-23T13:50:17Z,2026-01-27T15:09:54Z,"sentry: desc_builder.go:148: expected mutated √ó, instead got a √ó (1) assertion failure wraps: (2) attached stack trace -- stack trace: | github.com/cockroachdb/cockroach/pkg/sql/catalog/descbuil... this issue was auto filed by sentry. it represents a crash or reported error on a live cluster with telemetry enabled. sentry link: [ panic message: stacktrace (expand for inline code snippets): [src/runtime/asm_arm64.s#l1222-l1224](src/runtime/asm_arm64.s#l1222-l1224) [pkg/sql/pgwire/server.go#l1213-l1215](pkg/sql/pgwire/server.go#l1213-l1215) [pkg/sql/pgwire/conn.go#l252-l254](pkg/sql/pgwire/conn.go#l252-l254) [pkg/sql/conn_executor.go#l1055-l1057](pkg/sql/conn_executor.go#l1055-l1057) [pkg/sql/conn_executor.go#l2277-l2279](pkg/sql/conn_executor.go#l2277-l2279) [pkg/sql/conn_executor.go#l2369-l2371](pkg/sql/conn_executor.go#l2369-l2371) [pkg/sql/conn_executor.go#l2364-l2366](pkg/sql/conn_executor.go#l2364-l2366) [pkg/sql/conn_executor_exec.go#l172-l174](pkg/sql/conn_executor_exec.go#l172-l174) [pkg/sql/conn_executor_exec.go#l4580-l4582](pkg/sql/conn_executor_exec.go#l4580-l4582) [pkg/sql/conn_executor_exec.go#l173-l175](pkg/sql/conn_executor_exec.go#l173-l175) [pkg/sql/conn_executor_exec.go#l1105-l1107](pkg/sql/conn_executor_exec.go#l1105-l1107) [pkg/sql/conn_executor_exec.go#l3080-l3082](pkg/sql/conn_executor_exec.go#l3080-l3082) [pkg/sql/conn_executor_exec.go#l3566-l3568](pkg/sql/conn_executor_exec.go#l3566-l3568) [pkg/sql/distsql_running.go#l1807-l1809](pkg/sql/distsql_running.go#l1807-l1809) [pkg/sql/distsql_running.go#l1804-l1806](pkg/sql/distsql_running.go#l1804-l1806) [pkg/sql/distsql_running.go#l2100-l2102](pkg/sql/distsql_running.go#l2100-l2102) [pkg/sql/distsql_running.go#l997-l999](pkg/sql/distsql_running.go#l997-l999) [pkg/sql/colflow/vectorized_flow.go#l300-l302](pkg/sql/colflow/vectorized_flow.go#l300-l302) [pkg/sql/flowinfra/flow.go#l573-l575](pkg/sql/flowinfra/flow.go#l573-l575) [pkg/sql/execinfra/processorsbase.go#l743-l745](pkg/sql/execinfra/processorsbase.go#l743-l745) [pkg/sql/colflow/flow_coordinator.go#l109-l111](pkg/sql/colflow/flow_coordinator.go#l109-l111) [pkg/sql/colexecerror/error.go#l161-l163](pkg/sql/colexecerror/error.go#l161-l163) [pkg/sql/colflow/flow_coordinator.go#l110-l112](pkg/sql/colflow/flow_coordinator.go#l110-l112) [pkg/sql/plan_node_to_row_source.go#l180-l182](pkg/sql/plan_node_to_row_source.go#l180-l182) [pkg/sql/plan.go#l579-l581](pkg/sql/plan.go#l579-l581) [pkg/sql/values.go#l85-l87](pkg/sql/values.go#l85-l87) [pkg/sql/sem/eval/expr.go#l21-l23](pkg/sql/sem/eval/expr.go#l21-l23) [bazel-out/k8-opt/bin/pkg/sql/sem/tree/eval_expr_generated.go#l286-l288](bazel-out/k8-opt/bin/pkg/sql/sem/tree/eval_expr_generated.go#l286-l288) [pkg/sql/sem/eval/expr.go#l503-l505](pkg/sql/sem/eval/expr.go#l503-l505) [pkg/sql/sem/builtins/builtins.go#l6997-l6999](pkg/sql/sem/builtins/builtins.go#l6997-l6999) [pkg/sql/repair.go#l114-l116](pkg/sql/repair.go#l114-l116) [pkg/sql/catalog/descbuilder/desc_builder.go#l147-l149](pkg/sql/catalog/descbuilder/desc_builder.go#l147-l149) ### tags | tag | value | | --- | --- | | command | demo | | environment | v25.3.0 | | go version | go1.23.7 x:nocoverageredesign | | platform | darwin arm64 | | distribution | ccl | | cockroach release | v25.3.0 | | cockroach sha | cb540b101146345c1df9f265f65ea321c62b10b3 | | # of cpus | 14 | | # of goroutines | 531 | jira issue: crdb-59029",7.8,Critical,1.0,crash-like behavior
openssl/openssl#29739,optimize base64 decoding when avx is available,"back in 2020, requested avx optimization for base64 processing the issue was finally resolved by -nuon and help from the community in 2025 but only for the *encoding* part. the base64 *decoding* remains unoptimized even when avx is detected. thus i believe we should now pursue decoding.",[],['FEATURE'],"['branch: master', 'help wanted', 'triaged: feature']",github,2026-01-23T14:41:53Z,,"optimize base64 decoding when avx is available back in 2020, requested avx optimization for base64 processing the issue was finally resolved by -nuon and help from the community in 2025 but only for the *encoding* part. the base64 *decoding* remains unoptimized even when avx is detected. thus i believe we should now pursue decoding.",1.4,Low,0.538,localized low-impact
python/cpython#144179,add value recording to the trace recording jit front-end,"as listed here: currently we use a bunch of ad-hoc tracking of objects in the tracing front-end to allow subsequent optimizations, we also maintain ancillary data structures for version number to object lookup for functions, code objects and classes. this is all very fragile, complicated and inefficient. by recording references to any necessary objects in the trace, we can: * eliminate optimization failures due to missing values in lookup caches * remove version lookup caches * remove ad-hoc handling of code objects around calls in the tracer * allow the tracer to record any references we feel are useful for optimization * remove tracking of dependencies during tracing",[],['PERFORMANCE'],"['performance', 'topic-JIT', '3.15']",github,2026-01-23T14:46:06Z,,"add value recording to the trace recording jit front-end as listed here: currently we use a bunch of ad-hoc tracking of objects in the tracing front-end to allow subsequent optimizations, we also maintain ancillary data structures for version number to object lookup for functions, code objects and classes. this is all very fragile, complicated and inefficient. by recording references to any necessary objects in the trace, we can: * eliminate optimization failures due to missing values in lookup caches * remove version lookup caches * remove ad-hoc handling of code objects around calls in the tracer * allow the tracer to record any references we feel are useful for optimization * remove tracking of dependencies during tracing",3.492,High,1.0,performance degradation
containerd/containerd#12812,oci/image volume source doesn't work with user namespaces,"### description this was reported by mike beaumont in the k8s slack: the root cause of the issue is that you can't idmap a mounted overlayfs, like the image source is. to idmap an overlayfs you need to idmap each of the lower layers. this is indeed what we are using for the rootfs. it seems support for oci image source was added here: in particular, it seems to get the mount for the image [here]( but it doesn't set the uidmap/gidmap options in mount struct. if it did, then the overlayfs will be mounted properly, doing an idmap of each of the lower layers. it seems that function doesn't have access to the mappings, so it should probably be modified to have them. then, if the mappings are added there, it should work. as if the options are set, the overlay will use them for the lower layers: you did the pr to support oci image as a source, can you have a look at fixing this issue? ### steps to reproduce the issue i'll let the slack user to fill these. 1. 2. 3. ### describe the results you received and expected we got: we expect the container to start just fine. ### what version of containerd are you using? containerd 2+ ### any other relevant information _no response_ ### show configuration if it is related to cri plugin. _no response_",[],['BUG'],"['kind/bug', 'area/cri', 'area/runtime', 'status/accepted']",github,2026-01-23T14:57:01Z,,"oci/image volume source doesn't work with user namespaces ### description this was reported by mike beaumont in the k8s slack: the root cause of the issue is that you can't idmap a mounted overlayfs, like the image source is. to idmap an overlayfs you need to idmap each of the lower layers. this is indeed what we are using for the rootfs. it seems support for oci image source was added here: in particular, it seems to get the mount for the image [here]( but it doesn't set the uidmap/gidmap options in mount struct. if it did, then the overlayfs will be mounted properly, doing an idmap of each of the lower layers. it seems that function doesn't have access to the mappings, so it should probably be modified to have them. then, if the mappings are added there, it should work. as if the options are set, the overlay will use them for the lower layers: you did the pr to support oci image as a source, can you have a look at fixing this issue? ### steps to reproduce the issue i'll let the slack user to fill these. 1. 2. 3. ### describe the results you received and expected we got: we expect the container to start just fine. ### what version of containerd are you using? containerd 2+ ### any other relevant information _no response_ ### show configuration if it is related to cri plugin. _no response_",4.2,Critical,1.0,system-wide impact
kubernetes/kubernetes#136471,kubelet static pod initialization doesn't replicate api defaulting+prepareforcreate+prepareforupdate,"when kubelet reads pod manifests from disk, it bypasses api server defaulting / strategy initialization it tries to replicate that by: 1. decoding with defaulting (which replicates api server defaulting): 2. applying its own calculated versions of generated fields like uid / generated name / etc: however, it is missing any fields set at pod creation time by podstrategy#prepareforcreate: this is likely to break assumptions in the kubelet that every pod it encounters has gone through those strategy methods. we should consider having the kubelet call both of those functions on static pods. cc /sig node /kind bug",[],['BUG'],"['kind/bug', 'sig/node', 'needs-triage']",github,2026-01-23T15:05:00Z,,"kubelet static pod initialization doesn't replicate api defaulting+prepareforcreate+prepareforupdate when kubelet reads pod manifests from disk, it bypasses api server defaulting / strategy initialization it tries to replicate that by: 1. decoding with defaulting (which replicates api server defaulting): 2. applying its own calculated versions of generated fields like uid / generated name / etc: however, it is missing any fields set at pod creation time by podstrategy#prepareforcreate: this is likely to break assumptions in the kubelet that every pod it encounters has gone through those strategy methods. we should consider having the kubelet call both of those functions on static pods. cc /sig node /kind bug",2.379,Medium,0.761,functional impact
kubernetes/kubernetes#136472,validatingadmissionpolicy causes kube-controller-manager to panic,"### what happened? i have a validatingadmissionpolicy which appears to cause kube-controller-manager to panic on a nil pointer dereference. this is the backtrace from the log: ### what did you expect to happen? not to crash. by looks of it, can (and does) return nil, and the caller at doesn't check the value before using it. ### how can we reproduce it (as minimally and precisely as possible)? probably x-kubernetes-preserve-unknown-fields extension is causing this. ### anything else we need to know? _no response_ ### kubernetes version ### cloud provider n/a ### os version ### install tools ### container runtime (cri) and version (if applicable) ### related plugins (cni, csi, ...) and versions (if applicable)",[],['BUG'],"['kind/bug', 'sig/api-machinery', 'needs-triage']",github,2026-01-23T15:21:08Z,,"validatingadmissionpolicy causes kube-controller-manager to panic ### what happened? i have a validatingadmissionpolicy which appears to cause kube-controller-manager to panic on a nil pointer dereference. this is the backtrace from the log: ### what did you expect to happen? not to crash. by looks of it, can (and does) return nil, and the caller at doesn't check the value before using it. ### how can we reproduce it (as minimally and precisely as possible)? probably x-kubernetes-preserve-unknown-fields extension is causing this. ### anything else we need to know? _no response_ ### kubernetes version ### cloud provider n/a ### os version ### install tools ### container runtime (cri) and version (if applicable) ### related plugins (cni, csi, ...) and versions (if applicable)",4.6,Critical,1.0,crash-like behavior
envoyproxy/envoy#43136,sse parser feature enrichment,"1. handle other fields like 'id', 'event', and 'retry' in sse parser 2. optimization : 3. code readability",[],['FEATURE'],['enhancement'],github,2026-01-23T15:36:02Z,,"sse parser feature enrichment 1. handle other fields like 'id', 'event', and 'retry' in sse parser 2. optimization : 3. code readability",1.4,Low,0.538,localized low-impact
cockroachdb/cockroach#161681,roachtest: jepsen/g2/subcritical-skews failed,roachtest.jepsen/g2/subcritical-skews [failed]( with [artifacts]( on master @ [e17af79b64bd9260c4963031439c92e6c7ecefc5]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21013953-1769152962-138-n6cpu4-0001 | 34.44.36.248 | 10.128.0.17 | | teamcity-21013953-1769152962-138-n6cpu4-0002 | 34.63.132.32 | 10.128.0.10 | | teamcity-21013953-1769152962-138-n6cpu4-0003 | 136.112.237.101 | 10.128.0.18 | | teamcity-21013953-1769152962-138-n6cpu4-0004 | 34.44.14.119 | 10.128.0.25 | | teamcity-21013953-1769152962-138-n6cpu4-0005 | 34.44.217.3 | 10.128.0.23 | | teamcity-21013953-1769152962-138-n6cpu4-0006 | 34.56.135.125 | 10.128.0.11 | parameters: - arch=arm64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - fs=ext4 - localssd=false - metamorphicleases=default - metamorphicwritebuffering=true - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-59030,[],['TESTING'],"['C-test-failure', 'O-robot', 'O-roachtest', 'branch-master', 'T-testeng', 'X-infra-flake']",github,2026-01-23T16:01:43Z,,roachtest: jepsen/g2/subcritical-skews failed roachtest.jepsen/g2/subcritical-skews [failed]( with [artifacts]( on master @ [e17af79b64bd9260c4963031439c92e6c7ecefc5]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21013953-1769152962-138-n6cpu4-0001 | 34.44.36.248 | 10.128.0.17 | | teamcity-21013953-1769152962-138-n6cpu4-0002 | 34.63.132.32 | 10.128.0.10 | | teamcity-21013953-1769152962-138-n6cpu4-0003 | 136.112.237.101 | 10.128.0.18 | | teamcity-21013953-1769152962-138-n6cpu4-0004 | 34.44.14.119 | 10.128.0.25 | | teamcity-21013953-1769152962-138-n6cpu4-0005 | 34.44.217.3 | 10.128.0.23 | | teamcity-21013953-1769152962-138-n6cpu4-0006 | 34.56.135.125 | 10.128.0.11 | parameters: - arch=arm64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - fs=ext4 - localssd=false - metamorphicleases=default - metamorphicwritebuffering=true - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-59030,3.208,High,0.949,system-wide impact
cockroachdb/cockroach#161685,sql/importer: testimportintocsv failed [timeout],sql/importer.testimportintocsv [failed]( on master @ [e602ffc2f639c36bdea7bae14cc7af0b6f61c4e3]( parameters: - attempt=1 - run=1 - shard=12 help see also: [how to investigate a go test failure \(internal\)]( [this test on roachdash]( | [improve this report!]( jira issue: crdb-59031,[],['TESTING'],"['C-test-failure', 'O-robot', 'branch-master', 'T-sql-queries']",github,2026-01-23T16:43:14Z,,sql/importer: testimportintocsv failed [timeout] sql/importer.testimportintocsv [failed]( on master @ [e602ffc2f639c36bdea7bae14cc7af0b6f61c4e3]( parameters: - attempt=1 - run=1 - shard=12 help see also: [how to investigate a go test failure \(internal\)]( [this test on roachdash]( | [improve this report!]( jira issue: crdb-59031,1.6,Low,0.584,localized low-impact
llvm/llvm-project#177611,[libc++] add instrumentation to track the number of retired instructions for the spec benchmarks,"currently we only track how long it takes to run the spec suite. this can be quite noisy unfortunately. however, we can additionally track the number of retired instructions, which should be much less noisy, providing us with another tool to spot performance regressions. this isn't a perfect tool and we should still check the time it takes to run benchmarks, but it should find cases where we unexpectedly hinder optimizations and allow us to pinpoint regressions faster.",[],"['PERFORMANCE', 'UI']","['libc++', 'test-suite', 'performance']",github,2026-01-23T16:44:29Z,2026-01-26T19:37:54Z,"[libc++] add instrumentation to track the number of retired instructions for the spec benchmarks currently we only track how long it takes to run the spec suite. this can be quite noisy unfortunately. however, we can additionally track the number of retired instructions, which should be much less noisy, providing us with another tool to spot performance regressions. this isn't a perfect tool and we should still check the time it takes to run benchmarks, but it should find cases where we unexpectedly hinder optimizations and allow us to pinpoint regressions faster.",3.489,High,1.0,"performance degradation, user-visible issue"
cockroachdb/cockroach#161686,sql: teststatementtimeoutforschemachangecommit failed,sql.teststatementtimeoutforschemachangecommit [failed]( with [artifacts]( on release-24.3 @ [0673cbfa60f764ed35ae3a91a167883845038f70]( help see also: [how to investigate a go test failure \(internal\)]( [this test on roachdash]( | [improve this report!]( jira issue: crdb-59032,[],['TESTING'],"['C-test-failure', 'O-robot', 'X-duplicate', 'release-blocker', 'T-sql-queries', 'branch-release-24.3']",github,2026-01-23T16:45:33Z,2026-01-23T20:07:30Z,sql: teststatementtimeoutforschemachangecommit failed sql.teststatementtimeoutforschemachangecommit [failed]( with [artifacts]( on release-24.3 @ [0673cbfa60f764ed35ae3a91a167883845038f70]( help see also: [how to investigate a go test failure \(internal\)]( [this test on roachdash]( | [improve this report!]( jira issue: crdb-59032,3.8,Critical,1.0,crash-like behavior
rust-lang/rust#151545,[ice]: icu_collections failed to compile in termux on android,"i tried to compile govctl-org/govctl in termux on android but ran into this bug. i haven't been able to reduce anymore. <!-- thank you for finding an internal compiler error! üßä if possible, try to provide a minimal verifiable example. you can read ""rust bug minimization patterns"" for how to create smaller examples. --> ### code ### meta <!-- if you're using the stable version of the compiler, you should also check if the bug also exists in the beta or nightly versions. --> : ### error output <!-- include a backtrace in the code block by setting in your environment. e.g. . --> backtrace",[],['BUG'],"['A-resolve', 'I-ICE', 'T-compiler', 'C-bug', 'needs-triage']",github,2026-01-23T16:48:11Z,,"[ice]: icu_collections failed to compile in termux on android i tried to compile govctl-org/govctl in termux on android but ran into this bug. i haven't been able to reduce anymore. <!-- thank you for finding an internal compiler error! üßä if possible, try to provide a minimal verifiable example. you can read ""rust bug minimization patterns"" for how to create smaller examples. --> ### code ### meta <!-- if you're using the stable version of the compiler, you should also check if the bug also exists in the beta or nightly versions. --> : ### error output <!-- include a backtrace in the code block by setting in your environment. e.g. . --> backtrace",2.58,Medium,0.806,functional impact
cilium/cilium#43956,cfp: allow mixed http/https ingress resources,"## cilium feature proposal **is your proposed feature related to a problem?** --> yes migrating from nginx ingress to cilium ingress runs into an issue due to cilium not handling a mix of http and http+https ingress resources in the same way. in nginx, the following setup is possible: - an ingress resource that defines both a http and a https listener - multiple ingress resources that define only a http listener for the same hostname, for a specific sub-path - this allows for sub-path urls to be directed to other services, even when that url is accessed over https in cilium, this isn't working. if no section is defined in the ingress resource, envoy will not accept an incoming https request for that resource. an example application that uses this is nvidia run:ai. it defines a generic https-enabled ingress resource for the fqdn, which points to the main management service. user-deployed workloads get their own ingress resources that look for sub-paths on the main fqdn, to capture that traffic and send it to the user's workload pods instead of the main management service. due to cilium ingress not accepting incoming https connections for those user-deployed ingresses, the application breaks when trying to replace nginx with cilium. manually defining the section on the user-deployed workload ingress also doesn't work, as the envoy pod will log that it already knows of an ssl listener with this hostname. **describe the feature you'd like** if an ingress resource exists that defines an https listener for a specific fqdn, any subsequent http-only ingress resources that use the same hostname but different sub-paths, should overrule the path routing from the ""parent"" https-enabled ingress for the paths defined in the http-only ingress resources. **notify relevant community channels** notify the members of any relevant code owners below from the [teams] list in the following form: - /envoy",[],['FEATURE'],"['kind/feature', 'kind/cfp', 'area/servicemesh', 'feature/k8s-ingress']",github,2026-01-23T16:49:34Z,,"cfp: allow mixed http/https ingress resources ## cilium feature proposal **is your proposed feature related to a problem?** --> yes migrating from nginx ingress to cilium ingress runs into an issue due to cilium not handling a mix of http and http+https ingress resources in the same way. in nginx, the following setup is possible: - an ingress resource that defines both a http and a https listener - multiple ingress resources that define only a http listener for the same hostname, for a specific sub-path - this allows for sub-path urls to be directed to other services, even when that url is accessed over https in cilium, this isn't working. if no section is defined in the ingress resource, envoy will not accept an incoming https request for that resource. an example application that uses this is nvidia run:ai. it defines a generic https-enabled ingress resource for the fqdn, which points to the main management service. user-deployed workloads get their own ingress resources that look for sub-paths on the main fqdn, to capture that traffic and send it to the user's workload pods instead of the main management service. due to cilium ingress not accepting incoming https connections for those user-deployed ingresses, the application breaks when trying to replace nginx with cilium. manually defining the section on the user-deployed workload ingress also doesn't work, as the envoy pod will log that it already knows of an ssl listener with this hostname. **describe the feature you'd like** if an ingress resource exists that defines an https listener for a specific fqdn, any subsequent http-only ingress resources that use the same hostname but different sub-paths, should overrule the path routing from the ""parent"" https-enabled ingress for the paths defined in the http-only ingress resources. **notify relevant community channels** notify the members of any relevant code owners below from the [teams] list in the following form: - /envoy",3.138,High,0.933,functional impact
cockroachdb/cockroach#161688,roachtest: rebalance/by-load/mode=replicas/mixed-version failed,"**note:** this build has runtime assertions enabled. if the same failure was hit in a run without assertions enabled, there should be a similar failure without this message. if there isn& ;t one, then this failure is likely due to an assertion violation or (assertion) timeout. roachtest.rebalance/by-load/mode=replicas/mixed-version [failed]( with [artifacts]( on master @ [e17af79b64bd9260c4963031439c92e6c7ecefc5]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21013953-1769152962-185-n7cpu4-0001 | 104.196.165.182 | 10.142.1.41 | | teamcity-21013953-1769152962-185-n7cpu4-0002 | 35.231.11.239 | 10.142.1.38 | | teamcity-21013953-1769152962-185-n7cpu4-0003 | 34.75.120.211 | 10.142.1.29 | | teamcity-21013953-1769152962-185-n7cpu4-0004 | 35.231.80.18 | 10.142.0.131 | | teamcity-21013953-1769152962-185-n7cpu4-0005 | 35.231.66.102 | 10.142.1.43 | | teamcity-21013953-1769152962-185-n7cpu4-0006 | 34.139.49.250 | 10.142.1.61 | | teamcity-21013953-1769152962-185-n7cpu4-0007 | 104.196.105.16 | 10.142.1.121 | parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - fs=ext4 - localssd=true - mvtdeploymentmode=system-only - mvtversions=v25.1.10 ‚Üí v25.2.11 ‚Üí v25.3.7 ‚Üí v25.4.3 ‚Üí v26.2.0-alpha.00000000-dev-e17af79b64bd9260c4963031439c92e6c7ecefc5 (master) - runtimeassertionsbuild=true help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( /cc /kv-triage [this test on roachdash]( | [improve this report!]( jira issue: crdb-59033",[],"['TESTING', 'BUG']","['C-bug', 'C-test-failure', 'O-robot', 'X-duplicate', 'A-testing', 'O-roachtest', 'branch-master', 'T-kv', 'B-runtime-assertions-enabled']",github,2026-01-23T17:12:20Z,2026-01-23T17:30:54Z,"roachtest: rebalance/by-load/mode=replicas/mixed-version failed **note:** this build has runtime assertions enabled. if the same failure was hit in a run without assertions enabled, there should be a similar failure without this message. if there isn& ;t one, then this failure is likely due to an assertion violation or (assertion) timeout. roachtest.rebalance/by-load/mode=replicas/mixed-version [failed]( with [artifacts]( on master @ [e17af79b64bd9260c4963031439c92e6c7ecefc5]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21013953-1769152962-185-n7cpu4-0001 | 104.196.165.182 | 10.142.1.41 | | teamcity-21013953-1769152962-185-n7cpu4-0002 | 35.231.11.239 | 10.142.1.38 | | teamcity-21013953-1769152962-185-n7cpu4-0003 | 34.75.120.211 | 10.142.1.29 | | teamcity-21013953-1769152962-185-n7cpu4-0004 | 35.231.80.18 | 10.142.0.131 | | teamcity-21013953-1769152962-185-n7cpu4-0005 | 35.231.66.102 | 10.142.1.43 | | teamcity-21013953-1769152962-185-n7cpu4-0006 | 34.139.49.250 | 10.142.1.61 | | teamcity-21013953-1769152962-185-n7cpu4-0007 | 104.196.105.16 | 10.142.1.121 | parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - fs=ext4 - localssd=true - mvtdeploymentmode=system-only - mvtversions=v25.1.10 ‚Üí v25.2.11 ‚Üí v25.3.7 ‚Üí v25.4.3 ‚Üí v26.2.0-alpha.00000000-dev-e17af79b64bd9260c4963031439c92e6c7ecefc5 (master) - runtimeassertionsbuild=true help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( /cc /kv-triage [this test on roachdash]( | [improve this report!]( jira issue: crdb-59033",2.941,Medium,0.888,functional impact
rust-lang/rust#151548,symbol of items instantiated w/ dyn trait w/ assoc const bindings not demangled (aka 's needs to be bumped),"#### reproducer #### program output #### background in pr i intentionally only bumped for *compiler*, not for any other project (library, tools). #### steps - [x] ~~merge - [ ] ~~optionally(?) release a new version of ~~ - [ ] ~~update submodule ,~~ and lock file",[],['BUG'],"['C-bug', 'T-libs', 'A-dyn-trait', 'A-backtrace', 'F-min_generic_const_args', 'A-name-mangling']",github,2026-01-23T17:14:55Z,,"symbol of items instantiated w/ dyn trait w/ assoc const bindings not demangled (aka 's needs to be bumped) #### reproducer #### program output #### background in pr i intentionally only bumped for *compiler*, not for any other project (library, tools). #### steps - [x] ~~merge - [ ] ~~optionally(?) release a new version of ~~ - [ ] ~~update submodule ,~~ and lock file",2.557,Medium,0.801,functional impact
cockroachdb/cockroach#161690,roachtest: import/cancellation/nodes=4 failed,"**note:** this build has runtime assertions enabled. if the same failure was hit in a run without assertions enabled, there should be a similar failure without this message. if there isn& ;t one, then this failure is likely due to an assertion violation or (assertion) timeout. roachtest.import/cancellation/nodes=4 [failed]( with [artifacts]( on release-26.1 @ [6fa7405f7bf2b301b0878b3b420e00248b450725]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21014103-1769152323-03-n4cpu4-0001 | 34.26.221.121 | 10.142.2.239 | | teamcity-21014103-1769152323-03-n4cpu4-0002 | 34.73.221.4 | 10.142.2.233 | | teamcity-21014103-1769152323-03-n4cpu4-0003 | 35.231.44.8 | 10.142.2.227 | | teamcity-21014103-1769152323-03-n4cpu4-0004 | 34.26.215.170 | 10.142.2.231 | parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=true - metamorphicleases=epoch - metamorphicwritebuffering=true - runtimeassertionsbuild=true help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( [this test on roachdash]( | [improve this report!]( jira issue: crdb-59034",[],['TESTING'],"['C-test-failure', 'O-robot', 'O-roachtest', 'T-sql-queries', 'B-runtime-assertions-enabled', 'branch-release-26.1']",github,2026-01-23T17:30:22Z,2026-01-27T19:51:29Z,"roachtest: import/cancellation/nodes=4 failed **note:** this build has runtime assertions enabled. if the same failure was hit in a run without assertions enabled, there should be a similar failure without this message. if there isn& ;t one, then this failure is likely due to an assertion violation or (assertion) timeout. roachtest.import/cancellation/nodes=4 [failed]( with [artifacts]( on release-26.1 @ [6fa7405f7bf2b301b0878b3b420e00248b450725]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21014103-1769152323-03-n4cpu4-0001 | 34.26.221.121 | 10.142.2.239 | | teamcity-21014103-1769152323-03-n4cpu4-0002 | 34.73.221.4 | 10.142.2.233 | | teamcity-21014103-1769152323-03-n4cpu4-0003 | 35.231.44.8 | 10.142.2.227 | | teamcity-21014103-1769152323-03-n4cpu4-0004 | 34.26.215.170 | 10.142.2.231 | parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=true - metamorphicleases=epoch - metamorphicwritebuffering=true - runtimeassertionsbuild=true help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( [this test on roachdash]( | [improve this report!]( jira issue: crdb-59034",2.922,Medium,0.884,functional impact
kubernetes/kubernetes#136475,"service creation fails ""provided ip is already allocated"" even if ip is not allocated","### what happened? i tried to create a new service to expose kube-dns on another ip. it seems to be impossible as it says that the clusterip (10.96.0.11) i set in the service is already allocated, even if it's not. below you'll find the smallest possible process to reproduce this problem. interestingly, if i create a service without specifying ip, an ip will be automatically allocated without any problem. i can then delete the service, the ip will be deleted, and if i create the same service but with this specific ip, it will work. it means that ip are correctly de-allocated when i remove a service. ### what did you expect to happen? service to be created with specified ip. ### how can we reproduce it (as minimally and precisely as possible)? test-svc.yaml ipaddress.yaml ### anything else we need to know? i also have direct access to etcd if needed to debug. ### kubernetes version ### cloud provider baremetal ### os version talos v1.11.6 linux: 6.12.62 runc: 1.3.4 ### install tools ### container runtime (cri) and version (if applicable) containerd: 2.1.5 ### related plugins (cni, csi, ...) and versions (if applicable) calico: 3.31.2",[],"['NETWORK', 'BUG']","['kind/bug', 'sig/network', 'needs-triage']",github,2026-01-23T17:42:58Z,,"service creation fails ""provided ip is already allocated"" even if ip is not allocated ### what happened? i tried to create a new service to expose kube-dns on another ip. it seems to be impossible as it says that the clusterip (10.96.0.11) i set in the service is already allocated, even if it's not. below you'll find the smallest possible process to reproduce this problem. interestingly, if i create a service without specifying ip, an ip will be automatically allocated without any problem. i can then delete the service, the ip will be deleted, and if i create the same service but with this specific ip, it will work. it means that ip are correctly de-allocated when i remove a service. ### what did you expect to happen? service to be created with specified ip. ### how can we reproduce it (as minimally and precisely as possible)? test-svc.yaml ipaddress.yaml ### anything else we need to know? i also have direct access to etcd if needed to debug. ### kubernetes version ### cloud provider baremetal ### os version talos v1.11.6 linux: 6.12.62 runc: 1.3.4 ### install tools ### container runtime (cri) and version (if applicable) containerd: 2.1.5 ### related plugins (cni, csi, ...) and versions (if applicable) calico: 3.31.2",2.881,Medium,0.875,affects communication layer
python/cpython#144190,fails for in 3.14,"# bug report ### bug description: this code works in 3.13, but fails in 3.14: failure in 3.14: works in older versions: ### cpython versions tested on: 3.14, 3.13 ### operating systems tested on: windows, linux",[],['DOCUMENTATION'],"['docs', 'topic-typing', '3.14', '3.15']",github,2026-01-23T17:53:56Z,,"fails for in 3.14 # bug report ### bug description: this code works in 3.13, but fails in 3.14: failure in 3.14: works in older versions: ### cpython versions tested on: 3.14, 3.13 ### operating systems tested on: windows, linux",1.2,Low,0.493,localized low-impact
cockroachdb/cockroach#161693,ccl/schemachangerccl: testpausemixedversion_ccl_add_column_multiple_regional_by_row failed,ccl/schemachangerccl.testpausemixedversion_ccl_add_column_multiple_regional_by_row [failed]( on master @ [17c82d0e79ce7759347ed22c5bc2cea2d782ae4f]( parameters: - attempt=1 - run=1 - shard=31 help see also: [how to investigate a go test failure \(internal\)]( /cc /sql-foundations [this test on roachdash]( | [improve this report!]( jira issue: crdb-59035,[],['TESTING'],"['C-test-failure', 'O-robot', 'branch-master', 'T-sql-foundations', 'P-2', 'branch-release-24.3', 'branch-release-25.2', 'branch-release-25.3', 'branch-release-25.4', 'branch-release-26.1', 'target-release-26.2.0', 'target-release-26.1.1', 'target-release-25.3.9', 'target-release-25.2.13', 'target-release-24.3.27']",github,2026-01-23T17:53:59Z,2026-01-27T21:41:59Z,ccl/schemachangerccl: testpausemixedversion_ccl_add_column_multiple_regional_by_row failed ccl/schemachangerccl.testpausemixedversion_ccl_add_column_multiple_regional_by_row [failed]( on master @ [17c82d0e79ce7759347ed22c5bc2cea2d782ae4f]( parameters: - attempt=1 - run=1 - shard=31 help see also: [how to investigate a go test failure \(internal\)]( /cc /sql-foundations [this test on roachdash]( | [improve this report!]( jira issue: crdb-59035,3.8,Critical,1.0,crash-like behavior
openssl/openssl#29741,suggestion for docs starting page,"as per here a concrete example to improve the initial experience for beginners opening up the documentation: see here the screenshots of where a new user is going to arrive and what they're likely going to be drawn to when they are looking for a starting point for e.g. tls with openssl (see red arrows for what user is probably going to click): then i end up on the page right below this paragraph. but that page 1. seems dangerously lowlevel for most newly arrived people who want to use tls for their apps (which i assume is going to be the majority of people landing on this page, apart from perhaps openssl command line tool users analyzing certs), and 2. has no examples, and 3. has no obvious listing for how to get to practical high-level beginner friendly examples. the red arrows show where the user needs to go for practical examples from here. not the easiest spots to find if you're completely new to this page. perhaps should be changed to link to a different manual entry point in the prominent center text? (this is a text suggestion for the center text with new links:) > a good starting point for learning openssl 3.0 is the [**openssl introduction page**]( for the underlying key concepts, there's the [lowlevel libssl manual]( notes about migrating existing applications to openssl 3.0 are in the [openssl 3.0 migration guide]( changes and rationale: - named the obvious starting point in bold, and made it an inviting link to click, saying ""**openssl introduction page**"". - kept the libssl link, but made it obvious that it's an expert topic by making the link include the word ""lowlevel"". - shortened the wording a bit. how about something like that? rationale for the most prominent link: has prominent examples, and it seems to be the most aware page of the fact that the user may want to use openssl for a variety of things, e.g. tls clients, tls servers, crypto primitives, and so on.",[],['DOCUMENTATION'],['triaged: documentation'],github,2026-01-23T17:58:45Z,2026-01-27T20:00:51Z,"suggestion for docs starting page as per here a concrete example to improve the initial experience for beginners opening up the documentation: see here the screenshots of where a new user is going to arrive and what they're likely going to be drawn to when they are looking for a starting point for e.g. tls with openssl (see red arrows for what user is probably going to click): then i end up on the page right below this paragraph. but that page 1. seems dangerously lowlevel for most newly arrived people who want to use tls for their apps (which i assume is going to be the majority of people landing on this page, apart from perhaps openssl command line tool users analyzing certs), and 2. has no examples, and 3. has no obvious listing for how to get to practical high-level beginner friendly examples. the red arrows show where the user needs to go for practical examples from here. not the easiest spots to find if you're completely new to this page. perhaps should be changed to link to a different manual entry point in the prominent center text? (this is a text suggestion for the center text with new links:) > a good starting point for learning openssl 3.0 is the [**openssl introduction page**]( for the underlying key concepts, there's the [lowlevel libssl manual]( notes about migrating existing applications to openssl 3.0 are in the [openssl 3.0 migration guide]( changes and rationale: - named the obvious starting point in bold, and made it an inviting link to click, saying ""**openssl introduction page**"". - kept the libssl link, but made it obvious that it's an expert topic by making the link include the word ""lowlevel"". - shortened the wording a bit. how about something like that? rationale for the most prominent link: has prominent examples, and it seems to be the most aware page of the fact that the user may want to use openssl for a variety of things, e.g. tls clients, tls servers, crypto primitives, and so on.",3.4,High,0.993,crash-like behavior
python/cpython#144191,can dataclass ordering avoid tuple wrappers for single field comparisons?,"proposal and rationale -------------- when , the generated rich comparison methods wrap the target field in a tuple. this makes sense when multiple fields are being compared. however for a single field, it adds unnecessary extra work: two extra tuple allocations and deallocations, a tuple compare, and superfluous equality check on the field. for example, the [heapq docs]( suggest making the following class: the generated code for less-than operation is: instead, it would be nicer to generate: behavior change ------------ when the tuple wrapper was removed from in favor of individual field comparisons linked by , it resulted in faster code that matched what people would write by hand. however, it also caused issues where users had been relying on the identity checks inside the tuple comparison. that affects us here as well: unlike the impact of the change, users of ordering operations are far less likely to be relying on identity-implies-equality. one reason is that is used with hashing but ordering operations are not. so the prior change affected and while the proposed change does not. another reason is that it only affects and but not , , , and , so it doesn't show-up 100% of the time. also python's ordering tools ( , , , , , , , and ) do not use and . so they are entirely unaffected. the upside of the behavior change is that it fixes an existing inconsistency between '<', '==', and '<=': this inconsistency is unexpected because it does not show up in hand-written code: ### linked prs * gh-144222",[],['FEATURE'],"['type-feature', 'stdlib', 'topic-dataclasses']",github,2026-01-23T18:12:41Z,,"can dataclass ordering avoid tuple wrappers for single field comparisons? proposal and rationale -------------- when , the generated rich comparison methods wrap the target field in a tuple. this makes sense when multiple fields are being compared. however for a single field, it adds unnecessary extra work: two extra tuple allocations and deallocations, a tuple compare, and superfluous equality check on the field. for example, the [heapq docs]( suggest making the following class: the generated code for less-than operation is: instead, it would be nicer to generate: behavior change ------------ when the tuple wrapper was removed from in favor of individual field comparisons linked by , it resulted in faster code that matched what people would write by hand. however, it also caused issues where users had been relying on the identity checks inside the tuple comparison. that affects us here as well: unlike the impact of the change, users of ordering operations are far less likely to be relying on identity-implies-equality. one reason is that is used with hashing but ordering operations are not. so the prior change affected and while the proposed change does not. another reason is that it only affects and but not , , , and , so it doesn't show-up 100% of the time. also python's ordering tools ( , , , , , , , and ) do not use and . so they are entirely unaffected. the upside of the behavior change is that it fixes an existing inconsistency between '<', '==', and '<=': this inconsistency is unexpected because it does not show up in hand-written code: ### linked prs * gh-144222",3.6,Critical,1.0,crash-like behavior
facebook/react#35613,[compiler bug]: expected memoization doesn't happen if multiple factors come into play,"### what kind of issue is this? - [x] react compiler core (the js output is incorrect, or your app works incorrectly after optimization) - [ ] babel-plugin-react-compiler (build issue installing or using the babel plugin) - [ ] eslint-plugin-react-hooks (build issue installing or using the eslint plugin) - [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script) ### link to repro ### repro steps when i combine optional chaining with a pattern, isn't stable anymore and the callback (hence: the logging) is invoked multiple times. ### how often does this bug happen? every time ### what version of react are you using? 19.2.3 ### what version of react compiler are you using? babel-plugin-react-compiler .0.0, if you mean that",[],['BUG'],"['Type: Bug', 'Status: Unconfirmed']",github,2026-01-23T18:14:29Z,2026-01-23T18:23:46Z,"[compiler bug]: expected memoization doesn't happen if multiple factors come into play ### what kind of issue is this? - [x] react compiler core (the js output is incorrect, or your app works incorrectly after optimization) - [ ] babel-plugin-react-compiler (build issue installing or using the babel plugin) - [ ] eslint-plugin-react-hooks (build issue installing or using the eslint plugin) - [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script) ### link to repro ### repro steps when i combine optional chaining with a pattern, isn't stable anymore and the callback (hence: the logging) is invoked multiple times. ### how often does this bug happen? every time ### what version of react are you using? 19.2.3 ### what version of react compiler are you using? babel-plugin-react-compiler .0.0, if you mean that",2.656,Medium,0.824,functional impact
electron/electron#49507,windows alt+space should get routed through 'system-context-menu' event,"### preflight checklist - [x] i have read the [contributing guidelines]( for this project. - [x] i agree to follow the [code of conduct]( that this project adheres to. - [x] i have searched the [issue tracker]( for a feature request that matches the one i want to file, without success. ### problem description as per - there currently is no way to prevent alt+space in windows from opening the system context menu even using webcontents.on(""before-input-event"")... though, even if it did, its less than ideal as preventing a keydown event means that the keyup will never fire, as per ### proposed solution proposes allowing an override specifically for alt+space, but i think it might be more meaningful to have the alt+space get routed through the existing ""before-input-event"" event, as it is more semantically appropriate and (hopefully) should allow the actual relevant keyboard events through. ### alternatives considered i feel the only viable alternative would be as suggests: have a specific option to override the alt+space keyboard combination, but i think that's less than ideal for the aforementioned reasons. i have tried using a global hotkey and an accelerator key, but neither were able to allow the key event through to the browser window. ### additional information _no response_",[],['FEATURE'],['enhancement :sparkles:'],github,2026-01-23T18:18:07Z,,"windows alt+space should get routed through 'system-context-menu' event ### preflight checklist - [x] i have read the [contributing guidelines]( for this project. - [x] i agree to follow the [code of conduct]( that this project adheres to. - [x] i have searched the [issue tracker]( for a feature request that matches the one i want to file, without success. ### problem description as per - there currently is no way to prevent alt+space in windows from opening the system context menu even using webcontents.on(""before-input-event"")... though, even if it did, its less than ideal as preventing a keydown event means that the keyup will never fire, as per ### proposed solution proposes allowing an override specifically for alt+space, but i think it might be more meaningful to have the alt+space get routed through the existing ""before-input-event"" event, as it is more semantically appropriate and (hopefully) should allow the actual relevant keyboard events through. ### alternatives considered i feel the only viable alternative would be as suggests: have a specific option to override the alt+space keyboard combination, but i think that's less than ideal for the aforementioned reasons. i have tried using a global hotkey and an accelerator key, but neither were able to allow the key event through to the browser window. ### additional information _no response_",1.4,Low,0.538,localized low-impact
openssl/openssl#29742,ssl_sendfile() has issues with bio_should_retry() raising and clearing,"there are two issues with ssl_sendfile() misreporting the state of the connection. 1. when the underlying socket is in non-blocking mode a short write can happen. in this case the bio_should_retry() is not set. the only way for the application to check why the write was short is to repeat it again. if the buffer space has not been cleared, the ssl_sendfile() will return -1 and will raise bio_should_retry(). however, on freebsd the sendfile(2) api allows to tell a short write due to insufficient buffer space from a short write due to end of file. bringing this logic up to the level of ssl_sendfile() will allow applications to issue 2x less syscalls. so this issue is a performance bug. 2. ssl_sendfile() does not reset the bio state. so, if ssl_sendfile() has raised the bio_should_retry() flag once, and no ssl_write() was ever issued on the connection, the flag will stay there indefinitely no matter if following ssl_sendfile()s were successful.",[],['BUG'],['issue: bug report'],github,2026-01-23T18:54:29Z,,"ssl_sendfile() has issues with bio_should_retry() raising and clearing there are two issues with ssl_sendfile() misreporting the state of the connection. 1. when the underlying socket is in non-blocking mode a short write can happen. in this case the bio_should_retry() is not set. the only way for the application to check why the write was short is to repeat it again. if the buffer space has not been cleared, the ssl_sendfile() will return -1 and will raise bio_should_retry(). however, on freebsd the sendfile(2) api allows to tell a short write due to insufficient buffer space from a short write due to end of file. bringing this logic up to the level of ssl_sendfile() will allow applications to issue 2x less syscalls. so this issue is a performance bug. 2. ssl_sendfile() does not reset the bio state. so, if ssl_sendfile() has raised the bio_should_retry() flag once, and no ssl_write() was ever issued on the connection, the flag will stay there indefinitely no matter if following ssl_sendfile()s were successful.",3.8,Critical,1.0,system-wide impact
istio/istio#58892,istioctl bug-report: gather previous container logs,"(this is used to request new product features, please visit < for questions on using istio) **describe the feature request** only gathers the current container logs. requesting that it be enhanced to gather any available previous logs too. previous container logs have a short lifetime and can be incredibly helpful to the troubleshooting process. **describe alternatives you've considered** running the following adhoc script. **affected product area (please put an x in all that apply)** [ ] ambient [ ] docs [ ] dual stack [ ] installation [ ] networking [ ] performance and scalability [ ] extensions and telemetry [ ] security [ ] test and release [x] user experience [ ] developer infrastructure **affected features (please put an x in all that apply)** [ ] multi cluster [ ] virtual machine [ ] multi control plane **additional context**",[],['FEATURE'],"['kind/enhancement', 'area/user experience']",github,2026-01-23T19:03:13Z,,"istioctl bug-report: gather previous container logs (this is used to request new product features, please visit < for questions on using istio) **describe the feature request** only gathers the current container logs. requesting that it be enhanced to gather any available previous logs too. previous container logs have a short lifetime and can be incredibly helpful to the troubleshooting process. **describe alternatives you've considered** running the following adhoc script. **affected product area (please put an x in all that apply)** [ ] ambient [ ] docs [ ] dual stack [ ] installation [ ] networking [ ] performance and scalability [ ] extensions and telemetry [ ] security [ ] test and release [x] user experience [ ] developer infrastructure **affected features (please put an x in all that apply)** [ ] multi cluster [ ] virtual machine [ ] multi control plane **additional context**",3.034,High,0.91,functional impact
python/cpython#144194,perf_jit_trampoline: mmap failure check uses wrong error value,"# bug report ### bug description: in , the function checks for to detect mmap failures, but mmap() returns (which is ) on error, not . this means mmap failures are never detected, and jitdump initialization proceeds with an invalid pointer. ### cpython versions tested on: cpython main branch, 3.15, 3.14, 3.13 ### operating systems tested on: linux ### linked prs * gh-143713",[],['BUG'],"['type-bug', 'interpreter-core', 'topic-JIT']",github,2026-01-23T19:18:03Z,,"perf_jit_trampoline: mmap failure check uses wrong error value # bug report ### bug description: in , the function checks for to detect mmap failures, but mmap() returns (which is ) on error, not . this means mmap failures are never detected, and jitdump initialization proceeds with an invalid pointer. ### cpython versions tested on: cpython main branch, 3.15, 3.14, 3.13 ### operating systems tested on: linux ### linked prs * gh-143713",2.26,Medium,0.734,functional impact
flutter/flutter#181408,[impeller] geometry_unittests.cc contains tests for too many other source files,the file started out as the tests for all of the somewhat limited geometry objects. since then the geometry code has grown and there are many more objects now and some of the tests for those objects are fairly extensive. some of the more egregious examples of consolidation for objects that required extensive testing have already been broken out into their own files. we should probably complete that work and break this file down fully into separate component tests.,[],['TESTING'],"['a: tests', 'engine', 'P3', 'e: impeller', 'team-engine', 'triaged-engine']",github,2026-01-23T19:26:17Z,,[impeller] geometry_unittests.cc contains tests for too many other source files the file started out as the tests for all of the somewhat limited geometry objects. since then the geometry code has grown and there are many more objects now and some of the tests for those objects are fairly extensive. some of the more egregious examples of consolidation for objects that required extensive testing have already been broken out into their own files. we should probably complete that work and break this file down fully into separate component tests.,3.4,High,0.993,system-wide impact
llvm/llvm-project#177666,[lldb] testscriptedframeprovider.py fails on 32-bit arm,"instead of 0xbad pc is 0xbac on arm. it looks like the least significant bit is being discarded, possibly because pc must always be at least 16-bit aligned. this happens since the introduction of the new test_chained_frame_providers test by , breaking the bot.",[],['UI'],"['lldb', 'test-suite']",github,2026-01-23T19:50:35Z,2026-01-23T21:04:58Z,"[lldb] testscriptedframeprovider.py fails on 32-bit arm instead of 0xbad pc is 0xbac on arm. it looks like the least significant bit is being discarded, possibly because pc must always be at least 16-bit aligned. this happens since the introduction of the new test_chained_frame_providers test by , breaking the bot.",1.8,Low,0.629,user-visible issue
kubernetes/kubernetes#136481,pod level resource managers integration work,"this issue has the purpose of tracking any open points related to pod level resource managers with any other features in development. - [x] * the container restart flow involves deleting the topology manager state from the corresponding container. it is needed to evaluate if this can have any repercussions to the overall cpu manager functionality, including pod level resource managers. * it seems that the does not affect the functionality of the resource managers, please refer to - [ ] * will introduce state changes to the cpu manager, creating a new state version v3, which may conflict with",[],['FEATURE'],"['sig/node', 'kind/feature', 'priority/important-longterm', 'triage/accepted']",github,2026-01-23T20:14:47Z,,"pod level resource managers integration work this issue has the purpose of tracking any open points related to pod level resource managers with any other features in development. - [x] * the container restart flow involves deleting the topology manager state from the corresponding container. it is needed to evaluate if this can have any repercussions to the overall cpu manager functionality, including pod level resource managers. * it seems that the does not affect the functionality of the resource managers, please refer to - [ ] * will introduce state changes to the cpu manager, creating a new state version v3, which may conflict with",6.8,Critical,1.0,crash-like behavior
cockroachdb/cockroach#161699,kv/kvserver: testflowcontroladmissionpostsplitmergev2 failed,kv/kvserver.testflowcontroladmissionpostsplitmergev2 [failed]( with [artifacts]( on master @ [a5b6f1b8420f45fafd7faee11255fd24c95b567a]( help see also: [how to investigate a go test failure \(internal\)]( /cc /kv-triage [this test on roachdash]( | [improve this report!]( jira issue: crdb-59036,[],"['TESTING', 'BUG']","['C-bug', 'C-test-failure', 'O-robot', 'A-testing', 'branch-master', 'T-kv', 'P-3']",github,2026-01-23T21:02:00Z,,kv/kvserver: testflowcontroladmissionpostsplitmergev2 failed kv/kvserver.testflowcontroladmissionpostsplitmergev2 [failed]( with [artifacts]( on master @ [a5b6f1b8420f45fafd7faee11255fd24c95b567a]( help see also: [how to investigate a go test failure \(internal\)]( /cc /kv-triage [this test on roachdash]( | [improve this report!]( jira issue: crdb-59036,2.0,Medium,0.675,localized low-impact
containerd/containerd#12814,[sig-node]: kep-5825: cri pagination,### kep/sig-node references - kep(s): kep-5825: cri pagination - stage: alpha - v1.36 - kep issue: - kep pr: - k8s-release: v1.36 - kep-owner: - sig-node member liason: - kep-shepherd: ### what is the sig-node problem you are trying to solve pagination in cri apis to avoid exceeding grpc message limits on nodes with high numbers of containers ### describe the solution you would like see and [aip-158]( ### additional context _no response_,[],['FEATURE'],"['kind/feature', 'area/cri']",github,2026-01-23T21:02:17Z,,[sig-node]: kep-5825: cri pagination ### kep/sig-node references - kep(s): kep-5825: cri pagination - stage: alpha - v1.36 - kep issue: - kep pr: - k8s-release: v1.36 - kep-owner: - sig-node member liason: - kep-shepherd: ### what is the sig-node problem you are trying to solve pagination in cri apis to avoid exceeding grpc message limits on nodes with high numbers of containers ### describe the solution you would like see and [aip-158]( ### additional context _no response_,1.4,Low,0.538,localized low-impact
cockroachdb/cockroach#161704,restore: database/table restore does not synthesize rbr index level zone configs,restore synthesizes mr table zone configs [here]( but this logic is missing a call to applyzoneconfigformultiregiontableoptionnewindexes. restore likely needs to port over the logic in the configurezoneconfigfornewindexpartitioning helper. jira issue: crdb-59037 epic crdb-58422,[],['BUG'],"['C-bug', 'A-disaster-recovery', 'T-disaster-recovery']",github,2026-01-23T21:36:00Z,,restore: database/table restore does not synthesize rbr index level zone configs restore synthesizes mr table zone configs [here]( but this logic is missing a call to applyzoneconfigformultiregiontableoptionnewindexes. restore likely needs to port over the logic in the configurezoneconfigfornewindexpartitioning helper. jira issue: crdb-59037 epic crdb-58422,2.563,Medium,0.803,functional impact
rust-lang/rust#151554,code coverage reports wrong count of loop header,a simple test of the feature returns the wrong count (9 instead of 10) of the loop header on line 4 when built with rust 1.93.0: main.rs: reproducer: this test works as expected with 1.92.0. ### meta,[],['BUG'],"['T-compiler', 'C-bug', 'A-code-coverage']",github,2026-01-23T22:05:43Z,,code coverage reports wrong count of loop header a simple test of the feature returns the wrong count (9 instead of 10) of the loop header on line 4 when built with rust 1.93.0: main.rs: reproducer: this test works as expected with 1.92.0. ### meta,2.478,Medium,0.783,functional impact
pytorch/pytorch#173225,[rfc] renaming mps backend as metal backend,"### üöÄ the feature, motivation and pitch currently using the name mps backend is a bit misleading. while metal performance shaders and metal performance shaders graph are used in many of the op implementations, most of the ops are nowadays going through custom metal kernels added to the backend. so i want to float around the idea to rename the backend from mps to metal to better convey where the developer interested in writing a gpu implementation of an operation on macos should head to, and metal might be more apt descriptor than mps. more concretely in order to avoid breaking existing code i think the initial renaming should just be adding a device in name only and have it map to the existing mps code. and alter the device printout of objects on to return instead of . the tedious work of going through the codebase to scrub the variable and filenames could be done in the background. once the move is otherwise done, a deprecation warning could be added to get printed if someone targets the device and eventually it would be removed from the list of valid target devices. this would be the minimal diff to start with the facade of having a device: happy to get feedback and comments if this sounds like a sensible move from the community perspective or would it just add confusion and waste effort in addressing a non-issue. ### alternatives alternative would be to keep the naming as is. ### additional context notably there already exists a metal backend in the code base but based on the discussion* the active development work seems to have ceased. mps would be taking over the namespace which would make it clearer where the developer who wants to add a metal kernel should head over to. * cc",[],['FEATURE'],"['low priority', 'triaged', 'enhancement', 'module: mps']",github,2026-01-23T23:10:11Z,,"[rfc] renaming mps backend as metal backend ### üöÄ the feature, motivation and pitch currently using the name mps backend is a bit misleading. while metal performance shaders and metal performance shaders graph are used in many of the op implementations, most of the ops are nowadays going through custom metal kernels added to the backend. so i want to float around the idea to rename the backend from mps to metal to better convey where the developer interested in writing a gpu implementation of an operation on macos should head to, and metal might be more apt descriptor than mps. more concretely in order to avoid breaking existing code i think the initial renaming should just be adding a device in name only and have it map to the existing mps code. and alter the device printout of objects on to return instead of . the tedious work of going through the codebase to scrub the variable and filenames could be done in the background. once the move is otherwise done, a deprecation warning could be added to get printed if someone targets the device and eventually it would be removed from the list of valid target devices. this would be the minimal diff to start with the facade of having a device: happy to get feedback and comments if this sounds like a sensible move from the community perspective or would it just add confusion and waste effort in addressing a non-issue. ### alternatives alternative would be to keep the naming as is. ### additional context notably there already exists a metal backend in the code base but based on the discussion* the active development work seems to have ceased. mps would be taking over the namespace which would make it clearer where the developer who wants to add a metal kernel should head over to. * cc",5.0,Critical,1.0,crash-like behavior
kubernetes/kubernetes#136485,waitforcertificate may block until timeout if certificate changes before approval,"### what happened? several years ago, there was an optimization made to the handling of waitforcertificate so that it would be interrupted when the desired certificate changes, ref: this optimization seems to have since regressed due to a race condition between when the is retrieved by the reconciler and when it's set by . if is read by before sets it (and rotate is not actively trying to read from ), then the cancel function obtains from will be invalid as soon as reaches the func. this means essentially fails to cancel the wait in , leading it to wait up to the full length of the timeout. only after independently exists the wait is finally able to signal it at the beginning of the next iteration. ### what did you expect to happen? should break on the change of the certificate tmplate ### how can we reproduce it (as minimally and precisely as possible)? one way to reproduce is just by creating a node, wait for kubelet to create the initial csr, and then update the node's addresses while the csr is pending. in this case, a new csr is not created until after the 15 minute timeout. ### anything else we need to know? attempted fix in but this should be done as part of a larger refactor ### kubernetes version ### cloud provider aws ### os version ### install tools ### container runtime (cri) and version (if applicable) ### related plugins (cni, csi, ...) and versions (if applicable)",[],['BUG'],"['kind/bug', 'needs-sig', 'needs-triage']",github,2026-01-23T23:16:17Z,,"waitforcertificate may block until timeout if certificate changes before approval ### what happened? several years ago, there was an optimization made to the handling of waitforcertificate so that it would be interrupted when the desired certificate changes, ref: this optimization seems to have since regressed due to a race condition between when the is retrieved by the reconciler and when it's set by . if is read by before sets it (and rotate is not actively trying to read from ), then the cancel function obtains from will be invalid as soon as reaches the func. this means essentially fails to cancel the wait in , leading it to wait up to the full length of the timeout. only after independently exists the wait is finally able to signal it at the beginning of the next iteration. ### what did you expect to happen? should break on the change of the certificate tmplate ### how can we reproduce it (as minimally and precisely as possible)? one way to reproduce is just by creating a node, wait for kubelet to create the initial csr, and then update the node's addresses while the csr is pending. in this case, a new csr is not created until after the 15 minute timeout. ### anything else we need to know? attempted fix in but this should be done as part of a larger refactor ### kubernetes version ### cloud provider aws ### os version ### install tools ### container runtime (cri) and version (if applicable) ### related plugins (cni, csi, ...) and versions (if applicable)",6.4,Critical,1.0,crash-like behavior
pandas-dev/pandas#63842,bug: returns object type instead of stringdtype.,i would expect the result to preserve a instead of return,[],['BUG'],"['Bug', 'Strings']",github,2026-01-24T01:10:02Z,,bug: returns object type instead of stringdtype. i would expect the result to preserve a instead of return,2.366,Medium,0.758,functional impact
cockroachdb/cockroach#161722,pkg/sql/logictest/tests/local/local_test: testlogic_swap_mutation failed,pkg/sql/logictest/tests/local/local_test.testlogic_swap_mutation [failed]( on master @ [f957652150dec24320428995ac8171301aba51a7]( parameters: - attempt=1 - run=1 - shard=2 help see also: [how to investigate a go test failure \(internal\)]( [this test on roachdash]( | [improve this report!]( jira issue: crdb-59038,[],['TESTING'],"['C-test-failure', 'O-robot', 'branch-master', 'T-sql-queries', 'A-buffered-writes']",github,2026-01-24T01:20:15Z,,pkg/sql/logictest/tests/local/local_test: testlogic_swap_mutation failed pkg/sql/logictest/tests/local/local_test.testlogic_swap_mutation [failed]( on master @ [f957652150dec24320428995ac8171301aba51a7]( parameters: - attempt=1 - run=1 - shard=2 help see also: [how to investigate a go test failure \(internal\)]( [this test on roachdash]( | [improve this report!]( jira issue: crdb-59038,1.6,Low,0.584,localized low-impact
python/cpython#144199,leak sanitizer errors when is passed to during build,# bug report ### bug description: configure command: with ubuntu 24.04 and gcc 13.3.0 build log: ### cpython versions tested on: cpython main branch ### operating systems tested on: linux,[],"['BUG', 'UI']","['type-bug', 'build']",github,2026-01-24T01:36:51Z,,leak sanitizer errors when is passed to during build # bug report ### bug description: configure command: with ubuntu 24.04 and gcc 13.3.0 build log: ### cpython versions tested on: cpython main branch ### operating systems tested on: linux,2.119,Medium,0.702,user-visible issue
python/cpython#144203,customizable venv prompt prefix and suffix,"# feature or enhancement ### proposal: in venv activate ps1 is updated with the fixed format. some user like me doesn't like such format for some reason. for my case, i'd like to keep empty line between the previous output and the current prompt like if i enable venv, it breaks my preference like i want to keep empty like between previous output and commands like this can be easily achieved keeping the compatibility by taking prefix and suffix for venv name in activate to set ps1 instaed of current ""("" and "") "". it makes someone to use explain venv name as their preferred way like with ### has this already been discussed elsewhere? this is a minor feature, which does not need previous discussion elsewhere ### links to previous discussion of this feature: _no response_ ### linked prs * gh-144205",[],['FEATURE'],"['type-feature', 'stdlib', 'topic-venv']",github,2026-01-24T03:24:48Z,,"customizable venv prompt prefix and suffix # feature or enhancement ### proposal: in venv activate ps1 is updated with the fixed format. some user like me doesn't like such format for some reason. for my case, i'd like to keep empty line between the previous output and the current prompt like if i enable venv, it breaks my preference like i want to keep empty like between previous output and commands like this can be easily achieved keeping the compatibility by taking prefix and suffix for venv name in activate to set ps1 instaed of current ""("" and "") "". it makes someone to use explain venv name as their preferred way like with ### has this already been discussed elsewhere? this is a minor feature, which does not need previous discussion elsewhere ### links to previous discussion of this feature: _no response_ ### linked prs * gh-144205",1.4,Low,0.538,localized low-impact
rust-lang/rust#151566,about 1 line (or more) of clipping in dropdown menus,"there is about a line of clipping in the dropdown menus, even when i scroll to the bottom after was fixed. ![image]( ![image](",[],"['BUG', 'UI']","['T-rustdoc', 'C-bug', 'A-rustdoc-ui', 'T-rustdoc-frontend']",github,2026-01-24T03:30:03Z,,"about 1 line (or more) of clipping in dropdown menus there is about a line of clipping in the dropdown menus, even when i scroll to the bottom after was fixed. ![image]( ![image](",2.181,Medium,0.716,user-visible issue
cockroachdb/cockroach#161726,util/admission: testbadioloadlistenerstats failed,util/admission.testbadioloadlistenerstats [failed]( with [artifacts]( on master @ [cf29a2da053a5025f6f7e68e01c70ab3f9aa8192]( failed with: fatal entries found in cockroach logs: help see also: [how to investigate a go test failure \(internal\)]( /cc /admission-control [this test on roachdash]( | [improve this report!]( jira issue: crdb-59039,[],['TESTING'],"['C-test-failure', 'O-robot', 'A-admission-control', 'branch-master', 'T-kv', 'X-infra-flake']",github,2026-01-24T04:40:19Z,2026-01-27T14:36:05Z,util/admission: testbadioloadlistenerstats failed util/admission.testbadioloadlistenerstats [failed]( with [artifacts]( on master @ [cf29a2da053a5025f6f7e68e01c70ab3f9aa8192]( failed with: fatal entries found in cockroach logs: help see also: [how to investigate a go test failure \(internal\)]( /cc /admission-control [this test on roachdash]( | [improve this report!]( jira issue: crdb-59039,3.8,Critical,1.0,crash-like behavior
cockroachdb/cockroach#161727,: failed,. [failed]( with [artifacts]( on refs/heads/master @ [f957652150dec24320428995ac8171301aba51a7]( help see also: [how to investigate a go test failure \(internal\)]( /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-59040,[],['TESTING'],"['C-test-failure', 'O-robot', 'release-blocker', 'T-testeng', 'branch-refs/heads/master']",github,2026-01-24T04:50:55Z,,: failed . [failed]( with [artifacts]( on refs/heads/master @ [f957652150dec24320428995ac8171301aba51a7]( help see also: [how to investigate a go test failure \(internal\)]( /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-59040,1.6,Low,0.584,localized low-impact
rust-lang/rust#151569,nested has incorrect hygiene,"i tried this code: another code snippet with the same behavior i expect the above code should compile, since the item and the call are in the same ""macro scope"". instead, i got the following compile error: however, if the item is defined in the macro instead, the call finds that item and uses it. for example, the following code compiles and prints ""outer"": this seems incorrect / undesirable. my (uninformed) guess is that the fix would require each identifier to keep track of the entire ""macro stack"" that it went through. ### meta reproducible on the playground with version",[],['BUG'],"['A-macros', 'T-lang', 'T-compiler', 'A-decl-macros-2-0', 'C-bug', 'F-decl_macro', 'A-hygiene']",github,2026-01-24T04:59:40Z,,"nested has incorrect hygiene i tried this code: another code snippet with the same behavior i expect the above code should compile, since the item and the call are in the same ""macro scope"". instead, i got the following compile error: however, if the item is defined in the macro instead, the call finds that item and uses it. for example, the following code compiles and prints ""outer"": this seems incorrect / undesirable. my (uninformed) guess is that the fix would require each identifier to keep track of the entire ""macro stack"" that it went through. ### meta reproducible on the playground with version",2.525,Medium,0.794,functional impact
cockroachdb/cockroach#161729,sql: crdb_rewrite_inline_hints fails on mutation statements,"calling with a mutation statement fails. here's an example: this happens when adding index hints to insert, upsert, update, and delete. i think we're just not visiting the correct aliasedtableexpr in walk.go. jira issue: crdb-59041",[],['BUG'],"['C-bug', 'GA-blocker', 'T-sql-queries', 'A-plan-management', 'branch-release-26.1', 'A-statement-hint']",github,2026-01-24T06:07:18Z,,"sql: crdb_rewrite_inline_hints fails on mutation statements calling with a mutation statement fails. here's an example: this happens when adding index hints to insert, upsert, update, and delete. i think we're just not visiting the correct aliasedtableexpr in walk.go. jira issue: crdb-59041",2.352,Medium,0.755,functional impact
rust-lang/rust#151573,rustc-1.93.0 does not build on macos 10.12,"rustc-1.93.0 does not build on macos 10.12. it fails during: building stage1 compiler artifacts (stage0 -> stage1, x86_64-apple-darwin) i believe you are using apis that are not supported. ### summary project does not build. ### command used ### expected behaviour i expected the build to not fail. ### actual behaviour see above. i also rebuilt rustc-1.92.0 on this system without any problems. ### bootstrap configuration (bootstrap.toml) i can attach if needed. it made the body too long. ### operating system macos 10.12.6 ### head this is a tarball build. <!-- include the complete build log in the section below. enable backtrace and verbose mode if possible for more detailed information e.g., with . --> build log",[],['BUG'],"['O-macos', 'T-compiler', 'regression-from-stable-to-stable', 'C-bug']",github,2026-01-24T06:38:46Z,,"rustc-1.93.0 does not build on macos 10.12 rustc-1.93.0 does not build on macos 10.12. it fails during: building stage1 compiler artifacts (stage0 -> stage1, x86_64-apple-darwin) i believe you are using apis that are not supported. ### summary project does not build. ### command used ### expected behaviour i expected the build to not fail. ### actual behaviour see above. i also rebuilt rustc-1.92.0 on this system without any problems. ### bootstrap configuration (bootstrap.toml) i can attach if needed. it made the body too long. ### operating system macos 10.12.6 ### head this is a tarball build. <!-- include the complete build log in the section below. enable backtrace and verbose mode if possible for more detailed information e.g., with . --> build log",2.456,Medium,0.778,functional impact
cockroachdb/cockroach#161730,sentry: pebble.go:1087: log.fatal: local corruption detected: pebble: file 097195: block 4942435/2668: √ó checksum mismatch d0ef35fc != d9493d93 (1) attached stack trace -- stack trace: | github.c...,this issue was auto filed by sentry. it represents a crash or reported error on a live cluster with telemetry enabled. sentry link: [ panic message: stacktrace (expand for inline code snippets): [src/runtime/asm_amd64.s#l1699-l1701](src/runtime/asm_amd64.s#l1699-l1701) [external/org_golang_x_sync/errgroup/errgroup.go#l77-l79](external/org_golang_x_sync/errgroup/errgroup.go#l77-l79) [debug_check_store.go#l159-l161](debug_check_store.go#l159-l161) [debug_check_store.go#l128-l130](debug_check_store.go#l128-l130) [pkg/kv/kvserver/rditer/stats.go#l20-l22](pkg/kv/kvserver/rditer/stats.go#l20-l22) [pkg/kv/kvserver/rditer/stats.go#l51-l53](pkg/kv/kvserver/rditer/stats.go#l51-l53) [pkg/kv/kvserver/rditer/stats.go#l89-l91](pkg/kv/kvserver/rditer/stats.go#l89-l91) [pkg/storage/mvcc.go#l7350-l7352](pkg/storage/mvcc.go#l7350-l7352) [pkg/storage/mvcc.go#l7398-l7400](pkg/storage/mvcc.go#l7398-l7400) [pkg/storage/intent_interleaving_iter.go#l849-l851](pkg/storage/intent_interleaving_iter.go#l849-l851) [pkg/storage/pebble_iterator.go#l439-l441](pkg/storage/pebble_iterator.go#l439-l441) [external/com_github_cockroachdb_pebble/iterator.go#l1700-l1702](external/com_github_cockroachdb_pebble/iterator.go#l1700-l1702) [external/com_github_cockroachdb_pebble/iterator.go#l1984-l1986](external/com_github_cockroachdb_pebble/iterator.go#l1984-l1986) [external/com_github_cockroachdb_pebble/iterator.go#l617-l619](external/com_github_cockroachdb_pebble/iterator.go#l617-l619) [external/com_github_cockroachdb_pebble/iterator.go#l722-l724](external/com_github_cockroachdb_pebble/iterator.go#l722-l724) [external/com_github_cockroachdb_pebble/iterator.go#l1188-l1190](external/com_github_cockroachdb_pebble/iterator.go#l1188-l1190) [external/com_github_cockroachdb_pebble/range_keys.go#l662-l664](external/com_github_cockroachdb_pebble/range_keys.go#l662-l664) [external/com_github_cockroachdb_pebble/merging_iter.go#l1188-l1190](external/com_github_cockroachdb_pebble/merging_iter.go#l1188-l1190) [external/com_github_cockroachdb_pebble/merging_iter.go#l540-l542](external/com_github_cockroachdb_pebble/merging_iter.go#l540-l542) [external/com_github_cockroachdb_pebble/level_iter.go#l752-l754](external/com_github_cockroachdb_pebble/level_iter.go#l752-l754) [external/com_github_cockroachdb_pebble/sstable/reader_iter_single_lvl.go#l1251-l1253](external/com_github_cockroachdb_pebble/sstable/reader_iter_single_lvl.go#l1251-l1253) [external/com_github_cockroachdb_pebble/sstable/reader_iter_single_lvl.go#l1355-l1357](external/com_github_cockroachdb_pebble/sstable/reader_iter_single_lvl.go#l1355-l1357) [external/com_github_cockroachdb_pebble/sstable/reader_iter_single_lvl.go#l497-l499](external/com_github_cockroachdb_pebble/sstable/reader_iter_single_lvl.go#l497-l499) [external/com_github_cockroachdb_pebble/sstable/reader.go#l328-l330](external/com_github_cockroachdb_pebble/sstable/reader.go#l328-l330) [external/com_github_cockroachdb_pebble/sstable/block/block.go#l539-l541](external/com_github_cockroachdb_pebble/sstable/block/block.go#l539-l541) [external/com_github_cockroachdb_pebble/sstable/block/block.go#l441-l443](external/com_github_cockroachdb_pebble/sstable/block/block.go#l441-l443) [external/com_github_cockroachdb_pebble/event.go#l1152-l1154](external/com_github_cockroachdb_pebble/event.go#l1152-l1154) [external/com_github_cockroachdb_pebble/event.go#l1193-l1195](external/com_github_cockroachdb_pebble/event.go#l1193-l1195) [external/com_github_cockroachdb_pebble/event.go#l993-l995](external/com_github_cockroachdb_pebble/event.go#l993-l995) [pkg/storage/pebble.go#l1086-l1088](pkg/storage/pebble.go#l1086-l1088) ### tags | tag | value | | --- | --- | | command | debug check-store | | environment | v25.2.10 | | go version | go1.23.12 x:nocoverageredesign | | platform | linux amd64 | | distribution | ccl | | cockroach release | v25.2.10 | | cockroach sha | a2cf0e12d277179df853e7e494c0155a6839edf3 | | # of cpus | 48 | | # of goroutines | 110 | jira issue: crdb-59042,[],['BUG'],"['C-bug', 'A-storage', 'O-sentry', 'X-blathers-triaged', 'T-storage', 'branch-release-25.2']",github,2026-01-24T06:57:39Z,,sentry: pebble.go:1087: log.fatal: local corruption detected: pebble: file 097195: block 4942435/2668: √ó checksum mismatch d0ef35fc != d9493d93 (1) attached stack trace -- stack trace: | github.c... this issue was auto filed by sentry. it represents a crash or reported error on a live cluster with telemetry enabled. sentry link: [ panic message: stacktrace (expand for inline code snippets): [src/runtime/asm_amd64.s#l1699-l1701](src/runtime/asm_amd64.s#l1699-l1701) [external/org_golang_x_sync/errgroup/errgroup.go#l77-l79](external/org_golang_x_sync/errgroup/errgroup.go#l77-l79) [debug_check_store.go#l159-l161](debug_check_store.go#l159-l161) [debug_check_store.go#l128-l130](debug_check_store.go#l128-l130) [pkg/kv/kvserver/rditer/stats.go#l20-l22](pkg/kv/kvserver/rditer/stats.go#l20-l22) [pkg/kv/kvserver/rditer/stats.go#l51-l53](pkg/kv/kvserver/rditer/stats.go#l51-l53) [pkg/kv/kvserver/rditer/stats.go#l89-l91](pkg/kv/kvserver/rditer/stats.go#l89-l91) [pkg/storage/mvcc.go#l7350-l7352](pkg/storage/mvcc.go#l7350-l7352) [pkg/storage/mvcc.go#l7398-l7400](pkg/storage/mvcc.go#l7398-l7400) [pkg/storage/intent_interleaving_iter.go#l849-l851](pkg/storage/intent_interleaving_iter.go#l849-l851) [pkg/storage/pebble_iterator.go#l439-l441](pkg/storage/pebble_iterator.go#l439-l441) [external/com_github_cockroachdb_pebble/iterator.go#l1700-l1702](external/com_github_cockroachdb_pebble/iterator.go#l1700-l1702) [external/com_github_cockroachdb_pebble/iterator.go#l1984-l1986](external/com_github_cockroachdb_pebble/iterator.go#l1984-l1986) [external/com_github_cockroachdb_pebble/iterator.go#l617-l619](external/com_github_cockroachdb_pebble/iterator.go#l617-l619) [external/com_github_cockroachdb_pebble/iterator.go#l722-l724](external/com_github_cockroachdb_pebble/iterator.go#l722-l724) [external/com_github_cockroachdb_pebble/iterator.go#l1188-l1190](external/com_github_cockroachdb_pebble/iterator.go#l1188-l1190) [external/com_github_cockroachdb_pebble/range_keys.go#l662-l664](external/com_github_cockroachdb_pebble/range_keys.go#l662-l664) [external/com_github_cockroachdb_pebble/merging_iter.go#l1188-l1190](external/com_github_cockroachdb_pebble/merging_iter.go#l1188-l1190) [external/com_github_cockroachdb_pebble/merging_iter.go#l540-l542](external/com_github_cockroachdb_pebble/merging_iter.go#l540-l542) [external/com_github_cockroachdb_pebble/level_iter.go#l752-l754](external/com_github_cockroachdb_pebble/level_iter.go#l752-l754) [external/com_github_cockroachdb_pebble/sstable/reader_iter_single_lvl.go#l1251-l1253](external/com_github_cockroachdb_pebble/sstable/reader_iter_single_lvl.go#l1251-l1253) [external/com_github_cockroachdb_pebble/sstable/reader_iter_single_lvl.go#l1355-l1357](external/com_github_cockroachdb_pebble/sstable/reader_iter_single_lvl.go#l1355-l1357) [external/com_github_cockroachdb_pebble/sstable/reader_iter_single_lvl.go#l497-l499](external/com_github_cockroachdb_pebble/sstable/reader_iter_single_lvl.go#l497-l499) [external/com_github_cockroachdb_pebble/sstable/reader.go#l328-l330](external/com_github_cockroachdb_pebble/sstable/reader.go#l328-l330) [external/com_github_cockroachdb_pebble/sstable/block/block.go#l539-l541](external/com_github_cockroachdb_pebble/sstable/block/block.go#l539-l541) [external/com_github_cockroachdb_pebble/sstable/block/block.go#l441-l443](external/com_github_cockroachdb_pebble/sstable/block/block.go#l441-l443) [external/com_github_cockroachdb_pebble/event.go#l1152-l1154](external/com_github_cockroachdb_pebble/event.go#l1152-l1154) [external/com_github_cockroachdb_pebble/event.go#l1193-l1195](external/com_github_cockroachdb_pebble/event.go#l1193-l1195) [external/com_github_cockroachdb_pebble/event.go#l993-l995](external/com_github_cockroachdb_pebble/event.go#l993-l995) [pkg/storage/pebble.go#l1086-l1088](pkg/storage/pebble.go#l1086-l1088) ### tags | tag | value | | --- | --- | | command | debug check-store | | environment | v25.2.10 | | go version | go1.23.12 x:nocoverageredesign | | platform | linux amd64 | | distribution | ccl | | cockroach release | v25.2.10 | | cockroach sha | a2cf0e12d277179df853e7e494c0155a6839edf3 | | # of cpus | 48 | | # of goroutines | 110 | jira issue: crdb-59042,6.0,Critical,1.0,crash-like behavior
cockroachdb/cockroach#161731,ccl/sqlproxyccl/acl: testparsingerrorhandling failed,ccl/sqlproxyccl/acl.testparsingerrorhandling [failed]( on release-25.2.12-rc @ [7fc1e95228ad6610c2ea21d8ba55d0d6513c319f]( parameters: - attempt=1 - run=5 - shard=1 help see also: [how to investigate a go test failure \(internal\)]( same failure on other branches - ccl/sqlproxyccl/acl: testparsingerrorhandling failed [c-test-failure o-robot t-cloud-platform branch-release-25.2.11-rc] - ccl/sqlproxyccl/acl: testparsingerrorhandling failed [c-test-failure o-robot t-cloud-platform branch-release-25.2] - ccl/sqlproxyccl/acl: testparsingerrorhandling failed [c-test-failure o-robot t-cloud-platform branch-release-24.3] /cc /sqlproxy-prs /server [this test on roachdash]( | [improve this report!]( jira issue: crdb-59043,[],['TESTING'],"['C-test-failure', 'O-robot', 'T-cloud-platform', 'branch-release-25.2.12-rc']",github,2026-01-24T07:14:29Z,,ccl/sqlproxyccl/acl: testparsingerrorhandling failed ccl/sqlproxyccl/acl.testparsingerrorhandling [failed]( on release-25.2.12-rc @ [7fc1e95228ad6610c2ea21d8ba55d0d6513c319f]( parameters: - attempt=1 - run=5 - shard=1 help see also: [how to investigate a go test failure \(internal\)]( same failure on other branches - ccl/sqlproxyccl/acl: testparsingerrorhandling failed [c-test-failure o-robot t-cloud-platform branch-release-25.2.11-rc] - ccl/sqlproxyccl/acl: testparsingerrorhandling failed [c-test-failure o-robot t-cloud-platform branch-release-25.2] - ccl/sqlproxyccl/acl: testparsingerrorhandling failed [c-test-failure o-robot t-cloud-platform branch-release-24.3] /cc /sqlproxy-prs /server [this test on roachdash]( | [improve this report!]( jira issue: crdb-59043,1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161732,roachtest: sqlsmith/setup=tpcc/setting=ddl-nodrop failed [drop column unable to make progress],"**note:** this build has runtime assertions enabled. if the same failure was hit in a run without assertions enabled, there should be a similar failure without this message. if there isn& ;t one, then this failure is likely due to an assertion violation or (assertion) timeout. roachtest.sqlsmith/setup=tpcc/setting=ddl-nodrop [failed]( with [artifacts]( on release-25.3.8-rc @ [129f86631b285cfd2bd8740db920947b5a9c89ab]( parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - encrypted=false - metamorphicbufferedsender=true - metamorphicleases=default - runtimeassertionsbuild=true - ssd=0 help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( [this test on roachdash]( | [improve this report!]( jira issue: crdb-59044",[],['TESTING'],"['C-test-failure', 'O-robot', 'O-roachtest', 'O-rsg', 'T-sql-foundations', 'P-2', 'B-runtime-assertions-enabled', 'branch-release-25.3.8-rc']",github,2026-01-24T07:29:39Z,,"roachtest: sqlsmith/setup=tpcc/setting=ddl-nodrop failed [drop column unable to make progress] **note:** this build has runtime assertions enabled. if the same failure was hit in a run without assertions enabled, there should be a similar failure without this message. if there isn& ;t one, then this failure is likely due to an assertion violation or (assertion) timeout. roachtest.sqlsmith/setup=tpcc/setting=ddl-nodrop [failed]( with [artifacts]( on release-25.3.8-rc @ [129f86631b285cfd2bd8740db920947b5a9c89ab]( parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - encrypted=false - metamorphicbufferedsender=true - metamorphicleases=default - runtimeassertionsbuild=true - ssd=0 help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( [this test on roachdash]( | [improve this report!]( jira issue: crdb-59044",2.951,Medium,0.891,functional impact
rust-lang/rust#151579,ice with when accessing hir place,"<!-- thank you for filing a bug report! üêõ please provide a short summary of the bug, along with any information you feel relevant to replicating the bug. --> i tried this code: (auto-reduce) the compiler hangs even with | release channel | result | |-----------------------------------|--------| | current stable | error | | current nightly (default solver) | error | | current nightly (+ | ice | ### meta <!-- if you're using the stable version of the compiler, you should also check if the bug also exists in the beta or nightly versions. --> : <!-- include a backtrace in the code block by setting in your environment. e.g. . --> backtrace",[],"['CLEANUP', 'BUG', 'COMPATIBILITY']","['A-lints', 'I-ICE', 'A-closures', 'C-bug', 'S-has-mcve', 'T-types', 'WG-trait-system-refactor', 'L-rust_2021_incompatible_closure_captures']",github,2026-01-24T07:39:17Z,,"ice with when accessing hir place <!-- thank you for filing a bug report! üêõ please provide a short summary of the bug, along with any information you feel relevant to replicating the bug. --> i tried this code: (auto-reduce) the compiler hangs even with | release channel | result | |-----------------------------------|--------| | current stable | error | | current nightly (default solver) | error | | current nightly (+ | ice | ### meta <!-- if you're using the stable version of the compiler, you should also check if the bug also exists in the beta or nightly versions. --> : <!-- include a backtrace in the code block by setting in your environment. e.g. . --> backtrace",3.085,High,0.921,crash-like behavior
python/cpython#144206,fcntl.ioctl raises systemerror: buffer overflow instead of valueerror when mutating undersized buffers,# bug report ### bug description: ### cpython versions tested on: cpython main branch ### operating systems tested on: linux ### linked prs * gh-144273,[],['BUG'],"['type-bug', 'extension-modules', 'pending']",github,2026-01-24T07:43:40Z,,fcntl.ioctl raises systemerror: buffer overflow instead of valueerror when mutating undersized buffers # bug report ### bug description: ### cpython versions tested on: cpython main branch ### operating systems tested on: linux ### linked prs * gh-144273,6.4,Critical,1.0,crash-like behavior
cilium/cilium#43964,ciliumenvoyconfig: missing implicit l7 policy state causes host‚Üínodeport failures and pod‚Üípod tcp resets,"### is there an existing issue for this? - [x] i have searched the existing issues ### version equal or higher than v1.18.6 and lower than v1.19.0 ### what happened? two related but distinct issues are observed when using cilium with envoy l7 proxy enabled. ### 1. host ‚Üí nodeport traffic misclassification traffic originating from the node host and sent to a service via **nodeport** is incorrectly treated as **east/west l7 load-balanced traffic**. in this case, envoy receives bpf metadata where the original source address is set to . cilium rejects this source address for non-local pods and emits: as a result: * no l3/l4 or l7 policy filter state is created * envoy logs: * requests fail and envoy returns http 500 or is terminated early (e.g. http 403 when an l7 filter (coraza is triggered) is present) this behavior occurs even when no l7/wasm filters are configured, indicating the failure happens before http filter execution. --- ### 2. pod ‚Üí pod tcp handshake reset loop (side effect) separately, pod-to-pod (east/west) traffic shows a repeated tcp handshake failure pattern. hubble consistently reports the following sequence: this sequence repeats periodically for the same connection. characteristics: * no packets are dropped * all packets are reported as * l3/l4 connectivity appears to be intact * the tcp connection never establishes successfully this suggests the connection is reset **after l3/l4 forwarding but before a valid l7 proxy / policy state is established**. ### how can we reproduce the issue? ### reproduce issue 1 (host ‚Üí nodeport) 1. deploy a kubernetes cluster with: * cilium installed v1.18.6 * kube-proxy replacement enabled * envoy admin enabled * envoy config enabled could be enabled these through following commands: 2. deploy a pod and expose it using a service of type . 3. prepare coraza for envoy pod. 4. deploy ciliumenvoyconfig to contorl http request. 5. from the node host, send an http request to the nodeport. 6. observe hubble and envoy logs. expected result: * traffic is forwarded to the pod and handled normally. actual result: * envoy reports invalid original source address ( ) * envoy reports missing policy filter state. * no response traffic is observed in hubble. * request fails (http 500). --- ### reproduce issue 2 (pod ‚Üí pod tcp reset loop) 1. deploy two pods in the same cluster (e.g. as client and as server). 2. expose the server on port 80. 3. request service on through curl. 4. observe hubble flows. expected result: * tcp handshake completes and traffic flows normally. actual result: * initiate repeated tcp/http connections from the client pod to the server pod. * tcp handshake repeatedly terminates with client-side rst after syn-ack. * no connection is established despite all packets being forwarded. ### cilium version ### kernel version ### kubernetes version ### regression regression: not confirmed. we have not yet tested earlier cilium versions to determine whether this behavior was present before. it is currently unclear whether this is a regression or a long-standing issue exposed by a specific envoy/l7 configuration. ### sysdump [cilium-sysdump-20260124-162346.zip]( ### relevant log output ### anything else? - the issue only occurs when using **cec mode**. - the failures seems occur before http filter logic is applied. - no explicit policy drops are observed. - the behavior appears to be related to missing or inconsistent l7 policy / proxy state creation. - seems could using explicit instead of auto-generated policies avoids the issue but don't know how to achieve it. - this appears to be related to how **cec implicitly creates and manages l7 policy / proxy state**, regardless of whether http wasm filters are present. ### cilium users document - [ ] are you a user of cilium? please add yourself to the [users doc]( ### code of conduct - [x] i agree to follow this project's code of conduct",[],['BUG'],"['kind/bug', 'area/proxy', 'kind/community-report', 'area/agent', 'area/servicemesh']",github,2026-01-24T08:39:24Z,,"ciliumenvoyconfig: missing implicit l7 policy state causes host‚Üínodeport failures and pod‚Üípod tcp resets ### is there an existing issue for this? - [x] i have searched the existing issues ### version equal or higher than v1.18.6 and lower than v1.19.0 ### what happened? two related but distinct issues are observed when using cilium with envoy l7 proxy enabled. ### 1. host ‚Üí nodeport traffic misclassification traffic originating from the node host and sent to a service via **nodeport** is incorrectly treated as **east/west l7 load-balanced traffic**. in this case, envoy receives bpf metadata where the original source address is set to . cilium rejects this source address for non-local pods and emits: as a result: * no l3/l4 or l7 policy filter state is created * envoy logs: * requests fail and envoy returns http 500 or is terminated early (e.g. http 403 when an l7 filter (coraza is triggered) is present) this behavior occurs even when no l7/wasm filters are configured, indicating the failure happens before http filter execution. --- ### 2. pod ‚Üí pod tcp handshake reset loop (side effect) separately, pod-to-pod (east/west) traffic shows a repeated tcp handshake failure pattern. hubble consistently reports the following sequence: this sequence repeats periodically for the same connection. characteristics: * no packets are dropped * all packets are reported as * l3/l4 connectivity appears to be intact * the tcp connection never establishes successfully this suggests the connection is reset **after l3/l4 forwarding but before a valid l7 proxy / policy state is established**. ### how can we reproduce the issue? ### reproduce issue 1 (host ‚Üí nodeport) 1. deploy a kubernetes cluster with: * cilium installed v1.18.6 * kube-proxy replacement enabled * envoy admin enabled * envoy config enabled could be enabled these through following commands: 2. deploy a pod and expose it using a service of type . 3. prepare coraza for envoy pod. 4. deploy ciliumenvoyconfig to contorl http request. 5. from the node host, send an http request to the nodeport. 6. observe hubble and envoy logs. expected result: * traffic is forwarded to the pod and handled normally. actual result: * envoy reports invalid original source address ( ) * envoy reports missing policy filter state. * no response traffic is observed in hubble. * request fails (http 500). --- ### reproduce issue 2 (pod ‚Üí pod tcp reset loop) 1. deploy two pods in the same cluster (e.g. as client and as server). 2. expose the server on port 80. 3. request service on through curl. 4. observe hubble flows. expected result: * tcp handshake completes and traffic flows normally. actual result: * initiate repeated tcp/http connections from the client pod to the server pod. * tcp handshake repeatedly terminates with client-side rst after syn-ack. * no connection is established despite all packets being forwarded. ### cilium version ### kernel version ### kubernetes version ### regression regression: not confirmed. we have not yet tested earlier cilium versions to determine whether this behavior was present before. it is currently unclear whether this is a regression or a long-standing issue exposed by a specific envoy/l7 configuration. ### sysdump [cilium-sysdump-20260124-162346.zip]( ### relevant log output ### anything else? - the issue only occurs when using **cec mode**. - the failures seems occur before http filter logic is applied. - no explicit policy drops are observed. - the behavior appears to be related to missing or inconsistent l7 policy / proxy state creation. - seems could using explicit instead of auto-generated policies avoids the issue but don't know how to achieve it. - this appears to be related to how **cec implicitly creates and manages l7 policy / proxy state**, regardless of whether http wasm filters are present. ### cilium users document - [ ] are you a user of cilium? please add yourself to the [users doc]( ### code of conduct - [x] i agree to follow this project's code of conduct",6.4,Critical,1.0,crash-like behavior
python/cpython#144207,syntax highlighting(theming support) for module,"# feature or enhancement ### proposal: > syntax highlighting is good addition to improve readability, and the way of giving a context to special parts of output text. adding it to dis module can be considered as developer experience enhancement. > \- from discuss.python.org (my thread on ideas section) for more details, you might also refer to forum. since there are no comments but have some likes (i can think of them like ""approvals"") i'm opening this issue to propose this enhancement. i'll link my pr into this issue. ### has this already been discussed elsewhere? i have already discussed this feature proposal on discourse ### links to previous discussion of this feature: ### linked prs * gh-144208",[],['FEATURE'],"['type-feature', 'stdlib']",github,2026-01-24T08:41:30Z,,"syntax highlighting(theming support) for module # feature or enhancement ### proposal: > syntax highlighting is good addition to improve readability, and the way of giving a context to special parts of output text. adding it to dis module can be considered as developer experience enhancement. > \- from discuss.python.org (my thread on ideas section) for more details, you might also refer to forum. since there are no comments but have some likes (i can think of them like ""approvals"") i'm opening this issue to propose this enhancement. i'll link my pr into this issue. ### has this already been discussed elsewhere? i have already discussed this feature proposal on discourse ### links to previous discussion of this feature: ### linked prs * gh-144208",1.4,Low,0.538,localized low-impact
cockroachdb/cockroach#161733,roachtest: pg_regress failed,roachtest.pg_regress [failed]( with [artifacts]( on master @ [cf29a2da053a5025f6f7e68e01c70ab3f9aa8192]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21019178-1769237883-27-n1cpu4-0001 | 34.26.12.55 | 10.142.2.3 | parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - metamorphicleases=epoch - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( [this test on roachdash]( | [improve this report!]( jira issue: crdb-59045,[],['TESTING'],"['C-test-failure', 'O-robot', 'O-roachtest', 'branch-master', 'T-sql-queries']",github,2026-01-24T08:50:43Z,,roachtest: pg_regress failed roachtest.pg_regress [failed]( with [artifacts]( on master @ [cf29a2da053a5025f6f7e68e01c70ab3f9aa8192]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21019178-1769237883-27-n1cpu4-0001 | 34.26.12.55 | 10.142.2.3 | parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - metamorphicleases=epoch - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( [this test on roachdash]( | [improve this report!]( jira issue: crdb-59045,3.204,High,0.948,system-wide impact
rust-lang/rust#151580,require optimization flag for codegen tests,new codegen tests often bounce on gnu-nopt. it would probably make sense to require that codegen tests explicitly specify . or possibly they should just always be run with optimizations unless explicitly overridden.,[],"['TESTING', 'FEATURE', 'UI']","['A-testsuite', 'T-bootstrap', 'C-feature-request', 'A-compiletest', 'A-test-infra']",github,2026-01-24T08:58:49Z,,require optimization flag for codegen tests new codegen tests often bounce on gnu-nopt. it would probably make sense to require that codegen tests explicitly specify . or possibly they should just always be run with optimizations unless explicitly overridden.,1.6,Low,0.584,user-visible issue
cockroachdb/cockroach#161734,sql/opt/xform: testrules failed,sql/opt/xform.testrules [failed]( on release-24.3.26-rc @ [77aa3fcad169b09166b9105d52c05ab0122988d1]( parameters: - attempt=1 - deadlock=true - run=1 - shard=1 help see also: [how to investigate a go test failure \(internal\)]( [this test on roachdash]( | [improve this report!]( jira issue: crdb-59046,[],['TESTING'],"['C-test-failure', 'O-robot', 'T-sql-queries', 'branch-release-24.3.26-rc']",github,2026-01-24T09:19:21Z,2026-01-24T17:24:16Z,sql/opt/xform: testrules failed sql/opt/xform.testrules [failed]( on release-24.3.26-rc @ [77aa3fcad169b09166b9105d52c05ab0122988d1]( parameters: - attempt=1 - deadlock=true - run=1 - shard=1 help see also: [how to investigate a go test failure \(internal\)]( [this test on roachdash]( | [improve this report!]( jira issue: crdb-59046,3.8,Critical,1.0,crash-like behavior
cockroachdb/cockroach#161735,roachtest: jepsen/monotonic/subcritical-skews-start-kill-2 failed,roachtest.jepsen/monotonic/subcritical-skews-start-kill-2 [failed]( with [artifacts]( on master @ [cf29a2da053a5025f6f7e68e01c70ab3f9aa8192]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21019196-1769237854-90-n6cpu4-0001 | 20.127.242.89 | 10.1.0.101 | | teamcity-21019196-1769237854-90-n6cpu4-0002 | 20.42.106.107 | 10.1.0.140 | | teamcity-21019196-1769237854-90-n6cpu4-0003 | 20.121.136.65 | 10.1.0.149 | | teamcity-21019196-1769237854-90-n6cpu4-0004 | 20.127.189.22 | 10.1.0.102 | | teamcity-21019196-1769237854-90-n6cpu4-0005 | 20.127.189.44 | 10.1.0.114 | | teamcity-21019196-1769237854-90-n6cpu4-0006 | 20.102.41.25 | 10.1.0.100 | parameters: - arch=amd64 - cloud=azure - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - fs=ext4 - localssd=true - metamorphicleases=default - metamorphicwritebuffering=true - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( _grafana is not yet available for azure clusters_ /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-59047,[],['TESTING'],"['C-test-failure', 'O-robot', 'O-roachtest', 'branch-master', 'release-blocker', 'T-testeng']",github,2026-01-24T09:21:50Z,,roachtest: jepsen/monotonic/subcritical-skews-start-kill-2 failed roachtest.jepsen/monotonic/subcritical-skews-start-kill-2 [failed]( with [artifacts]( on master @ [cf29a2da053a5025f6f7e68e01c70ab3f9aa8192]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21019196-1769237854-90-n6cpu4-0001 | 20.127.242.89 | 10.1.0.101 | | teamcity-21019196-1769237854-90-n6cpu4-0002 | 20.42.106.107 | 10.1.0.140 | | teamcity-21019196-1769237854-90-n6cpu4-0003 | 20.121.136.65 | 10.1.0.149 | | teamcity-21019196-1769237854-90-n6cpu4-0004 | 20.127.189.22 | 10.1.0.102 | | teamcity-21019196-1769237854-90-n6cpu4-0005 | 20.127.189.44 | 10.1.0.114 | | teamcity-21019196-1769237854-90-n6cpu4-0006 | 20.102.41.25 | 10.1.0.100 | parameters: - arch=amd64 - cloud=azure - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - fs=ext4 - localssd=true - metamorphicleases=default - metamorphicwritebuffering=true - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( _grafana is not yet available for azure clusters_ /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-59047,3.091,High,0.923,functional impact
cockroachdb/cockroach#161736,roachtest: c2c/tpcc/warehouses=1000/duration=60/cutover=30 failed,roachtest.c2c/tpcc/warehouses=1000/duration=60/cutover=30 [failed]( with [artifacts]( on release-25.4.4-rc @ [688cda9084f2924ec4e22044c861aa0a3f34e956]( parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=8 - encrypted=false - fs=ext4 - localssd=false - metamorphicleases=default - runtimeassertionsbuild=false - ssd=0 help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( same failure on other branches - roachtest: c2c/tpcc/warehouses=1000/duration=60/cutover=30 failed [a-disaster-recovery c-test-failure o-roachtest o-robot t-disaster-recovery branch-release-25.4 release-blocker] - roachtest: c2c/tpcc/warehouses=1000/duration=60/cutover=30 failed [a-disaster-recovery c-test-failure o-roachtest o-robot p-2 t-disaster-recovery branch-master release-blocker] /cc /disaster-recovery [this test on roachdash]( | [improve this report!]( jira issue: crdb-59048,[],['TESTING'],"['C-test-failure', 'O-robot', 'A-disaster-recovery', 'O-roachtest', 'release-blocker', 'T-disaster-recovery', 'branch-release-25.4.4-rc']",github,2026-01-24T09:48:28Z,,roachtest: c2c/tpcc/warehouses=1000/duration=60/cutover=30 failed roachtest.c2c/tpcc/warehouses=1000/duration=60/cutover=30 [failed]( with [artifacts]( on release-25.4.4-rc @ [688cda9084f2924ec4e22044c861aa0a3f34e956]( parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=8 - encrypted=false - fs=ext4 - localssd=false - metamorphicleases=default - runtimeassertionsbuild=false - ssd=0 help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( same failure on other branches - roachtest: c2c/tpcc/warehouses=1000/duration=60/cutover=30 failed [a-disaster-recovery c-test-failure o-roachtest o-robot t-disaster-recovery branch-release-25.4 release-blocker] - roachtest: c2c/tpcc/warehouses=1000/duration=60/cutover=30 failed [a-disaster-recovery c-test-failure o-roachtest o-robot p-2 t-disaster-recovery branch-master release-blocker] /cc /disaster-recovery [this test on roachdash]( | [improve this report!]( jira issue: crdb-59048,3.213,High,0.95,system-wide impact
llvm/llvm-project#177750,warnings on windows with flag -gdwarf,,[],['BUG'],"['debuginfo', 'platform:windows']",github,2026-01-24T09:51:04Z,,warnings on windows with flag -gdwarf,2.479,Medium,0.783,functional impact
nodejs/node#61501,`test-inspector-network- is flaky,### test test-inspector-network- ### platform _no response_ ### console output ### additional information it's only reported in and indicating it's quite new and can probably be bisected.,[],['TESTING'],['flaky-test'],github,2026-01-24T10:25:39Z,2026-01-26T11:10:25Z,`test-inspector-network- is flaky ### test test-inspector-network- ### platform _no response_ ### console output ### additional information it's only reported in and indicating it's quite new and can probably be bisected.,1.6,Low,0.584,localized low-impact
rust-lang/rust#151582,ice: panic in evaluate_trait_predicate_recursively with specialization and associated type projection,"<!-- thank you for filing a bug report! üêõ please provide a short summary of the bug, along with any information you feel relevant to replicating the bug. --> i tried this code: | release channel | result | |-----------------------------------|--------| | current stable | crash | | current nightly (default solver) | crash | | current nightly (+ | error | ### meta <!-- if you're using the stable version of the compiler, you should also check if the bug also exists in the beta or nightly versions. --> : <!-- include a backtrace in the code block by setting in your environment. e.g. . --> backtrace",[],['BUG'],"['I-ICE', 'A-trait-system', 'A-associated-items', 'T-compiler', 'C-bug', 'fixed-by-next-solver', 'F-min_specialization']",github,2026-01-24T10:28:11Z,2026-01-25T14:53:27Z,"ice: panic in evaluate_trait_predicate_recursively with specialization and associated type projection <!-- thank you for filing a bug report! üêõ please provide a short summary of the bug, along with any information you feel relevant to replicating the bug. --> i tried this code: | release channel | result | |-----------------------------------|--------| | current stable | crash | | current nightly (default solver) | crash | | current nightly (+ | error | ### meta <!-- if you're using the stable version of the compiler, you should also check if the bug also exists in the beta or nightly versions. --> : <!-- include a backtrace in the code block by setting in your environment. e.g. . --> backtrace",4.6,Critical,1.0,crash-like behavior
rust-lang/rust#151583,lint against inherent methods on types implementing and,"when inherent methods are added to types that implement or today, they take precedence over methods with the same name on the type they receive for or deref to. this means that it becomes impossible to call those with the method calling syntax, resulting in poorer ergonomics for the type that implements or . for [example]( since exists purely for these ergonomic reasons -- allowing methods to be called using the method syntax with custom receiver types -- this goes directly against the spirit of that trait. in the standard library it is already standard practice to not add inherent methods to types like and and instead make them associated functions. for this reason, there should be a lint that triggers on inherent methods on types that implement or . in the example above, the lint could look like this: the lint should only fire on methods that are (or the maximal publicity of the type, so a type would also trigger on functions). another important consideration is that is often used for the subtyping pattern or for dereferencing to a ""morally contained type"". in this case, the target type is not a generic, but a concrete type or a rigid type containing a generic (for example ). for this reason the lint for should not trigger in those cases. the rationale is that since one knows the concrete type, avoiding name collisions is much easier. it also avoids false positives, since in the subtyping pattern, both types want to have inherent methods. in particular this will ensure that the implementation for will not trigger the lint. [zulip discussion](",[],['FEATURE'],"['A-lints', 'T-lang', 'C-feature-request', 'I-lang-nominated']",github,2026-01-24T10:36:12Z,,"lint against inherent methods on types implementing and when inherent methods are added to types that implement or today, they take precedence over methods with the same name on the type they receive for or deref to. this means that it becomes impossible to call those with the method calling syntax, resulting in poorer ergonomics for the type that implements or . for [example]( since exists purely for these ergonomic reasons -- allowing methods to be called using the method syntax with custom receiver types -- this goes directly against the spirit of that trait. in the standard library it is already standard practice to not add inherent methods to types like and and instead make them associated functions. for this reason, there should be a lint that triggers on inherent methods on types that implement or . in the example above, the lint could look like this: the lint should only fire on methods that are (or the maximal publicity of the type, so a type would also trigger on functions). another important consideration is that is often used for the subtyping pattern or for dereferencing to a ""morally contained type"". in this case, the target type is not a generic, but a concrete type or a rigid type containing a generic (for example ). for this reason the lint for should not trigger in those cases. the rationale is that since one knows the concrete type, avoiding name collisions is much easier. it also avoids false positives, since in the subtyping pattern, both types want to have inherent methods. in particular this will ensure that the implementation for will not trigger the lint. [zulip discussion](",1.4,Low,0.538,localized low-impact
python/cpython#144211,extend module to support platform-specific events,"the module api is deliberately very narrow: - the / methods only accept as a bitmask of | - the method only ever returns combinations of | this design makes it impossible to use many platform-specific events provided by the underlying os apis ( , , ). for example, describes a use case for on linux. the biggest missed opportunity, however, is the inability to fully exploit on bsd / macos, which would allow async handling of regular files, signals, and others (see [kq_event_*]( this limitation is relevant for , which uses the module. for instance, on bsd / macos, could be notified when a process terminates, enabling an optimization similar to currenly on linux already takes advantage of this approach via + (see [source]( but can't do the same for bsd / macos because doesn't allow registering custom objects. whereas it would be relatively easy to add a method, or to pass events directly to , it's less clear how the method should behave in this case, since it's apparently bound to the promise of returning either or .",[],['FEATURE'],"['type-feature', 'extension-modules']",github,2026-01-24T12:00:15Z,,"extend module to support platform-specific events the module api is deliberately very narrow: - the / methods only accept as a bitmask of | - the method only ever returns combinations of | this design makes it impossible to use many platform-specific events provided by the underlying os apis ( , , ). for example, describes a use case for on linux. the biggest missed opportunity, however, is the inability to fully exploit on bsd / macos, which would allow async handling of regular files, signals, and others (see [kq_event_*]( this limitation is relevant for , which uses the module. for instance, on bsd / macos, could be notified when a process terminates, enabling an optimization similar to currenly on linux already takes advantage of this approach via + (see [source]( but can't do the same for bsd / macos because doesn't allow registering custom objects. whereas it would be relatively easy to add a method, or to pass events directly to , it's less clear how the method should behave in this case, since it's apparently bound to the promise of returning either or .",5.4,Critical,1.0,crash-like behavior
kubernetes/kubernetes#136497,curl is okÔºåbut kubectl is not right,"### what happened? ### what did you expect to happen? kubectl is ok ### how can we reproduce it (as minimally and precisely as possible)? install k8s 1.34.0 create a sa bind sa with cluster-admin cluster role create token use command ### anything else we need to know? k8s1.34.0 ### kubernetes version root :~# kubectl version client version: v1.34.0 kustomize version: v5.7.1 server version: v1.34.1 ### cloud provider kvm ### os version root :~# cat /etc/os-release pretty_name=""ubuntu 24.04.2 lts"" name=""ubuntu"" version_id=""24.04"" version=""24.04.2 lts (noble numbat)"" version_codename=noble id=ubuntu id_like=debian home_url="" support_url="" bug_report_url="" privacy_policy_url="" ubuntu_codename=noble logo=ubuntu-logo ### install tools ### container runtime (cri) and version (if applicable) ### related plugins (cni, csi, ...) and versions (if applicable)",[],['BUG'],"['kind/bug', 'needs-sig', 'needs-triage']",github,2026-01-24T12:44:40Z,,"curl is okÔºåbut kubectl is not right ### what happened? ### what did you expect to happen? kubectl is ok ### how can we reproduce it (as minimally and precisely as possible)? install k8s 1.34.0 create a sa bind sa with cluster-admin cluster role create token use command ### anything else we need to know? k8s1.34.0 ### kubernetes version root :~# kubectl version client version: v1.34.0 kustomize version: v5.7.1 server version: v1.34.1 ### cloud provider kvm ### os version root :~# cat /etc/os-release pretty_name=""ubuntu 24.04.2 lts"" name=""ubuntu"" version_id=""24.04"" version=""24.04.2 lts (noble numbat)"" version_codename=noble id=ubuntu id_like=debian home_url="" support_url="" bug_report_url="" privacy_policy_url="" ubuntu_codename=noble logo=ubuntu-logo ### install tools ### container runtime (cri) and version (if applicable) ### related plugins (cni, csi, ...) and versions (if applicable)",2.238,Medium,0.729,functional impact
electron/electron#49517,tray icon can't be destroyed on linux,"### preflight checklist - [x] i have read the [contributing guidelines]( for this project. - [x] i agree to follow the [code of conduct]( that this project adheres to. - [x] i have searched the [issue tracker]( for a bug report that matches the one i want to file, without success. ### electron version 40.0.0 ### what operating system(s) are you using? other linux ### operating system version arch linux with gnome 49 and opensuse tumbleweed with kde plasma 6.5.5 ### what arch are you using? x64 ### last known working electron version _no response_ ### does the issue also appear in chromium / google chrome? i don't know how to test ### expected behavior the tray icon gets destroyed and removed from the system tray when calling . ### actual behavior the tray icon stays in the system tray and its menu items are unresponsive. ### testcase gist url ### additional information the provided testcase is based on the electron fiddle provided in the tray guide: i only added an additional menu option to destroy the tray. using that fiddle, i was able to reproduce it on both gnome and kde plasma, other desktop environments may be affected as well. a similar issue (though only for gnome and without testcase) was already reported in . ### steps to reproduce: 1. run the provided electron fiddle. 2. verify the tray is working by clicking the ""set green icon"" tray menu item. 3. click the ""destroy icon"" tray menu item. 4. try clicking ""set green icon"" or any other tray menu item again. 5. observe how nothing happens, but the icon is still displayed and not removed from the system tray.",[],"['BUG', 'UI']","['platform/linux', 'component/tray', 'bug :beetle:', 'has-repro-gist', '40-x-y']",github,2026-01-24T13:06:24Z,,"tray icon can't be destroyed on linux ### preflight checklist - [x] i have read the [contributing guidelines]( for this project. - [x] i agree to follow the [code of conduct]( that this project adheres to. - [x] i have searched the [issue tracker]( for a bug report that matches the one i want to file, without success. ### electron version 40.0.0 ### what operating system(s) are you using? other linux ### operating system version arch linux with gnome 49 and opensuse tumbleweed with kde plasma 6.5.5 ### what arch are you using? x64 ### last known working electron version _no response_ ### does the issue also appear in chromium / google chrome? i don't know how to test ### expected behavior the tray icon gets destroyed and removed from the system tray when calling . ### actual behavior the tray icon stays in the system tray and its menu items are unresponsive. ### testcase gist url ### additional information the provided testcase is based on the electron fiddle provided in the tray guide: i only added an additional menu option to destroy the tray. using that fiddle, i was able to reproduce it on both gnome and kde plasma, other desktop environments may be affected as well. a similar issue (though only for gnome and without testcase) was already reported in . ### steps to reproduce: 1. run the provided electron fiddle. 2. verify the tray is working by clicking the ""set green icon"" tray menu item. 3. click the ""destroy icon"" tray menu item. 4. try clicking ""set green icon"" or any other tray menu item again. 5. observe how nothing happens, but the icon is still displayed and not removed from the system tray.",2.058,Medium,0.688,user-visible issue
cockroachdb/cockroach#161737,ccl/serverccl: testexecsql failed,ccl/serverccl.testexecsql [failed]( on release-26.1 @ [069b26313036c62a4af23d3172b8175971d84829]( parameters: - attempt=1 - race=true - run=2 - shard=1 help see also: [how to investigate a go test failure \(internal\)]( /cc /obs-prs [this test on roachdash]( | [improve this report!]( jira issue: crdb-59049,[],['TESTING'],"['C-test-failure', 'O-robot', 'X-infra-flake', 'T-observability', 'branch-release-26.1']",github,2026-01-24T13:36:19Z,2026-01-26T15:15:14Z,ccl/serverccl: testexecsql failed ccl/serverccl.testexecsql [failed]( on release-26.1 @ [069b26313036c62a4af23d3172b8175971d84829]( parameters: - attempt=1 - race=true - run=2 - shard=1 help see also: [how to investigate a go test failure \(internal\)]( /cc /obs-prs [this test on roachdash]( | [improve this report!]( jira issue: crdb-59049,1.6,Low,0.584,localized low-impact
rust-lang/rust#151591,[ice]: assertion failed: layout.is_sized(),"<!-- thank you for finding an internal compiler error! üßä if possible, try to provide a minimal verifiable example. you can read ""rust bug minimization patterns"" for how to create smaller examples. --> it is similar to , but this example seems to require specialization, and i'm not sure if it is a duplicate. ### code ### meta <!-- if you're using the stable version of the compiler, you should also check if the bug also exists in the beta or nightly versions. --> : <!-- include a backtrace in the code block by setting in your environment. e.g. . --> program output",[],['BUG'],"['I-ICE', 'T-compiler', 'C-bug', 'needs-triage']",github,2026-01-24T14:26:29Z,,"[ice]: assertion failed: layout.is_sized() <!-- thank you for finding an internal compiler error! üßä if possible, try to provide a minimal verifiable example. you can read ""rust bug minimization patterns"" for how to create smaller examples. --> it is similar to , but this example seems to require specialization, and i'm not sure if it is a duplicate. ### code ### meta <!-- if you're using the stable version of the compiler, you should also check if the bug also exists in the beta or nightly versions. --> : <!-- include a backtrace in the code block by setting in your environment. e.g. . --> program output",2.643,Medium,0.821,functional impact
rust-lang/rust#151592,is displayed incorrectly,### location (url) ### summary this signature in source code is displayed as so it looks like this isn't correctly displayed,[],['DOCUMENTATION'],['A-docs'],github,2026-01-24T14:29:45Z,2026-01-24T14:53:07Z,is displayed incorrectly ### location (url) ### summary this signature in source code is displayed as so it looks like this isn't correctly displayed,3.033,High,0.909,functional impact
rust-lang/rust#151595,[ice]: rustc panicked at compiler\rustc_mir_build\src\builder\matches\mod.rs:2176:44,"<!-- thank you for finding an internal compiler error! üßä if possible, try to provide a minimal verifiable example. you can read ""rust bug minimization patterns"" for how to create smaller examples. --> the code is generated by a fuzzer and reduced manually ### code ### meta <!-- if you're using the stable version of the compiler, you should also check if the bug also exists in the beta or nightly versions. --> : ### error output <!-- include a backtrace in the code block by setting in your environment. e.g. . --> backtrace",[],['BUG'],"['I-ICE', 'A-closures', 'T-lang', 'T-compiler', 'regression-from-stable-to-beta', 'A-impl-trait', 'C-bug', 'I-prioritize', 'A-patterns', 'T-types']",github,2026-01-24T14:37:32Z,,"[ice]: rustc panicked at compiler\rustc_mir_build\src\builder\matches\mod.rs:2176:44 <!-- thank you for finding an internal compiler error! üßä if possible, try to provide a minimal verifiable example. you can read ""rust bug minimization patterns"" for how to create smaller examples. --> the code is generated by a fuzzer and reduced manually ### code ### meta <!-- if you're using the stable version of the compiler, you should also check if the bug also exists in the beta or nightly versions. --> : ### error output <!-- include a backtrace in the code block by setting in your environment. e.g. . --> backtrace",4.6,Critical,1.0,crash-like behavior
python/cpython#144212,add to,"# feature or enhancement ### proposal: jpeg xl is a new image format designed to replace jpeg, png, gif, and other formats. it is already supported in safari and will be supported in chrome and firefox soon. official website: iana registry: ### has this already been discussed elsewhere? this is a minor feature, which does not need previous discussion elsewhere ### links to previous discussion of this feature: _no response_ ### linked prs * gh-144213",[],['FEATURE'],"['type-feature', 'stdlib']",github,2026-01-24T15:09:33Z,2026-01-26T16:06:01Z,"add to # feature or enhancement ### proposal: jpeg xl is a new image format designed to replace jpeg, png, gif, and other formats. it is already supported in safari and will be supported in chrome and firefox soon. official website: iana registry: ### has this already been discussed elsewhere? this is a minor feature, which does not need previous discussion elsewhere ### links to previous discussion of this feature: _no response_ ### linked prs * gh-144213",1.4,Low,0.538,localized low-impact
openssl/openssl#29748,ossl_http_req_ctx_nbio incorrectly handles http errors when expecting asn1 encoded response.,"openssl 3.5/3.6 (introduced in 3.5, also appears on 3.6 latest commit: f99912c4a463dffa8d8fc4536d22dd34d5293cdf) the state machine in the function of the is done in such way to allow reading the response's content even when receiving an error code (>=400); see openssl/openssl , exactly [this]( change which sets the next state after parsing the first http response line: however when this scenario (receving 4xx or 5xx) happens with an ossl_http_req_ctx which has expect_asn1 (as would happen when using the new openssl3.0+ macro `x509_crl_load_ the state machine will still try to parse the response as asn1 encoded content, resulting a couple of issues: 1. if the 4xx or 5xx response has at least 2 bytes of content, you will get the following error message: which is extremely misleading (this is the case i faced and it started quite a long investigation) 2. if the response content is empty you will probably get 3. i am in no way, shape, or form an openssl expert (this is my first time diving it its code) so there might be other side effects i am missing to reproduce you can use the following c code which creates an ""http server"" always responding with a 500: 500 http server and the following which simply does x509_crl_load_http to the server: crl request my feeling is that if there is an error code, but the request expects asn1 encoding, we should¬¥t try and decode that response and just stop the processing there.",[],['BUG'],"['branch: master', 'triaged: bug', 'branch: 3.5', 'branch: 3.6']",github,2026-01-24T15:16:43Z,,"ossl_http_req_ctx_nbio incorrectly handles http errors when expecting asn1 encoded response. openssl 3.5/3.6 (introduced in 3.5, also appears on 3.6 latest commit: f99912c4a463dffa8d8fc4536d22dd34d5293cdf) the state machine in the function of the is done in such way to allow reading the response's content even when receiving an error code (>=400); see openssl/openssl , exactly [this]( change which sets the next state after parsing the first http response line: however when this scenario (receving 4xx or 5xx) happens with an ossl_http_req_ctx which has expect_asn1 (as would happen when using the new openssl3.0+ macro `x509_crl_load_ the state machine will still try to parse the response as asn1 encoded content, resulting a couple of issues: 1. if the 4xx or 5xx response has at least 2 bytes of content, you will get the following error message: which is extremely misleading (this is the case i faced and it started quite a long investigation) 2. if the response content is empty you will probably get 3. i am in no way, shape, or form an openssl expert (this is my first time diving it its code) so there might be other side effects i am missing to reproduce you can use the following c code which creates an ""http server"" always responding with a 500: 500 http server and the following which simply does x509_crl_load_http to the server: crl request my feeling is that if there is an error code, but the request expects asn1 encoding, we should¬¥t try and decode that response and just stop the processing there.",4.6,Critical,1.0,crash-like behavior
pytorch/pytorch#173255,torch.compile is decreasing training speed instead increasing it,"### üêõ describe the bug hi, everyone i turned into linux to can work with torch.compile as windows doesn't support to test how much will torch.compile helping me speed my training, but the result was disappointed. that's my code # enable tensorfloat32 for better performance torch.set_float32_matmul_precision('high') # enable cudnn benchmarking for optimal performance torch.backends.cudnn.benchmark = true batch_size = 32 train_data = torchvision.datasets.imagefolder(root=train_dataset_path, transform=train_transforms, target_transform=none) test_data = torchvision.datasets.imagefolder(root=test_dataset_path, transform=test_transforms, target_transform=none) class_names = train_data.classes print(len(train_data)), print(len(test_data)) # create dataloader for preloaded data train_loader = dataloader( dataset=train_data, batch_size=batch_size, num_workers=5, pin_memory=true, persistent_workers=true, prefetch_factor=3, shuffle=true ) test_loader = dataloader( dataset=test_data, batch_size=batch_size, num_workers=5, pin_memory=true, persistent_workers=true, prefetch_factor=3, shuffle=false ) def set_device(): if torch.cuda.is_available(): dev = ""cuda"" else: dev = ""cpu"" return torch.device(dev) device = set_device() scaler = torch.amp.gradscaler('cuda') def train_nn(model, train_loader, test_loader, criterion, optimizer, n_epochs, is_compiled=false) -> dict[str, list[float]]: device = set_device() best_acc = 0 # initialize the learning rate scheduler scheduler = reducelronplateau( optimizer, mode='min', factor=0.5, patience=3) early_stopping = earlystopping(patience=11, min_delta=0.001) results = { 'train_loss': [], 'train_acc': [], 'test_loss': [], 'test_acc': [] } for epoch in range(n_epochs): print(""epoch number %d"" % (epoch + 1)) start_time = timer() model.train() running_loss = 0.0 running_correct = 0.0 total = 0 for data in (train_loader): images, labels = data images = images.to(device) labels = labels.to(device) total += labels.size(0) optimizer.zero_grad(set_to_none=true) # use amp for forward and backward pass with torch.amp.autocast(device_type='cuda'): outputs = model(images) loss = criterion(outputs, labels) scaler.scale(loss).backward() scaler.step(optimizer) scaler.update() running_loss += loss.item() _, predicted = torch.max(outputs.data, 1) running_correct += (labels == predicted).sum().item() epoch_loss = running_loss / len(train_loader) epoch_acc = 100.00 * running_correct / total print("" - training dataset. got %d out of %d images correctly (%.3f%%). epoch loss: %.3f"" % (running_correct, total, epoch_acc, epoch_loss)) test_dataset_acc, test_loss, all_preds, all_labels = evaluate_model_on_test_set( model, test_loader, criterion) end_time = timer() elapsed_time = end_time - start_time print('execution time:', time.strftime( ""%h:%m:%s"", time.gmtime(elapsed_time))) print( f""checking epoch_loss {epoch_loss} test_loss {test_loss} epoch_acc {epoch_acc} test_dataset_acc {test_dataset_acc}"") if (epoch_loss <= 0.0001 and test_loss <= 0.0001 and epoch_acc >= 99.999 and test_dataset_acc >= 99.999): best_acc = test_dataset_acc save_checkpoint(model, epoch, optimizer, best_acc, is_compiled) print(""perfect performance achieved! training stopped."") break if test_dataset_acc > best_acc or (test_dataset_acc == best_acc and epoch_loss < results['train_loss'][-1] if results['train_loss'] else true): best_acc = test_dataset_acc save_checkpoint(model, epoch, optimizer, best_acc, is_compiled) results['train_loss'].append(epoch_loss) results['train_acc'].append(epoch_acc) results['test_loss'].append(test_loss) results['test_acc'].append(test_dataset_acc) scheduler.step(test_loss) print(f""current learning rate: {optimizer.param_groups[0]['lr']}"") if early_stopping.check_early_stop(test_loss): print(""early stopping triggered. training stopped."") break print(""training complete"") return model, results, all_preds, all_labels efficientnetv2_s_model = models.efficientnet_v2_s( weights=models.efficientnet_v2_s_weights.imagenet1k_v1) # modify the first convolutional layer to accept 1-channel (grayscale) input original_first_conv = efficientnetv2_s_model.features[0][0] new_first_conv = nn.conv2d( in_channels=1, out_channels=original_first_conv.out_channels, kernel_size=original_first_conv.kernel_size, stride=original_first_conv.stride, padding=original_first_conv.padding, bias=false ) # initialize the new first layer's weights by averaging the pretrained weights across the rgb channels with torch.no_grad(): new_first_conv.weight[:, :] = original_first_conv.weight.mean( dim=1, keepdim=true) efficientnetv2_s_model.features[0][0] = new_first_conv # modify the classifier's final layer for 4 output classes num_ftrs = efficientnetv2_s_model.classifier[1].in_features num_classes = 4 efficientnetv2_s_model.classifier[1] = nn.linear(num_ftrs, num_classes) efficientnetv2_s_model = efficientnetv2_s_model.to(device) # compile the model print(""compiling model with torch.compile..."") efficientnetv2_s_model = torch.compile( efficientnetv2_s_model, mode=""max-autotune"") print(""model compilation complete!"") loss_fn = nn.crossentropyloss() optimizer = optim.adam(efficientnetv2_s_model.parameters(), lr=0.0003, weight_decay=1e-5) model, results, all_preds, all_labels = train_nn( efficientnetv2_s_model, train_loader, test_loader, loss_fn, optimizer, 40, is_compiled) i hope i'm wrong and help me speeding my training with compile ### error logs _no response_ ### versions python 3.13.11 torch2.9.1+cuda13.0 cc -chen -nrv",[],['PERFORMANCE'],"['module: performance', 'triaged', 'oncall: pt2', 'module: dynamo']",github,2026-01-24T15:39:48Z,,"torch.compile is decreasing training speed instead increasing it ### üêõ describe the bug hi, everyone i turned into linux to can work with torch.compile as windows doesn't support to test how much will torch.compile helping me speed my training, but the result was disappointed. that's my code # enable tensorfloat32 for better performance torch.set_float32_matmul_precision('high') # enable cudnn benchmarking for optimal performance torch.backends.cudnn.benchmark = true batch_size = 32 train_data = torchvision.datasets.imagefolder(root=train_dataset_path, transform=train_transforms, target_transform=none) test_data = torchvision.datasets.imagefolder(root=test_dataset_path, transform=test_transforms, target_transform=none) class_names = train_data.classes print(len(train_data)), print(len(test_data)) # create dataloader for preloaded data train_loader = dataloader( dataset=train_data, batch_size=batch_size, num_workers=5, pin_memory=true, persistent_workers=true, prefetch_factor=3, shuffle=true ) test_loader = dataloader( dataset=test_data, batch_size=batch_size, num_workers=5, pin_memory=true, persistent_workers=true, prefetch_factor=3, shuffle=false ) def set_device(): if torch.cuda.is_available(): dev = ""cuda"" else: dev = ""cpu"" return torch.device(dev) device = set_device() scaler = torch.amp.gradscaler('cuda') def train_nn(model, train_loader, test_loader, criterion, optimizer, n_epochs, is_compiled=false) -> dict[str, list[float]]: device = set_device() best_acc = 0 # initialize the learning rate scheduler scheduler = reducelronplateau( optimizer, mode='min', factor=0.5, patience=3) early_stopping = earlystopping(patience=11, min_delta=0.001) results = { 'train_loss': [], 'train_acc': [], 'test_loss': [], 'test_acc': [] } for epoch in range(n_epochs): print(""epoch number %d"" % (epoch + 1)) start_time = timer() model.train() running_loss = 0.0 running_correct = 0.0 total = 0 for data in (train_loader): images, labels = data images = images.to(device) labels = labels.to(device) total += labels.size(0) optimizer.zero_grad(set_to_none=true) # use amp for forward and backward pass with torch.amp.autocast(device_type='cuda'): outputs = model(images) loss = criterion(outputs, labels) scaler.scale(loss).backward() scaler.step(optimizer) scaler.update() running_loss += loss.item() _, predicted = torch.max(outputs.data, 1) running_correct += (labels == predicted).sum().item() epoch_loss = running_loss / len(train_loader) epoch_acc = 100.00 * running_correct / total print("" - training dataset. got %d out of %d images correctly (%.3f%%). epoch loss: %.3f"" % (running_correct, total, epoch_acc, epoch_loss)) test_dataset_acc, test_loss, all_preds, all_labels = evaluate_model_on_test_set( model, test_loader, criterion) end_time = timer() elapsed_time = end_time - start_time print('execution time:', time.strftime( ""%h:%m:%s"", time.gmtime(elapsed_time))) print( f""checking epoch_loss {epoch_loss} test_loss {test_loss} epoch_acc {epoch_acc} test_dataset_acc {test_dataset_acc}"") if (epoch_loss <= 0.0001 and test_loss <= 0.0001 and epoch_acc >= 99.999 and test_dataset_acc >= 99.999): best_acc = test_dataset_acc save_checkpoint(model, epoch, optimizer, best_acc, is_compiled) print(""perfect performance achieved! training stopped."") break if test_dataset_acc > best_acc or (test_dataset_acc == best_acc and epoch_loss < results['train_loss'][-1] if results['train_loss'] else true): best_acc = test_dataset_acc save_checkpoint(model, epoch, optimizer, best_acc, is_compiled) results['train_loss'].append(epoch_loss) results['train_acc'].append(epoch_acc) results['test_loss'].append(test_loss) results['test_acc'].append(test_dataset_acc) scheduler.step(test_loss) print(f""current learning rate: {optimizer.param_groups[0]['lr']}"") if early_stopping.check_early_stop(test_loss): print(""early stopping triggered. training stopped."") break print(""training complete"") return model, results, all_preds, all_labels efficientnetv2_s_model = models.efficientnet_v2_s( weights=models.efficientnet_v2_s_weights.imagenet1k_v1) # modify the first convolutional layer to accept 1-channel (grayscale) input original_first_conv = efficientnetv2_s_model.features[0][0] new_first_conv = nn.conv2d( in_channels=1, out_channels=original_first_conv.out_channels, kernel_size=original_first_conv.kernel_size, stride=original_first_conv.stride, padding=original_first_conv.padding, bias=false ) # initialize the new first layer's weights by averaging the pretrained weights across the rgb channels with torch.no_grad(): new_first_conv.weight[:, :] = original_first_conv.weight.mean( dim=1, keepdim=true) efficientnetv2_s_model.features[0][0] = new_first_conv # modify the classifier's final layer for 4 output classes num_ftrs = efficientnetv2_s_model.classifier[1].in_features num_classes = 4 efficientnetv2_s_model.classifier[1] = nn.linear(num_ftrs, num_classes) efficientnetv2_s_model = efficientnetv2_s_model.to(device) # compile the model print(""compiling model with torch.compile..."") efficientnetv2_s_model = torch.compile( efficientnetv2_s_model, mode=""max-autotune"") print(""model compilation complete!"") loss_fn = nn.crossentropyloss() optimizer = optim.adam(efficientnetv2_s_model.parameters(), lr=0.0003, weight_decay=1e-5) model, results, all_preds, all_labels = train_nn( efficientnetv2_s_model, train_loader, test_loader, loss_fn, optimizer, 40, is_compiled) i hope i'm wrong and help me speeding my training with compile ### error logs _no response_ ### versions python 3.13.11 torch2.9.1+cuda13.0 cc -chen -nrv",6.8,Critical,1.0,"performance degradation, crash-like behavior"
rust-lang/rust#151599,hang: trait solver hangs indefinitely with nested struct bounds in where clause,"<!-- thank you for filing a bug report! üêõ please provide a short summary of the bug, along with any information you feel relevant to replicating the bug. --> i tried this code: | release channel | result | |-----------------------------------|--------| | current stable | hang | | current nightly (default solver) | hang | | current nightly (+ | error | ### meta <!-- if you're using the stable version of the compiler, you should also check if the bug also exists in the beta or nightly versions. --> : <!-- include a backtrace in the code block by setting in your environment. e.g. . --> backtrace",[],['BUG'],"['A-trait-system', 'T-compiler', 'C-bug', 'I-hang', 'T-types', 'fixed-by-next-solver']",github,2026-01-24T15:42:43Z,,"hang: trait solver hangs indefinitely with nested struct bounds in where clause <!-- thank you for filing a bug report! üêõ please provide a short summary of the bug, along with any information you feel relevant to replicating the bug. --> i tried this code: | release channel | result | |-----------------------------------|--------| | current stable | hang | | current nightly (default solver) | hang | | current nightly (+ | error | ### meta <!-- if you're using the stable version of the compiler, you should also check if the bug also exists in the beta or nightly versions. --> : <!-- include a backtrace in the code block by setting in your environment. e.g. . --> backtrace",4.6,Critical,1.0,crash-like behavior
openssl/openssl#29749,the .rodata section name is incorrect on windows,".rodata is not a standard read-only data section name for coff, so .rodata sections will get image_scn_cnt_initialized_data|image_scn_mem_read|image_scn_mem_write. for coff, .rodata should be renamed to .rdata",[],"['BUG', 'FEATURE']","['triaged: bug', 'triaged: feature']",github,2026-01-24T16:35:39Z,,"the .rodata section name is incorrect on windows .rodata is not a standard read-only data section name for coff, so .rodata sections will get image_scn_cnt_initialized_data|image_scn_mem_read|image_scn_mem_write. for coff, .rodata should be renamed to .rdata",1.9,Low,0.652,localized low-impact
rust-lang/rust#151604,tracking issue for release notes of : promote powerpc64-unknown-linux-musl to tier 2 with host tools,"this issue tracks the release notes text for . cc , -simulacrum -- original issue/pr authors and assignees for drafting text see the forge.rust-lang.org chapter about [release notes]( for an overview of how the release team makes use of these tracking issues. ### release notes text this section should be edited to specify the correct category(s) for the change, with succinct description(s) of what changed. some things worth considering: - does this need an additional compat notes section? - was this a libs stabilization that should have additional headers to list new apis under and ? ` > [!tip] > use the [previous releases]( for inspiration on how to write the release notes text and which categories to pick. ### release blog section if this change is notable enough for inclusion in the blog post then this section should be edited to contain a draft for the blog post. *otherwise leave it empty.* ` > [!note] > > if a blog post section is required the label should be added ( ) to this issue as otherwise it may be missed by the release team.",[],['UI'],"['A-testsuite', 'T-compiler', 'relnotes', 'T-bootstrap', 'T-infra', 'needs-triage', 'A-CI', 'relnotes-tracking-issue']",github,2026-01-24T18:42:26Z,,"tracking issue for release notes of : promote powerpc64-unknown-linux-musl to tier 2 with host tools this issue tracks the release notes text for . cc , -simulacrum -- original issue/pr authors and assignees for drafting text see the forge.rust-lang.org chapter about [release notes]( for an overview of how the release team makes use of these tracking issues. ### release notes text this section should be edited to specify the correct category(s) for the change, with succinct description(s) of what changed. some things worth considering: - does this need an additional compat notes section? - was this a libs stabilization that should have additional headers to list new apis under and ? ` > [!tip] > use the [previous releases]( for inspiration on how to write the release notes text and which categories to pick. ### release blog section if this change is notable enough for inclusion in the blog post then this section should be edited to contain a draft for the blog post. *otherwise leave it empty.* ` > [!note] > > if a blog post section is required the label should be added ( ) to this issue as otherwise it may be missed by the release team.",4.0,Critical,1.0,"user-visible issue, crash-like behavior"
rust-lang/rust#151607,[ice]:,"<!-- [31mice[0m: rustc ./a.rs '' 'assertion failed: all spans must be disjoint ' left: some([substitutionpart { span: /tmp/im/2/a.rs:5:13: 5:16 ( ), snippet: """" }, substitutionpart { span: /tmp/im/2/a.rs:5:15: 5:16 ( ), snippet: ""/* e */"" }])' ' right: none'', 'assertion failed: all spans must be disjoint ' left: some([substitutionpart { span: /tmp/im/2/a.rs:5:13: 5:16 ( ), snippet: """" }, substitutionpart { span: /tmp/im/2/a.rs:5:15: 5:16 ( ), snippet: ""/* e */"" }])' ' right: none'' file: /tmp/im/2/a.rs --> snippet: ` version information ` possibly related line of code: command: program output <!-- query stack: [typeck] type-checking [analysis] running analysis passes on crate -->",[],['BUG'],"['A-diagnostics', 'I-ICE', 'T-compiler', 'C-bug', 'requires-debug-assertions']",github,2026-01-24T18:53:40Z,,"[ice]: <!-- [31mice[0m: rustc ./a.rs '' 'assertion failed: all spans must be disjoint ' left: some([substitutionpart { span: /tmp/im/2/a.rs:5:13: 5:16 ( ), snippet: """" }, substitutionpart { span: /tmp/im/2/a.rs:5:15: 5:16 ( ), snippet: ""/* e */"" }])' ' right: none'', 'assertion failed: all spans must be disjoint ' left: some([substitutionpart { span: /tmp/im/2/a.rs:5:13: 5:16 ( ), snippet: """" }, substitutionpart { span: /tmp/im/2/a.rs:5:15: 5:16 ( ), snippet: ""/* e */"" }])' ' right: none'' file: /tmp/im/2/a.rs --> snippet: ` version information ` possibly related line of code: command: program output <!-- query stack: [typeck] type-checking [analysis] running analysis passes on crate -->",2.455,Medium,0.778,functional impact
rust-lang/rust#151608,[ice]: segfault when compiling naked_asm fn with unclosed .cfi_startproc and symbol item,"<!-- thank you for finding an internal compiler error! üßä if possible, try to provide a minimal verifiable example. you can read ""rust bug minimization patterns"" for how to create smaller examples. --> trying to compile a naked function with 1. an opened directive but a missing , and 2. an instruction that references a mangled symbol (here ) causes rust nightly to segfault. note that removing the instruction below makes the compiler correctly output the error . adding at the end also fixes the issue. ### code [playground]( ### meta : also reproducible on stable 1.93.0, but **not** on the following build of 1.92.0 ([godbolt]( ### error output from the backtrace, this looks more like an llvm bug, but wanted to report it here first just in case.",[],['BUG'],"['I-crash', 'A-LLVM', 'A-inline-assembly', 'T-compiler', 'A-naked', 'C-bug', 'S-waiting-on-LLVM']",github,2026-01-24T19:15:18Z,,"[ice]: segfault when compiling naked_asm fn with unclosed .cfi_startproc and symbol item <!-- thank you for finding an internal compiler error! üßä if possible, try to provide a minimal verifiable example. you can read ""rust bug minimization patterns"" for how to create smaller examples. --> trying to compile a naked function with 1. an opened directive but a missing , and 2. an instruction that references a mangled symbol (here ) causes rust nightly to segfault. note that removing the instruction below makes the compiler correctly output the error . adding at the end also fixes the issue. ### code [playground]( ### meta : also reproducible on stable 1.93.0, but **not** on the following build of 1.92.0 ([godbolt]( ### error output from the backtrace, this looks more like an llvm bug, but wanted to report it here first just in case.",4.6,Critical,1.0,crash-like behavior
rust-lang/rust#151610,[ice]:,"<!-- [31mice[0m: rustc ./a.rs '-znext-solver=globally -ooutputfile -zdump-mir-dir=dir' 'thread 'rustc' ($tid) panicked at compiler/rustc_errors/src/diagnostic.rs:998:9: 'span must not be empty and have no suggestion'', 'thread 'rustc' ($tid) panicked at compiler/rustc_errors/src/diagnostic.rs:998:9: 'span must not be empty and have no suggestion'' file: /tmp/im/a.rs --> auto-reduced (treereduce-rust): ` original: ` version information ` possibly related line of code: command: program output <!-- query stack: [typeck] type-checking [analysis] running analysis passes on crate --> label +wg-trait-system-refactor",[],"['CLEANUP', 'BUG']","['A-diagnostics', 'I-ICE', 'T-compiler', 'C-bug', 'requires-debug-assertions', 'WG-trait-system-refactor']",github,2026-01-24T19:18:21Z,,"[ice]: <!-- [31mice[0m: rustc ./a.rs '-znext-solver=globally -ooutputfile -zdump-mir-dir=dir' 'thread 'rustc' ($tid) panicked at compiler/rustc_errors/src/diagnostic.rs:998:9: 'span must not be empty and have no suggestion'', 'thread 'rustc' ($tid) panicked at compiler/rustc_errors/src/diagnostic.rs:998:9: 'span must not be empty and have no suggestion'' file: /tmp/im/a.rs --> auto-reduced (treereduce-rust): ` original: ` version information ` possibly related line of code: command: program output <!-- query stack: [typeck] type-checking [analysis] running analysis passes on crate --> label +wg-trait-system-refactor",2.99,Medium,0.9,crash-like behavior
cockroachdb/cockroach#161742,ccl/schemachangerccl: testpause_ccl_add_column_multiple_regional_by_row failed,ccl/schemachangerccl.testpause_ccl_add_column_multiple_regional_by_row [failed]( with [artifacts]( on master @ [a47c92a7669ef729fb17854978d781852dd9afaa]( help see also: [how to investigate a go test failure \(internal\)]( /cc /sql-foundations [this test on roachdash]( | [improve this report!]( jira issue: crdb-59050,[],['TESTING'],"['C-test-failure', 'O-robot', 'branch-master', 'T-sql-foundations', 'P-2', 'branch-release-24.3', 'branch-release-25.2', 'branch-release-25.3', 'branch-release-25.4', 'branch-release-26.1', 'target-release-26.2.0', 'target-release-26.1.1', 'target-release-25.3.9', 'target-release-25.2.13', 'target-release-24.3.27']",github,2026-01-24T19:37:10Z,2026-01-27T21:41:59Z,ccl/schemachangerccl: testpause_ccl_add_column_multiple_regional_by_row failed ccl/schemachangerccl.testpause_ccl_add_column_multiple_regional_by_row [failed]( with [artifacts]( on master @ [a47c92a7669ef729fb17854978d781852dd9afaa]( help see also: [how to investigate a go test failure \(internal\)]( /cc /sql-foundations [this test on roachdash]( | [improve this report!]( jira issue: crdb-59050,3.8,Critical,1.0,crash-like behavior
cockroachdb/cockroach#161743,ccl/schemachangerccl: testpausemixedversion_ccl_alter_table_alter_primary_key_rbr failed,ccl/schemachangerccl.testpausemixedversion_ccl_alter_table_alter_primary_key_rbr [failed]( with [artifacts]( on master @ [a47c92a7669ef729fb17854978d781852dd9afaa]( help see also: [how to investigate a go test failure \(internal\)]( same failure on other branches - ccl/schemachangerccl: testpausemixedversion_ccl_alter_table_alter_primary_key_rbr failed [c-test-failure o-robot t-sql-foundations branch-release-25.3 release-blocker] /cc /sql-foundations [this test on roachdash]( | [improve this report!]( jira issue: crdb-59051,[],['TESTING'],"['C-test-failure', 'O-robot', 'branch-master', 'T-sql-foundations', 'P-2', 'branch-release-24.3', 'branch-release-25.2', 'branch-release-25.3', 'branch-release-25.4', 'branch-release-26.1', 'target-release-26.2.0', 'target-release-26.1.1', 'target-release-25.3.9', 'target-release-25.2.13', 'target-release-24.3.27']",github,2026-01-24T19:37:11Z,2026-01-27T21:41:58Z,ccl/schemachangerccl: testpausemixedversion_ccl_alter_table_alter_primary_key_rbr failed ccl/schemachangerccl.testpausemixedversion_ccl_alter_table_alter_primary_key_rbr [failed]( with [artifacts]( on master @ [a47c92a7669ef729fb17854978d781852dd9afaa]( help see also: [how to investigate a go test failure \(internal\)]( same failure on other branches - ccl/schemachangerccl: testpausemixedversion_ccl_alter_table_alter_primary_key_rbr failed [c-test-failure o-robot t-sql-foundations branch-release-25.3 release-blocker] /cc /sql-foundations [this test on roachdash]( | [improve this report!]( jira issue: crdb-59051,3.8,Critical,1.0,crash-like behavior
tikv/tikv#19322,to support offline convert backup into parquet files,"## development task currently, tidb/tikv does not have native tooling to export data to parquet files. we need a tikv-side pipeline that converts br backup ssts into parquet files and optionally emits iceberg-compatible json manifests. the conversion runs offline via , reads from the backup storage, and writes parquet data to a separate output storage prefix.",[],['FEATURE'],"['type/enhancement', 'contribution']",github,2026-01-24T20:06:11Z,,"to support offline convert backup into parquet files ## development task currently, tidb/tikv does not have native tooling to export data to parquet files. we need a tikv-side pipeline that converts br backup ssts into parquet files and optionally emits iceberg-compatible json manifests. the conversion runs offline via , reads from the backup storage, and writes parquet data to a separate output storage prefix.",1.4,Low,0.538,localized low-impact
openssl/openssl#29751,ts -verify : rsa_padding_check_pkcs1_type_1:invalid padding,"i tried testing one of the eu's eidas trusted timestamping providers. i got a signed response, but validation with openssl fails. anyone has a pointer how to inspect this further? the response is attached [exampleresp.zip]( (needs zip for allowed files in github)",[],['BUG'],['issue: bug report'],github,2026-01-24T21:48:48Z,2026-01-24T23:09:09Z,"ts -verify : rsa_padding_check_pkcs1_type_1:invalid padding i tried testing one of the eu's eidas trusted timestamping providers. i got a signed response, but validation with openssl fails. anyone has a pointer how to inspect this further? the response is attached [exampleresp.zip]( (needs zip for allowed files in github)",2.54,Medium,0.797,functional impact
pandas-dev/pandas#63855,bug: exponentialmovingwindow.agg/aggregate fails when passing a callable,"### pandas version checks - [x] i have checked that this issue has not already been reported. - [x] i have confirmed this bug exists on the [latest version]( of pandas. - [x] i have confirmed this bug exists on the [main branch]( of pandas. ### reproducible example ### issue description fails when using a function (strings are fine). seems like there is an issue in how the aggregate call is wired, rolling does not suffer from the same issue (but any exponentialgroupby and alike will). the traceback is the following: ### expected behavior the use of a callable should not fail since other functions passed as string (mean, cov, corr, sum, ...). are working fine. ### installed versions replace this line with the output of pd.show_versions()",[],['BUG'],"['Bug', 'Window']",github,2026-01-24T22:32:50Z,,"bug: exponentialmovingwindow.agg/aggregate fails when passing a callable ### pandas version checks - [x] i have checked that this issue has not already been reported. - [x] i have confirmed this bug exists on the [latest version]( of pandas. - [x] i have confirmed this bug exists on the [main branch]( of pandas. ### reproducible example ### issue description fails when using a function (strings are fine). seems like there is an issue in how the aggregate call is wired, rolling does not suffer from the same issue (but any exponentialgroupby and alike will). the traceback is the following: ### expected behavior the use of a callable should not fail since other functions passed as string (mean, cov, corr, sum, ...). are working fine. ### installed versions replace this line with the output of pd.show_versions()",2.601,Medium,0.811,functional impact
kubernetes/minikube#22537,"minikube fails to detect podman container ip when boltdb deprecation warning is emitted (fedora 42, podman 5.7.0, boltdb)","### cosa √® successo? [log.txt]( minikube fails to start when using the podman driver if podman emits the boltdb deprecation warning. the warning interferes with minikube‚Äôs parsing of output, causing minikube to incorrectly fall back to the interface (127.0.0.1) and fail with: unable to select an ip from lo network interface this appears to be a parsing/robustness issue in minikube when handling non-empty stderr output from podman. --- ### environment - os: fedora 42 - minikube: v1.36.0 - driver: podman - podman: 5.7.0 - podman database backend: boltdb (legacy) - runtime: rootful (via sudo) --- ### steps to reproduce 1. install fedora 42 with podman 5.7.0 still configured to use boltdb. 2. ensure the boltdb deprecation warning is **not** suppressed. 3. run: or ### observed behavior - minikube startup fails with: - debug logs (--alsologtostderr --v=8) show minikube retrieving network data using: - podman emits the following warning on stderr: ### expected behavior minikube should correctly detect the podman container ip regardless of non-fatal warnings printed to stderr by podman. ### workarounds any of the following restore correct behavior: 1. suppress the boltdb warning for sudo commands and wait for safe db migration 2. **disruptive way** migrate boltdb, but lost your current minikube profile ### allegare il file di log [log.txt]( ### sistema operativo redhat/fedora ### driver podman",[],['BUG'],"['kind/bug', 'priority/awaiting-more-evidence', 'co/podman-driver']",github,2026-01-25T00:29:07Z,,"minikube fails to detect podman container ip when boltdb deprecation warning is emitted (fedora 42, podman 5.7.0, boltdb) ### cosa √® successo? [log.txt]( minikube fails to start when using the podman driver if podman emits the boltdb deprecation warning. the warning interferes with minikube‚Äôs parsing of output, causing minikube to incorrectly fall back to the interface (127.0.0.1) and fail with: unable to select an ip from lo network interface this appears to be a parsing/robustness issue in minikube when handling non-empty stderr output from podman. --- ### environment - os: fedora 42 - minikube: v1.36.0 - driver: podman - podman: 5.7.0 - podman database backend: boltdb (legacy) - runtime: rootful (via sudo) --- ### steps to reproduce 1. install fedora 42 with podman 5.7.0 still configured to use boltdb. 2. ensure the boltdb deprecation warning is **not** suppressed. 3. run: or ### observed behavior - minikube startup fails with: - debug logs (--alsologtostderr --v=8) show minikube retrieving network data using: - podman emits the following warning on stderr: ### expected behavior minikube should correctly detect the podman container ip regardless of non-fatal warnings printed to stderr by podman. ### workarounds any of the following restore correct behavior: 1. suppress the boltdb warning for sudo commands and wait for safe db migration 2. **disruptive way** migrate boltdb, but lost your current minikube profile ### allegare il file di log [log.txt]( ### sistema operativo redhat/fedora ### driver podman",4.6,Critical,1.0,crash-like behavior
rust-lang/rust#151625,[ice]:,"<!-- thank you for finding an internal compiler error! üßä if possible, try to provide a minimal verifiable example. you can read ""rust bug minimization patterns"" for how to create smaller examples. --> ### code ### meta <!-- if you're using the stable version of the compiler, you should also check if the bug also exists in the beta or nightly versions. --> : ### error output <!-- include a backtrace in the code block by setting in your environment. e.g. . --> backtrace",[],['BUG'],"['I-ICE', 'T-compiler', 'C-bug', 'F-adt_const_params', 'F-unsized_const_params', 'F-min_generic_const_args']",github,2026-01-25T01:52:24Z,,"[ice]: <!-- thank you for finding an internal compiler error! üßä if possible, try to provide a minimal verifiable example. you can read ""rust bug minimization patterns"" for how to create smaller examples. --> ### code ### meta <!-- if you're using the stable version of the compiler, you should also check if the bug also exists in the beta or nightly versions. --> : ### error output <!-- include a backtrace in the code block by setting in your environment. e.g. . --> backtrace",2.453,Medium,0.778,functional impact
python/cpython#144217,add dicom to mimetypes (medical imaging),"# feature or enhancement ### proposal: small feature request: add native support for the dicom (.dcm) mime type using application/dicom. ### has this already been discussed elsewhere? this is a minor feature, which does not need previous discussion elsewhere ### links to previous discussion of this feature: _no response_ ### linked prs * gh-144218",[],['FEATURE'],"['type-feature', 'stdlib']",github,2026-01-25T02:50:59Z,2026-01-25T22:47:30Z,"add dicom to mimetypes (medical imaging) # feature or enhancement ### proposal: small feature request: add native support for the dicom (.dcm) mime type using application/dicom. ### has this already been discussed elsewhere? this is a minor feature, which does not need previous discussion elsewhere ### links to previous discussion of this feature: _no response_ ### linked prs * gh-144218",1.4,Low,0.538,localized low-impact
kubernetes/kubernetes#136507,exeple.com,### what would you like to be added? comandi ### why is this needed? root,[],['FEATURE'],"['kind/feature', 'needs-sig', 'needs-triage']",github,2026-01-25T05:09:53Z,2026-01-25T07:50:35Z,exeple.com ### what would you like to be added? comandi ### why is this needed? root,1.4,Low,0.538,localized low-impact
rust-lang/rust#151631,ice: panic in with and associated const equality,"<!-- thank you for filing a bug report! üêõ please provide a short summary of the bug, along with any information you feel relevant to replicating the bug. --> this issue is similar to but this one seems to involve a different query stack, so i‚Äôm not sure whether it is a duplicate of it. i tried this code: ### meta <!-- if you're using the stable version of the compiler, you should also check if the bug also exists in the beta or nightly versions. --> : <!-- include a backtrace in the code block by setting in your environment. e.g. . --> backtrace",[],['BUG'],"['I-ICE', 'T-compiler', 'C-bug', 'F-generic_const_items', 'F-min_generic_const_args']",github,2026-01-25T05:14:08Z,,"ice: panic in with and associated const equality <!-- thank you for filing a bug report! üêõ please provide a short summary of the bug, along with any information you feel relevant to replicating the bug. --> this issue is similar to but this one seems to involve a different query stack, so i‚Äôm not sure whether it is a duplicate of it. i tried this code: ### meta <!-- if you're using the stable version of the compiler, you should also check if the bug also exists in the beta or nightly versions. --> : <!-- include a backtrace in the code block by setting in your environment. e.g. . --> backtrace",4.6,Critical,1.0,crash-like behavior
rust-lang/rust#151632,hang in trait resolution with and without,"<!-- thank you for filing a bug report! üêõ please provide a short summary of the bug, along with any information you feel relevant to replicating the bug. --> i tried this code: ### meta <!-- if you're using the stable version of the compiler, you should also check if the bug also exists in the beta or nightly versions. --> : <!-- include a backtrace in the code block by setting in your environment. e.g. . --> backtrace",[],['BUG'],"['A-trait-system', 'C-bug', 'I-hang', 'T-types']",github,2026-01-25T05:22:08Z,,"hang in trait resolution with and without <!-- thank you for filing a bug report! üêõ please provide a short summary of the bug, along with any information you feel relevant to replicating the bug. --> i tried this code: ### meta <!-- if you're using the stable version of the compiler, you should also check if the bug also exists in the beta or nightly versions. --> : <!-- include a backtrace in the code block by setting in your environment. e.g. . --> backtrace",4.6,Critical,1.0,crash-like behavior
electron/electron#49522,[bug]: exc_breakpoint crash on macos 26.2 tahoe during electronmain initialization,"### preflight checklist - [x] i have read the [contributing guidelines]( for this project. - [x] i agree to follow the [code of conduct]( that this project adheres to. - [x] i have searched the [issue tracker]( for a bug report that matches the one i want to file, without success. ### electron version 40.0.0 (also tested with 39.3.0) ### what operating system(s) are you using? macos ### operating system version arm64 (apple silicon - m4 pro) ### what arch are you using? arm64 (including apple silicon) ### last known working electron version unknown - the app worked on macos 15.x (sequoia) with electron 39.3.0 ### does the issue also appear in chromium / google chrome? no ### expected behavior the electron application should start normally without crashing. ### actual behavior the application crashes immediately on startup with exception. the crash occurs during electron framework initialization, before any application code is executed. ### testcase gist url _no response_ ### additional information #### crash report summary the crash occurs in the function during v8 engine initialization: exception type: exc_breakpoint (sigtrap) exception codes: 0x0000000000000001, 0x0000000110fab7c0 thread 0 crashed:: dispatch queue: com.apple.main-thread 0 electron framework 0x110fab7c0 ares_dns_rr_get_ttl + 3406988 1 electron framework 0x110fab6fc ares_dns_rr_get_ttl + 3406792 2 electron framework 0x10cbc8a34 v8::internal::compiler::compilationdependencies::dependoncontextcell(...) + 64872 3 electron framework 0x110fab878 ares_dns_rr_get_ttl + 3407172 4 electron framework 0x10d1ade50 cxxbridge1$box$rust_png$resultofreader$drop + 20268 5 electron framework 0x10e60a1f0 node::principalrealm::messaging_deserialize_create_object() const + 532192 6 electron framework 0x10e43d2ec electronmain + 84 7 dyld 0x181cd9d54 start + 7184 #### environment details - **hardware**: macbook pro with apple m4 pro (mac16,7) - **memory**: 48 gb lpddr5 - **macos version**: 26.2 tahoe (25c56) - **node.js version**: 22.13.1 (arm64) - **electron version**: 40.0.0 (also tested with 39.3.0, same crash) - **electron-builder version**: 26.4.0 #### application details - the application uses native modules: , , - all native modules are properly rebuilt for electron 40.0.0 and arm64 - the application is code-signed and notarized - hardened runtime is enabled #### entitlements configuration `xml <!doctype plist public ""-//apple//dtd plist 1.0//en"" "" com.apple.security.network.client com.apple.security.network.server com.apple.security.cs.allow-unsigned-executable-memory com.apple.security.cs.allow-jit com.apple.security.cs.disable-library-validation steps to reproduce 1. build an electron 40.0.0 application for macos arm64 2. code sign and notarize the application 3. install the application on macos 26.2 tahoe 4. launch the application 5. the application crashes immediately with exc_breakpoint what i've tried 1. upgraded from electron 39.3.0 to 40.0.0 - same crash 2. upgraded node.js from 20.19.0 to 22.13.1 - same crash 3. rebuilt all native modules - same crash 4. verified code signing and notarization - all valid full crash report process: soumall ai [98593] path: /applications/soumall ai.app/contents/macos/soumall ai identifier: org.rainbow.soumall version: 1.0.1 (1.0.1.1) code type: arm-64 (native) role: background parent process: launchd [1] coalition: org.rainbow.soumall [9681] user id: 501 date/time: 2026-01-25 13:25:50.9571 +0800 launch time: 2026-01-25 13:25:45.8359 +0800 hardware model: mac16,7 os version: macos 26.2 (25c56) release type: user exception type: exc_breakpoint (sigtrap) exception codes: 0x0000000000000001, 0x0000000110fab7c0 termination reason: namespace signal, code 5, trace/bpt trap: 5 terminating process: exc handler [98593] thread 0 crashed:: dispatch queue: com.apple.main-thread 0 electron framework 0x110fab7c0 ares_dns_rr_get_ttl + 3406988 1 electron framework 0x110fab6fc ares_dns_rr_get_ttl + 3406792 2 electron framework 0x10cbc8a34 v8::internal::compiler::compilationdependencies::dependoncontextcell(v8::internal::compiler::contextcellref, v8::internal::contextcell::state) + 64872 3 electron framework 0x110fab878 ares_dns_rr_get_ttl + 3407172 4 electron framework 0x10d1ade50 cxxbridge1$box$rust_png$resultofreader$drop + 20268 5 electron framework 0x10e60a1f0 node::principalrealm::messaging_deserialize_create_object() const + 532192 6 electron framework 0x10e43d2ec electronmain + 84 7 dyld 0x181cd9d54 start + 7184 binary images: 0x100fb0000 - 0x100fb3fff org.rainbow.soumall (1.0.1) /applications/soumall ai.app/contents/macos/soumall ai 0x10baf0000 - 0x11593bfff com.github.electron.framework (40.0.0) /applications/soumall ai.app/contents/frameworks/electron framework.framework/versions/a/electron framework notes - the crash occurs before any application code is executed - this appears to be a v8 engine initialization issue specific to macos 26.2 tahoe - the same application works correctly on macos 15.x (sequoia) - system integrity protection is enabled - the application is properly code-signed with developer id certificate ---",[],['BUG'],"['platform/macOS', 'crash :boom:', 'blocked/need-info ‚ùå', 'bug :beetle:', '39-x-y', '40-x-y']",github,2026-01-25T06:07:36Z,,"[bug]: exc_breakpoint crash on macos 26.2 tahoe during electronmain initialization ### preflight checklist - [x] i have read the [contributing guidelines]( for this project. - [x] i agree to follow the [code of conduct]( that this project adheres to. - [x] i have searched the [issue tracker]( for a bug report that matches the one i want to file, without success. ### electron version 40.0.0 (also tested with 39.3.0) ### what operating system(s) are you using? macos ### operating system version arm64 (apple silicon - m4 pro) ### what arch are you using? arm64 (including apple silicon) ### last known working electron version unknown - the app worked on macos 15.x (sequoia) with electron 39.3.0 ### does the issue also appear in chromium / google chrome? no ### expected behavior the electron application should start normally without crashing. ### actual behavior the application crashes immediately on startup with exception. the crash occurs during electron framework initialization, before any application code is executed. ### testcase gist url _no response_ ### additional information #### crash report summary the crash occurs in the function during v8 engine initialization: exception type: exc_breakpoint (sigtrap) exception codes: 0x0000000000000001, 0x0000000110fab7c0 thread 0 crashed:: dispatch queue: com.apple.main-thread 0 electron framework 0x110fab7c0 ares_dns_rr_get_ttl + 3406988 1 electron framework 0x110fab6fc ares_dns_rr_get_ttl + 3406792 2 electron framework 0x10cbc8a34 v8::internal::compiler::compilationdependencies::dependoncontextcell(...) + 64872 3 electron framework 0x110fab878 ares_dns_rr_get_ttl + 3407172 4 electron framework 0x10d1ade50 cxxbridge1$box$rust_png$resultofreader$drop + 20268 5 electron framework 0x10e60a1f0 node::principalrealm::messaging_deserialize_create_object() const + 532192 6 electron framework 0x10e43d2ec electronmain + 84 7 dyld 0x181cd9d54 start + 7184 #### environment details - **hardware**: macbook pro with apple m4 pro (mac16,7) - **memory**: 48 gb lpddr5 - **macos version**: 26.2 tahoe (25c56) - **node.js version**: 22.13.1 (arm64) - **electron version**: 40.0.0 (also tested with 39.3.0, same crash) - **electron-builder version**: 26.4.0 #### application details - the application uses native modules: , , - all native modules are properly rebuilt for electron 40.0.0 and arm64 - the application is code-signed and notarized - hardened runtime is enabled #### entitlements configuration `xml <!doctype plist public ""-//apple//dtd plist 1.0//en"" "" com.apple.security.network.client com.apple.security.network.server com.apple.security.cs.allow-unsigned-executable-memory com.apple.security.cs.allow-jit com.apple.security.cs.disable-library-validation steps to reproduce 1. build an electron 40.0.0 application for macos arm64 2. code sign and notarize the application 3. install the application on macos 26.2 tahoe 4. launch the application 5. the application crashes immediately with exc_breakpoint what i've tried 1. upgraded from electron 39.3.0 to 40.0.0 - same crash 2. upgraded node.js from 20.19.0 to 22.13.1 - same crash 3. rebuilt all native modules - same crash 4. verified code signing and notarization - all valid full crash report process: soumall ai [98593] path: /applications/soumall ai.app/contents/macos/soumall ai identifier: org.rainbow.soumall version: 1.0.1 (1.0.1.1) code type: arm-64 (native) role: background parent process: launchd [1] coalition: org.rainbow.soumall [9681] user id: 501 date/time: 2026-01-25 13:25:50.9571 +0800 launch time: 2026-01-25 13:25:45.8359 +0800 hardware model: mac16,7 os version: macos 26.2 (25c56) release type: user exception type: exc_breakpoint (sigtrap) exception codes: 0x0000000000000001, 0x0000000110fab7c0 termination reason: namespace signal, code 5, trace/bpt trap: 5 terminating process: exc handler [98593] thread 0 crashed:: dispatch queue: com.apple.main-thread 0 electron framework 0x110fab7c0 ares_dns_rr_get_ttl + 3406988 1 electron framework 0x110fab6fc ares_dns_rr_get_ttl + 3406792 2 electron framework 0x10cbc8a34 v8::internal::compiler::compilationdependencies::dependoncontextcell(v8::internal::compiler::contextcellref, v8::internal::contextcell::state) + 64872 3 electron framework 0x110fab878 ares_dns_rr_get_ttl + 3407172 4 electron framework 0x10d1ade50 cxxbridge1$box$rust_png$resultofreader$drop + 20268 5 electron framework 0x10e60a1f0 node::principalrealm::messaging_deserialize_create_object() const + 532192 6 electron framework 0x10e43d2ec electronmain + 84 7 dyld 0x181cd9d54 start + 7184 binary images: 0x100fb0000 - 0x100fb3fff org.rainbow.soumall (1.0.1) /applications/soumall ai.app/contents/macos/soumall ai 0x10baf0000 - 0x11593bfff com.github.electron.framework (40.0.0) /applications/soumall ai.app/contents/frameworks/electron framework.framework/versions/a/electron framework notes - the crash occurs before any application code is executed - this appears to be a v8 engine initialization issue specific to macos 26.2 tahoe - the same application works correctly on macos 15.x (sequoia) - system integrity protection is enabled - the application is properly code-signed with developer id certificate ---",4.6,Critical,1.0,crash-like behavior
cockroachdb/cockroach#161748,roachtest: vecindex/dbpedia-100k/nodes=3/prefix=3 failed,roachtest.vecindex/dbpedia-100k/nodes=3/prefix=3 [failed]( with [artifacts]( on release-26.1.0-rc @ [6c9b2f778bd29895383d493a2fa4490323499ecf]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21021043-1769322675-03-n3cpu4-0001 | 34.139.233.254 | 10.142.1.51 | | teamcity-21021043-1769322675-03-n3cpu4-0002 | 34.139.146.50 | 10.142.1.54 | | teamcity-21021043-1769322675-03-n3cpu4-0003 | 35.243.128.228 | 10.142.1.48 | parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - fs=ext4 - localssd=true - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( same failure on other branches - roachtest: vecindex/dbpedia-100k/nodes=3/prefix=3 failed [readfile unexpected eof] [c-test-failure o-roachtest o-robot p-2 t-sql-queries branch-master branch-release-26.1] [this test on roachdash]( | [improve this report!]( jira issue: crdb-59052,[],['TESTING'],"['C-test-failure', 'O-robot', 'O-roachtest', 'T-sql-queries', 'branch-release-26.1.0-rc']",github,2026-01-25T06:39:37Z,,roachtest: vecindex/dbpedia-100k/nodes=3/prefix=3 failed roachtest.vecindex/dbpedia-100k/nodes=3/prefix=3 [failed]( with [artifacts]( on release-26.1.0-rc @ [6c9b2f778bd29895383d493a2fa4490323499ecf]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21021043-1769322675-03-n3cpu4-0001 | 34.139.233.254 | 10.142.1.51 | | teamcity-21021043-1769322675-03-n3cpu4-0002 | 34.139.146.50 | 10.142.1.54 | | teamcity-21021043-1769322675-03-n3cpu4-0003 | 35.243.128.228 | 10.142.1.48 | parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - fs=ext4 - localssd=true - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( same failure on other branches - roachtest: vecindex/dbpedia-100k/nodes=3/prefix=3 failed [readfile unexpected eof] [c-test-failure o-roachtest o-robot p-2 t-sql-queries branch-master branch-release-26.1] [this test on roachdash]( | [improve this report!]( jira issue: crdb-59052,2.935,Medium,0.887,functional impact
cockroachdb/cockroach#161749,roachtest: vecindex/dbpedia-100k/nodes=3/prefix=0 failed,roachtest.vecindex/dbpedia-100k/nodes=3/prefix=0 [failed]( with [artifacts]( on release-26.1.0-rc @ [6c9b2f778bd29895383d493a2fa4490323499ecf]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21021043-1769322675-01-n3cpu4-0001 | 35.185.91.11 | 10.142.1.52 | | teamcity-21021043-1769322675-01-n3cpu4-0002 | 34.26.114.103 | 10.142.1.41 | | teamcity-21021043-1769322675-01-n3cpu4-0003 | 34.23.134.106 | 10.142.1.49 | parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - fs=ext4 - localssd=true - metamorphicwritebuffering=true - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( same failure on other branches - roachtest: vecindex/dbpedia-100k/nodes=3/prefix=0 failed [readfile unexpected eof] [c-test-failure o-roachtest o-robot p-2 t-sql-queries branch-master branch-release-26.1] [this test on roachdash]( | [improve this report!]( jira issue: crdb-59053,[],['TESTING'],"['C-test-failure', 'O-robot', 'O-roachtest', 'T-sql-queries', 'branch-release-26.1.0-rc']",github,2026-01-25T06:39:41Z,,roachtest: vecindex/dbpedia-100k/nodes=3/prefix=0 failed roachtest.vecindex/dbpedia-100k/nodes=3/prefix=0 [failed]( with [artifacts]( on release-26.1.0-rc @ [6c9b2f778bd29895383d493a2fa4490323499ecf]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21021043-1769322675-01-n3cpu4-0001 | 35.185.91.11 | 10.142.1.52 | | teamcity-21021043-1769322675-01-n3cpu4-0002 | 34.26.114.103 | 10.142.1.41 | | teamcity-21021043-1769322675-01-n3cpu4-0003 | 34.23.134.106 | 10.142.1.49 | parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - fs=ext4 - localssd=true - metamorphicwritebuffering=true - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( same failure on other branches - roachtest: vecindex/dbpedia-100k/nodes=3/prefix=0 failed [readfile unexpected eof] [c-test-failure o-roachtest o-robot p-2 t-sql-queries branch-master branch-release-26.1] [this test on roachdash]( | [improve this report!]( jira issue: crdb-59053,2.86,Medium,0.87,functional impact
rust-lang/rust#151636,next solver hangs on cyclic associated type projection in blanket implementation bounds,"<!-- thank you for filing a bug report! üêõ please provide a short summary of the bug, along with any information you feel relevant to replicating the bug. --> i tried this code: | release channel | result | |-----------------------------------|--------| | current stable | error | | current nightly (default solver) | error | | current nightly (+ | hang | ### meta <!-- if you're using the stable version of the compiler, you should also check if the bug also exists in the beta or nightly versions. --> : <!-- include a backtrace in the code block by setting in your environment. e.g. . --> backtrace",[],"['CLEANUP', 'BUG']","['A-trait-system', 'C-bug', 'I-hang', 'T-types', 'WG-trait-system-refactor']",github,2026-01-25T07:13:26Z,,"next solver hangs on cyclic associated type projection in blanket implementation bounds <!-- thank you for filing a bug report! üêõ please provide a short summary of the bug, along with any information you feel relevant to replicating the bug. --> i tried this code: | release channel | result | |-----------------------------------|--------| | current stable | error | | current nightly (default solver) | error | | current nightly (+ | hang | ### meta <!-- if you're using the stable version of the compiler, you should also check if the bug also exists in the beta or nightly versions. --> : <!-- include a backtrace in the code block by setting in your environment. e.g. . --> backtrace",2.981,Medium,0.898,crash-like behavior
cockroachdb/cockroach#161750,pkg/ccl/testccl/sqlccl/sqlccl_test: testexplaingist failed,pkg/ccl/testccl/sqlccl/sqlccl_test.testexplaingist [failed]( on release-26.1 @ [2dede1c13f1bf518511d790900cbd47e0557472f]( parameters: - attempt=1 - run=7 - shard=2 help see also: [how to investigate a go test failure \(internal\)]( same failure on other branches - pkg/ccl/testccl/sqlccl/sqlccl_test: testexplaingist failed [c-test-failure o-robot t-sql-queries branch-master] [this test on roachdash]( | [improve this report!]( jira issue: crdb-59054,[],['TESTING'],"['C-test-failure', 'O-robot', 'X-duplicate', 'release-blocker', 'T-sql-queries', 'branch-release-26.1']",github,2026-01-25T07:17:20Z,2026-01-26T16:56:26Z,pkg/ccl/testccl/sqlccl/sqlccl_test: testexplaingist failed pkg/ccl/testccl/sqlccl/sqlccl_test.testexplaingist [failed]( on release-26.1 @ [2dede1c13f1bf518511d790900cbd47e0557472f]( parameters: - attempt=1 - run=7 - shard=2 help see also: [how to investigate a go test failure \(internal\)]( same failure on other branches - pkg/ccl/testccl/sqlccl/sqlccl_test: testexplaingist failed [c-test-failure o-robot t-sql-queries branch-master] [this test on roachdash]( | [improve this report!]( jira issue: crdb-59054,1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161751,sql: testrelocatenonvoters failed,sql.testrelocatenonvoters [failed]( on release-25.4.4-rc @ [688cda9084f2924ec4e22044c861aa0a3f34e956]( parameters: - attempt=1 - run=7 - shard=7 help see also: [how to investigate a go test failure \(internal\)]( same failure on other branches - sql: testrelocatenonvoters failed [trying to add non_voter to a store that already has a learner] [c-test-failure o-robot t-kv branch-release-26.1] /cc /sql-foundations [this test on roachdash]( | [improve this report!]( jira issue: crdb-59055,[],['TESTING'],"['C-test-failure', 'O-robot', 'T-kv', 'branch-release-25.4.4-rc']",github,2026-01-25T07:21:46Z,,sql: testrelocatenonvoters failed sql.testrelocatenonvoters [failed]( on release-25.4.4-rc @ [688cda9084f2924ec4e22044c861aa0a3f34e956]( parameters: - attempt=1 - run=7 - shard=7 help see also: [how to investigate a go test failure \(internal\)]( same failure on other branches - sql: testrelocatenonvoters failed [trying to add non_voter to a store that already has a learner] [c-test-failure o-robot t-kv branch-release-26.1] /cc /sql-foundations [this test on roachdash]( | [improve this report!]( jira issue: crdb-59055,1.6,Low,0.584,localized low-impact
pytorch/pytorch#173308,massive performance regression on rocm with certain conv2d,"### üêõ describe the bug on an rx 9070/fedora 43, with the following code: output with : elapsed time: 26.50116009299927 seconds output with : elapsed time: 1.0386801980002929 seconds profiling reveals the difference comes from conv2d. [flamegraphs.zip]( ### versions collecting environment information... pytorch version: 2.9.1+rocm6.4 is debug build: false cuda used to build pytorch: n/a rocm used to build pytorch: 6.4.43484-123eb5128 os: fedora linux 43 (workstation edition) (x86_64) gcc version: (gcc) 15.2.1 20251211 (red hat 15.2.1-5) clang version: could not collect cmake version: could not collect libc version: glibc-2.42 python version: 3.13.11 | packaged by conda-forge | (main, dec 6 2025, 11:24:03) [gcc 14.3.0] (64-bit runtime) python platform: linux-6.18.5-200.fc43.x86_64-x86_64-with-glibc2.42 is cuda available: true cuda runtime version: could not collect cuda_module_loading set to: gpu models and configuration: amd radeon graphics (gfx1201) nvidia driver version: could not collect cudnn version: could not collect is xpu available: false hip runtime version: 6.4.43484 miopen runtime version: 3.4.0 is xnnpack available: true caching allocator config: n/a cpu: architecture: x86_64 cpu op-mode(s): 32-bit, 64-bit address sizes: 48 bits physical, 48 bits virtual byte order: little endian cpu(s): 16 on-line cpu(s) list: 0-15 vendor id: authenticamd model name: amd ryzen 7 9800x3d 8-core processor cpu family: 26 model: 68 thread(s) per core: 2 core(s) per socket: 8 socket(s): 1 stepping: 0 frequency boost: enabled cpu(s) scaling mhz: 68% cpu max mhz: 5271,6221 cpu min mhz: 603,3790 bogomips: 9381,75 flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good amd_lbr_v2 nopl xtopology nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpuid_fault cpb cat_l3 cdp_l3 hw_pstate ssbd mba perfmon_v2 ibrs ibpb stibp ibrs_enhanced vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local user_shstk avx_vnni avx512_bf16 clzero irperf xsaveerptr rdpru wbnoinvd cppc arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif x2avic v_spec_ctrl vnmi avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid bus_lock_detect movdiri movdir64b overflow_recov succor smca fsrm avx512_vp2intersect flush_l1d amd_lbr_pmc_freeze virtualization: amd-v l1d cache: 384 kib (8 instances) l1i cache: 256 kib (8 instances) l2 cache: 8 mib (8 instances) l3 cache: 96 mib (1 instance) numa node(s): 1 numa node0 cpu(s): 0-15 vulnerability gather data sampling: not affected vulnerability ghostwrite: not affected vulnerability indirect target selection: not affected vulnerability itlb multihit: not affected vulnerability l1tf: not affected vulnerability mds: not affected vulnerability meltdown: not affected vulnerability mmio stale data: not affected vulnerability old microcode: not affected vulnerability reg file data sampling: not affected vulnerability retbleed: not affected vulnerability spec rstack overflow: mitigation; ibpb on vmexit only vulnerability spec store bypass: mitigation; speculative store bypass disabled via prctl vulnerability spectre v1: mitigation; usercopy/swapgs barriers and __user pointer sanitization vulnerability spectre v2: mitigation; enhanced / automatic ibrs; ibpb conditional; stibp always-on; pbrsb-eibrs not affected; bhi not affected vulnerability srbds: not affected vulnerability tsa: not affected vulnerability tsx async abort: not affected vulnerability vmscape: mitigation; ibpb on vmexit versions of relevant libraries: [pip3] dctorch==0.1.2 [pip3] numpy==2.3.5 [pip3] onnx==1.20.1 [pip3] onnxruntime==1.23.2 [pip3] open_clip_torch==3.2.0 [pip3] pytorch-lightning==2.6.0 [pip3] pytorch-triton-rocm==3.5.1 [pip3] torch==2.9.1+rocm6.4 [pip3] torchmetrics==1.8.2 [pip3] torchsde==0.2.6 [pip3] torchvision==0.24.1+rocm6.4 [pip3] triton-rocm==3.6.0 [conda] could not collect cc -amd",[],['PERFORMANCE'],"['module: performance', 'module: rocm', 'triaged', 'module: regression']",github,2026-01-25T07:24:58Z,,"massive performance regression on rocm with certain conv2d ### üêõ describe the bug on an rx 9070/fedora 43, with the following code: output with : elapsed time: 26.50116009299927 seconds output with : elapsed time: 1.0386801980002929 seconds profiling reveals the difference comes from conv2d. [flamegraphs.zip]( ### versions collecting environment information... pytorch version: 2.9.1+rocm6.4 is debug build: false cuda used to build pytorch: n/a rocm used to build pytorch: 6.4.43484-123eb5128 os: fedora linux 43 (workstation edition) (x86_64) gcc version: (gcc) 15.2.1 20251211 (red hat 15.2.1-5) clang version: could not collect cmake version: could not collect libc version: glibc-2.42 python version: 3.13.11 | packaged by conda-forge | (main, dec 6 2025, 11:24:03) [gcc 14.3.0] (64-bit runtime) python platform: linux-6.18.5-200.fc43.x86_64-x86_64-with-glibc2.42 is cuda available: true cuda runtime version: could not collect cuda_module_loading set to: gpu models and configuration: amd radeon graphics (gfx1201) nvidia driver version: could not collect cudnn version: could not collect is xpu available: false hip runtime version: 6.4.43484 miopen runtime version: 3.4.0 is xnnpack available: true caching allocator config: n/a cpu: architecture: x86_64 cpu op-mode(s): 32-bit, 64-bit address sizes: 48 bits physical, 48 bits virtual byte order: little endian cpu(s): 16 on-line cpu(s) list: 0-15 vendor id: authenticamd model name: amd ryzen 7 9800x3d 8-core processor cpu family: 26 model: 68 thread(s) per core: 2 core(s) per socket: 8 socket(s): 1 stepping: 0 frequency boost: enabled cpu(s) scaling mhz: 68% cpu max mhz: 5271,6221 cpu min mhz: 603,3790 bogomips: 9381,75 flags: fpu vme de pse tsc msr pae mce cx8 apic sep mtrr pge mca cmov pat pse36 clflush mmx fxsr sse sse2 ht syscall nx mmxext fxsr_opt pdpe1gb rdtscp lm constant_tsc rep_good amd_lbr_v2 nopl xtopology nonstop_tsc cpuid extd_apicid aperfmperf rapl pni pclmulqdq monitor ssse3 fma cx16 sse4_1 sse4_2 movbe popcnt aes xsave avx f16c rdrand lahf_lm cmp_legacy svm extapic cr8_legacy abm sse4a misalignsse 3dnowprefetch osvw ibs skinit wdt tce topoext perfctr_core perfctr_nb bpext perfctr_llc mwaitx cpuid_fault cpb cat_l3 cdp_l3 hw_pstate ssbd mba perfmon_v2 ibrs ibpb stibp ibrs_enhanced vmmcall fsgsbase tsc_adjust bmi1 avx2 smep bmi2 erms invpcid cqm rdt_a avx512f avx512dq rdseed adx smap avx512ifma clflushopt clwb avx512cd sha_ni avx512bw avx512vl xsaveopt xsavec xgetbv1 xsaves cqm_llc cqm_occup_llc cqm_mbm_total cqm_mbm_local user_shstk avx_vnni avx512_bf16 clzero irperf xsaveerptr rdpru wbnoinvd cppc arat npt lbrv svm_lock nrip_save tsc_scale vmcb_clean flushbyasid decodeassists pausefilter pfthreshold avic v_vmsave_vmload vgif x2avic v_spec_ctrl vnmi avx512vbmi umip pku ospke avx512_vbmi2 gfni vaes vpclmulqdq avx512_vnni avx512_bitalg avx512_vpopcntdq rdpid bus_lock_detect movdiri movdir64b overflow_recov succor smca fsrm avx512_vp2intersect flush_l1d amd_lbr_pmc_freeze virtualization: amd-v l1d cache: 384 kib (8 instances) l1i cache: 256 kib (8 instances) l2 cache: 8 mib (8 instances) l3 cache: 96 mib (1 instance) numa node(s): 1 numa node0 cpu(s): 0-15 vulnerability gather data sampling: not affected vulnerability ghostwrite: not affected vulnerability indirect target selection: not affected vulnerability itlb multihit: not affected vulnerability l1tf: not affected vulnerability mds: not affected vulnerability meltdown: not affected vulnerability mmio stale data: not affected vulnerability old microcode: not affected vulnerability reg file data sampling: not affected vulnerability retbleed: not affected vulnerability spec rstack overflow: mitigation; ibpb on vmexit only vulnerability spec store bypass: mitigation; speculative store bypass disabled via prctl vulnerability spectre v1: mitigation; usercopy/swapgs barriers and __user pointer sanitization vulnerability spectre v2: mitigation; enhanced / automatic ibrs; ibpb conditional; stibp always-on; pbrsb-eibrs not affected; bhi not affected vulnerability srbds: not affected vulnerability tsa: not affected vulnerability tsx async abort: not affected vulnerability vmscape: mitigation; ibpb on vmexit versions of relevant libraries: [pip3] dctorch==0.1.2 [pip3] numpy==2.3.5 [pip3] onnx==1.20.1 [pip3] onnxruntime==1.23.2 [pip3] open_clip_torch==3.2.0 [pip3] pytorch-lightning==2.6.0 [pip3] pytorch-triton-rocm==3.5.1 [pip3] torch==2.9.1+rocm6.4 [pip3] torchmetrics==1.8.2 [pip3] torchsde==0.2.6 [pip3] torchvision==0.24.1+rocm6.4 [pip3] triton-rocm==3.6.0 [conda] could not collect cc -amd",8.6,Critical,1.0,"performance degradation, crash-like behavior"
cockroachdb/cockroach#161752,pkg/server/status/status_test_/status_test: pkg failed,pkg/server/status/status_test_/status_test.pkg [failed]( with [artifacts]( on release-26.1 @ [2dede1c13f1bf518511d790900cbd47e0557472f]( help see also: [how to investigate a go test failure \(internal\)]( /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-59056,[],['TESTING'],"['C-test-failure', 'O-robot', 'T-testeng', 's390x-test-failure', 'branch-release-26.1']",github,2026-01-25T07:34:24Z,2026-01-28T06:54:52Z,pkg/server/status/status_test_/status_test: pkg failed pkg/server/status/status_test_/status_test.pkg [failed]( with [artifacts]( on release-26.1 @ [2dede1c13f1bf518511d790900cbd47e0557472f]( help see also: [how to investigate a go test failure \(internal\)]( /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-59056,1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161753,pkg/util/tracing/tracing_test_/tracing_test: pkg failed,pkg/util/tracing/tracing_test_/tracing_test.pkg [failed]( with [artifacts]( on release-26.1 @ [2dede1c13f1bf518511d790900cbd47e0557472f]( help see also: [how to investigate a go test failure \(internal\)]( /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-59057,[],['TESTING'],"['C-test-failure', 'O-robot', 'X-duplicate', 'T-testeng', 's390x-test-failure', 'branch-release-26.1']",github,2026-01-25T07:34:25Z,2026-01-28T06:55:30Z,pkg/util/tracing/tracing_test_/tracing_test: pkg failed pkg/util/tracing/tracing_test_/tracing_test.pkg [failed]( with [artifacts]( on release-26.1 @ [2dede1c13f1bf518511d790900cbd47e0557472f]( help see also: [how to investigate a go test failure \(internal\)]( /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-59057,1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161754,pkg/sql/execinfra/execinfra_test_/execinfra_test: pkg failed,pkg/sql/execinfra/execinfra_test_/execinfra_test.pkg [failed]( with [artifacts]( on release-26.1 @ [2dede1c13f1bf518511d790900cbd47e0557472f]( help see also: [how to investigate a go test failure \(internal\)]( /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-59058,[],['TESTING'],"['C-test-failure', 'O-robot', 'X-duplicate', 'T-testeng', 's390x-test-failure', 'branch-release-26.1']",github,2026-01-25T07:34:26Z,2026-01-28T06:55:58Z,pkg/sql/execinfra/execinfra_test_/execinfra_test: pkg failed pkg/sql/execinfra/execinfra_test_/execinfra_test.pkg [failed]( with [artifacts]( on release-26.1 @ [2dede1c13f1bf518511d790900cbd47e0557472f]( help see also: [how to investigate a go test failure \(internal\)]( /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-59058,1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161755,kv/kvnemesis: testkvnemesismultinode_bufferedwritesnolockdurabilityupgrades failed,kv/kvnemesis.testkvnemesismultinode_bufferedwritesnolockdurabilityupgrades [failed]( on release-25.4 @ [27249807fb41bbb90ab7d7b184c077a442fd824d]( parameters: - attempt=1 - run=10 - shard=1 help see also: [how to investigate a go test failure \(internal\)]( /cc /kv-triage [this test on roachdash]( | [improve this report!]( jira issue: crdb-59059,[],"['TESTING', 'BUG']","['C-bug', 'C-test-failure', 'O-robot', 'T-kv', 'P-3', 'branch-release-25.4']",github,2026-01-25T07:39:08Z,,kv/kvnemesis: testkvnemesismultinode_bufferedwritesnolockdurabilityupgrades failed kv/kvnemesis.testkvnemesismultinode_bufferedwritesnolockdurabilityupgrades [failed]( on release-25.4 @ [27249807fb41bbb90ab7d7b184c077a442fd824d]( parameters: - attempt=1 - run=10 - shard=1 help see also: [how to investigate a go test failure \(internal\)]( /cc /kv-triage [this test on roachdash]( | [improve this report!]( jira issue: crdb-59059,2.0,Medium,0.675,localized low-impact
cockroachdb/cockroach#161756,roachtest: apt_problem failed,roachtest.apt_problem [failed]( with [artifacts]( on release-25.4.4-rc @ [688cda9084f2924ec4e22044c861aa0a3f34e956]( parameters: - arch=fips - cloud=gce - coveragebuild=false - cpu=16 - encrypted=false - fs=ext4 - localssd=true - metamorphicbufferedsender=true - metamorphicleases=default - runtimeassertionsbuild=false - ssd=0 help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( same failure on other branches - roachtest: apt_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.3.7-rc] - roachtest: apt_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.2.11-rc] - roachtest: apt_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.4.3-rc] /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-59060,[],['TESTING'],"['O-robot', 'O-roachtest', 'T-testeng', 'X-infra-flake', 'branch-release-25.4.4-rc']",github,2026-01-25T07:40:18Z,,roachtest: apt_problem failed roachtest.apt_problem [failed]( with [artifacts]( on release-25.4.4-rc @ [688cda9084f2924ec4e22044c861aa0a3f34e956]( parameters: - arch=fips - cloud=gce - coveragebuild=false - cpu=16 - encrypted=false - fs=ext4 - localssd=true - metamorphicbufferedsender=true - metamorphicleases=default - runtimeassertionsbuild=false - ssd=0 help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( same failure on other branches - roachtest: apt_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.3.7-rc] - roachtest: apt_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.2.11-rc] - roachtest: apt_problem failed [o-roachtest o-robot t-testeng x-infra-flake branch-release-25.4.3-rc] /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-59060,2.985,Medium,0.898,functional impact
rust-lang/rust#151637,unsoundness due to closure return value not being checked for wf,this unsoundness is an exploitation of the weirdness in . see also for a similar unsoundness. the below code causes a use-after-free (segfault in my testing). cc ### meta reproducible on the playground with version,[],['BUG'],"['A-lifetimes', 'A-closures', 'A-borrow-checker', 'P-high', 'T-compiler', 'I-unsound', 'A-NLL', 'C-bug', 'T-types']",github,2026-01-25T07:47:08Z,,unsoundness due to closure return value not being checked for wf this unsoundness is an exploitation of the weirdness in . see also for a similar unsoundness. the below code causes a use-after-free (segfault in my testing). cc ### meta reproducible on the playground with version,6.4,Critical,1.0,crash-like behavior
cockroachdb/cockroach#161757,roachtest: rebalance/by-load/leases/mixed-version failed,roachtest.rebalance/by-load/leases/mixed-version [failed]( with [artifacts]( on release-25.2 @ [7ea976da96e5c9f41820090cabb6777ba04f7607]( parameters: - arch=arm64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - metamorphicbufferedsender=false - mvtdeploymentmode=system-only - mvtversions=v24.3.25 ‚Üí v25.1.10 ‚Üí release-25.2 - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( /cc /kv-triage [this test on roachdash]( | [improve this report!]( jira issue: crdb-59061,[],['TESTING'],"['C-test-failure', 'O-robot', 'O-roachtest', 'release-blocker', 'T-kv', 'branch-release-25.2']",github,2026-01-25T07:58:08Z,,roachtest: rebalance/by-load/leases/mixed-version failed roachtest.rebalance/by-load/leases/mixed-version [failed]( with [artifacts]( on release-25.2 @ [7ea976da96e5c9f41820090cabb6777ba04f7607]( parameters: - arch=arm64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - metamorphicbufferedsender=false - mvtdeploymentmode=system-only - mvtversions=v24.3.25 ‚Üí v25.1.10 ‚Üí release-25.2 - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( /cc /kv-triage [this test on roachdash]( | [improve this report!]( jira issue: crdb-59061,2.8,Medium,0.856,functional impact
cockroachdb/cockroach#161758,roachtest: gopg failed,"**note:** this build has runtime assertions enabled. if the same failure was hit in a run without assertions enabled, there should be a similar failure without this message. if there isn& ;t one, then this failure is likely due to an assertion violation or (assertion) timeout. roachtest.gopg [failed]( with [artifacts]( on master @ [36e5eb89571f7eee1b489ab407b1ea06629ca26b]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21020877-1769324153-44-n1cpu4-0001 | 20.55.1.137 | 10.1.0.172 | parameters: - arch=amd64 - cloud=azure - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - metamorphicleases=leader - metamorphicwritebuffering=true - runtimeassertionsbuild=true help see: [roachtest readme]( see: [how to investigate \(internal\)]( _grafana is not yet available for azure clusters_ /cc /sql-foundations [this test on roachdash]( | [improve this report!]( jira issue: crdb-59062",[],['TESTING'],"['C-test-failure', 'O-robot', 'O-roachtest', 'branch-master', 'release-blocker', 'T-sql-foundations', 'B-runtime-assertions-enabled']",github,2026-01-25T08:02:43Z,2026-01-26T03:41:28Z,"roachtest: gopg failed **note:** this build has runtime assertions enabled. if the same failure was hit in a run without assertions enabled, there should be a similar failure without this message. if there isn& ;t one, then this failure is likely due to an assertion violation or (assertion) timeout. roachtest.gopg [failed]( with [artifacts]( on master @ [36e5eb89571f7eee1b489ab407b1ea06629ca26b]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21020877-1769324153-44-n1cpu4-0001 | 20.55.1.137 | 10.1.0.172 | parameters: - arch=amd64 - cloud=azure - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - metamorphicleases=leader - metamorphicwritebuffering=true - runtimeassertionsbuild=true help see: [roachtest readme]( see: [how to investigate \(internal\)]( _grafana is not yet available for azure clusters_ /cc /sql-foundations [this test on roachdash]( | [improve this report!]( jira issue: crdb-59062",2.971,Medium,0.895,functional impact
envoyproxy/envoy#43148,newer release available : 1.8.2 (current: 1.7.0),package name: rules_python .7.0 current version: 1.7.0 -11-14 available version: 1.8.2 -01-25 upstream releases:,[],['UI'],"['dependencies', 'no stalebot', 'area/build']",github,2026-01-25T08:09:49Z,2026-01-28T08:11:07Z,newer release available : 1.8.2 (current: 1.7.0) package name: rules_python .7.0 current version: 1.7.0 -11-14 available version: 1.8.2 -01-25 upstream releases:,1.8,Low,0.629,user-visible issue
kubernetes/minikube#22541,remove legacy docker code from the minikube code base,"the minikube code still has a lot of workarounds for kubernetes version < 1.24 and 1.27. * those were the versions when the ""dockershim"" was removed, and replaced by cri. * there is also code for ""kubenet"" and similar network plugins when not using cni. those conditionals are not needed anymore (1.28+), so they can be removed from the code.",[],['CLEANUP'],"['co/runtime/docker', 'kind/cleanup']",github,2026-01-25T09:17:04Z,,"remove legacy docker code from the minikube code base the minikube code still has a lot of workarounds for kubernetes version < 1.24 and 1.27. * those were the versions when the ""dockershim"" was removed, and replaced by cri. * there is also code for ""kubenet"" and similar network plugins when not using cni. those conditionals are not needed anymore (1.28+), so they can be removed from the code.",1.2,Low,0.493,localized low-impact
cockroachdb/cockroach#161759,sql: testindexbackfillaftergc failed,sql.testindexbackfillaftergc [failed]( on release-26.1 @ [2dede1c13f1bf518511d790900cbd47e0557472f]( parameters: - attempt=1 - deadlock=true - run=1 - shard=6 help see also: [how to investigate a go test failure \(internal\)]( same failure on other branches - sql: testindexbackfillaftergc failed [c-test-failure o-robot p-3 t-sql-foundations branch-master] /cc /sql-foundations [this test on roachdash]( | [improve this report!]( jira issue: crdb-59063,[],['TESTING'],"['C-test-failure', 'O-robot', 'T-sql-foundations', 'P-2', 'branch-release-26.1']",github,2026-01-25T09:24:01Z,,sql: testindexbackfillaftergc failed sql.testindexbackfillaftergc [failed]( on release-26.1 @ [2dede1c13f1bf518511d790900cbd47e0557472f]( parameters: - attempt=1 - deadlock=true - run=1 - shard=6 help see also: [how to investigate a go test failure \(internal\)]( same failure on other branches - sql: testindexbackfillaftergc failed [c-test-failure o-robot p-3 t-sql-foundations branch-master] /cc /sql-foundations [this test on roachdash]( | [improve this report!]( jira issue: crdb-59063,3.8,Critical,1.0,crash-like behavior
pandas-dev/pandas#63863,enh: data entropy & information loss tracking for dataframe transformations,"### feature type - [x] adding new functionality to pandas - [ ] changing existing functionality in pandas - [ ] removing existing functionality in pandas ### problem description pandas does not provide any method to measure the information loss which occurs when dataframe transformations are applied. data resolution decreases permanently through common operations because users cannot observe this reduction which results from rounding and binning and aggregation and encoding and filtering and deduplication. users create data compression that exceeds proper limits which leads to the loss of valuable information without their awareness. i want pandas to provide users with information loss measurements which show how transformations affect data processing because this will help them choose preprocessing methods and identify excessive operations which will improve their data pipeline trust. ### feature description the system will start tracking operational data transformations through its mandatory tracking system which will record every dataframe transformation that occurs. the feature would: - the system will calculate entropy values at the column level using shannon entropy for discrete data and histogram-based entropy for continuous data. - the system will measure information loss through its normalized entropy difference calculation which compares two different states. - the system will track information loss from operations that occur within a monitoring environment. example api: with df.track_information() as info: df[""price""] = df[""price""].round(0) df[""age_group""] = pd.cut(df[""age""], bins=5) info.report() conceptual implementation def entropy(series): if is_discrete(series): p = value_counts(series) / len(series) return -sum(p * log2(p)) else: bins = adaptive_bins(series) p = histogram(series, bins) / len(series) return -sum(p * log2(p)) the system performs lazy entropy calculations which apply to specific columns that undergo changes. the system applies adaptive binning to continuous data in order to produce consistent estimation results. the system calculates dataset-level information loss by summing all column-specific information losses. the feature requires user activation because it creates no performance impact until the user decides to use it. the system enables users to monitor dataframe transformation impacts on information content while maintaining the original pandas functional behavior. ### alternative solutions multiple different methods exist as potential alternatives for evaluation. relying on documentation or user discipline users would need to decide how much information they lost through their transformation work. the current system proves unmanageable because it creates errors while users cannot track the total impact which develops throughout their different workflows. heuristic warnings for specific operations pandas should develop a system which generates alerts about dangerous functions which include binning and rounding as destructive actions. the system would operate at a broad level but its results would not show the actual information loss degree which occurs through a specific dataset. downstream model metrics (e.g., feature importance, performance) the model behavior permits users to infer information loss yet this method only works during the machine learning stage and it does not serve general data analysis or reporting purposes. external profiling or data quality tools existing tools examine missing data and data distribution patterns and schema validation processes, but they lack the ability to monitor how data transformations create information loss which they can trace back to particular processes. the available alternatives failed to meet requirements because they lacked the ability to show information loss which occurs during pandas operations at both direct and transformation-level contexts. ### additional context _no response_",[],['FEATURE'],"['Enhancement', 'Closing Candidate']",github,2026-01-25T09:58:56Z,2026-01-27T17:50:37Z,"enh: data entropy & information loss tracking for dataframe transformations ### feature type - [x] adding new functionality to pandas - [ ] changing existing functionality in pandas - [ ] removing existing functionality in pandas ### problem description pandas does not provide any method to measure the information loss which occurs when dataframe transformations are applied. data resolution decreases permanently through common operations because users cannot observe this reduction which results from rounding and binning and aggregation and encoding and filtering and deduplication. users create data compression that exceeds proper limits which leads to the loss of valuable information without their awareness. i want pandas to provide users with information loss measurements which show how transformations affect data processing because this will help them choose preprocessing methods and identify excessive operations which will improve their data pipeline trust. ### feature description the system will start tracking operational data transformations through its mandatory tracking system which will record every dataframe transformation that occurs. the feature would: - the system will calculate entropy values at the column level using shannon entropy for discrete data and histogram-based entropy for continuous data. - the system will measure information loss through its normalized entropy difference calculation which compares two different states. - the system will track information loss from operations that occur within a monitoring environment. example api: with df.track_information() as info: df[""price""] = df[""price""].round(0) df[""age_group""] = pd.cut(df[""age""], bins=5) info.report() conceptual implementation def entropy(series): if is_discrete(series): p = value_counts(series) / len(series) return -sum(p * log2(p)) else: bins = adaptive_bins(series) p = histogram(series, bins) / len(series) return -sum(p * log2(p)) the system performs lazy entropy calculations which apply to specific columns that undergo changes. the system applies adaptive binning to continuous data in order to produce consistent estimation results. the system calculates dataset-level information loss by summing all column-specific information losses. the feature requires user activation because it creates no performance impact until the user decides to use it. the system enables users to monitor dataframe transformation impacts on information content while maintaining the original pandas functional behavior. ### alternative solutions multiple different methods exist as potential alternatives for evaluation. relying on documentation or user discipline users would need to decide how much information they lost through their transformation work. the current system proves unmanageable because it creates errors while users cannot track the total impact which develops throughout their different workflows. heuristic warnings for specific operations pandas should develop a system which generates alerts about dangerous functions which include binning and rounding as destructive actions. the system would operate at a broad level but its results would not show the actual information loss degree which occurs through a specific dataset. downstream model metrics (e.g., feature importance, performance) the model behavior permits users to infer information loss yet this method only works during the machine learning stage and it does not serve general data analysis or reporting purposes. external profiling or data quality tools existing tools examine missing data and data distribution patterns and schema validation processes, but they lack the ability to monitor how data transformations create information loss which they can trace back to particular processes. the available alternatives failed to meet requirements because they lacked the ability to show information loss which occurs during pandas operations at both direct and transformation-level contexts. ### additional context _no response_",5.0,Critical,1.0,crash-like behavior
rust-lang/rust#151642,mgca: we don't properly check associated const bindings for well-formedness,uplifted from follow links for further details. credits to . --- example reproducers: rhs is a const param. rhs is a const param. rhs is a (normalizable) const projection (that doesn't contain any generic params).,[],['BUG'],"['T-compiler', 'C-bug', 'T-types', 'F-min_generic_const_args']",github,2026-01-25T12:20:38Z,,mgca: we don't properly check associated const bindings for well-formedness uplifted from follow links for further details. credits to . --- example reproducers: rhs is a const param. rhs is a const param. rhs is a (normalizable) const projection (that doesn't contain any generic params).,2.492,Medium,0.786,functional impact
rust-lang/rust#151643,rustc doesn't always output color,"zulip discussion: [#t-compiler > rustc & ;--color always& ; doesn't always color output]( while working on , i discovered that some parts of that use flag don't always output color when the flag is set to . ## relevant examples: the check for terminal support should only occur when the option is set to . there may be more examples for this as well. these are just the two that i could find.",[],['BUG'],"['T-compiler', 'C-bug']",github,2026-01-25T13:27:25Z,,"rustc doesn't always output color zulip discussion: [#t-compiler > rustc & ;--color always& ; doesn't always color output]( while working on , i discovered that some parts of that use flag don't always output color when the flag is set to . ## relevant examples: the check for terminal support should only occur when the option is set to . there may be more examples for this as well. these are just the two that i could find.",2.256,Medium,0.733,functional impact
docker/docker#51924,docker 29.x streaming api responses contain null bytes between json documents,"## description docker engine 29.x appears to include null bytes (0x00) between json documents in streaming api responses. this causes json parsing failures in client libraries that strictly validate utf-8/json, including system.text.json (.net) and potentially others. ## affected endpoints - (image pulling) - - - ## error examples ## expected behavior streaming json responses should contain only valid json documents separated by newlines (ndjson format), with no null bytes. ## actual behavior responses include null bytes (0x00) between json documents, which violates rfc 8259 (json must be valid utf-8, and null bytes are not valid within json text). ## environment - docker engine: 29.x - affected clients: docker.dotnet, potentially others using strict json parsers ## references - [testcontainers/docker.dotnet ]( - related historical issue: [moby/moby ]( (streaming json format)",[],['BUG'],"['status/0-triage', 'kind/bug']",github,2026-01-25T13:35:19Z,,"docker 29.x streaming api responses contain null bytes between json documents ## description docker engine 29.x appears to include null bytes (0x00) between json documents in streaming api responses. this causes json parsing failures in client libraries that strictly validate utf-8/json, including system.text.json (.net) and potentially others. ## affected endpoints - (image pulling) - - - ## error examples ## expected behavior streaming json responses should contain only valid json documents separated by newlines (ndjson format), with no null bytes. ## actual behavior responses include null bytes (0x00) between json documents, which violates rfc 8259 (json must be valid utf-8, and null bytes are not valid within json text). ## environment - docker engine: 29.x - affected clients: docker.dotnet, potentially others using strict json parsers ## references - [testcontainers/docker.dotnet ]( - related historical issue: [moby/moby ]( (streaming json format)",2.611,Medium,0.813,functional impact
facebook/react#35622,[compiler bug]: useref - error: this value cannot be modified,"### what kind of issue is this? - [ ] react compiler core (the js output is incorrect, or your app works incorrectly after optimization) - [ ] babel-plugin-react-compiler (build issue installing or using the babel plugin) - [ ] eslint-plugin-react-hooks (build issue installing or using the eslint plugin) - [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script) ### link to repro ### repro steps look the playground **note**: even if the error is triggered for line (body of the ) , removing line make the compiler happy. it's weird for me. --- code error --- ### how often does this bug happen? every time ### what version of react are you using? 19.2.3 ### what version of react compiler are you using? ?",[],['BUG'],"['Type: Bug', 'Status: Unconfirmed']",github,2026-01-25T13:42:21Z,2026-01-26T05:10:04Z,"[compiler bug]: useref - error: this value cannot be modified ### what kind of issue is this? - [ ] react compiler core (the js output is incorrect, or your app works incorrectly after optimization) - [ ] babel-plugin-react-compiler (build issue installing or using the babel plugin) - [ ] eslint-plugin-react-hooks (build issue installing or using the eslint plugin) - [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script) ### link to repro ### repro steps look the playground **note**: even if the error is triggered for line (body of the ) , removing line make the compiler happy. it's weird for me. --- code error --- ### how often does this bug happen? every time ### what version of react are you using? 19.2.3 ### what version of react compiler are you using? ?",2.598,Medium,0.811,functional impact
cockroachdb/cockroach#161760,pkg/sql/sqlstats/insights/integration/integration_test: testinsightspriorityintegration failed,pkg/sql/sqlstats/insights/integration/integration_test.testinsightspriorityintegration [failed]( on release-26.1.0-rc @ [6c9b2f778bd29895383d493a2fa4490323499ecf]( parameters: - attempt=1 - race=true - run=2 - shard=1 help see also: [how to investigate a go test failure \(internal\)]( same failure on other branches - pkg/sql/sqlstats/insights/integration/integration_test: testinsightspriorityintegration failed [c-test-failure o-robot t-observability branch-release-25.4.3-rc release-blocker] /cc /obs-prs [this test on roachdash]( | [improve this report!]( jira issue: crdb-59064,[],['TESTING'],"['C-test-failure', 'O-robot', 'T-observability', 'branch-release-26.1.0-rc']",github,2026-01-25T13:55:43Z,,pkg/sql/sqlstats/insights/integration/integration_test: testinsightspriorityintegration failed pkg/sql/sqlstats/insights/integration/integration_test.testinsightspriorityintegration [failed]( on release-26.1.0-rc @ [6c9b2f778bd29895383d493a2fa4490323499ecf]( parameters: - attempt=1 - race=true - run=2 - shard=1 help see also: [how to investigate a go test failure \(internal\)]( same failure on other branches - pkg/sql/sqlstats/insights/integration/integration_test: testinsightspriorityintegration failed [c-test-failure o-robot t-observability branch-release-25.4.3-rc release-blocker] /cc /obs-prs [this test on roachdash]( | [improve this report!]( jira issue: crdb-59064,1.6,Low,0.584,localized low-impact
rust-lang/rust#151644,atomic missing wait and notify_* while c++20 supports that,,[],['FEATURE'],"['A-concurrency', 'T-libs-api', 'C-feature-request', 'A-atomic']",github,2026-01-25T14:28:06Z,2026-01-25T19:36:19Z,atomic missing wait and notify_* while c++20 supports that,1.4,Low,0.538,localized low-impact
microsoft/vscode#290266,need a way to inspect every tool call to understand how the llm operates / what bugs the tools have,"the current log view is not sufficient. this shouln't be for debugging, but for understanding the tools and getting a feeling for how to improve the tools. for example, github mcp tools generally perform poorly, because they overload the llm with unnecessary metadata (e.g. when querying an issue, it returns ~10 urls for one issue about everything).",[],['FEATURE'],['feature-request'],github,2026-01-25T14:53:17Z,,"need a way to inspect every tool call to understand how the llm operates / what bugs the tools have the current log view is not sufficient. this shouln't be for debugging, but for understanding the tools and getting a feeling for how to improve the tools. for example, github mcp tools generally perform poorly, because they overload the llm with unnecessary metadata (e.g. when querying an issue, it returns ~10 urls for one issue about everything).",1.4,Low,0.538,localized low-impact
python/cpython#144220,fails on bcachefs case insensitive directories,# bug report ### bug description: on bcachefs create case insensitive directory with some file inside: then do this fails with this is because of happening after contents were copied and case insensitivity can't be enabled afterwards. note: on ext4 copytree results in a directory without casefolding flag. ### cpython versions tested on: 3.14 ### operating systems tested on: linux,[],['BUG'],"['type-bug', 'stdlib']",github,2026-01-25T15:04:31Z,,fails on bcachefs case insensitive directories # bug report ### bug description: on bcachefs create case insensitive directory with some file inside: then do this fails with this is because of happening after contents were copied and case insensitivity can't be enabled afterwards. note: on ext4 copytree results in a directory without casefolding flag. ### cpython versions tested on: 3.14 ### operating systems tested on: linux,2.308,Medium,0.745,functional impact
python/cpython#144223,"refactor compileall: improve docstrings, add type hints, comments, and tests","# feature or enhancement ### proposal: ## proposal: refactor compileall module this proposal aims to improve the module with: - more informative docstrings for all public functions - better inline comments explaining complex logic - type hints for function signatures - minor pep8 formatting fixes - additional unit tests for new helper functions the module is currently not very well documented, which makes it harder for both python users and contributors to understand and maintain. improving docstrings, comments, and typing will make the module more accessible and maintainable. **no logic changes are intended this is a pure refactoring and documentation improvement.** feedback welcome ‚Äî happy to iterate and split this into smaller prs if needed! ### has this already been discussed elsewhere? this is a minor feature, which does not need previous discussion elsewhere ### linked prs * gh-144224",[],['CLEANUP'],"['stdlib', 'pending', 'type-refactor']",github,2026-01-25T15:41:59Z,2026-01-26T21:40:48Z,"refactor compileall: improve docstrings, add type hints, comments, and tests # feature or enhancement ### proposal: ## proposal: refactor compileall module this proposal aims to improve the module with: - more informative docstrings for all public functions - better inline comments explaining complex logic - type hints for function signatures - minor pep8 formatting fixes - additional unit tests for new helper functions the module is currently not very well documented, which makes it harder for both python users and contributors to understand and maintain. improving docstrings, comments, and typing will make the module more accessible and maintainable. **no logic changes are intended this is a pure refactoring and documentation improvement.** feedback welcome ‚Äî happy to iterate and split this into smaller prs if needed! ### has this already been discussed elsewhere? this is a minor feature, which does not need previous discussion elsewhere ### linked prs * gh-144224",3.4,High,0.993,crash-like behavior
prometheus/prometheus#17931,promql: info() function incorrectly handles negated __name__ matchers,"### bug description the function incorrectly handles negated matchers in the second argument. when using or , base metric series are incorrectly added to the set and returned unchanged instead of being filtered out. ### steps to reproduce ### expected behavior since no info metrics match (all info metrics end in ), and the matcher doesn't match the empty string, the query should return no series. ### actual behavior the query returns series unchanged (without any info labels added): ### root cause in (lines 50-60), series are added to if their name matches any : the intent is ""don't enrich info metrics with themselves"". however, with negated matchers like : 1. does not match the pattern 2. the negated matcher therefore returns 3. is incorrectly added to 4. series in are returned unchanged (lines 372-378) ### suggested fix the logic should only consider positive matchers ( and ), or the matching logic needs to be inverted for the purpose of determining which series to skip.",[],['BUG'],"['kind/bug', 'component/promql']",github,2026-01-25T15:58:32Z,,"promql: info() function incorrectly handles negated __name__ matchers ### bug description the function incorrectly handles negated matchers in the second argument. when using or , base metric series are incorrectly added to the set and returned unchanged instead of being filtered out. ### steps to reproduce ### expected behavior since no info metrics match (all info metrics end in ), and the matcher doesn't match the empty string, the query should return no series. ### actual behavior the query returns series unchanged (without any info labels added): ### root cause in (lines 50-60), series are added to if their name matches any : the intent is ""don't enrich info metrics with themselves"". however, with negated matchers like : 1. does not match the pattern 2. the negated matcher therefore returns 3. is incorrectly added to 4. series in are returned unchanged (lines 372-378) ### suggested fix the logic should only consider positive matchers ( and ), or the matching logic needs to be inverted for the purpose of determining which series to skip.",4.6,Critical,1.0,crash-like behavior
rust-lang/rust#151647,ice: mgca+gci: broken mir: equate_normalized_input_or_output: nosolution,found while investigating an unrelated mgca issue. ---,[],['BUG'],"['I-ICE', 'T-compiler', 'C-bug', 'S-has-mcve', 'F-generic_const_items', 'F-min_generic_const_args']",github,2026-01-25T16:13:44Z,,ice: mgca+gci: broken mir: equate_normalized_input_or_output: nosolution found while investigating an unrelated mgca issue. ---,2.467,Medium,0.781,functional impact
microsoft/vscode#290270,add 2 more buttons to improve the toggle of panel sizes,"type: feature request currently there are only two buttons in the vs code panel to toggle size: either maximize panel or hide panel. but with the increase usage of command line coding tools (e.g. github copilot cli, et al.) there is more use of the panel in vs code to access the terminal than ever before. so i propose adding 2 more toggle buttons to make the panel more effectively used: minimize and restore. with that change there would 4 total toggles for panel size: minimize, hide, restore, and maximize. this approach is already implemented in a vs code fork called positron ( for which i've made screenshots of and are attached to this issue. please see that tool for an idea of how well that works. summary: increasing the options for toggle of panel sizes in vs code will go a long way toward improving the ui towards easier leveraging of agent cli coding tools like github copilot cli. vs code version: code 1.108.2 (c9d77990917f3102ada88be140d28b038d1dd7c7, 2026-01-21t13:52:09.270z) os version: windows_nt x64 10.0.26200 modes: remote os version: linux x64 5.15.167.4-microsoft-standard-wsl2 [positron terminal buttons to vs code team.pdf](",[],"['FEATURE', 'UI']","['feature-request', 'terminal', 'layout']",github,2026-01-25T16:24:41Z,,"add 2 more buttons to improve the toggle of panel sizes type: feature request currently there are only two buttons in the vs code panel to toggle size: either maximize panel or hide panel. but with the increase usage of command line coding tools (e.g. github copilot cli, et al.) there is more use of the panel in vs code to access the terminal than ever before. so i propose adding 2 more toggle buttons to make the panel more effectively used: minimize and restore. with that change there would 4 total toggles for panel size: minimize, hide, restore, and maximize. this approach is already implemented in a vs code fork called positron ( for which i've made screenshots of and are attached to this issue. please see that tool for an idea of how well that works. summary: increasing the options for toggle of panel sizes in vs code will go a long way toward improving the ui towards easier leveraging of agent cli coding tools like github copilot cli. vs code version: code 1.108.2 (c9d77990917f3102ada88be140d28b038d1dd7c7, 2026-01-21t13:52:09.270z) os version: windows_nt x64 10.0.26200 modes: remote os version: linux x64 5.15.167.4-microsoft-standard-wsl2 [positron terminal buttons to vs code team.pdf](",2.969,Medium,0.895,"user-visible issue, crash-like behavior"
pytorch/pytorch#173318,[docs] link broken on 2.10+,### üìö the doc issue docs do not work after 2.9.1: - [working] - [not working] - [not working] first link in google points me to which is one of the non-working links. ### suggest a potential alternative/fix make the link work or make it not possible to route to the broken link cc,[],['DOCUMENTATION'],"['high priority', 'module: docs', 'triaged', 'module: regression']",github,2026-01-25T17:01:42Z,,[docs] link broken on 2.10+ ### üìö the doc issue docs do not work after 2.9.1: - [working] - [not working] - [not working] first link in google points me to which is one of the non-working links. ### suggest a potential alternative/fix make the link work or make it not possible to route to the broken link cc,1.2,Low,0.493,localized low-impact
rust-lang/rust#151648,[ice]:,"<!-- [31mice[0m: rustc ./4f1eba110fd32311c040ef9dafe6dd33e8db11f5be29df542c2ea99a15dc200d.rs '-znext-solver=globally -ooutputfile -zdump-mir-dir=dir' 'error: internal compiler error: compiler/rustc_trait_selection/src/solve/fulfill/derive_errors.rs:121:17: did not expect successful goal when collecting ambiguity errors for ', 'error: internal compiler error: compiler/rustc_trait_selection/src/solve/fulfill/derive_errors.rs:121:17: did not expect successful goal when collecting ambiguity errors for ' file: /tmp/im/4f1eba110fd32311c040ef9dafe6dd33e8db11f5be29df542c2ea99a15dc200d.rs --> auto-reduced (treereduce-rust): ` original: ` version information ` possibly related line of code: command: program output <!-- query stack: error: internal compiler error: /rustc-dev/9415853279dda1394c1f7d4114e1fe1e3ced76ec/compiler/rustc_trait_selection/src/solve/fulfill/derive_errors.rs:121:17: did not expect successful goal when collecting ambiguity errors for [typeck] type-checking [analysis] running analysis passes on crate --> label +wg-trait-system-refactor",[],"['CLEANUP', 'BUG']","['I-ICE', 'T-compiler', 'C-bug', 'WG-trait-system-refactor', 'needs-triage']",github,2026-01-25T17:29:59Z,,"[ice]: <!-- [31mice[0m: rustc ./4f1eba110fd32311c040ef9dafe6dd33e8db11f5be29df542c2ea99a15dc200d.rs '-znext-solver=globally -ooutputfile -zdump-mir-dir=dir' 'error: internal compiler error: compiler/rustc_trait_selection/src/solve/fulfill/derive_errors.rs:121:17: did not expect successful goal when collecting ambiguity errors for ', 'error: internal compiler error: compiler/rustc_trait_selection/src/solve/fulfill/derive_errors.rs:121:17: did not expect successful goal when collecting ambiguity errors for ' file: /tmp/im/4f1eba110fd32311c040ef9dafe6dd33e8db11f5be29df542c2ea99a15dc200d.rs --> auto-reduced (treereduce-rust): ` original: ` version information ` possibly related line of code: command: program output <!-- query stack: error: internal compiler error: /rustc-dev/9415853279dda1394c1f7d4114e1fe1e3ced76ec/compiler/rustc_trait_selection/src/solve/fulfill/derive_errors.rs:121:17: did not expect successful goal when collecting ambiguity errors for [typeck] type-checking [analysis] running analysis passes on crate --> label +wg-trait-system-refactor",1.8,Low,0.629,localized low-impact
cockroachdb/cockroach#161761,roachtest: import/concurrency/distmerge=false/nodes=4 failed,roachtest.import/concurrency/distmerge=false/nodes=4 [failed]( with [artifacts]( on master @ [36e5eb89571f7eee1b489ab407b1ea06629ca26b]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21020866-1769325003-08-n4cpu4-0001 | 34.23.139.55 | 10.142.2.199 | | teamcity-21020866-1769325003-08-n4cpu4-0002 | 35.243.142.228 | 10.142.2.201 | | teamcity-21020866-1769325003-08-n4cpu4-0003 | 34.148.244.134 | 10.142.2.200 | | teamcity-21020866-1769325003-08-n4cpu4-0004 | 35.231.7.212 | 10.142.0.150 | parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=true - metamorphicleases=default - metamorphicwritebuffering=true - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( [this test on roachdash]( | [improve this report!]( jira issue: crdb-59065,[],['TESTING'],"['C-test-failure', 'O-robot', 'O-roachtest', 'branch-master', 'release-blocker', 'T-sql-queries']",github,2026-01-25T17:36:54Z,2026-01-27T19:38:22Z,roachtest: import/concurrency/distmerge=false/nodes=4 failed roachtest.import/concurrency/distmerge=false/nodes=4 [failed]( with [artifacts]( on master @ [36e5eb89571f7eee1b489ab407b1ea06629ca26b]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21020866-1769325003-08-n4cpu4-0001 | 34.23.139.55 | 10.142.2.199 | | teamcity-21020866-1769325003-08-n4cpu4-0002 | 35.243.142.228 | 10.142.2.201 | | teamcity-21020866-1769325003-08-n4cpu4-0003 | 34.148.244.134 | 10.142.2.200 | | teamcity-21020866-1769325003-08-n4cpu4-0004 | 35.231.7.212 | 10.142.0.150 | parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=true - metamorphicleases=default - metamorphicwritebuffering=true - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( [this test on roachdash]( | [improve this report!]( jira issue: crdb-59065,3.268,High,0.963,system-wide impact
rust-lang/rust#151649,mgca: assoc const projections don't induce ambient object lifetime defaults,this is the assoc const version of (assoc tys). blocking this on pr which provides fundamental fixes and the necessary framework. --- the following snippets should all compile but don't: #### reproducer [1/4] #### reproducer [2/4] #### reproducer [3/4] (with gac under gci) in 's signature currently elaborates to but it should elaborate to since (resolved) assoc consts should be *eligible generic containers* and induce ambient object lifetime defaults to its constituent types. #### reproducer [4/4] (with gac under gci),[],['BUG'],"['A-lifetimes', 'T-compiler', 'C-bug', 'S-blocked', 'T-types', 'F-generic_const_items', 'A-dyn-trait', 'F-min_generic_const_args']",github,2026-01-25T17:52:25Z,,mgca: assoc const projections don't induce ambient object lifetime defaults this is the assoc const version of (assoc tys). blocking this on pr which provides fundamental fixes and the necessary framework. --- the following snippets should all compile but don't: #### reproducer [1/4] #### reproducer [2/4] #### reproducer [3/4] (with gac under gci) in 's signature currently elaborates to but it should elaborate to since (resolved) assoc consts should be *eligible generic containers* and induce ambient object lifetime defaults to its constituent types. #### reproducer [4/4] (with gac under gci),2.203,Medium,0.721,functional impact
rust-lang/rust#151652,tracking issue for release notes of : enable by default on more aarch64 platforms,"this issue tracks the release notes text for . cc , -- original issue/pr authors and assignees for drafting text see the forge.rust-lang.org chapter about [release notes]( for an overview of how the release team makes use of these tracking issues. ### release notes text this section should be edited to specify the correct category(s) for the change, with succinct description(s) of what changed. some things worth considering: - does this need an additional compat notes section? - was this a libs stabilization that should have additional headers to list new apis under and ? ` > [!tip] > use the [previous releases]( for inspiration on how to write the release notes text and which categories to pick. ### release blog section if this change is notable enough for inclusion in the blog post then this section should be edited to contain a draft for the blog post. *otherwise leave it empty.* ` > [!note] > > if a blog post section is required the label should be added ( ) to this issue as otherwise it may be missed by the release team.",[],['UI'],"['T-compiler', 'relnotes', 'T-libs', 'relnotes-tracking-issue', 'A-compiler-builtins']",github,2026-01-25T18:13:11Z,,"tracking issue for release notes of : enable by default on more aarch64 platforms this issue tracks the release notes text for . cc , -- original issue/pr authors and assignees for drafting text see the forge.rust-lang.org chapter about [release notes]( for an overview of how the release team makes use of these tracking issues. ### release notes text this section should be edited to specify the correct category(s) for the change, with succinct description(s) of what changed. some things worth considering: - does this need an additional compat notes section? - was this a libs stabilization that should have additional headers to list new apis under and ? ` > [!tip] > use the [previous releases]( for inspiration on how to write the release notes text and which categories to pick. ### release blog section if this change is notable enough for inclusion in the blog post then this section should be edited to contain a draft for the blog post. *otherwise leave it empty.* ` > [!note] > > if a blog post section is required the label should be added ( ) to this issue as otherwise it may be missed by the release team.",4.0,Critical,1.0,"user-visible issue, crash-like behavior"
microsoft/vscode#290280,mcp tool editors are not layed out correctly at first,"- have a chat view which is somewhat wide - expand an mcp tool - the editors are wrapped at about 300px, when the chat view is wider than this",[],['BUG'],"['bug', 'insiders-released', 'chat']",github,2026-01-25T18:59:00Z,2026-01-25T19:19:59Z,"mcp tool editors are not layed out correctly at first - have a chat view which is somewhat wide - expand an mcp tool - the editors are wrapped at about 300px, when the chat view is wider than this",2.608,Medium,0.813,functional impact
microsoft/vscode#290284,rendering image output on tool calls is slow,"- have some tool calls with image output in a past chat response- eg from playwright mcp, or from the mcp everything server - take a perf profile, scroll past them - see slow decodebase64 calls we can lazy-render these to avoid rendering pauses",[],"['PERFORMANCE', 'BUG']","['bug', 'perf', 'insiders-released', 'chat']",github,2026-01-25T19:30:42Z,2026-01-25T19:54:30Z,"rendering image output on tool calls is slow - have some tool calls with image output in a past chat response- eg from playwright mcp, or from the mcp everything server - take a perf profile, scroll past them - see slow decodebase64 calls we can lazy-render these to avoid rendering pauses",3.5,High,1.0,performance degradation
microsoft/vscode#290288,chat input scroll shadow seems reversed,"the top shadow looks correct but i think the bottom should go away when we are scrolled all the way down, and still be visible when scrolled up? i see this was added in is this right or should the logic be reversed of the top shadow? also i don't see this shadow in the scm editor so i was curious where you copied it from",[],['BUG'],"['bug', 'insiders-released', 'chat-input']",github,2026-01-25T19:41:44Z,2026-01-26T09:44:51Z,"chat input scroll shadow seems reversed the top shadow looks correct but i think the bottom should go away when we are scrolled all the way down, and still be visible when scrolled up? i see this was added in is this right or should the logic be reversed of the top shadow? also i don't see this shadow in the scm editor so i was curious where you copied it from",2.226,Medium,0.726,functional impact
microsoft/vscode#290290,agent skill: allow users to enable/disable specific skills,"useful to quickly install more skills and explore them, but disabling them when they get in the way. we already have this for extensions, for user-facing custom agents; but not for any of the agent-controlled primitives. claude's list has a toggle to enable/disable:",[],['FEATURE'],"['feature-request', 'chat-prompts']",github,2026-01-25T20:11:45Z,,"agent skill: allow users to enable/disable specific skills useful to quickly install more skills and explore them, but disabling them when they get in the way. we already have this for extensions, for user-facing custom agents; but not for any of the agent-controlled primitives. claude's list has a toggle to enable/disable:",1.4,Low,0.538,localized low-impact
envoyproxy/envoy#43150,[feature request] health aware weighted clusters: do not route to unhealthy clusters,"*title*: *health aware weighted cluster routing* *description*: weighted clusters are frequently used to route to remote ingress endpoints which tend to be single vips. if one of the clusters is ""unhealthy"" traffic is still sent to the cluster even though it has no healthy endpoints. for the above use case this is not the expected or desired behaviour. so potentially this is a *bug*. **api change**. add a bool to weighted cluster: do not route to a cluster with no healthy endpoints. *implementation plan* thread thru and update to also take and then something like this.",[],['FEATURE'],"['enhancement', 'area/upstream', 'help wanted']",github,2026-01-25T21:22:14Z,,"[feature request] health aware weighted clusters: do not route to unhealthy clusters *title*: *health aware weighted cluster routing* *description*: weighted clusters are frequently used to route to remote ingress endpoints which tend to be single vips. if one of the clusters is ""unhealthy"" traffic is still sent to the cluster even though it has no healthy endpoints. for the above use case this is not the expected or desired behaviour. so potentially this is a *bug*. **api change**. add a bool to weighted cluster: do not route to a cluster with no healthy endpoints. *implementation plan* thread thru and update to also take and then something like this.",3.6,Critical,1.0,crash-like behavior
pandas-dev/pandas#63876,bug: attributeerror: 'arrowstringarray' object has no attribute 'item',"### pandas version checks - [x] i have checked that this issue has not already been reported. - [x] i have confirmed this bug exists on the [latest version]( of pandas. - [x] i have confirmed this bug exists on the [main branch]( of pandas. ### reproducible example ### issue description in pandas 2.3.3 and earlier worked as a way to get a single unique value that appeared in a series ‚Äîor dataframe column, or as the result of other operation that resulted in a series. this was concise and preferable to, for instance, because it would raise an exception if earlier code produced a *non*-unique series. with pandas 3.0.0, when the dtype of the series is , the result of is arrowstringarray, which [lacks an method]( if this is a deliberate breaking change, then i would expect that (a) it would be mentioned in [the relevant section of the release notes]( and/or (b) version < 3.0.0 would give some kind of deprecationwarning to draw attention to expected breakage with the release of 3.0.0. as far as i can tell, neither of these happened. ### expected behavior the example above gives on the last line with pandas 3.0.0‚Äîsame as with pandas 2.3.3. ### installed versions",[],['BUG'],"['Bug', 'Strings', 'Needs Discussion', 'ExtensionArray']",github,2026-01-25T21:49:16Z,,"bug: attributeerror: 'arrowstringarray' object has no attribute 'item' ### pandas version checks - [x] i have checked that this issue has not already been reported. - [x] i have confirmed this bug exists on the [latest version]( of pandas. - [x] i have confirmed this bug exists on the [main branch]( of pandas. ### reproducible example ### issue description in pandas 2.3.3 and earlier worked as a way to get a single unique value that appeared in a series ‚Äîor dataframe column, or as the result of other operation that resulted in a series. this was concise and preferable to, for instance, because it would raise an exception if earlier code produced a *non*-unique series. with pandas 3.0.0, when the dtype of the series is , the result of is arrowstringarray, which [lacks an method]( if this is a deliberate breaking change, then i would expect that (a) it would be mentioned in [the relevant section of the release notes]( and/or (b) version < 3.0.0 would give some kind of deprecationwarning to draw attention to expected breakage with the release of 3.0.0. as far as i can tell, neither of these happened. ### expected behavior the example above gives on the last line with pandas 3.0.0‚Äîsame as with pandas 2.3.3. ### installed versions",4.6,Critical,1.0,crash-like behavior
rust-lang/rust#151658,ice in in rust 1.93: unwrap on,as noted in is now panicking on bevy's source code. ### code ### version it worked on v1.92 ### version with regression v1.93 ### backtrace,[],['BUG'],"['I-ICE', 'T-compiler', 'regression-from-stable-to-stable', 'C-bug']",github,2026-01-25T22:21:04Z,2026-01-25T23:56:39Z,ice in in rust 1.93: unwrap on as noted in is now panicking on bevy's source code. ### code ### version it worked on v1.92 ### version with regression v1.93 ### backtrace,6.4,Critical,1.0,crash-like behavior
microsoft/vscode#290296,tool selection quickpick dissapears on scroll,- copilot chat extension version: 0.37.2026012305 - vs code version: dfb16008e8bbec7b994c49c6865d4594fa5030ad - os version: macos steps to repro: 1. click the tool selection icon in the chat window. 2. de-select a random tool in the top of the quickpick. 3. scroll down in the quickpick to try to find other tools to deselect problem: the quickpick suddenly dissapears and won't let me save or select/de-select.,[],['BUG'],['bug'],github,2026-01-25T22:24:35Z,,tool selection quickpick dissapears on scroll - copilot chat extension version: 0.37.2026012305 - vs code version: dfb16008e8bbec7b994c49c6865d4594fa5030ad - os version: macos steps to repro: 1. click the tool selection icon in the chat window. 2. de-select a random tool in the top of the quickpick. 3. scroll down in the quickpick to try to find other tools to deselect problem: the quickpick suddenly dissapears and won't let me save or select/de-select.,2.236,Medium,0.728,functional impact
flutter/flutter#181471,[go_router_builder] add support for onenter introduced on [go_router] 16.3.0,"**package** go_router_builder **problem** go_router 16.3.0 introduced onenter callback for navigation guards, but go_router_builder doesn't support defining route-level onenter in goroutedata classes (unlike redirect and onexit). ### proposal",[],"['FEATURE', 'UI']","['c: new feature', 'package', 'c: proposal', 'P2', 'p: go_router_builder', 'team-framework', 'triaged-framework']",github,2026-01-25T22:48:12Z,,"[go_router_builder] add support for onenter introduced on [go_router] 16.3.0 **package** go_router_builder **problem** go_router 16.3.0 introduced onenter callback for navigation guards, but go_router_builder doesn't support defining route-level onenter in goroutedata classes (unlike redirect and onexit). ### proposal",1.6,Low,0.584,user-visible issue
rust-lang/rust#151662,rtn paths don't induce ambient object lifetime defaults,this is the rtn version of (assoc consts) and (assoc tys). blocking this on pr which provides fundamental fixes and the necessary framework. --- the following snippet should compile but it doesn't: in the rtn path should get elaborated to but it gets elaborated to instead because associated functions aren't considered *eligible generic containers*.,[],['BUG'],"['A-lifetimes', 'T-compiler', 'C-bug', 'S-blocked', 'T-types', 'F-return_type_notation', 'A-dyn-trait']",github,2026-01-25T23:35:55Z,,rtn paths don't induce ambient object lifetime defaults this is the rtn version of (assoc consts) and (assoc tys). blocking this on pr which provides fundamental fixes and the necessary framework. --- the following snippet should compile but it doesn't: in the rtn path should get elaborated to but it gets elaborated to instead because associated functions aren't considered *eligible generic containers*.,2.651,Medium,0.823,functional impact
microsoft/vscode#290300,code space not loading,"code space won't load it keeps saying it's currently in recovery mode. i have tried different computers and browsers. version: 1.108.2 commit: c9d77990917f3102ada88be140d28b038d1dd7c7 user agent: mozilla/5.0 (windows nt 10.0; win64; x64) applewebkit/537.36 (khtml, like gecko) chrome/144.0.0.0 safari/537.36 embedder: codespaces",[],['BUG'],"['bug', '*caused-by-extension', 'github-codespaces', 'triage-needed']",github,2026-01-25T23:42:09Z,2026-01-26T22:06:35Z,"code space not loading code space won't load it keeps saying it's currently in recovery mode. i have tried different computers and browsers. version: 1.108.2 commit: c9d77990917f3102ada88be140d28b038d1dd7c7 user agent: mozilla/5.0 (windows nt 10.0; win64; x64) applewebkit/537.36 (khtml, like gecko) chrome/144.0.0.0 safari/537.36 embedder: codespaces",2.297,Medium,0.742,functional impact
microsoft/vscode#290302,sticky scroll shouldn't show for popular agent repls in terminal,,[],"['FEATURE', 'UI']","['feature-request', 'verified', 'verification-needed', 'insiders-released', 'terminal-sticky-scroll']",github,2026-01-25T23:53:42Z,2026-01-26T02:29:35Z,sticky scroll shouldn't show for popular agent repls in terminal,1.6,Low,0.584,user-visible issue
microsoft/vscode#290307,mark all sessions as read,"i love the new ui and updates in insiders, but add a ""mark all as read"" as i have a bunch of cli stuff that i already read and don't need.",[],['FEATURE'],"['feature-request', 'info-needed']",github,2026-01-26T01:55:12Z,2026-01-28T11:17:58Z,"mark all sessions as read i love the new ui and updates in insiders, but add a ""mark all as read"" as i have a bunch of cli stuff that i already read and don't need.",1.4,Low,0.538,localized low-impact
cockroachdb/cockroach#161764,kv/kvserver: teststorerangemergeabandonedfollowersautomaticallygarbagecollected failed,kv/kvserver.teststorerangemergeabandonedfollowersautomaticallygarbagecollected [failed]( with [artifacts]( on master @ [867004a4311025b7e0de44bc8c72fcbc8442f041]( help see also: [how to investigate a go test failure \(internal\)]( /cc /kv-triage [this test on roachdash]( | [improve this report!]( jira issue: crdb-59066,[],['TESTING'],"['C-test-failure', 'O-robot', 'X-unactionable', 'branch-master', 'T-kv', 'P-3']",github,2026-01-26T02:17:29Z,,kv/kvserver: teststorerangemergeabandonedfollowersautomaticallygarbagecollected failed kv/kvserver.teststorerangemergeabandonedfollowersautomaticallygarbagecollected [failed]( with [artifacts]( on master @ [867004a4311025b7e0de44bc8c72fcbc8442f041]( help see also: [how to investigate a go test failure \(internal\)]( /cc /kv-triage [this test on roachdash]( | [improve this report!]( jira issue: crdb-59066,1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161766,pkg/bench/rttanalysis: benchmarkvirtualtablequeries failed,pkg/bench/rttanalysis.benchmarkvirtualtablequeries [failed]( with [artifacts]( on master @ [867004a4311025b7e0de44bc8c72fcbc8442f041]( help see also: [how to investigate a go test failure \(internal\)]( /cc /sql-foundations [this test on roachdash]( | [improve this report!]( jira issue: crdb-59067,[],['TESTING'],"['C-test-failure', 'O-robot', 'branch-master', 'T-sql-foundations', 'P-2', 'O-microbench']",github,2026-01-26T02:48:00Z,,pkg/bench/rttanalysis: benchmarkvirtualtablequeries failed pkg/bench/rttanalysis.benchmarkvirtualtablequeries [failed]( with [artifacts]( on master @ [867004a4311025b7e0de44bc8c72fcbc8442f041]( help see also: [how to investigate a go test failure \(internal\)]( /cc /sql-foundations [this test on roachdash]( | [improve this report!]( jira issue: crdb-59067,1.6,Low,0.584,localized low-impact
python/cpython#144228,consider removing some unused build flags,"## description tripped up by in our configure file while debugging gh-142353, i was curious if we have any other unused build flags. removing them could slightly shorten preprocessing work, reduce cognitive load when debugging platform‚Äëspecific issues, and reduce the risk of accidentally relying on dead configuration paths. after running , i put together this script with [ripgrep]( then i ran this command: i got the following results: expand results these macros are not referenced by any cpython source files outside configuration headers ( , and files) and are not part of the documented public api. a few of them are actually used and cannot be removed. most of them, however, weren't used in years (or decades) and appear to be safely removable, because they no longer affect the behavior of python. here's what i found about every single one of them. ### pc/pyconfig.h and pyconfig.h - [ ] , unused since 58395759b04273edccf3d199606088e0703ae6b1. ### pc/pyconfig.h - [ ] ( this is commented out: but we already have for that: which is used. let's remove the comment. - [ ] , ( superseded by and since 1997, see 1171ee6aaf55fbb7f314a36798c0835ea0900ce9. removed from in ab70e2ab3264c444ac70a1c98e869b0de0b22c4a (when os/2 support was dropped). - [ ] ( unused since ab70e2ab3264c444ac70a1c98e869b0de0b22c4a (when os/2 support was dropped). - [ ] ( last occurences removed in ab70e2ab3264c444ac70a1c98e869b0de0b22c4a as well. this was meant to be removed in 2008 according to the description of ada8c3b046aa6f3684cbc32a4a140a38c204c050: - [ ] i'm not sure if it was or was not ever used, because i haven't bisected this. seems unused at the moment. first added in 61fe8619fb084a6cb3ac699609ad772a801b8195. - [ ] ( unused since b8d1262e8afe7b907b4a394a191739571092acdb (when and became universally available). - [ ] ( unused since 39258d3595300bc7b952854c915f63ae2d4b9c3e (when openssl was bumped to 1.1.1). - [ ] no longer used since 11fc411f98a04947a2a21329c29fe0f35ff52dba. - [ ] obsolete since 1997, see b06df27843611d9b1a9988da7aacf01073a1fbc4. the presence of this flag was very confusing to me when i was debugging gh-142353. ### pyconfig.h - [ ] , , , , unused since fa26245a1c1aa938cce391348d6bd879da357522. - [ ] unused since 58395759b04273edccf3d199606088e0703ae6b1. ### ambiguous not touching these before i get additional feedback. consider them out of the scope of my proposal for now! - , (pc/pyconfig.h) added in fe393f47c662c307d2d3e90f785edf5821d54a8d, never used since. maybe it's intentionally reserved for future use? cc: , , can this be removed? - (pc/pyconfig.h) allowing this seems to have caused a stack corruption fixed in aa26b275034c07784c4d64e9a2bc26c742577327. since the fix, the flag has not been used. cc: , -natali, can this be removed? - (pc/pyconfig.h) [standardized by posix.1 (ieee std 1003.1).]( not sure if it should be in pc/. maybe it is used downstream? cc: , , do we need this in pc/pyconfig.h? ### still used or won't fix - ~ ~ (pc/pyconfig.h) this was identified by the script, but it's an include guard. - ~ ~ (pc/pyconfig.h) used in pc/pyconfig.h to define . - ~ ~ (pc/pyconfig) used for defining . - ~ ~ (pc/pyconfig.h) used for defining . - ~ , ~ (pc/pyconfig.h) i'd not touch these, as they are used for defining . these correspond to and from the pymacro.h header. making include pymacro.h just for doesn't seem like a sensible option. ## conclusion i'm going to start from prs one per every item (grouping one or more related macros) in the bullet lists of flags certainly removable. going forward, i'll appreciate feedback about the ambiguous flags or any things that i've overlooked. are there any flags listed for removal that still have known downstream or platform‚Äëspecific uses? i pinged some changeset authors to help decide the most ambiguous cases. finally, i considered adding automated checks for unused configuration macros, but given the amount of variables involved, this seems non-trivial and not worth it. thanks for reading through all of this! cc i think you will have useful insights into this one. ### linked prs * gh-144229 * gh-144230 * gh-144231 * gh-144232 * gh-144235 * gh-144236",[],"['FEATURE', 'UI']","['type-feature', 'build']",github,2026-01-26T04:36:55Z,2026-01-27T02:29:00Z,"consider removing some unused build flags ## description tripped up by in our configure file while debugging gh-142353, i was curious if we have any other unused build flags. removing them could slightly shorten preprocessing work, reduce cognitive load when debugging platform‚Äëspecific issues, and reduce the risk of accidentally relying on dead configuration paths. after running , i put together this script with [ripgrep]( then i ran this command: i got the following results: expand results these macros are not referenced by any cpython source files outside configuration headers ( , and files) and are not part of the documented public api. a few of them are actually used and cannot be removed. most of them, however, weren't used in years (or decades) and appear to be safely removable, because they no longer affect the behavior of python. here's what i found about every single one of them. ### pc/pyconfig.h and pyconfig.h - [ ] , unused since 58395759b04273edccf3d199606088e0703ae6b1. ### pc/pyconfig.h - [ ] ( this is commented out: but we already have for that: which is used. let's remove the comment. - [ ] , ( superseded by and since 1997, see 1171ee6aaf55fbb7f314a36798c0835ea0900ce9. removed from in ab70e2ab3264c444ac70a1c98e869b0de0b22c4a (when os/2 support was dropped). - [ ] ( unused since ab70e2ab3264c444ac70a1c98e869b0de0b22c4a (when os/2 support was dropped). - [ ] ( last occurences removed in ab70e2ab3264c444ac70a1c98e869b0de0b22c4a as well. this was meant to be removed in 2008 according to the description of ada8c3b046aa6f3684cbc32a4a140a38c204c050: - [ ] i'm not sure if it was or was not ever used, because i haven't bisected this. seems unused at the moment. first added in 61fe8619fb084a6cb3ac699609ad772a801b8195. - [ ] ( unused since b8d1262e8afe7b907b4a394a191739571092acdb (when and became universally available). - [ ] ( unused since 39258d3595300bc7b952854c915f63ae2d4b9c3e (when openssl was bumped to 1.1.1). - [ ] no longer used since 11fc411f98a04947a2a21329c29fe0f35ff52dba. - [ ] obsolete since 1997, see b06df27843611d9b1a9988da7aacf01073a1fbc4. the presence of this flag was very confusing to me when i was debugging gh-142353. ### pyconfig.h - [ ] , , , , unused since fa26245a1c1aa938cce391348d6bd879da357522. - [ ] unused since 58395759b04273edccf3d199606088e0703ae6b1. ### ambiguous not touching these before i get additional feedback. consider them out of the scope of my proposal for now! - , (pc/pyconfig.h) added in fe393f47c662c307d2d3e90f785edf5821d54a8d, never used since. maybe it's intentionally reserved for future use? cc: , , can this be removed? - (pc/pyconfig.h) allowing this seems to have caused a stack corruption fixed in aa26b275034c07784c4d64e9a2bc26c742577327. since the fix, the flag has not been used. cc: , -natali, can this be removed? - (pc/pyconfig.h) [standardized by posix.1 (ieee std 1003.1).]( not sure if it should be in pc/. maybe it is used downstream? cc: , , do we need this in pc/pyconfig.h? ### still used or won't fix - ~ ~ (pc/pyconfig.h) this was identified by the script, but it's an include guard. - ~ ~ (pc/pyconfig.h) used in pc/pyconfig.h to define . - ~ ~ (pc/pyconfig) used for defining . - ~ ~ (pc/pyconfig.h) used for defining . - ~ , ~ (pc/pyconfig.h) i'd not touch these, as they are used for defining . these correspond to and from the pymacro.h header. making include pymacro.h just for doesn't seem like a sensible option. ## conclusion i'm going to start from prs one per every item (grouping one or more related macros) in the bullet lists of flags certainly removable. going forward, i'll appreciate feedback about the ambiguous flags or any things that i've overlooked. are there any flags listed for removal that still have known downstream or platform‚Äëspecific uses? i pinged some changeset authors to help decide the most ambiguous cases. finally, i considered adding automated checks for unused configuration macros, but given the amount of variables involved, this seems non-trivial and not worth it. thanks for reading through all of this! cc i think you will have useful insights into this one. ### linked prs * gh-144229 * gh-144230 * gh-144231 * gh-144232 * gh-144235 * gh-144236",3.6,Critical,1.0,"user-visible issue, crash-like behavior"
cilium/cilium#44001,docs for host-firewall have a typo,### is there an existing issue for this? - [x] i have searched the existing issues ### version equal or higher than v1.18.6 and lower than v1.19.0 ### what happened? typo in cilium-dbg in docs for host-firewall ### how can we reproduce the issue? cilium-dbg endpoint list -o jsonpath='{[?(@.status.identity.id==1)].id}') should be cilium-dbg endpoint list -o jsonpath='{[?(@.status.identity.id==1)].id}' ### cilium version irrelevant ### kernel version irrelevant ### kubernetes version irrelevant ### regression _no response_ ### sysdump _no response_ ### relevant log output ### anything else? _no response_ ### cilium users document - [ ] are you a user of cilium? please add yourself to the [users doc]( ### code of conduct - [x] i agree to follow this project's code of conduct,[],['BUG'],"['kind/bug', 'needs/triage', 'kind/community-report']",github,2026-01-26T05:00:32Z,2026-01-26T05:16:16Z,docs for host-firewall have a typo ### is there an existing issue for this? - [x] i have searched the existing issues ### version equal or higher than v1.18.6 and lower than v1.19.0 ### what happened? typo in cilium-dbg in docs for host-firewall ### how can we reproduce the issue? cilium-dbg endpoint list -o jsonpath='{[?(@.status.identity.id==1)].id}') should be cilium-dbg endpoint list -o jsonpath='{[?(@.status.identity.id==1)].id}' ### cilium version irrelevant ### kernel version irrelevant ### kubernetes version irrelevant ### regression _no response_ ### sysdump _no response_ ### relevant log output ### anything else? _no response_ ### cilium users document - [ ] are you a user of cilium? please add yourself to the [users doc]( ### code of conduct - [x] i agree to follow this project's code of conduct,4.6,Critical,1.0,crash-like behavior
rust-lang/rust#151671,compiler hangs when evaluating where clause with self-referential dyn trait,"<!-- thank you for filing a bug report! üêõ please provide a short summary of the bug, along with any information you feel relevant to replicating the bug. --> i tried this code: | release channel | result | |-----------------------------------|--------| | current stable | hang | | current nightly (default solver) | hang | | current nightly (+ | error | ### meta <!-- if you're using the stable version of the compiler, you should also check if the bug also exists in the beta or nightly versions. --> : <!-- include a backtrace in the code block by setting in your environment. e.g. . --> backtrace",[],['BUG'],"['C-bug', 'I-hang', 'needs-triage']",github,2026-01-26T05:37:25Z,,"compiler hangs when evaluating where clause with self-referential dyn trait <!-- thank you for filing a bug report! üêõ please provide a short summary of the bug, along with any information you feel relevant to replicating the bug. --> i tried this code: | release channel | result | |-----------------------------------|--------| | current stable | hang | | current nightly (default solver) | hang | | current nightly (+ | error | ### meta <!-- if you're using the stable version of the compiler, you should also check if the bug also exists in the beta or nightly versions. --> : <!-- include a backtrace in the code block by setting in your environment. e.g. . --> backtrace",4.6,Critical,1.0,crash-like behavior
microsoft/vscode#290323,[unhandled error] uncaught typeerror: cannot read properties of undefined (reading 'substring'),"issue created from vs code errors analysis dashboard ## error bucket ## error message ## stack trace raw stack trace (minified) ## details | property | value | | --- | --- | | version | 1.108.2 | | commit | [c9d77990]( | | last seen | 2026-01-25t23:59:24.330z | | total hits | 1.2m | | affected users | 5.7k | | platforms | windows, linux | | product | vscode | --- *this issue was automatically created from the vs code errors dashboard*",[],['BUG'],"['bug', 'error-telemetry', '*duplicate']",github,2026-01-26T06:19:04Z,2026-01-26T19:37:10Z,"[unhandled error] uncaught typeerror: cannot read properties of undefined (reading 'substring') issue created from vs code errors analysis dashboard ## error bucket ## error message ## stack trace raw stack trace (minified) ## details | property | value | | --- | --- | | version | 1.108.2 | | commit | [c9d77990]( | | last seen | 2026-01-25t23:59:24.330z | | total hits | 1.2m | | affected users | 5.7k | | platforms | windows, linux | | product | vscode | --- *this issue was automatically created from the vs code errors dashboard*",2.513,Medium,0.791,functional impact
microsoft/vscode#290324,[unhandled error] cannot read properties of null (reading 'getlinecontent'),"issue created from vs code errors analysis dashboard ## error bucket ## error message ## stack trace raw stack trace (minified) ## details | property | value | | --- | --- | | version | 1.108.2 | | commit | [c9d77990]( | | last seen | 2026-01-25t23:58:33.428z | | total hits | 621.5k | | affected users | 342.6k | | platforms | linux, mac, windows | | product | vscode | --- *this issue was automatically created from the vs code errors dashboard*",[],['BUG'],"['bug', 'error-telemetry']",github,2026-01-26T06:19:59Z,,"[unhandled error] cannot read properties of null (reading 'getlinecontent') issue created from vs code errors analysis dashboard ## error bucket ## error message ## stack trace raw stack trace (minified) ## details | property | value | | --- | --- | | version | 1.108.2 | | commit | [c9d77990]( | | last seen | 2026-01-25t23:58:33.428z | | total hits | 621.5k | | affected users | 342.6k | | platforms | linux, mac, windows | | product | vscode | --- *this issue was automatically created from the vs code errors dashboard*",2.534,Medium,0.796,functional impact
microsoft/vscode#290326,[unhandled error] cannot read properties of undefined (reading 'tostring'),"issue created from vs code errors analysis dashboard ## error bucket ## error message ## stack trace raw stack trace (minified) ## details | property | value | | --- | --- | | version | 1.108.2 | | commit | [c9d77990]( | | last seen | 2026-01-25t23:58:48.348z | | total hits | 1.1m | | affected users | 42.7k | | platforms | linux, mac, windows | | product | vscode | --- *this issue was automatically created from the vs code errors dashboard*",[],['BUG'],"['bug', 'error-telemetry']",github,2026-01-26T06:22:30Z,,"[unhandled error] cannot read properties of undefined (reading 'tostring') issue created from vs code errors analysis dashboard ## error bucket ## error message ## stack trace raw stack trace (minified) ## details | property | value | | --- | --- | | version | 1.108.2 | | commit | [c9d77990]( | | last seen | 2026-01-25t23:58:48.348z | | total hits | 1.1m | | affected users | 42.7k | | platforms | linux, mac, windows | | product | vscode | --- *this issue was automatically created from the vs code errors dashboard*",2.332,Medium,0.75,functional impact
cockroachdb/cockroach#161767,pkg/kv/kvserver/tscache: potential performance regression,"pkg/kv/kvserver/tscache [performance regression]( on master @ [867004a4311025b7e0de44bc8c72fcbc8442f041]( performance regressions detected in package pkg/kv comparison: v25.4.3 (71d85362) -&gt; refs/heads/master (867004a4) found 26 benchmark(s) with regressions ‚â•20%: - [ ] [pkg/kv/kvserver/tscache‚Üíintervalskldecodevalue/logical=true,txnid=false-32 (sec/op): +100.00% (100.0%)]( - [ ] [pkg/kv/kvserver/tscache‚Üíintervalskldecodevalue/logical=true,txnid=true-32 (sec/op): +99.98% (100.0%)]( - [ ] [pkg/kv/kvserver/tscache‚Üíintervalskldecodevalue/logical=false,txnid=false-32 (sec/op): +99.95% (99.9%)]( - [ ] [pkg/kv/kvserver/tscache‚Üíintervalskldecodevalue/logical=false,txnid=true-32 (sec/op): +99.90% (99.9%)]( - [ ] [pkg/kv/kvserver/spanlatch‚Üíbtreeiterseeklt/count=65536-32 (sec/op): +43.73% (43.7%)]( - [ ] [pkg/kv/kvserver/raftlog‚Üívisit-32 (sec/op): +39.60% (39.6%)]( - [ ] [pkg/kv/kvserver/spanlatch‚Üíbtreedeleteinsertcloneonce/count=65536-32 (sec/op): +36.19% (36.2%)]( - [ ] [pkg/kv/kvclient/kvcoord‚Üíbtreeinsert/count=65536-32 (sec/op): +35.73% (35.7%)]( - [ ] [pkg/kv/kvserver‚Üístorepoolreadswithconcurrentupdates-32 (sec/op): +35.52% (35.5%)]( - [ ] [pkg/kv/kvserver/spanlatch‚Üíbtreeiterseekge/count=65536-32 (sec/op): +34.40% (34.4%)]( - [ ] [pkg/kv/kvserver‚Üíreplicaproposal/bytes=512_kib,withfollower=true-32 (sec/op): +31.17% (31.2%)]( - [ ] [pkg/kv/kvclient/kvcoord‚Üíbtreeiterseeklt/count=65536-32 (sec/op): +29.97% (30.0%)]( - [ ] [pkg/kv/kvserver/raftlog‚Üíiterator/next-32 (sec/op): +26.76% (26.8%)]( - [ ] [pkg/kv/kvserver/raftlog‚Üíiterator/seekge-32 (sec/op): +23.59% (23.6%)]( - [ ] [pkg/kv/kvserver/concurrency‚Üíbtreedeleteinsertcloneeachtime/count=8192/reset=false-32 (sec/op): +21.35% (21.3%)]( - [ ] [pkg/kv/kvserver/raftlog‚Üíraftadmissionmetaoverhead/bytes=256_kib,raft-ac=false-32 (sec/op): +20.48% (20.5%)]( - [ ] [pkg/kv/kvserver‚Üíreplicaproposal/bytes=256_b,withfollower=true-32 (allocs/op): +60.71% (60.7%)]( - [ ] [pkg/kv/kvserver‚Üíreplicaproposal/bytes=512_b,withfollower=true-32 (allocs/op): +59.41% (59.4%)]( - [ ] [pkg/kv/kvserver‚Üíflowcontrolv2basic/kvadmission.flow_control.mode=apply_to_elastic-32 (allocs/op): +31.33% (31.3%)]( - [ ] [pkg/kv/kvserver‚Üístorerangemerge-32 (allocs/op): +28.55% (28.6%)]( - [ ] [pkg/kv/kvserver‚Üíreplicaproposal/bytes=512_kib,withfollower=true-32 (b/s): -24.00% (24.0%)]( - [ ] [pkg/kv/kvserver/rangefeed‚Üíeventqueuempsc/eventqueue-32 (b/op): +408.84% (408.8%)]( - [ ] [pkg/kv/kvserver‚Üíflowcontrolv2basic/kvadmission.flow_control.mode=apply_to_all-32 (b/op): +61.68% (61.7%)]( - [ ] [pkg/kv/kvserver‚Üíreplicaproposal/bytes=256_b,withfollower=true-32 (b/op): +35.52% (35.5%)]( - [ ] [pkg/kv/kvserver‚Üíreplicaproposal/bytes=512_b,withfollower=true-32 (b/op): +30.95% (30.9%)]( - [ ] [pkg/kv/kvserver‚Üíflowcontrolv2basic/kvadmission.flow_control.mode=apply_to_elastic-32 (b/op): +29.02% (29.0%)]( help see also: [how to investigate a go test failure \(internal\)]( /cc /kv-triage jira issue: crdb-59068",[],"['PERFORMANCE', 'TESTING']","['C-test-failure', 'O-robot', 'C-performance', 'branch-master', 'release-blocker', 'T-kv', 'O-microbench', 'branch-release-25.4']",github,2026-01-26T06:25:07Z,,"pkg/kv/kvserver/tscache: potential performance regression pkg/kv/kvserver/tscache [performance regression]( on master @ [867004a4311025b7e0de44bc8c72fcbc8442f041]( performance regressions detected in package pkg/kv comparison: v25.4.3 (71d85362) -&gt; refs/heads/master (867004a4) found 26 benchmark(s) with regressions ‚â•20%: - [ ] [pkg/kv/kvserver/tscache‚Üíintervalskldecodevalue/logical=true,txnid=false-32 (sec/op): +100.00% (100.0%)]( - [ ] [pkg/kv/kvserver/tscache‚Üíintervalskldecodevalue/logical=true,txnid=true-32 (sec/op): +99.98% (100.0%)]( - [ ] [pkg/kv/kvserver/tscache‚Üíintervalskldecodevalue/logical=false,txnid=false-32 (sec/op): +99.95% (99.9%)]( - [ ] [pkg/kv/kvserver/tscache‚Üíintervalskldecodevalue/logical=false,txnid=true-32 (sec/op): +99.90% (99.9%)]( - [ ] [pkg/kv/kvserver/spanlatch‚Üíbtreeiterseeklt/count=65536-32 (sec/op): +43.73% (43.7%)]( - [ ] [pkg/kv/kvserver/raftlog‚Üívisit-32 (sec/op): +39.60% (39.6%)]( - [ ] [pkg/kv/kvserver/spanlatch‚Üíbtreedeleteinsertcloneonce/count=65536-32 (sec/op): +36.19% (36.2%)]( - [ ] [pkg/kv/kvclient/kvcoord‚Üíbtreeinsert/count=65536-32 (sec/op): +35.73% (35.7%)]( - [ ] [pkg/kv/kvserver‚Üístorepoolreadswithconcurrentupdates-32 (sec/op): +35.52% (35.5%)]( - [ ] [pkg/kv/kvserver/spanlatch‚Üíbtreeiterseekge/count=65536-32 (sec/op): +34.40% (34.4%)]( - [ ] [pkg/kv/kvserver‚Üíreplicaproposal/bytes=512_kib,withfollower=true-32 (sec/op): +31.17% (31.2%)]( - [ ] [pkg/kv/kvclient/kvcoord‚Üíbtreeiterseeklt/count=65536-32 (sec/op): +29.97% (30.0%)]( - [ ] [pkg/kv/kvserver/raftlog‚Üíiterator/next-32 (sec/op): +26.76% (26.8%)]( - [ ] [pkg/kv/kvserver/raftlog‚Üíiterator/seekge-32 (sec/op): +23.59% (23.6%)]( - [ ] [pkg/kv/kvserver/concurrency‚Üíbtreedeleteinsertcloneeachtime/count=8192/reset=false-32 (sec/op): +21.35% (21.3%)]( - [ ] [pkg/kv/kvserver/raftlog‚Üíraftadmissionmetaoverhead/bytes=256_kib,raft-ac=false-32 (sec/op): +20.48% (20.5%)]( - [ ] [pkg/kv/kvserver‚Üíreplicaproposal/bytes=256_b,withfollower=true-32 (allocs/op): +60.71% (60.7%)]( - [ ] [pkg/kv/kvserver‚Üíreplicaproposal/bytes=512_b,withfollower=true-32 (allocs/op): +59.41% (59.4%)]( - [ ] [pkg/kv/kvserver‚Üíflowcontrolv2basic/kvadmission.flow_control.mode=apply_to_elastic-32 (allocs/op): +31.33% (31.3%)]( - [ ] [pkg/kv/kvserver‚Üístorerangemerge-32 (allocs/op): +28.55% (28.6%)]( - [ ] [pkg/kv/kvserver‚Üíreplicaproposal/bytes=512_kib,withfollower=true-32 (b/s): -24.00% (24.0%)]( - [ ] [pkg/kv/kvserver/rangefeed‚Üíeventqueuempsc/eventqueue-32 (b/op): +408.84% (408.8%)]( - [ ] [pkg/kv/kvserver‚Üíflowcontrolv2basic/kvadmission.flow_control.mode=apply_to_all-32 (b/op): +61.68% (61.7%)]( - [ ] [pkg/kv/kvserver‚Üíreplicaproposal/bytes=256_b,withfollower=true-32 (b/op): +35.52% (35.5%)]( - [ ] [pkg/kv/kvserver‚Üíreplicaproposal/bytes=512_b,withfollower=true-32 (b/op): +30.95% (30.9%)]( - [ ] [pkg/kv/kvserver‚Üíflowcontrolv2basic/kvadmission.flow_control.mode=apply_to_elastic-32 (b/op): +29.02% (29.0%)]( help see also: [how to investigate a go test failure \(internal\)]( /cc /kv-triage jira issue: crdb-59068",2.926,Medium,0.885,performance degradation
cockroachdb/cockroach#161768,pkg/spanconfig/spanconfigstore: potential performance regression,pkg/spanconfig/spanconfigstore [performance regression]( on master @ [867004a4311025b7e0de44bc8c72fcbc8442f041]( performance regressions detected in package pkg/spanconfig comparison: v25.4.3 (71d85362) -&gt; refs/heads/master (867004a4) found 5 benchmark(s) with regressions ‚â•20%: - [ ] [pkg/spanconfig/spanconfigstore‚Üíbtreedeleteinsertcloneonce/count=65536-32 (sec/op): +53.26% (53.3%)]( - [ ] [pkg/spanconfig/spanconfigstore‚Üíbtreedeleteinsert/count=65536-32 (sec/op): +39.84% (39.8%)]( - [ ] [pkg/spanconfig/spanconfigstore‚Üíbtreedelete/count=65536-32 (sec/op): +35.75% (35.8%)]( - [ ] [pkg/spanconfig/spanconfigstore‚Üíbtreedeleteinsertcloneeachtime/count=65536/reset=false-32 (sec/op): +23.21% (23.2%)]( - [ ] [pkg/spanconfig/spanconfigstore‚Üíbtreedeleteinsertcloneonce/count=65536-32 (b/op): ? (+inf%)]( help see also: [how to investigate a go test failure \(internal\)]( /cc /kv-triage /kv-triage jira issue: crdb-59069,[],"['PERFORMANCE', 'TESTING']","['C-test-failure', 'O-robot', 'C-performance', 'branch-master', 'release-blocker', 'T-kv', 'O-microbench', 'branch-release-25.4']",github,2026-01-26T06:25:08Z,,pkg/spanconfig/spanconfigstore: potential performance regression pkg/spanconfig/spanconfigstore [performance regression]( on master @ [867004a4311025b7e0de44bc8c72fcbc8442f041]( performance regressions detected in package pkg/spanconfig comparison: v25.4.3 (71d85362) -&gt; refs/heads/master (867004a4) found 5 benchmark(s) with regressions ‚â•20%: - [ ] [pkg/spanconfig/spanconfigstore‚Üíbtreedeleteinsertcloneonce/count=65536-32 (sec/op): +53.26% (53.3%)]( - [ ] [pkg/spanconfig/spanconfigstore‚Üíbtreedeleteinsert/count=65536-32 (sec/op): +39.84% (39.8%)]( - [ ] [pkg/spanconfig/spanconfigstore‚Üíbtreedelete/count=65536-32 (sec/op): +35.75% (35.8%)]( - [ ] [pkg/spanconfig/spanconfigstore‚Üíbtreedeleteinsertcloneeachtime/count=65536/reset=false-32 (sec/op): +23.21% (23.2%)]( - [ ] [pkg/spanconfig/spanconfigstore‚Üíbtreedeleteinsertcloneonce/count=65536-32 (b/op): ? (+inf%)]( help see also: [how to investigate a go test failure \(internal\)]( /cc /kv-triage /kv-triage jira issue: crdb-59069,3.311,High,0.972,performance degradation
cockroachdb/cockroach#161769,pkg/raft: potential performance regression,pkg/raft [performance regression]( on master @ [867004a4311025b7e0de44bc8c72fcbc8442f041]( performance regressions detected in package pkg/raft comparison: v25.4.3 (71d85362) -&gt; refs/heads/master (867004a4) found 1 benchmark(s) with regressions ‚â•20%: - [ ] [pkg/raft‚Üístatus/members=1/withprogress-32 (sec/op): +21.50% (21.5%)]( help see also: [how to investigate a go test failure \(internal\)]( /cc /kv-triage jira issue: crdb-59070,[],"['PERFORMANCE', 'TESTING']","['C-test-failure', 'O-robot', 'C-performance', 'branch-master', 'release-blocker', 'T-kv', 'O-microbench', 'branch-release-25.4']",github,2026-01-26T06:25:09Z,,pkg/raft: potential performance regression pkg/raft [performance regression]( on master @ [867004a4311025b7e0de44bc8c72fcbc8442f041]( performance regressions detected in package pkg/raft comparison: v25.4.3 (71d85362) -&gt; refs/heads/master (867004a4) found 1 benchmark(s) with regressions ‚â•20%: - [ ] [pkg/raft‚Üístatus/members=1/withprogress-32 (sec/op): +21.50% (21.5%)]( help see also: [how to investigate a go test failure \(internal\)]( /cc /kv-triage jira issue: crdb-59070,3.133,High,0.932,performance degradation
cockroachdb/cockroach#161770,pkg/util/span: potential performance regression,"pkg/util/span [performance regression]( on master @ [867004a4311025b7e0de44bc8c72fcbc8442f041]( performance regressions detected in package pkg/util comparison: v25.4.3 (71d85362) -&gt; refs/heads/master (867004a4) found 24 benchmark(s) with regressions ‚â•20%: - [ ] [pkg/util/span‚Üíbtreeiterseeklt/count=65536-32 (sec/op): +43.48% (43.5%)]( - [ ] [pkg/util/syncutil‚Üíloadmostlyhits/*syncutil.deepcopymap-32 (sec/op): +40.05% (40.0%)]( - [ ] [pkg/util/interval/generic‚Üíbtreeiterseeklt/count=65536-32 (sec/op): +39.29% (39.3%)]( - [ ] [pkg/util/interval‚Üíbtreeinsert/count=65536-32 (sec/op): +37.98% (38.0%)]( - [ ] [pkg/util/span‚Üíbtreedelete/count=65536-32 (sec/op): +32.04% (32.0%)]( - [ ] [pkg/util/interval/generic‚Üíbtreedelete/count=65536-32 (sec/op): +31.56% (31.6%)]( - [ ] [pkg/util/quotapool‚Üíconcurrentintpool/workers=512,quota=511-32 (sec/op): +30.85% (30.9%)]( - [ ] [pkg/util/span‚Üíbtreedeleteinsertcloneeachtime/count=65536/reset=true-32 (sec/op): +30.30% (30.3%)]( - [ ] [pkg/util/quotapool‚Üíconcurrentintpool/workers=512,quota=513-32 (sec/op): +26.68% (26.7%)]( - [ ] [pkg/util/log‚Üíexpensivelogenabled-32 (sec/op): +26.48% (26.5%)]( - [ ] [pkg/util/interval/generic‚Üíbtreedeleteinsertcloneeachtime/count=65536/reset=true-32 (sec/op): +25.38% (25.4%)]( - [ ] [pkg/util/tracing‚Üíspancreation/detached-child=false-32 (sec/op): +23.60% (23.6%)]( - [ ] [pkg/util/syncutil‚Üíadversarialalloc/*syncutil.rwmutexmap-32 (sec/op): +22.71% (22.7%)]( - [ ] [pkg/util/encoding‚Üípeektype-32 (sec/op): +20.16% (20.2%)]( - [ ] [pkg/util‚Üífastintmap/10x10-4/map-sized-32 (allocs/op): +200.00% (200.0%)]( - [ ] [pkg/util‚Üífastintmap/32x15-10/map-32 (allocs/op): +200.00% (200.0%)]( - [ ] [pkg/util‚Üífastintmap/32x15-10/map-sized-32 (allocs/op): +200.00% (200.0%)]( - [ ] [pkg/util‚Üífastintmap/1000x1000-500/map-sized-32 (allocs/op): +150.00% (150.0%)]( - [ ] [pkg/util‚Üífastintmap/100x100-50/map-sized-32 (allocs/op): +50.00% (50.0%)]( - [ ] [pkg/util‚Üífastintmap/100x100-50/fastintmap-32 (allocs/op): +33.33% (33.3%)]( - [ ] [pkg/util/parquet‚Üíparquetwriter/int-32 (b/op): +46.06% (46.1%)]( - [ ] [pkg/util/parquet‚Üíparquetwriter/time-32 (b/op): +44.68% (44.7%)]( - [ ] [pkg/util/parquet‚Üíparquetwriter/pg_lsn-32 (b/op): +43.92% (43.9%)]( - [ ] [pkg/util/parquet‚Üíparquetwriter/geography-32 (b/op): +20.33% (20.3%)]( help see also: [how to investigate a go test failure \(internal\)]( /cc /unowned jira issue: crdb-59071",[],"['PERFORMANCE', 'TESTING']","['C-test-failure', 'O-robot', 'C-performance', 'branch-master', 'release-blocker', 'O-microbench', 'branch-release-25.4']",github,2026-01-26T06:25:10Z,,"pkg/util/span: potential performance regression pkg/util/span [performance regression]( on master @ [867004a4311025b7e0de44bc8c72fcbc8442f041]( performance regressions detected in package pkg/util comparison: v25.4.3 (71d85362) -&gt; refs/heads/master (867004a4) found 24 benchmark(s) with regressions ‚â•20%: - [ ] [pkg/util/span‚Üíbtreeiterseeklt/count=65536-32 (sec/op): +43.48% (43.5%)]( - [ ] [pkg/util/syncutil‚Üíloadmostlyhits/*syncutil.deepcopymap-32 (sec/op): +40.05% (40.0%)]( - [ ] [pkg/util/interval/generic‚Üíbtreeiterseeklt/count=65536-32 (sec/op): +39.29% (39.3%)]( - [ ] [pkg/util/interval‚Üíbtreeinsert/count=65536-32 (sec/op): +37.98% (38.0%)]( - [ ] [pkg/util/span‚Üíbtreedelete/count=65536-32 (sec/op): +32.04% (32.0%)]( - [ ] [pkg/util/interval/generic‚Üíbtreedelete/count=65536-32 (sec/op): +31.56% (31.6%)]( - [ ] [pkg/util/quotapool‚Üíconcurrentintpool/workers=512,quota=511-32 (sec/op): +30.85% (30.9%)]( - [ ] [pkg/util/span‚Üíbtreedeleteinsertcloneeachtime/count=65536/reset=true-32 (sec/op): +30.30% (30.3%)]( - [ ] [pkg/util/quotapool‚Üíconcurrentintpool/workers=512,quota=513-32 (sec/op): +26.68% (26.7%)]( - [ ] [pkg/util/log‚Üíexpensivelogenabled-32 (sec/op): +26.48% (26.5%)]( - [ ] [pkg/util/interval/generic‚Üíbtreedeleteinsertcloneeachtime/count=65536/reset=true-32 (sec/op): +25.38% (25.4%)]( - [ ] [pkg/util/tracing‚Üíspancreation/detached-child=false-32 (sec/op): +23.60% (23.6%)]( - [ ] [pkg/util/syncutil‚Üíadversarialalloc/*syncutil.rwmutexmap-32 (sec/op): +22.71% (22.7%)]( - [ ] [pkg/util/encoding‚Üípeektype-32 (sec/op): +20.16% (20.2%)]( - [ ] [pkg/util‚Üífastintmap/10x10-4/map-sized-32 (allocs/op): +200.00% (200.0%)]( - [ ] [pkg/util‚Üífastintmap/32x15-10/map-32 (allocs/op): +200.00% (200.0%)]( - [ ] [pkg/util‚Üífastintmap/32x15-10/map-sized-32 (allocs/op): +200.00% (200.0%)]( - [ ] [pkg/util‚Üífastintmap/1000x1000-500/map-sized-32 (allocs/op): +150.00% (150.0%)]( - [ ] [pkg/util‚Üífastintmap/100x100-50/map-sized-32 (allocs/op): +50.00% (50.0%)]( - [ ] [pkg/util‚Üífastintmap/100x100-50/fastintmap-32 (allocs/op): +33.33% (33.3%)]( - [ ] [pkg/util/parquet‚Üíparquetwriter/int-32 (b/op): +46.06% (46.1%)]( - [ ] [pkg/util/parquet‚Üíparquetwriter/time-32 (b/op): +44.68% (44.7%)]( - [ ] [pkg/util/parquet‚Üíparquetwriter/pg_lsn-32 (b/op): +43.92% (43.9%)]( - [ ] [pkg/util/parquet‚Üíparquetwriter/geography-32 (b/op): +20.33% (20.3%)]( help see also: [how to investigate a go test failure \(internal\)]( /cc /unowned jira issue: crdb-59071",3.116,High,0.928,performance degradation
cockroachdb/cockroach#161771,pkg/rpc: potential performance regression,pkg/rpc [performance regression]( on master @ [867004a4311025b7e0de44bc8c72fcbc8442f041]( performance regressions detected in package pkg/rpc comparison: v25.4.3 (71d85362) -&gt; refs/heads/master (867004a4) found 4 benchmark(s) with regressions ‚â•20%: - [ ] [pkg/rpc‚Üígrpcping/bytes=__65536/rpc=streamstream-32 (sec/op): +22.65% (22.6%)]( - [ ] [pkg/rpc‚Üígrpcping/bytes=1048576/rpc=streamstream-32 (sec/op): +21.79% (21.8%)]( - [ ] [pkg/rpc‚Üígrpcping/bytes=__65536/rpc=unaryunary-32 (sec/op): +20.31% (20.3%)]( - [ ] [pkg/rpc‚Üígrpcping/bytes=__32768/rpc=streamstream-32 (sec/op): +20.26% (20.3%)]( help see also: [how to investigate a go test failure \(internal\)]( /cc /kv-triage jira issue: crdb-59072,[],"['PERFORMANCE', 'TESTING']","['C-test-failure', 'O-robot', 'C-performance', 'branch-master', 'release-blocker', 'T-kv', 'O-microbench', 'branch-release-25.4']",github,2026-01-26T06:25:16Z,,pkg/rpc: potential performance regression pkg/rpc [performance regression]( on master @ [867004a4311025b7e0de44bc8c72fcbc8442f041]( performance regressions detected in package pkg/rpc comparison: v25.4.3 (71d85362) -&gt; refs/heads/master (867004a4) found 4 benchmark(s) with regressions ‚â•20%: - [ ] [pkg/rpc‚Üígrpcping/bytes=__65536/rpc=streamstream-32 (sec/op): +22.65% (22.6%)]( - [ ] [pkg/rpc‚Üígrpcping/bytes=1048576/rpc=streamstream-32 (sec/op): +21.79% (21.8%)]( - [ ] [pkg/rpc‚Üígrpcping/bytes=__65536/rpc=unaryunary-32 (sec/op): +20.31% (20.3%)]( - [ ] [pkg/rpc‚Üígrpcping/bytes=__32768/rpc=streamstream-32 (sec/op): +20.26% (20.3%)]( help see also: [how to investigate a go test failure \(internal\)]( /cc /kv-triage jira issue: crdb-59072,3.133,High,0.932,performance degradation
cockroachdb/cockroach#161772,pkg/ccl/changefeedccl: potential performance regression,pkg/ccl/changefeedccl [performance regression]( on master @ [867004a4311025b7e0de44bc8c72fcbc8442f041]( performance regressions detected in package pkg/ccl comparison: v25.4.3 (71d85362) -&gt; refs/heads/master (867004a4) found 20 benchmark(s) with regressions ‚â•20%: - [ ] [pkg/ccl/changefeedccl‚Üíencoders/json/envelope=wrapped -32 (sec/op): +88.84% (88.8%)]( - [ ] [pkg/ccl/changefeedccl‚Üíencoders/json/envelope=row -32 (sec/op): +85.93% (85.9%)]( - [ ] [pkg/ccl/storageccl‚Üíencryption/encryptfile/1.0_kib/chunk=1.0_mib-32 (sec/op): +20.74% (20.7%)]( - [ ] [pkg/ccl/changefeedccl‚Üíencoders/json/envelope=row -32 (allocs/op): +115.10% (115.1%)]( - [ ] [pkg/ccl/changefeedccl‚Üíencoders/json/envelope=wrapped -32 (allocs/op): +76.21% (76.2%)]( - [ ] [pkg/ccl/changefeedccl‚Üíencoders/json/envelope=row -32 (allocs/op): +72.34% (72.3%)]( - [ ] [pkg/ccl/changefeedccl‚Üíencoders/json/envelope=row -32 (b/op): +114.62% (114.6%)]( - [ ] [pkg/ccl/changefeedccl‚Üíencoders/json/envelope=wrapped -32 (b/op): +75.57% (75.6%)]( - [ ] [pkg/ccl/changefeedccl‚Üíencoders/json/envelope=row -32 (b/op): +71.72% (71.7%)]( - [ ] [pkg/ccl/changefeedccl/schemafeed‚Üípauseorresumepolling/not_schema_locked-32 (b/op): +50.00% (50.0%)]( - [ ] [pkg/ccl/pgcryptoccl/pgcryptocipherccl‚Üíencrypt/aes-128_with_iv-32 (b/op): +49.88% (49.9%)]( - [ ] [pkg/ccl/pgcryptoccl/pgcryptocipherccl‚Üíencrypt/aes-128-32 (b/op): +48.90% (48.9%)]( - [ ] [pkg/ccl/storageccl‚Üíencryption/decryptfile/1.2_kib/chunk=100_b-32 (b/op): +40.29% (40.3%)]( - [ ] [pkg/ccl/storageccl‚Üíencryption/decryptfile/116_kib/chunk=100_b-32 (b/op): +40.25% (40.3%)]( - [ ] [pkg/ccl/storageccl‚Üíencryption/decryptfile/1.2_mib/chunk=100_b-32 (b/op): +39.72% (39.7%)]( - [ ] [pkg/ccl/pgcryptoccl/pgcryptocipherccl‚Üíencrypt/aes-256-32 (b/op): +39.12% (39.1%)]( - [ ] [pkg/ccl/pgcryptoccl/pgcryptocipherccl‚Üíencrypt/aes-192-32 (b/op): +32.95% (32.9%)]( - [ ] [pkg/ccl/pgcryptoccl/pgcryptocipherccl‚Üíencrypt/aes-192_with_iv-32 (b/op): +32.95% (32.9%)]( - [ ] [pkg/ccl/pgcryptoccl/pgcryptocipherccl‚Üíencrypt/aes-256_with_iv-32 (b/op): +21.96% (22.0%)]( - [ ] [pkg/ccl/storageccl‚Üíencryption/decryptfile/74_mib/chunk=100_b-32 (b/op): +20.31% (20.3%)]( help see also: [how to investigate a go test failure \(internal\)]( /cc /cdc jira issue: crdb-59073,[],"['PERFORMANCE', 'TESTING']","['C-test-failure', 'O-robot', 'C-performance', 'A-cdc', 'branch-master', 'release-blocker', 'T-cdc', 'O-microbench', 'branch-release-25.4']",github,2026-01-26T06:25:17Z,,pkg/ccl/changefeedccl: potential performance regression pkg/ccl/changefeedccl [performance regression]( on master @ [867004a4311025b7e0de44bc8c72fcbc8442f041]( performance regressions detected in package pkg/ccl comparison: v25.4.3 (71d85362) -&gt; refs/heads/master (867004a4) found 20 benchmark(s) with regressions ‚â•20%: - [ ] [pkg/ccl/changefeedccl‚Üíencoders/json/envelope=wrapped -32 (sec/op): +88.84% (88.8%)]( - [ ] [pkg/ccl/changefeedccl‚Üíencoders/json/envelope=row -32 (sec/op): +85.93% (85.9%)]( - [ ] [pkg/ccl/storageccl‚Üíencryption/encryptfile/1.0_kib/chunk=1.0_mib-32 (sec/op): +20.74% (20.7%)]( - [ ] [pkg/ccl/changefeedccl‚Üíencoders/json/envelope=row -32 (allocs/op): +115.10% (115.1%)]( - [ ] [pkg/ccl/changefeedccl‚Üíencoders/json/envelope=wrapped -32 (allocs/op): +76.21% (76.2%)]( - [ ] [pkg/ccl/changefeedccl‚Üíencoders/json/envelope=row -32 (allocs/op): +72.34% (72.3%)]( - [ ] [pkg/ccl/changefeedccl‚Üíencoders/json/envelope=row -32 (b/op): +114.62% (114.6%)]( - [ ] [pkg/ccl/changefeedccl‚Üíencoders/json/envelope=wrapped -32 (b/op): +75.57% (75.6%)]( - [ ] [pkg/ccl/changefeedccl‚Üíencoders/json/envelope=row -32 (b/op): +71.72% (71.7%)]( - [ ] [pkg/ccl/changefeedccl/schemafeed‚Üípauseorresumepolling/not_schema_locked-32 (b/op): +50.00% (50.0%)]( - [ ] [pkg/ccl/pgcryptoccl/pgcryptocipherccl‚Üíencrypt/aes-128_with_iv-32 (b/op): +49.88% (49.9%)]( - [ ] [pkg/ccl/pgcryptoccl/pgcryptocipherccl‚Üíencrypt/aes-128-32 (b/op): +48.90% (48.9%)]( - [ ] [pkg/ccl/storageccl‚Üíencryption/decryptfile/1.2_kib/chunk=100_b-32 (b/op): +40.29% (40.3%)]( - [ ] [pkg/ccl/storageccl‚Üíencryption/decryptfile/116_kib/chunk=100_b-32 (b/op): +40.25% (40.3%)]( - [ ] [pkg/ccl/storageccl‚Üíencryption/decryptfile/1.2_mib/chunk=100_b-32 (b/op): +39.72% (39.7%)]( - [ ] [pkg/ccl/pgcryptoccl/pgcryptocipherccl‚Üíencrypt/aes-256-32 (b/op): +39.12% (39.1%)]( - [ ] [pkg/ccl/pgcryptoccl/pgcryptocipherccl‚Üíencrypt/aes-192-32 (b/op): +32.95% (32.9%)]( - [ ] [pkg/ccl/pgcryptoccl/pgcryptocipherccl‚Üíencrypt/aes-192_with_iv-32 (b/op): +32.95% (32.9%)]( - [ ] [pkg/ccl/pgcryptoccl/pgcryptocipherccl‚Üíencrypt/aes-256_with_iv-32 (b/op): +21.96% (22.0%)]( - [ ] [pkg/ccl/storageccl‚Üíencryption/decryptfile/74_mib/chunk=100_b-32 (b/op): +20.31% (20.3%)]( help see also: [how to investigate a go test failure \(internal\)]( /cc /cdc jira issue: crdb-59073,4.2,Critical,1.0,"performance degradation, crash-like behavior"
pytorch/pytorch#173337,torch.nn.batchnorm expects positive eps in eval mode,"### üêõ describe the bug torch 2.10.0 enforces to have positive eps in eval mode, and are frozen and can be constant-folded and hence eps can be 0 **repro** there are couple of options here 1. check along with to avoid 2. non-zero check only in case of training mode option 1 is more general but comes with added complexity of checking ### versions torch==2.10.0 cc",[],['FEATURE'],"['module: nn', 'triaged', 'enhancement', 'module: norms and normalization']",github,2026-01-26T06:26:55Z,,"torch.nn.batchnorm expects positive eps in eval mode ### üêõ describe the bug torch 2.10.0 enforces to have positive eps in eval mode, and are frozen and can be constant-folded and hence eps can be 0 **repro** there are couple of options here 1. check along with to avoid 2. non-zero check only in case of training mode option 1 is more general but comes with added complexity of checking ### versions torch==2.10.0 cc",3.176,High,0.942,functional impact
microsoft/vscode#290327,lifecycle of and,found an error telemetry report about many undisposed of these: at a quick glance it was not obvious to me that we ever dispose of these 2?,[],['BUG'],"['bug', 'debt']",github,2026-01-26T06:28:12Z,,lifecycle of and found an error telemetry report about many undisposed of these: at a quick glance it was not obvious to me that we ever dispose of these 2?,2.263,Medium,0.734,functional impact
microsoft/vscode#290328,bad use of ?,"there are instances of where from the factory method, disposables are registered to the outer container (via ), not the thing itself: a good use seems to be:",[],['PERFORMANCE'],"['freeze-slow-crash-leak', 'insiders-released']",github,2026-01-26T06:31:29Z,2026-01-26T19:35:58Z,"bad use of ? there are instances of where from the factory method, disposables are registered to the outer container (via ), not the thing itself: a good use seems to be:",3.043,High,0.911,performance degradation
microsoft/vscode#290329,error: [filesystemservice] file:///users/tyleonha/.claude/projects/-users-tyleonha-code-microsoft-vscode-copilot-chat-2/803b9cb6-9b94-4893-9100-19c47978ec9b.jsonl exceeds max file size. failed to read 6mb > 5mb,we should handle big files fine.,[],['BUG'],"['bug', 'chat-external-agent']",github,2026-01-26T06:37:46Z,2026-01-26T08:39:46Z,error: [filesystemservice] file:///users/tyleonha/.claude/projects/-users-tyleonha-code-microsoft-vscode-copilot-chat-2/803b9cb6-9b94-4893-9100-19c47978ec9b.jsonl exceeds max file size. failed to read 6mb > 5mb we should handle big files fine.,2.613,Medium,0.814,functional impact
openssl/openssl#29753,ldflags gets duplicated in windows builds,"[commit 722c976]( (""harmonize the make variables across all known platforms families"") changes the handling of ldflags so that it is joined into bin_ldflags, lib_ldflags, etc... however, while the unix makefile template is changed in this commit to no longer use ldflags directly when linking shared libs and bins, the windows makefile template is not. henceforth, ldflags is introduced both by directly using it when linking shared libs and bins and via bin_ldflags/lib_ldflags/etc. for example, on windows i'm attempting to use a custom manifest to enable long path support so that openssl tests pass with my current directory layout. i'm setting the ldflags environment variable similar to: but in the build output, i'm seeing: in my case, this leads to an invalid merged manifest being generated since the embedded manifest includes duplicated nodes from the duplicate-specified manifest file.",[],['BUG'],['issue: bug report'],github,2026-01-26T06:43:27Z,,"ldflags gets duplicated in windows builds [commit 722c976]( (""harmonize the make variables across all known platforms families"") changes the handling of ldflags so that it is joined into bin_ldflags, lib_ldflags, etc... however, while the unix makefile template is changed in this commit to no longer use ldflags directly when linking shared libs and bins, the windows makefile template is not. henceforth, ldflags is introduced both by directly using it when linking shared libs and bins and via bin_ldflags/lib_ldflags/etc. for example, on windows i'm attempting to use a custom manifest to enable long path support so that openssl tests pass with my current directory layout. i'm setting the ldflags environment variable similar to: but in the build output, i'm seeing: in my case, this leads to an invalid merged manifest being generated since the embedded manifest includes duplicated nodes from the duplicate-specified manifest file.",4.6,Critical,1.0,crash-like behavior
python/cpython#144233,typo in os.eventfd documentation example,see the line should be changed from ### linked prs * gh-144234 * gh-144237 * gh-144238,[],['DOCUMENTATION'],['docs'],github,2026-01-26T06:58:40Z,2026-01-26T08:10:41Z,typo in os.eventfd documentation example see the line should be changed from ### linked prs * gh-144234 * gh-144237 * gh-144238,3.4,High,0.993,crash-like behavior
pandas-dev/pandas#63879,bug: conversion from numpy masked arrays to pandas array does not preserve missing values,"### pandas version checks - [x] i have checked that this issue has not already been reported. - [x] i have confirmed this bug exists on the [latest version]( of pandas. - [ ] i have confirmed this bug exists on the [main branch]( of pandas. ### reproducible example ### issue description numpy [masked arrays]( can be used to define numpy arrays with missing values, similar to pandas extension arrays. however, conversion to does not handle missing values. conversion to does, but does not preserve the correct type in all cases. fyi -sheep -gold ### expected behavior `python >>> pd.array(a) [1, , 3, ] length: 4, dtype: int64 >>> pd.array(b) ['foo', , 'bar', ] length: 4, dtype: string ### installed versions installed versions ------------------ commit : 366ccdfcd8ed1e5543bfb6d4ee0c9bc519898670 python : 3.12.9 python-bits : 64 os : linux os-release : 6.18.6-arch1-1 version : smp preempt_dynamic sun, 18 jan 2026 00:34:07 +0000 machine : x86_64 processor : byteorder : little lc_all : none lang : en_us.utf-8 locale : en_us.utf-8 pandas : 3.0.0 numpy : 2.3.5 dateutil : 2.9.0.post0 pip : 25.3 cython : none sphinx : none ipython : 9.2.0 adbc-driver-postgresql: none adbc-driver-sqlite : none bs4 : none bottleneck : none fastparquet : none fsspec : 2025.12.0 html5lib : none hypothesis : none gcsfs : none jinja2 : none lxml.etree : none matplotlib : 3.10.8 numba : 0.63.1 numexpr : none odfpy : none openpyxl : none psycopg2 : none pymysql : none pyarrow : none pyiceberg : none pyreadstat : none pytest : 9.0.2 python-calamine : none pytz : 2025.2 pyxlsb : none s3fs : none scipy : 1.16.3 sqlalchemy : none tables : none tabulate : none xarray : none xlrd : none xlsxwriter : none zstandard : none qtpy : none pyqt5 : none",[],['BUG'],"['Bug', 'Constructors']",github,2026-01-26T07:01:05Z,,"bug: conversion from numpy masked arrays to pandas array does not preserve missing values ### pandas version checks - [x] i have checked that this issue has not already been reported. - [x] i have confirmed this bug exists on the [latest version]( of pandas. - [ ] i have confirmed this bug exists on the [main branch]( of pandas. ### reproducible example ### issue description numpy [masked arrays]( can be used to define numpy arrays with missing values, similar to pandas extension arrays. however, conversion to does not handle missing values. conversion to does, but does not preserve the correct type in all cases. fyi -sheep -gold ### expected behavior `python >>> pd.array(a) [1, , 3, ] length: 4, dtype: int64 >>> pd.array(b) ['foo', , 'bar', ] length: 4, dtype: string ### installed versions installed versions ------------------ commit : 366ccdfcd8ed1e5543bfb6d4ee0c9bc519898670 python : 3.12.9 python-bits : 64 os : linux os-release : 6.18.6-arch1-1 version : smp preempt_dynamic sun, 18 jan 2026 00:34:07 +0000 machine : x86_64 processor : byteorder : little lc_all : none lang : en_us.utf-8 locale : en_us.utf-8 pandas : 3.0.0 numpy : 2.3.5 dateutil : 2.9.0.post0 pip : 25.3 cython : none sphinx : none ipython : 9.2.0 adbc-driver-postgresql: none adbc-driver-sqlite : none bs4 : none bottleneck : none fastparquet : none fsspec : 2025.12.0 html5lib : none hypothesis : none gcsfs : none jinja2 : none lxml.etree : none matplotlib : 3.10.8 numba : 0.63.1 numexpr : none odfpy : none openpyxl : none psycopg2 : none pymysql : none pyarrow : none pyiceberg : none pyreadstat : none pytest : 9.0.2 python-calamine : none pytz : 2025.2 pyxlsb : none s3fs : none scipy : 1.16.3 sqlalchemy : none tables : none tabulate : none xarray : none xlrd : none xlsxwriter : none zstandard : none qtpy : none pyqt5 : none",2.434,Medium,0.773,functional impact
cockroachdb/cockroach#161774,ccl/workloadccl/allccl: testallregisteredsetup failed,ccl/workloadccl/allccl.testallregisteredsetup [failed]( on release-25.4 @ [27249807fb41bbb90ab7d7b184c077a442fd824d]( parameters: - attempt=1 - run=7 - shard=2 help see also: [how to investigate a go test failure \(internal\)]( /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-59074,[],['TESTING'],"['C-test-failure', 'O-robot', 'release-blocker', 'T-db-server', 'branch-release-25.4']",github,2026-01-26T07:11:16Z,,ccl/workloadccl/allccl: testallregisteredsetup failed ccl/workloadccl/allccl.testallregisteredsetup [failed]( on release-25.4 @ [27249807fb41bbb90ab7d7b184c077a442fd824d]( parameters: - attempt=1 - run=7 - shard=2 help see also: [how to investigate a go test failure \(internal\)]( /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-59074,1.6,Low,0.584,localized low-impact
kubernetes/kubernetes#136527,job validation error message is misleading when updating .status.starttime,"### what happened? i encountered a confusing validation error while trying to update the of an existing job. the error message received was: however, i am not trying to remove the , but rather update it. i checked the code and found that the validation logic triggers this error for any inequality, not just when the is removed (set to nil). ### what did you expect to happen? i expected the error message to reflect the actual validation constraint accurately. ### how can we reproduce it (as minimally and precisely as possible)? 1. create a non-suspended job 2. update to a different value using: ### anything else we need to know? if this is confirmed as an issue, i would be happy to submit a pr to correct the error message and potentially the error type to make it more user-friendly. ### kubernetes version ### cloud provider ### os version ### install tools ### container runtime (cri) and version (if applicable) ### related plugins (cni, csi, ...) and versions (if applicable)",[],['BUG'],"['kind/bug', 'priority/backlog', 'needs-sig', 'triage/accepted']",github,2026-01-26T07:21:46Z,,"job validation error message is misleading when updating .status.starttime ### what happened? i encountered a confusing validation error while trying to update the of an existing job. the error message received was: however, i am not trying to remove the , but rather update it. i checked the code and found that the validation logic triggers this error for any inequality, not just when the is removed (set to nil). ### what did you expect to happen? i expected the error message to reflect the actual validation constraint accurately. ### how can we reproduce it (as minimally and precisely as possible)? 1. create a non-suspended job 2. update to a different value using: ### anything else we need to know? if this is confirmed as an issue, i would be happy to submit a pr to correct the error message and potentially the error type to make it more user-friendly. ### kubernetes version ### cloud provider ### os version ### install tools ### container runtime (cri) and version (if applicable) ### related plugins (cni, csi, ...) and versions (if applicable)",2.403,Medium,0.766,functional impact
cockroachdb/cockroach#161775,ccl/changefeedccl: testchangefeednemeses failed,ccl/changefeedccl.testchangefeednemeses [failed]( on release-26.1.0-rc @ [6c9b2f778bd29895383d493a2fa4490323499ecf]( parameters: - attempt=1 - run=3 - shard=11 help see also: [how to investigate a go test failure \(internal\)]( same failure on other branches - ccl/changefeedccl: testchangefeednemeses failed [a-cdc c-test-failure o-robot p-2 t-cdc branch-master] /cc /cdc [this test on roachdash]( | [improve this report!]( jira issue: crdb-59075,[],['TESTING'],"['C-test-failure', 'O-robot', 'A-cdc', 'T-cdc', 'P-2', 'branch-release-26.1.0-rc']",github,2026-01-26T07:43:42Z,,ccl/changefeedccl: testchangefeednemeses failed ccl/changefeedccl.testchangefeednemeses [failed]( on release-26.1.0-rc @ [6c9b2f778bd29895383d493a2fa4490323499ecf]( parameters: - attempt=1 - run=3 - shard=11 help see also: [how to investigate a go test failure \(internal\)]( same failure on other branches - ccl/changefeedccl: testchangefeednemeses failed [a-cdc c-test-failure o-robot p-2 t-cdc branch-master] /cc /cdc [this test on roachdash]( | [improve this report!]( jira issue: crdb-59075,3.8,Critical,1.0,crash-like behavior
envoyproxy/envoy#43158,newer release available : 2.18 (current: gperftools-2.17.2),package name: gperftools .17.2 current version: gperftools-2.17.2 -08-15 available version: 2.18 -01-25 upstream releases:,[],['UI'],"['dependencies', 'no stalebot', 'area/build']",github,2026-01-26T08:11:33Z,,newer release available : 2.18 (current: gperftools-2.17.2) package name: gperftools .17.2 current version: gperftools-2.17.2 -08-15 available version: 2.18 -01-25 upstream releases:,1.8,Low,0.629,user-visible issue
scikit-learn/scikit-learn#33135,bug: error message raised by validate_params cannot pass check_estimator,"### describe the bug i maintain the scikit-learn compatible projects [fastcan]( and use to check its compatibility. when i use to validate the argument of , the error message of input is accepted by . however, when i use , the error message of input is invalid for . both and are standard ways to validate the function arguments. the error messages of both should be valid. ### steps/code to reproduce ### expected results ### actual results ### versions",[],['BUG'],"['Bug', 'Needs Triage']",github,2026-01-26T08:18:28Z,,"bug: error message raised by validate_params cannot pass check_estimator ### describe the bug i maintain the scikit-learn compatible projects [fastcan]( and use to check its compatibility. when i use to validate the argument of , the error message of input is accepted by . however, when i use , the error message of input is invalid for . both and are standard ways to validate the function arguments. the error messages of both should be valid. ### steps/code to reproduce ### expected results ### actual results ### versions",2.557,Medium,0.801,functional impact
cockroachdb/cockroach#161776,roachtest: drain-and-decommission/nodes=9 failed,"**note:** this build has runtime assertions enabled. if the same failure was hit in a run without assertions enabled, there should be a similar failure without this message. if there isn& ;t one, then this failure is likely due to an assertion violation or (assertion) timeout. roachtest.drain-and-decommission/nodes=9 [failed]( with [artifacts]( on master @ [867004a4311025b7e0de44bc8c72fcbc8442f041]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21021555-1769410111-50-n9cpu4-0001 | 57.151.100.195 | 10.1.0.127 | | teamcity-21021555-1769410111-50-n9cpu4-0002 | 172.203.244.237 | 10.1.0.125 | | teamcity-21021555-1769410111-50-n9cpu4-0003 | 20.55.84.49 | 10.1.0.123 | | teamcity-21021555-1769410111-50-n9cpu4-0004 | 20.115.98.153 | 10.1.0.134 | | teamcity-21021555-1769410111-50-n9cpu4-0005 | 20.127.192.203 | 10.1.0.124 | | teamcity-21021555-1769410111-50-n9cpu4-0006 | 20.127.164.9 | 10.1.0.14 | | teamcity-21021555-1769410111-50-n9cpu4-0007 | 20.102.96.188 | 10.1.0.129 | | teamcity-21021555-1769410111-50-n9cpu4-0008 | 20.127.237.87 | 10.1.0.137 | | teamcity-21021555-1769410111-50-n9cpu4-0009 | 40.76.232.61 | 10.1.0.173 | parameters: - arch=amd64 - cloud=azure - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - fs=ext4 - localssd=true - metamorphicleases=leader - runtimeassertionsbuild=true help see: [roachtest readme]( see: [how to investigate \(internal\)]( _grafana is not yet available for azure clusters_ /cc /kv-triage [this test on roachdash]( | [improve this report!]( jira issue: crdb-59076",[],['TESTING'],"['C-test-failure', 'O-robot', 'O-roachtest', 'branch-master', 'release-blocker', 'T-kv', 'B-runtime-assertions-enabled']",github,2026-01-26T08:21:04Z,,"roachtest: drain-and-decommission/nodes=9 failed **note:** this build has runtime assertions enabled. if the same failure was hit in a run without assertions enabled, there should be a similar failure without this message. if there isn& ;t one, then this failure is likely due to an assertion violation or (assertion) timeout. roachtest.drain-and-decommission/nodes=9 [failed]( with [artifacts]( on master @ [867004a4311025b7e0de44bc8c72fcbc8442f041]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21021555-1769410111-50-n9cpu4-0001 | 57.151.100.195 | 10.1.0.127 | | teamcity-21021555-1769410111-50-n9cpu4-0002 | 172.203.244.237 | 10.1.0.125 | | teamcity-21021555-1769410111-50-n9cpu4-0003 | 20.55.84.49 | 10.1.0.123 | | teamcity-21021555-1769410111-50-n9cpu4-0004 | 20.115.98.153 | 10.1.0.134 | | teamcity-21021555-1769410111-50-n9cpu4-0005 | 20.127.192.203 | 10.1.0.124 | | teamcity-21021555-1769410111-50-n9cpu4-0006 | 20.127.164.9 | 10.1.0.14 | | teamcity-21021555-1769410111-50-n9cpu4-0007 | 20.102.96.188 | 10.1.0.129 | | teamcity-21021555-1769410111-50-n9cpu4-0008 | 20.127.237.87 | 10.1.0.137 | | teamcity-21021555-1769410111-50-n9cpu4-0009 | 40.76.232.61 | 10.1.0.173 | parameters: - arch=amd64 - cloud=azure - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - fs=ext4 - localssd=true - metamorphicleases=leader - runtimeassertionsbuild=true help see: [roachtest readme]( see: [how to investigate \(internal\)]( _grafana is not yet available for azure clusters_ /cc /kv-triage [this test on roachdash]( | [improve this report!]( jira issue: crdb-59076",2.913,Medium,0.882,functional impact
cockroachdb/cockroach#161777,roachtest: tpcc-nowait/isolation-level=mixed/nodes=3/w=1 failed,roachtest.tpcc-nowait/isolation-level=mixed/nodes=3/w=1 [failed]( with [artifacts]( on release-25.3.8-rc @ [129f86631b285cfd2bd8740db920947b5a9c89ab]( parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=16 - encrypted=true - metamorphicleases=epoch - runtimeassertionsbuild=false - ssd=0 help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( same failure on other branches - roachtest: tpcc-nowait/isolation-level=mixed/nodes=3/w=1 failed [a-kv-transactions c-test-failure o-roachtest o-robot p-3 t-kv x-duplicate branch-master] /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-59077,[],['TESTING'],"['C-test-failure', 'O-robot', 'X-duplicate', 'O-roachtest', 'release-blocker', 'T-testeng', 'X-infra-flake', 'branch-release-25.3.8-rc']",github,2026-01-26T08:22:55Z,2026-01-27T20:13:46Z,roachtest: tpcc-nowait/isolation-level=mixed/nodes=3/w=1 failed roachtest.tpcc-nowait/isolation-level=mixed/nodes=3/w=1 [failed]( with [artifacts]( on release-25.3.8-rc @ [129f86631b285cfd2bd8740db920947b5a9c89ab]( parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=16 - encrypted=true - metamorphicleases=epoch - runtimeassertionsbuild=false - ssd=0 help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( same failure on other branches - roachtest: tpcc-nowait/isolation-level=mixed/nodes=3/w=1 failed [a-kv-transactions c-test-failure o-roachtest o-robot p-3 t-kv x-duplicate branch-master] /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-59077,3.229,High,0.954,system-wide impact
numpy/numpy#30726,bug: forces the array type to be numpy,"### describe the issue: i'm not sure if this is strictly speaking a bug. i am working on a project where we are trying to adopt the [array api]( i encountered a bug which confused me for a while ( but when stepping through the code i realised that was forcing the data type of my [array-api-strict]( array to be numpy. ### reproduce the code example: ### error message: ### python and numpy versions: 2.4.0 3.14.0 (main, oct 10 2025, 12:54:13) [clang 20.1.4 ] ### runtime environment: [{'numpy_version': '2.4.0', 'python': '3.14.0 (main, oct 10 2025, 12:54:13) [clang 20.1.4 ]', 'uname': uname_result(system='darwin', node='macbookpro.mynet', release='25.2.0', version='darwin kernel version 25.2.0: tue nov 18 21:09:55 pst 2025; root:xnu-12377.61.12~1/release_arm64_t8103', machine='arm64')}, {'simd_extensions': {'baseline': ['neon', 'neon_fp16', 'neon_vfpv4', 'asimd'], 'found': ['asimdhp', 'asimddp'], 'not_found': ['asimdfhm']}}, {'ignore_floating_point_errors_in_matmul': true}] ### how does this issue affect you or how did you find it: the bug caused some confusion, with the type being changed outside our control. this was confused further as we were using which hid away the problem even more. ended up ""solving"" it by changing our function to accept a function + arguments as separate inputs.",[],['BUG'],['00 - Bug'],github,2026-01-26T08:27:33Z,,"bug: forces the array type to be numpy ### describe the issue: i'm not sure if this is strictly speaking a bug. i am working on a project where we are trying to adopt the [array api]( i encountered a bug which confused me for a while ( but when stepping through the code i realised that was forcing the data type of my [array-api-strict]( array to be numpy. ### reproduce the code example: ### error message: ### python and numpy versions: 2.4.0 3.14.0 (main, oct 10 2025, 12:54:13) [clang 20.1.4 ] ### runtime environment: [{'numpy_version': '2.4.0', 'python': '3.14.0 (main, oct 10 2025, 12:54:13) [clang 20.1.4 ]', 'uname': uname_result(system='darwin', node='macbookpro.mynet', release='25.2.0', version='darwin kernel version 25.2.0: tue nov 18 21:09:55 pst 2025; root:xnu-12377.61.12~1/release_arm64_t8103', machine='arm64')}, {'simd_extensions': {'baseline': ['neon', 'neon_fp16', 'neon_vfpv4', 'asimd'], 'found': ['asimdhp', 'asimddp'], 'not_found': ['asimdfhm']}}, {'ignore_floating_point_errors_in_matmul': true}] ### how does this issue affect you or how did you find it: the bug caused some confusion, with the type being changed outside our control. this was confused further as we were using which hid away the problem even more. ended up ""solving"" it by changing our function to accept a function + arguments as separate inputs.",6.4,Critical,1.0,crash-like behavior
microsoft/vscode#290338,"verifies that ""hot exit"" works for dirty files",,[],['TESTING'],['smoke-test-failure'],github,2026-01-26T08:28:59Z,2026-01-26T08:36:48Z,"verifies that ""hot exit"" works for dirty files",1.6,Low,0.584,localized low-impact
microsoft/vscode#290342,"support atomic undo for ""accept all"" copilot changes (like cursor ide)","feature request: i hope copilot for vs code can support an ""atomic undo"" feature: when i click ""accept all"" or ""keep all"" suggestions in a single copilot session, all code changes from this session should be merged into a single undo operation. this way, i only need to press ctrl+z once to revert all changes made by this ai session, instead of having to undo each individual change one by one. use case: currently, each copilot modification is recorded as a separate undo step. if a session modifies many places, i have to press ctrl+z multiple times to revert all changes, which is very inconvenient. other ai ides like cursor already support ""batch undo after bulk modification,"" which greatly improves the user experience. expected behavior: copilot should merge all changes from a single ai session into one undo point. users should be able to revert all changes from a session with a single undo action.",[],['FEATURE'],"['feature-request', 'chat-agent-editing']",github,2026-01-26T09:04:11Z,,"support atomic undo for ""accept all"" copilot changes (like cursor ide) feature request: i hope copilot for vs code can support an ""atomic undo"" feature: when i click ""accept all"" or ""keep all"" suggestions in a single copilot session, all code changes from this session should be merged into a single undo operation. this way, i only need to press ctrl+z once to revert all changes made by this ai session, instead of having to undo each individual change one by one. use case: currently, each copilot modification is recorded as a separate undo step. if a session modifies many places, i have to press ctrl+z multiple times to revert all changes, which is very inconvenient. other ai ides like cursor already support ""batch undo after bulk modification,"" which greatly improves the user experience. expected behavior: copilot should merge all changes from a single ai session into one undo point. users should be able to revert all changes from a session with a single undo action.",3.6,Critical,1.0,crash-like behavior
python/cpython#144240,typo in configure comment,"i noticed a small typo in a comment in . at line **26978**, the word ** ** is misspelled and should be ** **: ### linked prs * gh-144241",[],['DOCUMENTATION'],['docs'],github,2026-01-26T09:12:28Z,2026-01-26T13:47:18Z,"typo in configure comment i noticed a small typo in a comment in . at line **26978**, the word ** ** is misspelled and should be ** **: ### linked prs * gh-144241",1.2,Low,0.493,localized low-impact
flutter/flutter#181479,stackoverflowerror running example app test in a container,### steps to reproduce 1. create an app with or clone [this repository]( 2. run inside a container ### expected results expectation is that the single test passes. ### actual results the test fails with stack overflow. ### code sample this bug is reproducible with the default code generated by . i've provided a [github repo]( with the code and instructions on how to reproduce. code sample ### logs logs command: log: ### flutter doctor output doctor output in container,[],['TESTING'],"['a: tests', 'framework', 'P3', 'team-framework', 'triaged-framework']",github,2026-01-26T09:15:37Z,,stackoverflowerror running example app test in a container ### steps to reproduce 1. create an app with or clone [this repository]( 2. run inside a container ### expected results expectation is that the single test passes. ### actual results the test fails with stack overflow. ### code sample this bug is reproducible with the default code generated by . i've provided a [github repo]( with the code and instructions on how to reproduce. code sample ### logs logs command: log: ### flutter doctor output doctor output in container,3.8,Critical,1.0,crash-like behavior
cockroachdb/cockroach#161779,kv/kvserver/batcheval: testdbaddsstable failed,kv/kvserver/batcheval.testdbaddsstable [failed]( on release-25.2 @ [7ea976da96e5c9f41820090cabb6777ba04f7607]( parameters: - attempt=1 - deadlock=true - run=3 - shard=1 help see also: [how to investigate a go test failure \(internal\)]( /cc /kv-triage [this test on roachdash]( | [improve this report!]( jira issue: crdb-59078,[],['TESTING'],"['C-test-failure', 'O-robot', 'release-blocker', 'T-kv', 'branch-release-25.2']",github,2026-01-26T09:15:48Z,,kv/kvserver/batcheval: testdbaddsstable failed kv/kvserver/batcheval.testdbaddsstable [failed]( on release-25.2 @ [7ea976da96e5c9f41820090cabb6777ba04f7607]( parameters: - attempt=1 - deadlock=true - run=3 - shard=1 help see also: [how to investigate a go test failure \(internal\)]( /cc /kv-triage [this test on roachdash]( | [improve this report!]( jira issue: crdb-59078,3.8,Critical,1.0,crash-like behavior
cockroachdb/cockroach#161780,sql/importer: testimportintocsvcancel failed,sql/importer.testimportintocsvcancel [failed]( on release-26.1.0-rc @ [6c9b2f778bd29895383d493a2fa4490323499ecf]( parameters: - attempt=1 - deadlock=true - run=3 - shard=1 help see also: [how to investigate a go test failure \(internal\)]( [this test on roachdash]( | [improve this report!]( jira issue: crdb-59079,[],['TESTING'],"['C-test-failure', 'O-robot', 'release-blocker', 'T-sql-queries', 'branch-release-26.1.0-rc']",github,2026-01-26T09:24:50Z,2026-01-26T16:40:31Z,sql/importer: testimportintocsvcancel failed sql/importer.testimportintocsvcancel [failed]( on release-26.1.0-rc @ [6c9b2f778bd29895383d493a2fa4490323499ecf]( parameters: - attempt=1 - deadlock=true - run=3 - shard=1 help see also: [how to investigate a go test failure \(internal\)]( [this test on roachdash]( | [improve this report!]( jira issue: crdb-59079,3.8,Critical,1.0,crash-like behavior
vercel/next.js#89033,"null cache entries have size 0, preventing lru eviction","### link to the code that reproduces this issue ### to reproduce 1. 2. 3. (sends 50k requests with unique ids) 4. take heap snapshot of the server process 5. search for - count matches request count ### current vs. expected behavior **current:** instances grow unbounded. each unique dynamic route path creates a new entry that never gets evicted. **expected:** lru cache should evict old entries when reaching (1mb). ### provide environment information ### which area(s) are affected? (select all that apply) route handlers, performance ### which stage(s) are affected? (select all that apply) other (deployed), next start (local) ### additional context i am running version 15.5.9 with self-hosting, and while investigating the cause of memory leaks, i discovered that lrunode instances were accumulating abnormally in heap dumps.",[],['PERFORMANCE'],"['Performance', 'Route Handlers']",github,2026-01-26T09:34:14Z,2026-01-27T11:06:09Z,"null cache entries have size 0, preventing lru eviction ### link to the code that reproduces this issue ### to reproduce 1. 2. 3. (sends 50k requests with unique ids) 4. take heap snapshot of the server process 5. search for - count matches request count ### current vs. expected behavior **current:** instances grow unbounded. each unique dynamic route path creates a new entry that never gets evicted. **expected:** lru cache should evict old entries when reaching (1mb). ### provide environment information ### which area(s) are affected? (select all that apply) route handlers, performance ### which stage(s) are affected? (select all that apply) other (deployed), next start (local) ### additional context i am running version 15.5.9 with self-hosting, and while investigating the cause of memory leaks, i discovered that lrunode instances were accumulating abnormally in heap dumps.",4.6,Critical,1.0,performance degradation
microsoft/vscode#290346,unread state seems flaky and random,"restarted insiders to this unread state. i am very certain i had all sessions read/worked on, esp local unread sessions are very confusing",['Unread state seems flaky and random (fix #290346) (#291227)'],['BUG'],"['bug', 'chat-agents-view']",github,2026-01-26T09:35:37Z,,"unread state seems flaky and random restarted insiders to this unread state. i am very certain i had all sessions read/worked on, esp local unread sessions are very confusing Unread state seems flaky and random (fix #290346) (#291227)",2.25,Medium,0.731,functional impact
python/cpython#144242,recommend direct prs for trivial changes in the issue template menu,"i find it that people often open issues just for typo fixes. sometimes these typo fixes should rather go unfixed, because they're not user-facing in any meaningful way. maybe this would help? or maybe there's too much on this list already? this one would redirect to preview it at / i'll submit a pr if i get a green light here. cc -turner . ### linked prs * gh-144288",[],['FEATURE'],"['type-feature', 'infra']",github,2026-01-26T09:45:35Z,,"recommend direct prs for trivial changes in the issue template menu i find it that people often open issues just for typo fixes. sometimes these typo fixes should rather go unfixed, because they're not user-facing in any meaningful way. maybe this would help? or maybe there's too much on this list already? this one would redirect to preview it at / i'll submit a pr if i get a green light here. cc -turner . ### linked prs * gh-144288",3.6,Critical,1.0,crash-like behavior
flutter/flutter#181483,[impeller] fail to render pixel buffer texture on linux,### steps to reproduce 1.down load pixel buffer sample code( 2.flutter run --enable-impeller on ubuntu 20.04 ### expected results can render a image successful by texture. ### actual results no image to show ### code sample code sample ### screenshots or video screenshots / video demonstration [upload media here] ### logs logs ### flutter doctor output doctor output,[],['UI'],"['engine', 'platform-linux', 'has reproducible steps', 'e: impeller', 'team-engine', 'fyi-linux', 'found in release: 3.38', 'found in release: 3.41']",github,2026-01-26T10:15:00Z,,[impeller] fail to render pixel buffer texture on linux ### steps to reproduce 1.down load pixel buffer sample code( 2.flutter run --enable-impeller on ubuntu 20.04 ### expected results can render a image successful by texture. ### actual results no image to show ### code sample code sample ### screenshots or video screenshots / video demonstration [upload media here] ### logs logs ### flutter doctor output doctor output,1.8,Low,0.629,user-visible issue
microsoft/vscode#290352,filter agent sessions resets when selecting additional session state filter,"type: bug 1. open chat view 1. update the list of session filters (e.g. uncheck completed) 1. update the list of session filters (e.g. uncheck in progress) the filters reset instead of applying the second filter. this only happens when filtering on session status. vs code version: code - insiders 1.109.0-insider (abb70fbc79435e892ca8e80654b47257de775c8f, 2026-01-26t05:02:47.498z) os version: windows_nt x64 10.0.26100 modes: system info |item|value| |---|---| |cpus|12th gen intel(r) core(tm) i7-1265u (12 x 2688)| |gpu status|2d_canvas: enabled gpu0: vendor= 0x8086, device=0x46a8 [intel(r) iris(r) xe graphics], driver_vendor=intel, driver_version=32.0.101.6737 *active* gpu1: vendor= 0x1414, device=0x008c [microsoft basic render driver], driver_version=10.0.26100.7309 machine model name: machine model version: direct_rendering_display_compositor: disabled_off_ok gpu_compositing: enabled multiple_raster_threads: enabled_on opengl: enabled_on rasterization: enabled raw_draw: disabled_off_ok skia_graphite: disabled_off trees_in_viz: disabled_off video_decode: enabled video_encode: enabled webgl: enabled webgl2: enabled webgpu: enabled webnn: disabled_off| |load (avg)|undefined| |memory (system)|31.83gb (9.84gb free)| |process argv|--transient --user-data-dir c:\\users\\nicktrog\\appdata\\local\\temp\\vscode-qpxfpprc\\data --extensions-dir c:\\users\\nicktrog\\appdata\\local\\temp\\vscode-qpxfpprc\\extensions --crash-reporter-id 55df780f-61b2-46ba-b106-d3f2bb356839| |screen reader|no| |vm|0%| extensions (1) extension|author (truncated)|version ---|---|--- copilot-chat|git|0.37.2026012601 a/b experiments",[],['BUG'],"['bug', 'confirmed', 'insiders-released', 'chat-agents-view']",github,2026-01-26T10:19:40Z,2026-01-26T19:09:36Z,"filter agent sessions resets when selecting additional session state filter type: bug 1. open chat view 1. update the list of session filters (e.g. uncheck completed) 1. update the list of session filters (e.g. uncheck in progress) the filters reset instead of applying the second filter. this only happens when filtering on session status. vs code version: code - insiders 1.109.0-insider (abb70fbc79435e892ca8e80654b47257de775c8f, 2026-01-26t05:02:47.498z) os version: windows_nt x64 10.0.26100 modes: system info |item|value| |---|---| |cpus|12th gen intel(r) core(tm) i7-1265u (12 x 2688)| |gpu status|2d_canvas: enabled gpu0: vendor= 0x8086, device=0x46a8 [intel(r) iris(r) xe graphics], driver_vendor=intel, driver_version=32.0.101.6737 *active* gpu1: vendor= 0x1414, device=0x008c [microsoft basic render driver], driver_version=10.0.26100.7309 machine model name: machine model version: direct_rendering_display_compositor: disabled_off_ok gpu_compositing: enabled multiple_raster_threads: enabled_on opengl: enabled_on rasterization: enabled raw_draw: disabled_off_ok skia_graphite: disabled_off trees_in_viz: disabled_off video_decode: enabled video_encode: enabled webgl: enabled webgl2: enabled webgpu: enabled webnn: disabled_off| |load (avg)|undefined| |memory (system)|31.83gb (9.84gb free)| |process argv|--transient --user-data-dir c:\\users\\nicktrog\\appdata\\local\\temp\\vscode-qpxfpprc\\data --extensions-dir c:\\users\\nicktrog\\appdata\\local\\temp\\vscode-qpxfpprc\\extensions --crash-reporter-id 55df780f-61b2-46ba-b106-d3f2bb356839| |screen reader|no| |vm|0%| extensions (1) extension|author (truncated)|version ---|---|--- copilot-chat|git|0.37.2026012601 a/b experiments",6.0,Critical,1.0,crash-like behavior
flutter/flutter#181484,"behavior difference from , pops up console windows","in work on jaspr+flutter we noticed that behaves differently to in a surprising way. the difference comes if is launched in a specific way, for example with this dart script: then run what happens is that every command dart.bat launches, including dart.exe, pops up a new console window and outputs to that window instead of to the parent process. if you launch directly you don't get that. apparently, windows has some weird behavior around automatically launching new console windows if there isn't already one ""attached"" to the process. and, it triggers in when it's launched without a console window, . (there is a similar bug internal to dart in 3.10 that can cause consoles to pop up on windows, fixed in if you run directly, instead of , you don't hit the same issue. . we can work around the issue for jaspr, but i thought it was worth filing for flutter in case someone can figure out a way to make behave more like in this regard, as it might help in other ways :) long thread where we figured all this out:",[],['UI'],"['tool', 'a: build', 'dependency: dart:io', 'team-tool']",github,2026-01-26T11:28:58Z,,"behavior difference from , pops up console windows in work on jaspr+flutter we noticed that behaves differently to in a surprising way. the difference comes if is launched in a specific way, for example with this dart script: then run what happens is that every command dart.bat launches, including dart.exe, pops up a new console window and outputs to that window instead of to the parent process. if you launch directly you don't get that. apparently, windows has some weird behavior around automatically launching new console windows if there isn't already one ""attached"" to the process. and, it triggers in when it's launched without a console window, . (there is a similar bug internal to dart in 3.10 that can cause consoles to pop up on windows, fixed in if you run directly, instead of , you don't hit the same issue. . we can work around the issue for jaspr, but i thought it was worth filing for flutter in case someone can figure out a way to make behave more like in this regard, as it might help in other ways :) long thread where we figured all this out:",1.8,Low,0.629,user-visible issue
microsoft/vscode#290714,support choosing / saving run configurations by file,"especially in multi-language projects, it is common to run different files with different run configurations. the same applies if different files are supposted to be run with different command line arguments. currently, this requires manually switching the run configuration prior to every run. this is tedious and currently one of the top things bothering me about vscode. i wish for the option to attach run configurations to individual files. ### potential implementation below are some notes on how this *could* be implemented. - this could be implemented via some workspace file containing a list / mapping from all already run files to the last used run configuration. - consistency of this file can easily be checked regularly and nonexisting files / run configurations be removed. - whenever a run configuration is changed manually for a file, the list is updated. - when files are removed or renamed, the list is updated as well. - a similar mechanism would be nice for renaming run configurations, but it would not be an issue to let this operation cause breaking the saved run configurations - after all, this is a mere convenience feature.",[],"['BUG', 'FEATURE']","['feature-request', 'debug']",github,2026-01-26T11:33:28Z,,"support choosing / saving run configurations by file especially in multi-language projects, it is common to run different files with different run configurations. the same applies if different files are supposted to be run with different command line arguments. currently, this requires manually switching the run configuration prior to every run. this is tedious and currently one of the top things bothering me about vscode. i wish for the option to attach run configurations to individual files. ### potential implementation below are some notes on how this *could* be implemented. - this could be implemented via some workspace file containing a list / mapping from all already run files to the last used run configuration. - consistency of this file can easily be checked regularly and nonexisting files / run configurations be removed. - whenever a run configuration is changed manually for a file, the list is updated. - when files are removed or renamed, the list is updated as well. - a similar mechanism would be nice for renaming run configurations, but it would not be an issue to let this operation cause breaking the saved run configurations - after all, this is a mere convenience feature.",2.989,Medium,0.899,crash-like behavior
cockroachdb/cockroach#161781,roachtest: hibernate failed,"**note:** this build has runtime assertions enabled. if the same failure was hit in a run without assertions enabled, there should be a similar failure without this message. if there isn& ;t one, then this failure is likely due to an assertion violation or (assertion) timeout. roachtest.hibernate [failed]( with [artifacts]( on master @ [867004a4311025b7e0de44bc8c72fcbc8442f041]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21021544-1769411655-19-n1cpu4-0001 | 35.231.103.214 | 10.142.0.238 | parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - metamorphicleases=expiration - metamorphicwritebuffering=true - runtimeassertionsbuild=true help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( /cc /sql-foundations [this test on roachdash]( | [improve this report!]( jira issue: crdb-59081",[],['TESTING'],"['C-test-failure', 'O-robot', 'O-roachtest', 'branch-master', 'T-sql-foundations', 'P-2', 'B-runtime-assertions-enabled']",github,2026-01-26T11:33:34Z,,"roachtest: hibernate failed **note:** this build has runtime assertions enabled. if the same failure was hit in a run without assertions enabled, there should be a similar failure without this message. if there isn& ;t one, then this failure is likely due to an assertion violation or (assertion) timeout. roachtest.hibernate [failed]( with [artifacts]( on master @ [867004a4311025b7e0de44bc8c72fcbc8442f041]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21021544-1769411655-19-n1cpu4-0001 | 35.231.103.214 | 10.142.0.238 | parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - metamorphicleases=expiration - metamorphicwritebuffering=true - runtimeassertionsbuild=true help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( /cc /sql-foundations [this test on roachdash]( | [improve this report!]( jira issue: crdb-59081",2.893,Medium,0.877,functional impact
kubernetes/kubernetes#136533,panic: assignment to entry in nil map in pkg/util/resource.maxresourcelist when pod container has nil resources.requests,"## what happened? when using to calculate pod resource requests, a panic occurs when comparing **spec** container resources with **status** container resources, and the spec has nil while the status has populated values. ## what did you expect to happen? the function should handle nil gracefully and return an empty or properly initialized instead of panicking. ## how can we reproduce it? the function compares spec and status resources: the panic occurs when: 1. is **nil** (pod doesn't define resource requests) 2. has **values** (populated by kubelet or admission controllers) 3. any code calls on that pod this state can occur when: - a limitrange with applies defaults to pods without explicit requests - the kubelet populates with actual values - virtual cluster (vcluster) syncers copy status back but don't modify spec example pod state that triggers the panic: ## root cause analysis in , the function doesn't handle nil first argument: then panics when trying to assign to the nil map: ## suggested fix add nil check in the function: ## environment - kubernetes version: v1.34.0 (kubectl v0.34.0) - discovered via: argocd v3 using",[],['BUG'],"['kind/bug', 'sig/cli', 'needs-triage']",github,2026-01-26T11:34:48Z,,"panic: assignment to entry in nil map in pkg/util/resource.maxresourcelist when pod container has nil resources.requests ## what happened? when using to calculate pod resource requests, a panic occurs when comparing **spec** container resources with **status** container resources, and the spec has nil while the status has populated values. ## what did you expect to happen? the function should handle nil gracefully and return an empty or properly initialized instead of panicking. ## how can we reproduce it? the function compares spec and status resources: the panic occurs when: 1. is **nil** (pod doesn't define resource requests) 2. has **values** (populated by kubelet or admission controllers) 3. any code calls on that pod this state can occur when: - a limitrange with applies defaults to pods without explicit requests - the kubelet populates with actual values - virtual cluster (vcluster) syncers copy status back but don't modify spec example pod state that triggers the panic: ## root cause analysis in , the function doesn't handle nil first argument: then panics when trying to assign to the nil map: ## suggested fix add nil check in the function: ## environment - kubernetes version: v1.34.0 (kubectl v0.34.0) - discovered via: argocd v3 using",6.4,Critical,1.0,crash-like behavior
flutter/flutter#181487,[windows] textfield cannot completely disable system ime,"### use case flutter's textfield widget lacks proper control over system input method editors (ime). while developers can configure keyboard layouts and input formatting, there's no reliable way to prevent ime activation - especially problematic for cjk languages where imes automatically engage. ### proposal current properties like keyboardtype, textinputaction, and inputformatters only control keyboard appearance and text filtering, but cannot prevent ime composition states. this breaks applications requiring direct keyboard input only (otp/pin entry, terminals, command interfaces). numeric-only fields still trigger ime composition real-time validation disrupted by ime intermediate states inconsistent behavior across platforms no workaround for cjk language environments",[],['FEATURE'],"['a: text input', 'c: new feature', 'framework', 'engine', 'a: internationalization', 'platform-windows', 'c: proposal', 'team-text-input', 'fyi-windows']",github,2026-01-26T12:39:25Z,,"[windows] textfield cannot completely disable system ime ### use case flutter's textfield widget lacks proper control over system input method editors (ime). while developers can configure keyboard layouts and input formatting, there's no reliable way to prevent ime activation - especially problematic for cjk languages where imes automatically engage. ### proposal current properties like keyboardtype, textinputaction, and inputformatters only control keyboard appearance and text filtering, but cannot prevent ime composition states. this breaks applications requiring direct keyboard input only (otp/pin entry, terminals, command interfaces). numeric-only fields still trigger ime composition real-time validation disrupted by ime intermediate states inconsistent behavior across platforms no workaround for cjk language environments",1.4,Low,0.538,localized low-impact
kubernetes/kubernetes#136536,"[flaking test] ci-kubernetes-ec2-arm64-conformance-latest.overall , kubetest2.test","### which jobs are flaking? sig-release-master-informing - [kubernetes-ec2-conformance-latest]( - [kubernetes-ec2-arm64-conformance-latest]( ### which tests are flaking? it fails in the very beginning of the test - kubernetes e2e suite: [synchronizedbeforesuite] ### since when has it been flaking? - [24/01/2026, 18:19:02]( - [21/01/2026, 12:16:03]( - [23/01/2026, 09:42:55]( - [21/01/2026, 15:41:06]( - [14/01/2026, 15:32:59]( - [13/01/2026, 15:33:02]( ### testgrid link , ### reason for failure (if possible) ### anything else we need to know? test fails in the very beginning when trying to list nodes: ### relevant sig(s) /sig testing",[],['TESTING'],"['kind/flake', 'sig/testing', 'needs-triage']",github,2026-01-26T12:42:15Z,,"[flaking test] ci-kubernetes-ec2-arm64-conformance-latest.overall , kubetest2.test ### which jobs are flaking? sig-release-master-informing - [kubernetes-ec2-conformance-latest]( - [kubernetes-ec2-arm64-conformance-latest]( ### which tests are flaking? it fails in the very beginning of the test - kubernetes e2e suite: [synchronizedbeforesuite] ### since when has it been flaking? - [24/01/2026, 18:19:02]( - [21/01/2026, 12:16:03]( - [23/01/2026, 09:42:55]( - [21/01/2026, 15:41:06]( - [14/01/2026, 15:32:59]( - [13/01/2026, 15:33:02]( ### testgrid link , ### reason for failure (if possible) ### anything else we need to know? test fails in the very beginning when trying to list nodes: ### relevant sig(s) /sig testing",1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161783,sentry: replica_consistency.go:832: log.fatal: attention: (1) attached stack trace -- stack trace: | github.com/cockroachdb/cockroach/pkg/kv/kvserver.(*replica).computechecksumpostapply.func2 | ...,this issue was auto filed by sentry. it represents a crash or reported error on a live cluster with telemetry enabled. sentry link: [ panic message: stacktrace (expand for inline code snippets): [src/runtime/asm_arm64.s#l1221-l1223](src/runtime/asm_arm64.s#l1221-l1223) [pkg/util/stop/stopper.go#l497-l499](pkg/util/stop/stopper.go#l497-l499) [pkg/kv/kvserver/replica_consistency.go#l831-l833](pkg/kv/kvserver/replica_consistency.go#l831-l833) ### tags | tag | value | | --- | --- | | command | server | | environment | v24.3.18 | | go version | go1.22.8 x:nocoverageredesign | | platform | linux arm64 | | distribution | ccl | | cockroach release | v24.3.18 | | cockroach sha | 37439c410d4583708c898b13a2f9e906f444867b | | # of cpus | 48 | | # of goroutines | 10442 |,[],['BUG'],"['C-bug', 'O-sentry', 'X-blathers-triaged', 'T-kv', 'branch-release-24.3']",github,2026-01-26T13:08:17Z,,sentry: replica_consistency.go:832: log.fatal: attention: (1) attached stack trace -- stack trace: | github.com/cockroachdb/cockroach/pkg/kv/kvserver.(*replica).computechecksumpostapply.func2 | ... this issue was auto filed by sentry. it represents a crash or reported error on a live cluster with telemetry enabled. sentry link: [ panic message: stacktrace (expand for inline code snippets): [src/runtime/asm_arm64.s#l1221-l1223](src/runtime/asm_arm64.s#l1221-l1223) [pkg/util/stop/stopper.go#l497-l499](pkg/util/stop/stopper.go#l497-l499) [pkg/kv/kvserver/replica_consistency.go#l831-l833](pkg/kv/kvserver/replica_consistency.go#l831-l833) ### tags | tag | value | | --- | --- | | command | server | | environment | v24.3.18 | | go version | go1.22.8 x:nocoverageredesign | | platform | linux arm64 | | distribution | ccl | | cockroach release | v24.3.18 | | cockroach sha | 37439c410d4583708c898b13a2f9e906f444867b | | # of cpus | 48 | | # of goroutines | 10442 |,6.0,Critical,1.0,crash-like behavior
cockroachdb/cockroach#161785,pkg/sql/logictest/tests/fakedist-disk/fakedist-disk_test: testlogic_optimizer_timeout failed,pkg/sql/logictest/tests/fakedist-disk/fakedist-disk_test.testlogic_optimizer_timeout [failed]( on release-25.3.8-rc @ [129f86631b285cfd2bd8740db920947b5a9c89ab]( parameters: - attempt=1 - race=true - run=1 - shard=35 help see also: [how to investigate a go test failure \(internal\)]( [this test on roachdash]( | [improve this report!]( jira issue: crdb-59083,[],['TESTING'],"['C-test-failure', 'O-robot', 'release-blocker', 'T-sql-queries', 'X-infra-flake', 'branch-release-25.3.8-rc']",github,2026-01-26T13:30:52Z,2026-01-26T16:50:53Z,pkg/sql/logictest/tests/fakedist-disk/fakedist-disk_test: testlogic_optimizer_timeout failed pkg/sql/logictest/tests/fakedist-disk/fakedist-disk_test.testlogic_optimizer_timeout [failed]( on release-25.3.8-rc @ [129f86631b285cfd2bd8740db920947b5a9c89ab]( parameters: - attempt=1 - race=true - run=1 - shard=35 help see also: [how to investigate a go test failure \(internal\)]( [this test on roachdash]( | [improve this report!]( jira issue: crdb-59083,1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161786,concurrency: improve lock table verification,"in test builds, we run verification of the lock table on every update. this verification is useful and has helped discover bugs in the lock table. however, we've recently encountered two problems we might want to address somehow: 1. under deadlock builds, we've seen failures that _appear_ to be related to lock table verification being very slow. 2. a lock table verification failure can actually happen because of some _other_ request other than the one doing the verification because of how locking works. for (1), one potential idea is to only run verification after a given proposal has done all of it's lock table updates. this would mean that a request that discovers or updates 10 locks would only have to run verification once rather than 10 times. jira issue: crdb-59084",[],['FEATURE'],['C-enhancement'],github,2026-01-26T13:38:51Z,,"concurrency: improve lock table verification in test builds, we run verification of the lock table on every update. this verification is useful and has helped discover bugs in the lock table. however, we've recently encountered two problems we might want to address somehow: 1. under deadlock builds, we've seen failures that _appear_ to be related to lock table verification being very slow. 2. a lock table verification failure can actually happen because of some _other_ request other than the one doing the verification because of how locking works. for (1), one potential idea is to only run verification after a given proposal has done all of it's lock table updates. this would mean that a request that discovers or updates 10 locks would only have to run verification once rather than 10 times. jira issue: crdb-59084",5.0,Critical,1.0,crash-like behavior
microsoft/vscode#290387,completions core inserts bogus similar files context,steps to reproduce: * checkout at commit e9a5b5af62d76c37a236e4f17e46d894e3dc876e * open the repo in latest insiders * open the file src/querydiagnosticsprovider.ts * close the file above * open the file src/treetraversal.ts * go to last line and press enter to create a new line this insert a context like the bogus code is inserted from:,[],['BUG'],['bug'],github,2026-01-26T13:47:40Z,,completions core inserts bogus similar files context steps to reproduce: * checkout at commit e9a5b5af62d76c37a236e4f17e46d894e3dc876e * open the repo in latest insiders * open the file src/querydiagnosticsprovider.ts * close the file above * open the file src/treetraversal.ts * go to last line and press enter to create a new line this insert a context like the bogus code is inserted from:,2.673,Medium,0.828,functional impact
cockroachdb/cockroach#161787,sentry: optimizer.go:290: top-level relational expression cannot have outer columns: (82) (1) wraps: (2) assertion failure wraps: (3) attached stack trace -- stack trace: | github.com/cockroachdb/...,this issue was auto filed by sentry. it represents a crash or reported error on a live cluster with telemetry enabled. sentry link: [ panic message: stacktrace (expand for inline code snippets): [src/runtime/asm_amd64.s#l1699-l1701](src/runtime/asm_amd64.s#l1699-l1701) [pkg/sql/pgwire/server.go#l1227-l1229](pkg/sql/pgwire/server.go#l1227-l1229) [pkg/sql/pgwire/conn.go#l252-l254](pkg/sql/pgwire/conn.go#l252-l254) [pkg/sql/conn_executor.go#l1066-l1068](pkg/sql/conn_executor.go#l1066-l1068) [pkg/sql/conn_executor.go#l2290-l2292](pkg/sql/conn_executor.go#l2290-l2292) [pkg/sql/conn_executor.go#l2382-l2384](pkg/sql/conn_executor.go#l2382-l2384) [pkg/sql/conn_executor.go#l2377-l2379](pkg/sql/conn_executor.go#l2377-l2379) [pkg/sql/conn_executor_exec.go#l173-l175](pkg/sql/conn_executor_exec.go#l173-l175) [pkg/sql/conn_executor_exec.go#l4414-l4416](pkg/sql/conn_executor_exec.go#l4414-l4416) [pkg/sql/conn_executor_exec.go#l174-l176](pkg/sql/conn_executor_exec.go#l174-l176) [pkg/sql/conn_executor_exec.go#l1066-l1068](pkg/sql/conn_executor_exec.go#l1066-l1068) [pkg/sql/conn_executor_exec.go#l2906-l2908](pkg/sql/conn_executor_exec.go#l2906-l2908) [pkg/sql/conn_executor_exec.go#l3380-l3382](pkg/sql/conn_executor_exec.go#l3380-l3382) [pkg/sql/distsql_running.go#l2035-l2037](pkg/sql/distsql_running.go#l2035-l2037) [pkg/sql/distsql_running.go#l2032-l2034](pkg/sql/distsql_running.go#l2032-l2034) [pkg/sql/distsql_running.go#l2333-l2335](pkg/sql/distsql_running.go#l2333-l2335) [pkg/sql/distsql_running.go#l1071-l1073](pkg/sql/distsql_running.go#l1071-l1073) [pkg/sql/colflow/vectorized_flow.go#l315-l317](pkg/sql/colflow/vectorized_flow.go#l315-l317) [pkg/sql/colflow/flow_coordinator.go#l276-l278](pkg/sql/colflow/flow_coordinator.go#l276-l278) [pkg/sql/colflow/flow_coordinator.go#l244-l246](pkg/sql/colflow/flow_coordinator.go#l244-l246) [pkg/sql/colexecerror/error.go#l161-l163](pkg/sql/colexecerror/error.go#l161-l163) [pkg/sql/colflow/flow_coordinator.go#l240-l242](pkg/sql/colflow/flow_coordinator.go#l240-l242) [pkg/sql/colflow/stats.go#l127-l129](pkg/sql/colflow/stats.go#l127-l129) [pkg/sql/colexecerror/error.go#l161-l163](pkg/sql/colexecerror/error.go#l161-l163) [pkg/sql/colflow/stats.go#l119-l121](pkg/sql/colflow/stats.go#l119-l121) [pkg/sql/colexec/columnarizer.go#l247-l249](pkg/sql/colexec/columnarizer.go#l247-l249) [pkg/sql/rowexec/project_set.go#l301-l303](pkg/sql/rowexec/project_set.go#l301-l303) [pkg/sql/rowexec/project_set.go#l207-l209](pkg/sql/rowexec/project_set.go#l207-l209) [pkg/sql/routine.go#l259-l261](pkg/sql/routine.go#l259-l261) [pkg/sql/routine.go#l309-l311](pkg/sql/routine.go#l309-l311) [pkg/sql/opt/exec/execbuilder/scalar.go#l1269-l1271](pkg/sql/opt/exec/execbuilder/scalar.go#l1269-l1271) [pkg/sql/opt/xform/optimizer.go#l289-l291](pkg/sql/opt/xform/optimizer.go#l289-l291) ### tags | tag | value | | --- | --- | | command | mt start-sql | | environment | v25.4.1 | | go version | go1.23.12 x:nocoverageredesign | | platform | linux amd64 | | distribution | ccl | | cockroach release | v25.4.1-1-g55e9e9d822f | | cockroach sha | 55e9e9d822f71cfe280f4772358f13859488aca6 | | # of cpus | 16 | | # of goroutines | 312 | jira issue: crdb-59085,[],['BUG'],"['C-bug', 'O-sentry', 'X-blathers-triaged', 'T-sql-queries', 'branch-release-25.4']",github,2026-01-26T14:11:03Z,2026-01-26T16:54:56Z,sentry: optimizer.go:290: top-level relational expression cannot have outer columns: (82) (1) wraps: (2) assertion failure wraps: (3) attached stack trace -- stack trace: | github.com/cockroachdb/... this issue was auto filed by sentry. it represents a crash or reported error on a live cluster with telemetry enabled. sentry link: [ panic message: stacktrace (expand for inline code snippets): [src/runtime/asm_amd64.s#l1699-l1701](src/runtime/asm_amd64.s#l1699-l1701) [pkg/sql/pgwire/server.go#l1227-l1229](pkg/sql/pgwire/server.go#l1227-l1229) [pkg/sql/pgwire/conn.go#l252-l254](pkg/sql/pgwire/conn.go#l252-l254) [pkg/sql/conn_executor.go#l1066-l1068](pkg/sql/conn_executor.go#l1066-l1068) [pkg/sql/conn_executor.go#l2290-l2292](pkg/sql/conn_executor.go#l2290-l2292) [pkg/sql/conn_executor.go#l2382-l2384](pkg/sql/conn_executor.go#l2382-l2384) [pkg/sql/conn_executor.go#l2377-l2379](pkg/sql/conn_executor.go#l2377-l2379) [pkg/sql/conn_executor_exec.go#l173-l175](pkg/sql/conn_executor_exec.go#l173-l175) [pkg/sql/conn_executor_exec.go#l4414-l4416](pkg/sql/conn_executor_exec.go#l4414-l4416) [pkg/sql/conn_executor_exec.go#l174-l176](pkg/sql/conn_executor_exec.go#l174-l176) [pkg/sql/conn_executor_exec.go#l1066-l1068](pkg/sql/conn_executor_exec.go#l1066-l1068) [pkg/sql/conn_executor_exec.go#l2906-l2908](pkg/sql/conn_executor_exec.go#l2906-l2908) [pkg/sql/conn_executor_exec.go#l3380-l3382](pkg/sql/conn_executor_exec.go#l3380-l3382) [pkg/sql/distsql_running.go#l2035-l2037](pkg/sql/distsql_running.go#l2035-l2037) [pkg/sql/distsql_running.go#l2032-l2034](pkg/sql/distsql_running.go#l2032-l2034) [pkg/sql/distsql_running.go#l2333-l2335](pkg/sql/distsql_running.go#l2333-l2335) [pkg/sql/distsql_running.go#l1071-l1073](pkg/sql/distsql_running.go#l1071-l1073) [pkg/sql/colflow/vectorized_flow.go#l315-l317](pkg/sql/colflow/vectorized_flow.go#l315-l317) [pkg/sql/colflow/flow_coordinator.go#l276-l278](pkg/sql/colflow/flow_coordinator.go#l276-l278) [pkg/sql/colflow/flow_coordinator.go#l244-l246](pkg/sql/colflow/flow_coordinator.go#l244-l246) [pkg/sql/colexecerror/error.go#l161-l163](pkg/sql/colexecerror/error.go#l161-l163) [pkg/sql/colflow/flow_coordinator.go#l240-l242](pkg/sql/colflow/flow_coordinator.go#l240-l242) [pkg/sql/colflow/stats.go#l127-l129](pkg/sql/colflow/stats.go#l127-l129) [pkg/sql/colexecerror/error.go#l161-l163](pkg/sql/colexecerror/error.go#l161-l163) [pkg/sql/colflow/stats.go#l119-l121](pkg/sql/colflow/stats.go#l119-l121) [pkg/sql/colexec/columnarizer.go#l247-l249](pkg/sql/colexec/columnarizer.go#l247-l249) [pkg/sql/rowexec/project_set.go#l301-l303](pkg/sql/rowexec/project_set.go#l301-l303) [pkg/sql/rowexec/project_set.go#l207-l209](pkg/sql/rowexec/project_set.go#l207-l209) [pkg/sql/routine.go#l259-l261](pkg/sql/routine.go#l259-l261) [pkg/sql/routine.go#l309-l311](pkg/sql/routine.go#l309-l311) [pkg/sql/opt/exec/execbuilder/scalar.go#l1269-l1271](pkg/sql/opt/exec/execbuilder/scalar.go#l1269-l1271) [pkg/sql/opt/xform/optimizer.go#l289-l291](pkg/sql/opt/xform/optimizer.go#l289-l291) ### tags | tag | value | | --- | --- | | command | mt start-sql | | environment | v25.4.1 | | go version | go1.23.12 x:nocoverageredesign | | platform | linux amd64 | | distribution | ccl | | cockroach release | v25.4.1-1-g55e9e9d822f | | cockroach sha | 55e9e9d822f71cfe280f4772358f13859488aca6 | | # of cpus | 16 | | # of goroutines | 312 | jira issue: crdb-59085,6.0,Critical,1.0,crash-like behavior
cockroachdb/cockroach#161794,roachtest: rebalance/by-load/leases/mixed-version failed,"**note:** this build has runtime assertions enabled. if the same failure was hit in a run without assertions enabled, there should be a similar failure without this message. if there isn& ;t one, then this failure is likely due to an assertion violation or (assertion) timeout. roachtest.rebalance/by-load/leases/mixed-version [failed]( with [artifacts]( on master @ [867004a4311025b7e0de44bc8c72fcbc8442f041]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21021544-1769411655-108-n4cpu4-0001 | 34.148.187.190 | 10.142.1.23 | | teamcity-21021544-1769411655-108-n4cpu4-0002 | 34.26.3.72 | 10.142.1.65 | | teamcity-21021544-1769411655-108-n4cpu4-0003 | 35.237.83.17 | 10.142.1.76 | | teamcity-21021544-1769411655-108-n4cpu4-0004 | 35.229.48.59 | 10.142.1.83 | parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - fs=ext4 - localssd=true - mvtdeploymentmode=shared-process - mvtversions=v25.4.3 ‚Üí v26.1.0-rc.1 ‚Üí v26.2.0-alpha.00000000-dev-867004a4311025b7e0de44bc8c72fcbc8442f041 (master) - runtimeassertionsbuild=true help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( same failure on other branches - roachtest: rebalance/by-load/leases/mixed-version failed [c-test-failure o-roachtest o-robot t-kv branch-release-25.2 release-blocker] /cc /kv-triage [this test on roachdash]( | [improve this report!]( jira issue: crdb-59086",[],['TESTING'],"['C-test-failure', 'O-robot', 'O-roachtest', 'branch-master', 'release-blocker', 'T-kv', 'B-runtime-assertions-enabled']",github,2026-01-26T14:28:57Z,,"roachtest: rebalance/by-load/leases/mixed-version failed **note:** this build has runtime assertions enabled. if the same failure was hit in a run without assertions enabled, there should be a similar failure without this message. if there isn& ;t one, then this failure is likely due to an assertion violation or (assertion) timeout. roachtest.rebalance/by-load/leases/mixed-version [failed]( with [artifacts]( on master @ [867004a4311025b7e0de44bc8c72fcbc8442f041]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21021544-1769411655-108-n4cpu4-0001 | 34.148.187.190 | 10.142.1.23 | | teamcity-21021544-1769411655-108-n4cpu4-0002 | 34.26.3.72 | 10.142.1.65 | | teamcity-21021544-1769411655-108-n4cpu4-0003 | 35.237.83.17 | 10.142.1.76 | | teamcity-21021544-1769411655-108-n4cpu4-0004 | 35.229.48.59 | 10.142.1.83 | parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - fs=ext4 - localssd=true - mvtdeploymentmode=shared-process - mvtversions=v25.4.3 ‚Üí v26.1.0-rc.1 ‚Üí v26.2.0-alpha.00000000-dev-867004a4311025b7e0de44bc8c72fcbc8442f041 (master) - runtimeassertionsbuild=true help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( same failure on other branches - roachtest: rebalance/by-load/leases/mixed-version failed [c-test-failure o-roachtest o-robot t-kv branch-release-25.2 release-blocker] /cc /kv-triage [this test on roachdash]( | [improve this report!]( jira issue: crdb-59086",3.101,High,0.925,functional impact
microsoft/vscode#290398,test: context window widget,"refs: - [x] anyos - [x] anyos -d - [x] anyos complexity: 3 authors: [create issue]( --- we added a context window widget in chat to show total context window utilization, with bucketized breakdowns for system and user context. this info is generated from: - overall token usage: from providers - bucket breakdown: estimates from our tokenizer test: - the widget shows in the chat title bar after a request is sent. the widget should appear in the title/action toolbar both in a maximized view and sidebar view. - total context usage shows when you hover over the widget - additional breakdown information is shown on click - when selecting models with different context windows, the denominator (total available context) changes - as you send subsequent requests, the widget shows a higher amount shaded in, the utilization increases, and the bucketized info changes accordingly. - the numbers feels accurate (i.e. is summarization kicking in at a reasonable context window utilization). please do some exploring and extra testing here as we want this info to be complimentary to the actual experience in chat. - what sort of actions might you expect to see in the hover menu / be the most actionable?",[],['TESTING'],['testplan-item'],github,2026-01-26T15:03:15Z,2026-01-28T08:01:32Z,"test: context window widget refs: - [x] anyos - [x] anyos -d - [x] anyos complexity: 3 authors: [create issue]( --- we added a context window widget in chat to show total context window utilization, with bucketized breakdowns for system and user context. this info is generated from: - overall token usage: from providers - bucket breakdown: estimates from our tokenizer test: - the widget shows in the chat title bar after a request is sent. the widget should appear in the title/action toolbar both in a maximized view and sidebar view. - total context usage shows when you hover over the widget - additional breakdown information is shown on click - when selecting models with different context windows, the denominator (total available context) changes - as you send subsequent requests, the widget shows a higher amount shaded in, the utilization increases, and the bucketized info changes accordingly. - the numbers feels accurate (i.e. is summarization kicking in at a reasonable context window utilization). please do some exploring and extra testing here as we want this info to be complimentary to the actual experience in chat. - what sort of actions might you expect to see in the hover menu / be the most actionable?",3.8,Critical,1.0,crash-like behavior
microsoft/vscode#290401,top borders are misaligned on dark 2026 experimental,üëé to trying to use round corners here too. if you are doing the depth thing it'll look better if it's flat and it's odd that the top would get rounded corners suddenly when selected if leaning into skeuomorphism. cc,[],['UI'],"['ux', 'polish', '2026-themes']",github,2026-01-26T15:21:52Z,,top borders are misaligned on dark 2026 experimental üëé to trying to use round corners here too. if you are doing the depth thing it'll look better if it's flat and it's odd that the top would get rounded corners suddenly when selected if leaning into skeuomorphism. cc,1.8,Low,0.629,user-visible issue
microsoft/vscode#290402,discarding file changes from scm view should work as for chat,"currently, keep/undo just show +0/-0 changes if i discard from scm version: 1.109.0-insider (universal) commit: abb70fbc79435e892ca8e80654b47257de775c8f date: 2026-01-26t05:02:47.498z electron: 39.2.7 electronbuildid: 13098910 chromium: 142.0.7444.235 node.js: 22.21.1 v8: 14.2.231.21-electron.0 os: darwin arm64 25.2.0",[],['FEATURE'],"['feature-request', 'git']",github,2026-01-26T15:22:09Z,,"discarding file changes from scm view should work as for chat currently, keep/undo just show +0/-0 changes if i discard from scm version: 1.109.0-insider (universal) commit: abb70fbc79435e892ca8e80654b47257de775c8f date: 2026-01-26t05:02:47.498z electron: 39.2.7 electronbuildid: 13098910 chromium: 142.0.7444.235 node.js: 22.21.1 v8: 14.2.231.21-electron.0 os: darwin arm64 25.2.0",3.6,Critical,1.0,crash-like behavior
python/cpython#144249,sslcontext.load_cert_chain non-existing filename report,"# feature or enhancement ### proposal: considering the following example the current implementation returns a simple *no such file or directory* exception without information about the file itself since filenotfounderror have a filename attribute that is correctly reported in its representation, it would be appreciable to have the filename attached to the exception being raised ### has this already been discussed elsewhere? no response given ### links to previous discussion of this feature: _no response_ ### linked prs * gh-144250",[],['FEATURE'],"['type-feature', 'extension-modules', 'topic-SSL']",github,2026-01-26T15:22:46Z,2026-01-28T10:21:24Z,"sslcontext.load_cert_chain non-existing filename report # feature or enhancement ### proposal: considering the following example the current implementation returns a simple *no such file or directory* exception without information about the file itself since filenotfounderror have a filename attribute that is correctly reported in its representation, it would be appreciable to have the filename attached to the exception being raised ### has this already been discussed elsewhere? no response given ### links to previous discussion of this feature: _no response_ ### linked prs * gh-144250",1.4,Low,0.538,localized low-impact
microsoft/vscode#290403,terminal tabs look bad in dark 2026 experimental,- odd shadow on top and right - no active terminal indicator leaves a weird gap on left of the tab,[],['UI'],"['ux', 'polish', '2026-themes']",github,2026-01-26T15:22:59Z,,terminal tabs look bad in dark 2026 experimental - odd shadow on top and right - no active terminal indicator leaves a weird gap on left of the tab,1.8,Low,0.629,user-visible issue
microsoft/vscode#290407,light 2026 experimental shows white brackets on light background,,[],['UI'],"['ux', 'polish', '2026-themes']",github,2026-01-26T15:26:42Z,,light 2026 experimental shows white brackets on light background,1.8,Low,0.629,user-visible issue
microsoft/vscode#290408,light 2026 experimental shows white badge on light background,,[],['UI'],"['ux', 'polish', '2026-themes']",github,2026-01-26T15:28:19Z,,light 2026 experimental shows white badge on light background,1.8,Low,0.629,user-visible issue
microsoft/vscode#290411,dark 2026 experimental doesn't have a colored bold color in quick pick,"no color for the bold parts: this aids me significantly in differentiating when filtering items, boldness changes are a lot more subtle that bold+color. compare:",[],['UI'],"['ux', 'polish', '2026-themes']",github,2026-01-26T15:31:15Z,,"dark 2026 experimental doesn't have a colored bold color in quick pick no color for the bold parts: this aids me significantly in differentiating when filtering items, boldness changes are a lot more subtle that bold+color. compare:",4.0,Critical,1.0,"user-visible issue, crash-like behavior"
microsoft/vscode#290412,2026 experimental themes don't differentiate inline titles,see light themes and dark themes pop as they're higher in the hierarchy: they look like regular text here:,[],['UI'],"['ux', 'polish', '2026-themes']",github,2026-01-26T15:32:57Z,,2026 experimental themes don't differentiate inline titles see light themes and dark themes pop as they're higher in the hierarchy: they look like regular text here:,1.8,Low,0.629,user-visible issue
microsoft/vscode#290413,2026 experimental themes don't color the quick pick separator,the the lines between sections: no lines:,[],['UI'],"['ux', 'polish', '2026-themes']",github,2026-01-26T15:33:25Z,,2026 experimental themes don't color the quick pick separator the the lines between sections: no lines:,1.8,Low,0.629,user-visible issue
envoyproxy/envoy#43159,allow extensions.access_loggers.stats.v3.config to use formatter plugins,*title*: allow extensions.access_loggers.stats.v3.config to use formatters plugins *description*: i'd like to use [extensions.formatter.req_without_query.v3.reqwithoutquery]( with [extensions.access_loggers.stats.v3.config]( to strip out query parameter from the http path used as a metrics value. [optional *relevant links*:] -,[],['FEATURE'],"['enhancement', 'triage']",github,2026-01-26T15:33:59Z,2026-01-27T10:30:47Z,allow extensions.access_loggers.stats.v3.config to use formatter plugins *title*: allow extensions.access_loggers.stats.v3.config to use formatters plugins *description*: i'd like to use [extensions.formatter.req_without_query.v3.reqwithoutquery]( with [extensions.access_loggers.stats.v3.config]( to strip out query parameter from the http path used as a metrics value. [optional *relevant links*:] -,1.4,Low,0.538,localized low-impact
microsoft/vscode#290414,"reason: tool limit exceeded (140/128). click ""configure tools"" in the chat input to disable 12 tools and retry.","type: bug hi this is oldie but a goodie. vs code version: code - insiders 1.109.0-insider (abb70fbc79435e892ca8e80654b47257de775c8f, 2026-01-26t05:02:47.498z) os version: windows_nt x64 10.0.26200 modes: system info |item|value| |---|---| |cpus|13th gen intel(r) core(tm) i7-13800h (20 x 2918)| |gpu status|2d_canvas: enabled gpu0: vendor= 0x8086, device=0xa7a0 [intel(r) iris(r) xe graphics], driver_vendor=intel, driver_version=32.0.101.6737 *active* gpu1: vendor= 0x10de, device=0x28a1 [nvidia geforce rtx 4050 laptop gpu], driver_version=32.0.15.8157 gpu2: vendor= 0x1414, device=0x008c [microsoft basic render driver], driver_version=10.0.26100.7309 machine model name: machine model version: direct_rendering_display_compositor: disabled_off_ok gpu_compositing: enabled multiple_raster_threads: enabled_on opengl: enabled_on rasterization: enabled raw_draw: disabled_off_ok skia_graphite: disabled_off trees_in_viz: disabled_off video_decode: enabled video_encode: enabled webgl: enabled webgl2: enabled webgpu: enabled webnn: disabled_off| |load (avg)|undefined| |memory (system)|31.83gb (12.82gb free)| |process argv|--crash-reporter-id 35ea611a-f63e-4f40-8f7e-6ae7779f9c07| |screen reader|no| |vm|0%| extensions (17) extension|author (truncated)|version ---|---|--- vscode-eslint|dba|3.0.20 editorconfig|edi|0.18.1 copilot-chat|git|0.37.2026012601 vscode-github-actions|git|0.30.0 vscode-pull-request-github|git|0.126.0 vscode-pr-pinger|jri|0.0.6 vscode-bicep|ms-|0.40.2 vscode-containers|ms-|2.4.0 csdevkit|ms-|1.90.2 csharp|ms-|2.110.4 vscode-dotnet-runtime|ms-|3.0.0 playwright|ms-|1.1.17 remote-containers|ms-|0.439.0 extension-test-runner|ms-|0.0.14 powershell|ms-|2025.4.0 vscode-github-issue-notebooks|ms-|0.0.134 native-preview|typ|0.20260126.1",[],['BUG'],"['bug', 'important']",github,2026-01-26T15:40:08Z,2026-01-26T17:56:25Z,"reason: tool limit exceeded (140/128). click ""configure tools"" in the chat input to disable 12 tools and retry. type: bug hi this is oldie but a goodie. vs code version: code - insiders 1.109.0-insider (abb70fbc79435e892ca8e80654b47257de775c8f, 2026-01-26t05:02:47.498z) os version: windows_nt x64 10.0.26200 modes: system info |item|value| |---|---| |cpus|13th gen intel(r) core(tm) i7-13800h (20 x 2918)| |gpu status|2d_canvas: enabled gpu0: vendor= 0x8086, device=0xa7a0 [intel(r) iris(r) xe graphics], driver_vendor=intel, driver_version=32.0.101.6737 *active* gpu1: vendor= 0x10de, device=0x28a1 [nvidia geforce rtx 4050 laptop gpu], driver_version=32.0.15.8157 gpu2: vendor= 0x1414, device=0x008c [microsoft basic render driver], driver_version=10.0.26100.7309 machine model name: machine model version: direct_rendering_display_compositor: disabled_off_ok gpu_compositing: enabled multiple_raster_threads: enabled_on opengl: enabled_on rasterization: enabled raw_draw: disabled_off_ok skia_graphite: disabled_off trees_in_viz: disabled_off video_decode: enabled video_encode: enabled webgl: enabled webgl2: enabled webgpu: enabled webnn: disabled_off| |load (avg)|undefined| |memory (system)|31.83gb (12.82gb free)| |process argv|--crash-reporter-id 35ea611a-f63e-4f40-8f7e-6ae7779f9c07| |screen reader|no| |vm|0%| extensions (17) extension|author (truncated)|version ---|---|--- vscode-eslint|dba|3.0.20 editorconfig|edi|0.18.1 copilot-chat|git|0.37.2026012601 vscode-github-actions|git|0.30.0 vscode-pull-request-github|git|0.126.0 vscode-pr-pinger|jri|0.0.6 vscode-bicep|ms-|0.40.2 vscode-containers|ms-|2.4.0 csdevkit|ms-|1.90.2 csharp|ms-|2.110.4 vscode-dotnet-runtime|ms-|3.0.0 playwright|ms-|1.1.17 remote-containers|ms-|0.439.0 extension-test-runner|ms-|0.0.14 powershell|ms-|2025.4.0 vscode-github-issue-notebooks|ms-|0.0.134 native-preview|typ|0.20260126.1",7.8,Critical,1.0,crash-like behavior
docker/docker#51934,docker load produces different image ids depening on the machine,"### description loading an image via will load the image and assign a different id based on the machine / distribution you're working on. ### reproduce ## prerequisites two different hosts are needed with different linux distributions (e.g. ubuntu and fedora). ## preparation * one one host, build an image and write it to a file via . * copy the image to the other host ## reproduction * load the image on each host via * check the image ids with (use the reference displayed when importing) ## results ### host a - ubuntu ### host b - fedora ### expected behavior the of an image that was stored and loaded should be equal with persistency across different machines (see ### docker version ### docker info ### additional info # docker buildx inspect ## host a - ubuntu ## host b - fedora # distribution info ## host a - ubuntu ## host b - fedora # other info i've reproduced the problem on the same hardware, with ubuntu running as the host os and fedora as a guest vm through kvm. # dockerfile issue should be reproducible with any dockerfile, but i've used the following:",[],['BUG'],"['status/0-triage', 'kind/bug', 'area/images', 'containerd-integration', 'version/29.1']",github,2026-01-26T15:42:07Z,2026-01-27T19:45:34Z,"docker load produces different image ids depening on the machine ### description loading an image via will load the image and assign a different id based on the machine / distribution you're working on. ### reproduce ## prerequisites two different hosts are needed with different linux distributions (e.g. ubuntu and fedora). ## preparation * one one host, build an image and write it to a file via . * copy the image to the other host ## reproduction * load the image on each host via * check the image ids with (use the reference displayed when importing) ## results ### host a - ubuntu ### host b - fedora ### expected behavior the of an image that was stored and loaded should be equal with persistency across different machines (see ### docker version ### docker info ### additional info # docker buildx inspect ## host a - ubuntu ## host b - fedora # distribution info ## host a - ubuntu ## host b - fedora # other info i've reproduced the problem on the same hardware, with ubuntu running as the host os and fedora as a guest vm through kvm. # dockerfile issue should be reproducible with any dockerfile, but i've used the following:",2.22,Medium,0.724,functional impact
cockroachdb/cockroach#161796,kv/kvserver/intentresolver: testintentresolutionunavailablerange failed,kv/kvserver/intentresolver.testintentresolutionunavailablerange [failed]( with [artifacts]( on master @ [01a893ef768e1b60b084318e27e6d9e0f5caa0ec]( help see also: [how to investigate a go test failure \(internal\)]( /cc /kv-triage [this test on roachdash]( | [improve this report!]( jira issue: crdb-59087,[],['TESTING'],"['C-test-failure', 'O-robot', 'branch-master', 'release-blocker', 'T-kv', 's390x-test-failure']",github,2026-01-26T15:45:29Z,,kv/kvserver/intentresolver: testintentresolutionunavailablerange failed kv/kvserver/intentresolver.testintentresolutionunavailablerange [failed]( with [artifacts]( on master @ [01a893ef768e1b60b084318e27e6d9e0f5caa0ec]( help see also: [how to investigate a go test failure \(internal\)]( /cc /kv-triage [this test on roachdash]( | [improve this report!]( jira issue: crdb-59087,1.6,Low,0.584,localized low-impact
microsoft/vscode#290417,auto approve set-location,,[],"['FEATURE', 'UI']","['feature-request', 'verified', 'verification-needed', 'insiders-released', 'chat-terminal']",github,2026-01-26T15:59:23Z,2026-01-26T16:35:41Z,auto approve set-location,1.6,Low,0.584,user-visible issue
microsoft/vscode#290429,add screen reader support to chat sessions view,,[],"['TESTING', 'FEATURE']","['feature-request', 'on-testplan']",github,2026-01-26T16:30:45Z,2026-01-26T16:30:58Z,add screen reader support to chat sessions view,1.5,Low,0.561,localized low-impact
microsoft/vscode#290431,test: chat sessions with a screen reader,"refs - [x] windows -d - [ ] macos - [x] linux complexity: 3 authors: , [create issue]( --- enable a screen reader and test out the experience in the chat sessions view. ensure anything available visually is communicated to the screen reader. open the chat response accessibility help dialog and ensure it has helpful information related to chat sessions.",[],['TESTING'],['testplan-item'],github,2026-01-26T16:32:25Z,,"test: chat sessions with a screen reader refs - [x] windows -d - [ ] macos - [x] linux complexity: 3 authors: , [create issue]( --- enable a screen reader and test out the experience in the chat sessions view. ensure anything available visually is communicated to the screen reader. open the chat response accessibility help dialog and ensure it has helpful information related to chat sessions.",1.6,Low,0.584,localized low-impact
microsoft/vscode#290434,chat: file bubbles miss padding when there's no file icon theme,,[],"['BUG', 'UI']","['bug', 'ux', 'insiders-released', 'chat']",github,2026-01-26T16:40:41Z,2026-01-27T01:03:00Z,chat: file bubbles miss padding when there's no file icon theme,1.964,Low,0.666,user-visible issue
rust-lang/rust#151698,type inference fails with multiple impl blocks despite explicit type annotation,"im encountering a situation where the rust compiler cannot infer which method to call, even when an explicit type annotation is provided for the variable receiving the result. this leads to the e0034 error, ""multiple applicable items in scope."" requiring the use of turbofish syntax to avoid. this is a pretty large example showcasing the issue and my use cases for it but for reproduction sake i have also [created an minimal example using rust's playground:]( ### **expected behavior:** i expect the rust compiler to use the explicit type annotation of the variable ( ) to disambiguate which impl block's function should be called. the type of the variable should provide enough information for the compiler to select the correct function. ### **current behavior:** the compiler fails to compile the code, citing error e0034, ""multiple applicable items in scope."" this forces the developer to use the turbofish syntax ( ) to manually specify the type at the call site. ### meta :",[],['BUG'],"['C-bug', 'needs-triage']",github,2026-01-26T16:40:49Z,,"type inference fails with multiple impl blocks despite explicit type annotation im encountering a situation where the rust compiler cannot infer which method to call, even when an explicit type annotation is provided for the variable receiving the result. this leads to the e0034 error, ""multiple applicable items in scope."" requiring the use of turbofish syntax to avoid. this is a pretty large example showcasing the issue and my use cases for it but for reproduction sake i have also [created an minimal example using rust's playground:]( ### **expected behavior:** i expect the rust compiler to use the explicit type annotation of the variable ( ) to disambiguate which impl block's function should be called. the type of the variable should provide enough information for the compiler to select the correct function. ### **current behavior:** the compiler fails to compile the code, citing error e0034, ""multiple applicable items in scope."" this forces the developer to use the turbofish syntax ( ) to manually specify the type at the call site. ### meta :",4.2,Critical,1.0,system-wide impact
microsoft/vscode#290443,test: post nes rename,"refs - [x] anyos - [x] anyos complexity: 4 authors: [create issue]( --- next edit suggestion now support renaming and identifier (variables, types, properties, ...) after the user has typed over the original declaration. a use case is like this: if you change to by appending 2 to the local next edit suggestion should suggest renaming local to local2 when suggestion on line . accepting the rename using should result in observe: the console.log(local2) has been changed as well. test this with all sort of declaration (e.g. type, locals param, ....) please note that the current implemention can only rename the last type identifier. if nes intermixes edits on identifiers then a rename refactoring is not provided. there are ideas how this can be improved further but it needs additional investigation to preserve semantics during rename",[],['TESTING'],['testplan-item'],github,2026-01-26T16:55:45Z,2026-01-27T22:11:41Z,"test: post nes rename refs - [x] anyos - [x] anyos complexity: 4 authors: [create issue]( --- next edit suggestion now support renaming and identifier (variables, types, properties, ...) after the user has typed over the original declaration. a use case is like this: if you change to by appending 2 to the local next edit suggestion should suggest renaming local to local2 when suggestion on line . accepting the rename using should result in observe: the console.log(local2) has been changed as well. test this with all sort of declaration (e.g. type, locals param, ....) please note that the current implemention can only rename the last type identifier. if nes intermixes edits on identifiers then a rename refactoring is not provided. there are ideas how this can be improved further but it needs additional investigation to preserve semantics during rename",3.8,Critical,1.0,crash-like behavior
microsoft/vscode#290446,shouldn't be set at the workspace level,only user level,[],"['TESTING', 'FEATURE']","['feature-request', 'tasks', 'on-testplan', 'insiders-released']",github,2026-01-26T17:07:54Z,2026-01-26T17:36:52Z,shouldn't be set at the workspace level only user level,1.5,Low,0.561,localized low-impact
microsoft/vscode#290456,[testplan-item] contributed chat context should have a tooltip,"refs - [ ] anyos - [x] anyos complexity: 3 [create issue]( --- ## test steps 1. we have a test sample extension for this: the sample provides a silly contributed context for json files. 2. try out the sample, and modify the tooltip. verify that the tooltip works as expected.",[],['TESTING'],['testplan-item'],github,2026-01-26T17:16:07Z,,"[testplan-item] contributed chat context should have a tooltip refs - [ ] anyos - [x] anyos complexity: 3 [create issue]( --- ## test steps 1. we have a test sample extension for this: the sample provides a silly contributed context for json files. 2. try out the sample, and modify the tooltip. verify that the tooltip works as expected.",1.6,Low,0.584,localized low-impact
containerd/containerd#12822,"oauth2 token fetch fails with ""invalid character '<'"" when auth service returns html for post","### description ## description containerd v2.2.0 fails to authenticate against auth services that only support get on their token endpoint: the oauth2 library tries to parse the response body as json before checking the http status code. when the auth service returns an html 404 for the unsupported post, json parsing chokes on and the get fallback never runs. ## root cause the [token authentication spec]( only defines get for the token endpoint. containerd tries post first and falls back to get on failure, but the oauth2 v0.30.0 bump in v2.2.0 broke the fallback: when the auth service returns html (e.g., 404), oauth2 tries to unmarshal it as json before checking the status code. the resulting parse error doesn't match the fallback conditions in , so get never runs. ## versions - **broken**: containerd v2.2.0 (oauth2 v0.30.0) - **working**: containerd v2.1.5 (oauth2 v0.27.0) ## reproduction ## fix the fallback logic in [ ]( checks for status codes 401, 404, 405. it should also catch json unmarshal errors‚Äîa non-json response means the auth service doesn't support the oauth2 post flow. ## references - - incompatible oauth behavior with docker's registry implementation - - added 403 handling for post fallback (similar pattern) ### steps to reproduce the issue ### describe the results you received and expected see, above ### what version of containerd are you using? v2.2.0 ### any other relevant information _no response_ ### show configuration if it is related to cri plugin. _no response_",[],['BUG'],"['kind/bug', 'area/distribution']",github,2026-01-26T17:16:14Z,,"oauth2 token fetch fails with ""invalid character '<'"" when auth service returns html for post ### description ## description containerd v2.2.0 fails to authenticate against auth services that only support get on their token endpoint: the oauth2 library tries to parse the response body as json before checking the http status code. when the auth service returns an html 404 for the unsupported post, json parsing chokes on and the get fallback never runs. ## root cause the [token authentication spec]( only defines get for the token endpoint. containerd tries post first and falls back to get on failure, but the oauth2 v0.30.0 bump in v2.2.0 broke the fallback: when the auth service returns html (e.g., 404), oauth2 tries to unmarshal it as json before checking the status code. the resulting parse error doesn't match the fallback conditions in , so get never runs. ## versions - **broken**: containerd v2.2.0 (oauth2 v0.30.0) - **working**: containerd v2.1.5 (oauth2 v0.27.0) ## reproduction ## fix the fallback logic in [ ]( checks for status codes 401, 404, 405. it should also catch json unmarshal errors‚Äîa non-json response means the auth service doesn't support the oauth2 post flow. ## references - - incompatible oauth behavior with docker's registry implementation - - added 403 handling for post fallback (similar pattern) ### steps to reproduce the issue ### describe the results you received and expected see, above ### what version of containerd are you using? v2.2.0 ### any other relevant information _no response_ ### show configuration if it is related to cri plugin. _no response_",2.423,Medium,0.771,functional impact
microsoft/vscode#290457,"[testplan-item] copilot chat extension should have actions in the sessions view to checkout, even if ghpr isn't installed","refs - [x] anyos - [x] anyos - [x] anyos complexity: 3 [create issue]( --- ## setup 1. install the latest pre-release of copilot chat 2. uninstall github pull requests 3. reload ## test steps 1. open an existing cloud agent session or start a new cloud agent session and wait for it to be done. 1. in the ""changes"" section, verify you see a ""checkout"" button. 1. use the ""checkout"" button. verify that you get a modal dialog explaining what will happen and that the dialog makes sense. 1. verify that github pull requests is installed, and that the pr is checked out.",[],['TESTING'],['testplan-item'],github,2026-01-26T17:17:18Z,2026-01-27T21:24:49Z,"[testplan-item] copilot chat extension should have actions in the sessions view to checkout, even if ghpr isn't installed refs - [x] anyos - [x] anyos - [x] anyos complexity: 3 [create issue]( --- ## setup 1. install the latest pre-release of copilot chat 2. uninstall github pull requests 3. reload ## test steps 1. open an existing cloud agent session or start a new cloud agent session and wait for it to be done. 1. in the ""changes"" section, verify you see a ""checkout"" button. 1. use the ""checkout"" button. verify that you get a modal dialog explaining what will happen and that the dialog makes sense. 1. verify that github pull requests is installed, and that the pr is checked out.",3.8,Critical,1.0,crash-like behavior
microsoft/vscode#290458,[testplan-item] pr extension doesn't honor swapping between repositories,"refs - [x] anyos -oikawa - [x] anyos complexity: 2 [create issue]( --- ## setup - ensure that the following settings are set: - scm.repositories.explorer, true - scm.repositories.selectionmode, single - install the latest pre-release of github pull requests ## test steps 1. open a workspace in vs code that contains two or more git repositories. 2. in more than one of the git repos, checkout a pull request. 3. in the scm view, select a repo. verify that the ""pull request"" status bar item reflects the pr associated with that repo 4. in the scm view, select a different repo. verify that the ""pull request"" status bar item reflects the pr associated with that repo",[],['TESTING'],['testplan-item'],github,2026-01-26T17:17:28Z,2026-01-28T00:23:50Z,"[testplan-item] pr extension doesn't honor swapping between repositories refs - [x] anyos -oikawa - [x] anyos complexity: 2 [create issue]( --- ## setup - ensure that the following settings are set: - scm.repositories.explorer, true - scm.repositories.selectionmode, single - install the latest pre-release of github pull requests ## test steps 1. open a workspace in vs code that contains two or more git repositories. 2. in more than one of the git repos, checkout a pull request. 3. in the scm view, select a repo. verify that the ""pull request"" status bar item reflects the pr associated with that repo 4. in the scm view, select a different repo. verify that the ""pull request"" status bar item reflects the pr associated with that repo",1.6,Low,0.584,localized low-impact
microsoft/vscode#290461,[testplan-item] set to by default,"refs - [x] windows - [ ] macos - [x] linux complexity: 3 [create issue]( --- ## context the default setting for 'task.allowautomatictasks' is now 'off', preventing automatic tasks from running when a workspace is opened. when a workspace contains automatic tasks and the user has not previously made a decision, a notification prompt appears asking for permission to run these tasks. additionally, we no longer allow workspace settings to define 'task.allowautomatictasks'. ## test steps 1. open vs code and ensure no user or workspace settings override 'task.allowautomatictasks'. 2. open a workspace containing a '.vscode/tasks.json' file with automatic tasks defined. - no automatic tasks should run immediately upon opening. 3. observe the notification prompt that appears. - the prompt should offer options: 'allow and run', 'disallow', and 'open file(s)'. 4. click 'allow and run'. - automatic tasks should run immediately and the setting should be updated to allow future automatic tasks in this workspace. 5. close and reopen the workspace. - automatic tasks should run immediately without prompting again. 6. open a different workspace with automatic tasks, and when prompted, click 'disallow'. - automatic tasks should not run, and future openings of this workspace should not prompt again. 7. open a workspace with automatic tasks and click 'open file(s)' in the prompt. - the 'tasks.json' file should open for review, and automatic tasks should not run until a decision is made. 8. set 'task.allowautomatictasks' to 'on' in user or workspace settings and open a workspace with automatic tasks. - automatic tasks should run immediately without any prompt. 9. set 'task.allowautomatictasks' to 'off' in user settings and open a workspace with automatic tasks where you have already chosen 'disallow'. - no automatic tasks should run and no prompt should appear. 10. repeat steps 2-9 on windows, macos, and linux to verify consistent behavior across platforms.",[],['TESTING'],['testplan-item'],github,2026-01-26T17:20:17Z,,"[testplan-item] set to by default refs - [x] windows - [ ] macos - [x] linux complexity: 3 [create issue]( --- ## context the default setting for 'task.allowautomatictasks' is now 'off', preventing automatic tasks from running when a workspace is opened. when a workspace contains automatic tasks and the user has not previously made a decision, a notification prompt appears asking for permission to run these tasks. additionally, we no longer allow workspace settings to define 'task.allowautomatictasks'. ## test steps 1. open vs code and ensure no user or workspace settings override 'task.allowautomatictasks'. 2. open a workspace containing a '.vscode/tasks.json' file with automatic tasks defined. - no automatic tasks should run immediately upon opening. 3. observe the notification prompt that appears. - the prompt should offer options: 'allow and run', 'disallow', and 'open file(s)'. 4. click 'allow and run'. - automatic tasks should run immediately and the setting should be updated to allow future automatic tasks in this workspace. 5. close and reopen the workspace. - automatic tasks should run immediately without prompting again. 6. open a different workspace with automatic tasks, and when prompted, click 'disallow'. - automatic tasks should not run, and future openings of this workspace should not prompt again. 7. open a workspace with automatic tasks and click 'open file(s)' in the prompt. - the 'tasks.json' file should open for review, and automatic tasks should not run until a decision is made. 8. set 'task.allowautomatictasks' to 'on' in user or workspace settings and open a workspace with automatic tasks. - automatic tasks should run immediately without any prompt. 9. set 'task.allowautomatictasks' to 'off' in user settings and open a workspace with automatic tasks where you have already chosen 'disallow'. - no automatic tasks should run and no prompt should appear. 10. repeat steps 2-9 on windows, macos, and linux to verify consistent behavior across platforms.",1.6,Low,0.584,localized low-impact
microsoft/vscode#290462,[testplan-item] dynamically stream thinking into chat response accessible view,"refs , - [x] windows - [x] macos - [x] linux complexity: 3 [create issue]( --- ## context the chat response accessible view now dynamically streams 'thinking' messages as part of the main chat response for screen reader users, prefixing them with 'thinking'. the separate thinking accessible view has been removed, ensuring that all response parts are processed in order for improved accessibility. ## test steps 1. open a chat session in vs code. 2. trigger a chat query that takes time to process (e.g., ask a complex question). 3. enable a screen reader (e.g., nvda on windows, voiceover on macos). 4. open the accessible view for the chat response. (focus the chat input box or chat response directly then use 5. as the response streams in to chat, the accessible view should dynamically update to have that content too 6. move your cursor. it should remain stable as content streams in. 7. navigate via arrow keys to prior chat responses, open the accessible view, and verify they show the relevant content",[],['TESTING'],['testplan-item'],github,2026-01-26T17:21:15Z,2026-01-28T01:10:55Z,"[testplan-item] dynamically stream thinking into chat response accessible view refs , - [x] windows - [x] macos - [x] linux complexity: 3 [create issue]( --- ## context the chat response accessible view now dynamically streams 'thinking' messages as part of the main chat response for screen reader users, prefixing them with 'thinking'. the separate thinking accessible view has been removed, ensuring that all response parts are processed in order for improved accessibility. ## test steps 1. open a chat session in vs code. 2. trigger a chat query that takes time to process (e.g., ask a complex question). 3. enable a screen reader (e.g., nvda on windows, voiceover on macos). 4. open the accessible view for the chat response. (focus the chat input box or chat response directly then use 5. as the response streams in to chat, the accessible view should dynamically update to have that content too 6. move your cursor. it should remain stable as content streams in. 7. navigate via arrow keys to prior chat responses, open the accessible view, and verify they show the relevant content",1.6,Low,0.584,localized low-impact
microsoft/vscode#290465,test inline chat affordance,"refs: - [x] anyos - [ ] anyos - [x] anyos -99 complexity: 4 [create issue]( --- we have added an inline chat affordance, in fact we added two variants and this tpi is about testing them. for that, there a new setting which can defined if there (1) no affordance, (2) an affordance in the gutter, or (3) an affordance in the editor. 1. configure to be (it also works with render mode zone but hover is the future) and test that * the setting and its values have understandable docs * when set to the affordance shows at the active corner (where the cursor blinks) of a selection * when set to the affordance show over/with the line number * when set to no affordance shows * the gutter never shows for empty selections * the affordance allows to start inline chat * the affordance strikes the balance of being useful but not annoying * the affordance teaches you the inline chat keybinding * when set to the affordance also includes the lightbulb (doesn't replace it tho/yet)",[],['TESTING'],['testplan-item'],github,2026-01-26T17:30:16Z,,"test inline chat affordance refs: - [x] anyos - [ ] anyos - [x] anyos -99 complexity: 4 [create issue]( --- we have added an inline chat affordance, in fact we added two variants and this tpi is about testing them. for that, there a new setting which can defined if there (1) no affordance, (2) an affordance in the gutter, or (3) an affordance in the editor. 1. configure to be (it also works with render mode zone but hover is the future) and test that * the setting and its values have understandable docs * when set to the affordance shows at the active corner (where the cursor blinks) of a selection * when set to the affordance show over/with the line number * when set to no affordance shows * the gutter never shows for empty selections * the affordance allows to start inline chat * the affordance strikes the balance of being useful but not annoying * the affordance teaches you the inline chat keybinding * when set to the affordance also includes the lightbulb (doesn't replace it tho/yet)",1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161803,sql: testprepareinexplicittransactiondoesnotdeadlock failed,sql.testprepareinexplicittransactiondoesnotdeadlock [failed]( with [artifacts]( on master @ [5b249523a12ee0b9b7b51b7db45398efe0211312]( help see also: [how to investigate a go test failure \(internal\)]( [this test on roachdash]( | [improve this report!]( jira issue: crdb-59088,[],['TESTING'],"['C-test-failure', 'O-robot', 'branch-master', 'T-sql-queries']",github,2026-01-26T17:31:31Z,,sql: testprepareinexplicittransactiondoesnotdeadlock failed sql.testprepareinexplicittransactiondoesnotdeadlock [failed]( with [artifacts]( on master @ [5b249523a12ee0b9b7b51b7db45398efe0211312]( help see also: [how to investigate a go test failure \(internal\)]( [this test on roachdash]( | [improve this report!]( jira issue: crdb-59088,3.8,Critical,1.0,crash-like behavior
microsoft/vscode#290471,[testplan-item] chat terminal streaming,"refs - [x] windows - [x] macos - [x] linux complexity: 5 authors: , [create issue]( --- ## terminal chat streaming - test plan ### feature overview this feature streams terminal command output directly into the chat view. users see live terminal output, can interact with prompts, and provide input within the chat interface. **auto-expand behavior:** - output auto-expands for long-running commands with real output (>500ms without data events, or >50ms after first data event) - output stays collapsed for fast commands to prevent flicker - on failure (non-zero exit code), output auto-expands (controlled by ) - on success (exit code 0), output auto-collapses (unless user manually toggled) - manual user toggle overrides all auto-expand/collapse behavior note that with enabled, the tool call needs to be expanded first to see expansion state once the command has finished. see ### test scenarios #### auto-expand: long-running commands with output 1. run - panel should expand after ~50ms (output appears immediately, command keeps running) 2. run streaming output: - panel should expand shortly after first output #### auto-expand: long-running commands without output 3. run - panel should not auto-expand (no visible output despite running >500ms) #### auto-expand: fast commands (no flicker) 4. run in a small folder - panel should not auto-expand 5. run - panel should not auto-expand 6. run (no output, exits immediately) - panel should not auto-expand #### auto-collapse on success 7. run (expands due to output + long-running) - after command succeeds, panel should auto-collapse #### failure handling 8. run or - panel should auto-expand showing failure 9. run - panel should auto-expand 10. set to , run - panel should not auto-expand on failure #### user manual toggle 11. manually expand output during a running command - auto-collapse on success should not occur 12. manually collapse output during a running command - panel should stay collapsed (no auto-expand) #### shell integration sequences 13. run (produces shell integration sequences but no visible output) - panel should not auto-expand #### cross-platform (windows) 14. run powershell command with streaming output - auto-expand behavior should match other platforms #### multiple commands 15. run multiple commands in succession (mix fast/slow with output) - only long-running commands with output should auto-expand #### edge cases 16. cancel a long-running command before timeout - panel should not auto-expand 17. multiple data events in quick succession - should only trigger one expand decision",[],['TESTING'],['testplan-item'],github,2026-01-26T17:35:39Z,2026-01-28T10:01:25Z,"[testplan-item] chat terminal streaming refs - [x] windows - [x] macos - [x] linux complexity: 5 authors: , [create issue]( --- ## terminal chat streaming - test plan ### feature overview this feature streams terminal command output directly into the chat view. users see live terminal output, can interact with prompts, and provide input within the chat interface. **auto-expand behavior:** - output auto-expands for long-running commands with real output (>500ms without data events, or >50ms after first data event) - output stays collapsed for fast commands to prevent flicker - on failure (non-zero exit code), output auto-expands (controlled by ) - on success (exit code 0), output auto-collapses (unless user manually toggled) - manual user toggle overrides all auto-expand/collapse behavior note that with enabled, the tool call needs to be expanded first to see expansion state once the command has finished. see ### test scenarios #### auto-expand: long-running commands with output 1. run - panel should expand after ~50ms (output appears immediately, command keeps running) 2. run streaming output: - panel should expand shortly after first output #### auto-expand: long-running commands without output 3. run - panel should not auto-expand (no visible output despite running >500ms) #### auto-expand: fast commands (no flicker) 4. run in a small folder - panel should not auto-expand 5. run - panel should not auto-expand 6. run (no output, exits immediately) - panel should not auto-expand #### auto-collapse on success 7. run (expands due to output + long-running) - after command succeeds, panel should auto-collapse #### failure handling 8. run or - panel should auto-expand showing failure 9. run - panel should auto-expand 10. set to , run - panel should not auto-expand on failure #### user manual toggle 11. manually expand output during a running command - auto-collapse on success should not occur 12. manually collapse output during a running command - panel should stay collapsed (no auto-expand) #### shell integration sequences 13. run (produces shell integration sequences but no visible output) - panel should not auto-expand #### cross-platform (windows) 14. run powershell command with streaming output - auto-expand behavior should match other platforms #### multiple commands 15. run multiple commands in succession (mix fast/slow with output) - only long-running commands with output should auto-expand #### edge cases 16. cancel a long-running command before timeout - panel should not auto-expand 17. multiple data events in quick succession - should only trigger one expand decision",2.859,Medium,0.87,functional impact
microsoft/vscode#290474,[testplan-item] chat input: allow to create new sessions easily,"refs - [x] anyos - [x] anyos complexity: 3 [create issue]( --- ## context a session type picker has been added near the chat input box in vs code's chat view, allowing users to choose where their agent session will run. users can select between local, background, or cloud sessions directly from the input area. the picker also supports changing the session type mid-conversation, enabling users to start in a local session and then continue in a background or cloud session. ## test steps 1. open the chat view in vs code 2. locate the session type picker near the chat input box at the bottom of the chat panel 3. verify that the picker displays the available session types (local, background, cloud) 4. select ""local"" session type and send a message - the session should run locally and respond appropriately 5. select ""background"" session type and start a new session - the session should run in the background as expected 6. select ""cloud"" session type and start a new session - the session should run in the cloud and respond appropriately 7. start a local session, send a few messages, then change the picker to ""background"" or ""cloud"" - the session should seamlessly continue in the new session type - previous context should be maintained 8. verify the session type indicator reflects the current selection 9. switch between multiple old sessions and make sure the picker updates correctly",[],['TESTING'],['testplan-item'],github,2026-01-26T17:41:16Z,2026-01-27T21:38:32Z,"[testplan-item] chat input: allow to create new sessions easily refs - [x] anyos - [x] anyos complexity: 3 [create issue]( --- ## context a session type picker has been added near the chat input box in vs code's chat view, allowing users to choose where their agent session will run. users can select between local, background, or cloud sessions directly from the input area. the picker also supports changing the session type mid-conversation, enabling users to start in a local session and then continue in a background or cloud session. ## test steps 1. open the chat view in vs code 2. locate the session type picker near the chat input box at the bottom of the chat panel 3. verify that the picker displays the available session types (local, background, cloud) 4. select ""local"" session type and send a message - the session should run locally and respond appropriately 5. select ""background"" session type and start a new session - the session should run in the background as expected 6. select ""cloud"" session type and start a new session - the session should run in the cloud and respond appropriately 7. start a local session, send a few messages, then change the picker to ""background"" or ""cloud"" - the session should seamlessly continue in the new session type - previous context should be maintained 8. verify the session type indicator reflects the current selection 9. switch between multiple old sessions and make sure the picker updates correctly",3.8,Critical,1.0,crash-like behavior
pandas-dev/pandas#63889,bug: handling of column name in is now all-nan instead of values,"### pandas version checks - [x] i have checked that this issue has not already been reported. - [x] i have confirmed this bug exists on the [latest version]( of pandas. - [ ] i have confirmed this bug exists on the [main branch]( of pandas. ### reproducible example ### issue description using that example list-of-dicts, which is the result of an incorrect csv file read with : on pandas 3.0.0, the output just has a **nan column name** with **nan values**, which doesn't seem useful.: even if that last field was not an array, and just and , the result is still the same nan column name with nan values. ### expected behavior it should have as the column name and the values should be present. while it may be more consistent with the behaviour of the param of dataframe.from_records() _""(any names not found in the data will become all-na columns)""_ - the column name is already different and it's a breaking change wrt how this used to be handled when no column param is provided: on version 2.3.3, the result of that same code is: which can be used to inform the end user of the missing header/extra column. when it's nan _name and values_, nothing can done with it except to inform the end user ""there probably an extra column"". ### installed versions installed versions ------------------ commit : 366ccdfcd8ed1e5543bfb6d4ee0c9bc519898670 python : 3.13.11 python-bits : 64 os : windows os-release : 10 version : 10.0.19045 machine : amd64 processor : intel64 family 6 model 141 stepping 1, genuineintel byteorder : little lc_all : none lang : none locale : english_united kingdom.1252 pandas : 3.0.0 numpy : 2.4.1 dateutil : 2.9.0.post0 pip : 25.3 cython : none sphinx : none ipython : 9.9.0 adbc-driver-postgresql: none adbc-driver-sqlite : none bs4 : 4.14.3 bottleneck : none fastparquet : none fsspec : none html5lib : 1.1 hypothesis : none gcsfs : none jinja2 : 3.1.6 lxml.etree : none matplotlib : none numba : none numexpr : none odfpy : none openpyxl : none psycopg2 : none pymysql : none pyarrow : none pyiceberg : none pyreadstat : none pytest : 9.0.2 python-calamine : none pytz : 2025.2 pyxlsb : none s3fs : none scipy : none sqlalchemy : 2.0.46 tables : none tabulate : none xarray : none xlrd : none xlsxwriter : none zstandard : none qtpy : none pyqt5 : none",[],['BUG'],"['Bug', 'IO Data', 'Regression', 'Closing Candidate']",github,2026-01-26T17:45:31Z,,"bug: handling of column name in is now all-nan instead of values ### pandas version checks - [x] i have checked that this issue has not already been reported. - [x] i have confirmed this bug exists on the [latest version]( of pandas. - [ ] i have confirmed this bug exists on the [main branch]( of pandas. ### reproducible example ### issue description using that example list-of-dicts, which is the result of an incorrect csv file read with : on pandas 3.0.0, the output just has a **nan column name** with **nan values**, which doesn't seem useful.: even if that last field was not an array, and just and , the result is still the same nan column name with nan values. ### expected behavior it should have as the column name and the values should be present. while it may be more consistent with the behaviour of the param of dataframe.from_records() _""(any names not found in the data will become all-na columns)""_ - the column name is already different and it's a breaking change wrt how this used to be handled when no column param is provided: on version 2.3.3, the result of that same code is: which can be used to inform the end user of the missing header/extra column. when it's nan _name and values_, nothing can done with it except to inform the end user ""there probably an extra column"". ### installed versions installed versions ------------------ commit : 366ccdfcd8ed1e5543bfb6d4ee0c9bc519898670 python : 3.13.11 python-bits : 64 os : windows os-release : 10 version : 10.0.19045 machine : amd64 processor : intel64 family 6 model 141 stepping 1, genuineintel byteorder : little lc_all : none lang : none locale : english_united kingdom.1252 pandas : 3.0.0 numpy : 2.4.1 dateutil : 2.9.0.post0 pip : 25.3 cython : none sphinx : none ipython : 9.9.0 adbc-driver-postgresql: none adbc-driver-sqlite : none bs4 : 4.14.3 bottleneck : none fastparquet : none fsspec : none html5lib : 1.1 hypothesis : none gcsfs : none jinja2 : 3.1.6 lxml.etree : none matplotlib : none numba : none numexpr : none odfpy : none openpyxl : none psycopg2 : none pymysql : none pyarrow : none pyiceberg : none pyreadstat : none pytest : 9.0.2 python-calamine : none pytz : 2025.2 pyxlsb : none s3fs : none scipy : none sqlalchemy : 2.0.46 tables : none tabulate : none xarray : none xlrd : none xlsxwriter : none zstandard : none qtpy : none pyqt5 : none",4.6,Critical,1.0,crash-like behavior
microsoft/vscode#290475,move tool picker out of top level chat input,move the chat tool picker into the mode picker as an action.,[],['FEATURE'],"['feature-request', 'verified', 'verification-needed', 'chat-tools', 'chat-input']",github,2026-01-26T17:59:04Z,2026-01-26T18:00:22Z,move tool picker out of top level chat input move the chat tool picker into the mode picker as an action.,1.4,Low,0.538,localized low-impact
cilium/cilium#44020,clustermesh-apiserver: ciliumendpoint out-of-order deletion during high pod churn,"### is there an existing issue for this? - [x] i have searched the existing issues ### version equal or higher than v1.18.6 and lower than v1.19.0 ### what happened? during cluster upgrade (with high pod churn rate), we encountered an increase of packet drops for cnp for one of our deployment pod, the other pods of the same deployment worked fine. after investigating and trying to find the root cause, we have noticed the pod being absent in remote agents' ipcaches (which correlates source ip and identity). we saw an out-of-order deletion in our clustermesh-apiserver logs: this happened for 2 pods that got rescheduled and reused ips that were previously used by other pods. their node combinations matched as well. from our metrics, the ips got reused within < 1m so maybe the ceps of the first pods were not deleted before the ceps of the second pods were added. we are not using ciliumendpointslices. note that we run clustermesh-apiserver with 2 replicas in hamode (using sessionaffinity) and the out-of-order delete happened on both instances simultaneously. ### how can we reproduce the issue? i don't know, we never encountered it before and were not yet able to reproduce it. ### cilium version 1.18.4 ### kernel version 6.12.58-82.121.amzn2023.x86_64 ### kubernetes version 1.33 ### regression _no response_ ### sysdump _no response_ ### relevant log output ### anything else? looks similar to ### cilium users document - [ ] are you a user of cilium? please add yourself to the [users doc]( ### code of conduct - [x] i agree to follow this project's code of conduct",[],['BUG'],"['kind/bug', 'kind/community-report', 'area/clustermesh']",github,2026-01-26T18:00:36Z,,"clustermesh-apiserver: ciliumendpoint out-of-order deletion during high pod churn ### is there an existing issue for this? - [x] i have searched the existing issues ### version equal or higher than v1.18.6 and lower than v1.19.0 ### what happened? during cluster upgrade (with high pod churn rate), we encountered an increase of packet drops for cnp for one of our deployment pod, the other pods of the same deployment worked fine. after investigating and trying to find the root cause, we have noticed the pod being absent in remote agents' ipcaches (which correlates source ip and identity). we saw an out-of-order deletion in our clustermesh-apiserver logs: this happened for 2 pods that got rescheduled and reused ips that were previously used by other pods. their node combinations matched as well. from our metrics, the ips got reused within < 1m so maybe the ceps of the first pods were not deleted before the ceps of the second pods were added. we are not using ciliumendpointslices. note that we run clustermesh-apiserver with 2 replicas in hamode (using sessionaffinity) and the out-of-order delete happened on both instances simultaneously. ### how can we reproduce the issue? i don't know, we never encountered it before and were not yet able to reproduce it. ### cilium version 1.18.4 ### kernel version 6.12.58-82.121.amzn2023.x86_64 ### kubernetes version 1.33 ### regression _no response_ ### sysdump _no response_ ### relevant log output ### anything else? looks similar to ### cilium users document - [ ] are you a user of cilium? please add yourself to the [users doc]( ### code of conduct - [x] i agree to follow this project's code of conduct",6.4,Critical,1.0,crash-like behavior
llvm/llvm-project#178007,[debuginfo] missed line info,"i found problem in one of tests. main.cpp when i set watchpoint on and hit it, lldb displays wrong location ( ), code compiled using from main with option. lldb.log also, shows the same problem: gcc has the same behavior, is it bug or not?",[],['BUG'],['debuginfo'],github,2026-01-26T18:03:20Z,,"[debuginfo] missed line info i found problem in one of tests. main.cpp when i set watchpoint on and hit it, lldb displays wrong location ( ), code compiled using from main with option. lldb.log also, shows the same problem: gcc has the same behavior, is it bug or not?",2.349,Medium,0.754,functional impact
microsoft/vscode#290480,sleep 3 showing output briefly,1. ask the agent to run 2. üêõ output briefly expands even though there isn't any,[],"['BUG', 'UI']","['bug', 'insiders-released', 'chat-terminal']",github,2026-01-26T18:05:19Z,2026-01-26T18:30:30Z,sleep 3 showing output briefly 1. ask the agent to run 2. üêõ output briefly expands even though there isn't any,2.394,Medium,0.764,user-visible issue
python/cpython#144254,enable tcp_nodelay by default for tcp sockets (disable nagle algorithm),"# bug report ### bug description: hello, opening a bug ticket before i send a pr. i just spent yet another day debugging a slow python application. a task that took 8 seconds out of the box, took 0.3 seconds after adding tcp_nodelay. nothing special, just two processes talking over a localhost tcp socket. for people not familiar with the matter, nagle algorithm adds a small delay (up to 500ms depending on os) before sending a packet in some circumstances, this can have a catastrophic effect on network applications depending on their read/write patterns. this has been a long-standing recurring issue across many applications and libraries. i'd like to set tcp_nodelay by default on all tcp sockets created by python ( module), to effectively disables nagle's algorithm by default. the last python discussion on this topic was in 2016 there was clear consensus that nagle algorithm should be disabled by default. this lead to a few patches in built-in modules and third party packages (asyncio and uvloop), some related tickets: 10 year have passed since that last discussion. the bug is still showing up regularly both in the python interpreter and in popular libraries. ( a quick search immediately shows that is affected ) we should fix it. for python, i think we can set tcp_nodelay when a socket is created in the module. it's just a few lines of code to change. thoughts? the rest of the world has been actively setting tcp_nodelay to mitigate the issue: * go has set tcp_nodelay by default since inception * curl has set tcp_nodelay since 2016 * nodejs has set tcp_nodelay by default since 2020 * rust has tcp_nodelay in the network libraries (main library is tokio * john nagle explained himself that the nagle algorithm has been broken since the moment the rfc was published * the rfc * one article to explain the issue regards ### cpython versions tested on: cpython main branch ### operating systems tested on: linux ### linked prs * gh-144255",[],['FEATURE'],"['type-feature', 'extension-modules']",github,2026-01-26T18:06:34Z,2026-01-28T01:09:09Z,"enable tcp_nodelay by default for tcp sockets (disable nagle algorithm) # bug report ### bug description: hello, opening a bug ticket before i send a pr. i just spent yet another day debugging a slow python application. a task that took 8 seconds out of the box, took 0.3 seconds after adding tcp_nodelay. nothing special, just two processes talking over a localhost tcp socket. for people not familiar with the matter, nagle algorithm adds a small delay (up to 500ms depending on os) before sending a packet in some circumstances, this can have a catastrophic effect on network applications depending on their read/write patterns. this has been a long-standing recurring issue across many applications and libraries. i'd like to set tcp_nodelay by default on all tcp sockets created by python ( module), to effectively disables nagle's algorithm by default. the last python discussion on this topic was in 2016 there was clear consensus that nagle algorithm should be disabled by default. this lead to a few patches in built-in modules and third party packages (asyncio and uvloop), some related tickets: 10 year have passed since that last discussion. the bug is still showing up regularly both in the python interpreter and in popular libraries. ( a quick search immediately shows that is affected ) we should fix it. for python, i think we can set tcp_nodelay when a socket is created in the module. it's just a few lines of code to change. thoughts? the rest of the world has been actively setting tcp_nodelay to mitigate the issue: * go has set tcp_nodelay by default since inception * curl has set tcp_nodelay since 2016 * nodejs has set tcp_nodelay by default since 2020 * rust has tcp_nodelay in the network libraries (main library is tokio * john nagle explained himself that the nagle algorithm has been broken since the moment the rfc was published * the rfc * one article to explain the issue regards ### cpython versions tested on: cpython main branch ### operating systems tested on: linux ### linked prs * gh-144255",5.0,Critical,1.0,crash-like behavior
kubernetes/kubernetes#136545,support tls server name overrides in egressselectorconfiguration,"## what would you like to be added? add a field to the structure in to allow overriding the hostname used for tls certificate verification, similar to the existing field in kubeconfig (added in[ ]( ## precedent this functionality already exists in: - **kubeconfig files**: the field was added in for similar use cases - **kubectl**: the flag provides command-line access - **go's **: the underlying field supports this the egress selector would benefit from the same capability for consistency. ## why is this needed? ### use case when configuring the api server's egress selector to connect to a proxy via a kubernetes service dns name, the tls certificate verification can fail if the certificate's subject alternative names (sans) don't include the service dns name. **example scenario:** - egress selector is configured to connect to: ` - the proxy's tls certificate contains sans for: , - connection fails because is not in the certificate sans ### current workaround limitations the current workaround of updating certificates to include all possible service dns names has significant drawbacks: - requires certificate regeneration and rotation across many clusters - service names may vary across environments (dev, staging, prod) - certificate management becomes complex in multi-tenant environments - may require coordination with external pki infrastructure ### proposed solution add an optional field to override the hostname used for certificate verification: this would allow connecting via the service dns name while verifying the certificate against the actual hostname in the sans. /sig api-machinery /sig auth /kind feature",[],['FEATURE'],"['sig/api-machinery', 'kind/feature', 'sig/auth', 'needs-triage']",github,2026-01-26T18:22:45Z,,"support tls server name overrides in egressselectorconfiguration ## what would you like to be added? add a field to the structure in to allow overriding the hostname used for tls certificate verification, similar to the existing field in kubeconfig (added in[ ]( ## precedent this functionality already exists in: - **kubeconfig files**: the field was added in for similar use cases - **kubectl**: the flag provides command-line access - **go's **: the underlying field supports this the egress selector would benefit from the same capability for consistency. ## why is this needed? ### use case when configuring the api server's egress selector to connect to a proxy via a kubernetes service dns name, the tls certificate verification can fail if the certificate's subject alternative names (sans) don't include the service dns name. **example scenario:** - egress selector is configured to connect to: ` - the proxy's tls certificate contains sans for: , - connection fails because is not in the certificate sans ### current workaround limitations the current workaround of updating certificates to include all possible service dns names has significant drawbacks: - requires certificate regeneration and rotation across many clusters - service names may vary across environments (dev, staging, prod) - certificate management becomes complex in multi-tenant environments - may require coordination with external pki infrastructure ### proposed solution add an optional field to override the hostname used for certificate verification: this would allow connecting via the service dns name while verifying the certificate against the actual hostname in the sans. /sig api-machinery /sig auth /kind feature",1.4,Low,0.538,localized low-impact
microsoft/vscode#290486,add inlinechat.defaultmodel setting,add a setting called that allows the user to configure which model they want to default to when using inline chat. as a note: changing the model should persist within a session but not after a reload,[],"['TESTING', 'FEATURE']","['feature-request', 'on-testplan']",github,2026-01-26T18:28:35Z,2026-01-26T18:29:32Z,add inlinechat.defaultmodel setting add a setting called that allows the user to configure which model they want to default to when using inline chat. as a note: changing the model should persist within a session but not after a reload,2.861,Medium,0.87,crash-like behavior
microsoft/vscode#290487,[testplan-item] add inlinechat.defaultmodel setting,"refs - [x] anyos - [x] anyos complexity: 2 [create issue]( --- ## context a new setting 'inlinechat.defaultmodel' has been added, allowing users to specify which ai model is used by default in inline chat. the selected model persists during the session but resets to the configured default after vs code is reloaded. ## test steps 1. open vs code and navigate to settings. 2. search for 'inlinechat.defaultmodel'. - the setting should be visible and allow selection from available models (e.g., 'gpt-3.5', 'gpt-4'). 3. confirm that all the models you have in your model picker appear in this dropdown as well (look for extra model providers you may have added or byok to confirm these) 4. set 'inlinechat.defaultmodel' to a specific model (e.g., 'gpt-4'). 5. open an inline chat session in the editor. - the selected model ('gpt-4') should be used by default. 6. change the model within the inline chat session to a different model (e.g., 'gpt-3.5'). - the new model should be used for subsequent inline chat requests in the same session. 7. close and reopen the inline chat (without reloading vs code). - the last selected model ('gpt-3.5') should still be used by default. 8. reload vs code (e.g., by closing and reopening the application). 9. open an inline chat session again. - the model should reset to the value set in 'inlinechat.defaultmodel' (e.g., 'gpt-4'). 10. verify that changing the setting updates the default model for new sessions after reload. 11. make the model you selected (eg gpt-4) not visable (the eye icon in the manage models view), check the setting and make sure it reset to the default (as it can't use gpt-4 or whatever you set anymore)",[],['TESTING'],['testplan-item'],github,2026-01-26T18:29:47Z,2026-01-27T19:12:03Z,"[testplan-item] add inlinechat.defaultmodel setting refs - [x] anyos - [x] anyos complexity: 2 [create issue]( --- ## context a new setting 'inlinechat.defaultmodel' has been added, allowing users to specify which ai model is used by default in inline chat. the selected model persists during the session but resets to the configured default after vs code is reloaded. ## test steps 1. open vs code and navigate to settings. 2. search for 'inlinechat.defaultmodel'. - the setting should be visible and allow selection from available models (e.g., 'gpt-3.5', 'gpt-4'). 3. confirm that all the models you have in your model picker appear in this dropdown as well (look for extra model providers you may have added or byok to confirm these) 4. set 'inlinechat.defaultmodel' to a specific model (e.g., 'gpt-4'). 5. open an inline chat session in the editor. - the selected model ('gpt-4') should be used by default. 6. change the model within the inline chat session to a different model (e.g., 'gpt-3.5'). - the new model should be used for subsequent inline chat requests in the same session. 7. close and reopen the inline chat (without reloading vs code). - the last selected model ('gpt-3.5') should still be used by default. 8. reload vs code (e.g., by closing and reopening the application). 9. open an inline chat session again. - the model should reset to the value set in 'inlinechat.defaultmodel' (e.g., 'gpt-4'). 10. verify that changing the setting updates the default model for new sessions after reload. 11. make the model you selected (eg gpt-4) not visable (the eye icon in the manage models view), check the setting and make sure it reset to the default (as it can't use gpt-4 or whatever you set anymore)",3.8,Critical,1.0,crash-like behavior
microsoft/vscode#290488,add setting to customize implementation agent model,"create setting . this setting defines the model to be used for the ""start implementation"" button coming out of plan mode.",[],"['TESTING', 'FEATURE']","['feature-request', 'on-testplan']",github,2026-01-26T18:31:15Z,2026-01-26T18:31:44Z,"add setting to customize implementation agent model create setting . this setting defines the model to be used for the ""start implementation"" button coming out of plan mode.",1.5,Low,0.561,localized low-impact
microsoft/vscode#290489,[testplan-item] add setting to customize implementation agent model,"refs - [x] anyos - [x] anyos complexity: 3 [create issue]( --- ## context a new setting 'github.copilot.config.implementagent.model' allows users to customize which ai model is used when they click the 'start implementation' button after plan mode in copilot. this gives users control over the model powering code generation. ## test steps 1. look at the mode dropdown- verify you can see ""plan"" mode as normal but do not see ""implementation"" mode 2. open vs code and navigate to the settings (file > preferences > settings). 3. search for 'github.copilot.config.implementagent.model'. - the setting should be visible and editable. 4. set the value to a valid model name e.g., note: you must do 5. enter plan mode in copilot, select a model that is not what you set for the implement model, and ask a planning question 6. now click the 'start implementation' button. - confirm the implementation uses your model from the setting 7. set the value to an invalid model name. - attempt to use 'start implementation'; an error or fallback behavior should occur (default model is used, and a warning is shown in the window logs 8. reset the setting to its default value and verify that implementation uses the default model. 9. click on ""configure custom agents"" in the dropdown in chat 10. select ""implement"" agent 11. give the instructions a read and thread any suggestions you have for them here",[],['TESTING'],['testplan-item'],github,2026-01-26T18:32:27Z,2026-01-27T22:41:00Z,"[testplan-item] add setting to customize implementation agent model refs - [x] anyos - [x] anyos complexity: 3 [create issue]( --- ## context a new setting 'github.copilot.config.implementagent.model' allows users to customize which ai model is used when they click the 'start implementation' button after plan mode in copilot. this gives users control over the model powering code generation. ## test steps 1. look at the mode dropdown- verify you can see ""plan"" mode as normal but do not see ""implementation"" mode 2. open vs code and navigate to the settings (file > preferences > settings). 3. search for 'github.copilot.config.implementagent.model'. - the setting should be visible and editable. 4. set the value to a valid model name e.g., note: you must do 5. enter plan mode in copilot, select a model that is not what you set for the implement model, and ask a planning question 6. now click the 'start implementation' button. - confirm the implementation uses your model from the setting 7. set the value to an invalid model name. - attempt to use 'start implementation'; an error or fallback behavior should occur (default model is used, and a warning is shown in the window logs 8. reset the setting to its default value and verify that implementation uses the default model. 9. click on ""configure custom agents"" in the dropdown in chat 10. select ""implement"" agent 11. give the instructions a read and thread any suggestions you have for them here",1.6,Low,0.584,localized low-impact
pytorch/pytorch#173389,inference latency increase on arm after upgrading from pytorch 1.13.1 to 2.x,"### üêõ describe the bug we are experiencing a significant increase in indexing latency on arm machines after upgrading from pytorch 1.13.1 to 2.5.1 or 2.7.1. this issue does not occur on x86 machines. **environment** - architecture: arm (aarch64) - djl version: 0.31.1 (also tested with 0.36.0) - pytorch versions tested: 1.13.1, 2.5.1, 2.7.1 **benchmark results** pytorch version | djl version | 50th percentile latency (ms) -- | -- | -- 1.13.1 | 0.31.1 | 7,418.92 2.5.1 | 0.31.1 | 11,778.60 2.7.1 | 0.36.0 | 10,850.20 **investigation done** - confirmed the latency increase originates from the inference portion, not indexing or other operations, by analyzing internal metrics. - issue is arm-specific; x86 machines do not exhibit this behavior. **additional info** - more details on the issue: - found similar pytorch forum thread: ### versions pytorch 2.x (e.g., 2.5.1 and 2.7.1) cc -arm",[],['PERFORMANCE'],"['needs reproduction', 'module: performance', 'triaged', 'module: arm']",github,2026-01-26T18:34:29Z,,"inference latency increase on arm after upgrading from pytorch 1.13.1 to 2.x ### üêõ describe the bug we are experiencing a significant increase in indexing latency on arm machines after upgrading from pytorch 1.13.1 to 2.5.1 or 2.7.1. this issue does not occur on x86 machines. **environment** - architecture: arm (aarch64) - djl version: 0.31.1 (also tested with 0.36.0) - pytorch versions tested: 1.13.1, 2.5.1, 2.7.1 **benchmark results** pytorch version | djl version | 50th percentile latency (ms) -- | -- | -- 1.13.1 | 0.31.1 | 7,418.92 2.5.1 | 0.31.1 | 11,778.60 2.7.1 | 0.36.0 | 10,850.20 **investigation done** - confirmed the latency increase originates from the inference portion, not indexing or other operations, by analyzing internal metrics. - issue is arm-specific; x86 machines do not exhibit this behavior. **additional info** - more details on the issue: - found similar pytorch forum thread: ### versions pytorch 2.x (e.g., 2.5.1 and 2.7.1) cc -arm",6.4,Critical,1.0,performance degradation
microsoft/vscode#290490,chat session perf issues,"notes for improvements that we didn't get to in - - don't re-render the tree all the time- just observe changes to chat viewmodels and update in place (more in the style of markdown ""progressive rendering"") - dataid and complicated tree diffing shouldn't be an issue - markdown rendering in general is slow - check whether there are labels that don't need to be rendered as markdown- skip it for strings that have no md characters - look into optimizations for the markdown renderer, disable features - chatmodels are sometimes still retained after being rendered and disposed",[],['PERFORMANCE'],"['perf', 'chat']",github,2026-01-26T18:36:23Z,,"chat session perf issues notes for improvements that we didn't get to in - - don't re-render the tree all the time- just observe changes to chat viewmodels and update in place (more in the style of markdown ""progressive rendering"") - dataid and complicated tree diffing shouldn't be an issue - markdown rendering in general is slow - check whether there are labels that don't need to be rendered as markdown- skip it for strings that have no md characters - look into optimizations for the markdown renderer, disable features - chatmodels are sometimes still retained after being rendered and disposed",6.8,Critical,1.0,"performance degradation, crash-like behavior"
microsoft/vscode#290493,integrated browser: tab / editor lifecycle,"refs: - [x] macos - [x] linux - [x] windows complexity: 2 authors: , [create issue]( --- the integrated browser enables rich browser-like functionality within vs code desktop. this tpi only covers the overall lifecycle of browser tabs such as creation, resize, reload, etc. **there is no need to test the general browsing behavior or new commands as part of this tpi.** but of course feel free to open other issues if you find them. [list of all integrated browser tpis]( ### setup: 1. open a browser tab using the ""browser: open integrated browser"" command 2. navigate to any valid url -- this could be local or a public site like google etc. - note: you may want to navigate to different urls when testing with multiple tabs **important**: ensure you perform testing with a page successfully loaded. testing against the welcome screen or an error screen may hide issues (though feel free to perform additional testing with these screens as well) ### scenarios to cover: 1. open multiple browser tabs and swap between them / arrange in split view 2. use the ""move into new window"" and ""copy into new window"" commands 3. layout changes: resize the window, toggle / resize panels, etc. 4. close a tab and then reopen it (ctrl/cmd+shift+t) 5. use the ""reload window"" command 6. close & reopen vs code the goal in this tpi is just to validate that pages are shown / hidden and resized as appropriate.",[],['TESTING'],"['testplan-item', 'browser-integration']",github,2026-01-26T18:37:30Z,2026-01-28T08:24:58Z,"integrated browser: tab / editor lifecycle refs: - [x] macos - [x] linux - [x] windows complexity: 2 authors: , [create issue]( --- the integrated browser enables rich browser-like functionality within vs code desktop. this tpi only covers the overall lifecycle of browser tabs such as creation, resize, reload, etc. **there is no need to test the general browsing behavior or new commands as part of this tpi.** but of course feel free to open other issues if you find them. [list of all integrated browser tpis]( ### setup: 1. open a browser tab using the ""browser: open integrated browser"" command 2. navigate to any valid url -- this could be local or a public site like google etc. - note: you may want to navigate to different urls when testing with multiple tabs **important**: ensure you perform testing with a page successfully loaded. testing against the welcome screen or an error screen may hide issues (though feel free to perform additional testing with these screens as well) ### scenarios to cover: 1. open multiple browser tabs and swap between them / arrange in split view 2. use the ""move into new window"" and ""copy into new window"" commands 3. layout changes: resize the window, toggle / resize panels, etc. 4. close a tab and then reopen it (ctrl/cmd+shift+t) 5. use the ""reload window"" command 6. close & reopen vs code the goal in this tpi is just to validate that pages are shown / hidden and resized as appropriate.",3.8,Critical,1.0,crash-like behavior
kubernetes/kubernetes#136546,"[failing test][sig node] ""ensure credential pulled images pulling images with credentials [featuregate:kubeletensuresecretpulledimages] [beta] [serial]"" error",### which jobs are failing? ci-kubernetes-node-arm64-ubuntu-serial ci-kubernetes-node-swap-ubuntu-serial ci-containerd-node-arm64-e2e-serial-ec2 ci-containerd-node-e2e-serial-ec2 ci-cos-containerd-node-e2e-serial ### which tests are failing? ### since when has it been failing? 1/13 for 1/23 for & & ### testgrid link _no response_ ### reason for failure (if possible) _no response_ ### anything else we need to know? there was a previous fix in ### relevant sig(s) /sig node,[],['TESTING'],"['sig/node', 'kind/failing-test', 'needs-triage']",github,2026-01-26T19:01:39Z,2026-01-26T20:39:23Z,"[failing test][sig node] ""ensure credential pulled images pulling images with credentials [featuregate:kubeletensuresecretpulledimages] [beta] [serial]"" error ### which jobs are failing? ci-kubernetes-node-arm64-ubuntu-serial ci-kubernetes-node-swap-ubuntu-serial ci-containerd-node-arm64-e2e-serial-ec2 ci-containerd-node-e2e-serial-ec2 ci-cos-containerd-node-e2e-serial ### which tests are failing? ### since when has it been failing? 1/13 for 1/23 for & & ### testgrid link _no response_ ### reason for failure (if possible) _no response_ ### anything else we need to know? there was a previous fix in ### relevant sig(s) /sig node",1.6,Low,0.584,localized low-impact
microsoft/vscode#290504,infinite [scmcontextresolver] cache invalidated log,- open copilot chat log - is logged forever because it's triggered by a document changing which in this case is the output document,[],['BUG'],"['bug', 'scm', 'inline-completions']",github,2026-01-26T19:06:58Z,2026-01-27T11:10:19Z,infinite [scmcontextresolver] cache invalidated log - open copilot chat log - is logged forever because it's triggered by a document changing which in this case is the output document,4.6,Critical,1.0,crash-like behavior
cockroachdb/cockroach#161808,roachtest: import/nodeshutdown/worker/distmerge=false/nodes=4 failed,roachtest.import/nodeshutdown/worker/distmerge=false/nodes=4 [failed]( with [artifacts]( on master @ [867004a4311025b7e0de44bc8c72fcbc8442f041]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21021544-1769411655-32-n4cpu4-0001 | 34.75.183.61 | 10.142.0.76 | | teamcity-21021544-1769411655-32-n4cpu4-0002 | 34.75.62.221 | 10.142.0.73 | | teamcity-21021544-1769411655-32-n4cpu4-0003 | 104.196.191.168 | 10.142.2.177 | | teamcity-21021544-1769411655-32-n4cpu4-0004 | 34.74.148.39 | 10.142.2.176 | parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - metamorphicleases=default - metamorphicwritebuffering=true - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( [this test on roachdash]( | [improve this report!]( jira issue: crdb-59089,[],['TESTING'],"['C-test-failure', 'O-robot', 'O-roachtest', 'branch-master', 'release-blocker', 'T-sql-queries']",github,2026-01-26T19:07:17Z,2026-01-27T19:44:22Z,roachtest: import/nodeshutdown/worker/distmerge=false/nodes=4 failed roachtest.import/nodeshutdown/worker/distmerge=false/nodes=4 [failed]( with [artifacts]( on master @ [867004a4311025b7e0de44bc8c72fcbc8442f041]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21021544-1769411655-32-n4cpu4-0001 | 34.75.183.61 | 10.142.0.76 | | teamcity-21021544-1769411655-32-n4cpu4-0002 | 34.75.62.221 | 10.142.0.73 | | teamcity-21021544-1769411655-32-n4cpu4-0003 | 104.196.191.168 | 10.142.2.177 | | teamcity-21021544-1769411655-32-n4cpu4-0004 | 34.74.148.39 | 10.142.2.176 | parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - metamorphicleases=default - metamorphicwritebuffering=true - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( [this test on roachdash]( | [improve this report!]( jira issue: crdb-59089,2.941,Medium,0.888,functional impact
microsoft/vscode#290506,[testplan-item] add api for providing dynamic prompt files,"refs - [x] anyos - [x] anyos complexity: 3 [create issue]( --- ## context added new extension api for contributing dynamic prompt file types. this is currently a proposed api, but we would like to make it stable in the feb release so feedback is appreciated. ## test steps 1. consider using the following branch as a base: **custom agents provider** 1. use to dynamically contribute a custom agent. 2. the custom agent appears in the dropdown; selecting the agent and sending a chat message should use the instructions for that agent. 3. when clicking on settings wheel at top of chat window > custom agents, the custom agent file should also appear and clicking it should also show an editor with the correct contents. 4. verify that calling refreshes the custom agent content properly. **instructions provider** 1. use to dynamically contribute instructions. make sure you use to always include the instructions for testing. 2. make sure the instruction reference shows up when any message is sent and the instructions are affect the response. 3. when clicking on settings wheel at top of chat window > chat instructions, the instruction file should also appear and clicking it should also show an editor with the correct contents. 4. verify that calling refreshes the instructions content properly. **prompt files provider** 1. use to dynamically contribute prompt files / slash commands. 2. make sure the slash commands show up and can be invoked. 3. when clicking on settings wheel at top of chat window > prompt files, the prompt file should also appear and clicking it should also show an editor with the correct contents. 4. verify that calling refreshes the prompt files content properly. **skills provider** 1. use to dynamically contribute skills. 2. make sure the skills show up when you ask ""what skills do you know?"" 3. when clicking on settings wheel at top of chat window > skills, the skill.md should also appear and clicking it should also show an editor with the correct contents. 4. verify that calling refreshes the skills content properly. you can ask the agent to re-read the skill file to see if it changed.",[],['TESTING'],['testplan-item'],github,2026-01-26T19:10:36Z,2026-01-27T21:18:06Z,"[testplan-item] add api for providing dynamic prompt files refs - [x] anyos - [x] anyos complexity: 3 [create issue]( --- ## context added new extension api for contributing dynamic prompt file types. this is currently a proposed api, but we would like to make it stable in the feb release so feedback is appreciated. ## test steps 1. consider using the following branch as a base: **custom agents provider** 1. use to dynamically contribute a custom agent. 2. the custom agent appears in the dropdown; selecting the agent and sending a chat message should use the instructions for that agent. 3. when clicking on settings wheel at top of chat window > custom agents, the custom agent file should also appear and clicking it should also show an editor with the correct contents. 4. verify that calling refreshes the custom agent content properly. **instructions provider** 1. use to dynamically contribute instructions. make sure you use to always include the instructions for testing. 2. make sure the instruction reference shows up when any message is sent and the instructions are affect the response. 3. when clicking on settings wheel at top of chat window > chat instructions, the instruction file should also appear and clicking it should also show an editor with the correct contents. 4. verify that calling refreshes the instructions content properly. **prompt files provider** 1. use to dynamically contribute prompt files / slash commands. 2. make sure the slash commands show up and can be invoked. 3. when clicking on settings wheel at top of chat window > prompt files, the prompt file should also appear and clicking it should also show an editor with the correct contents. 4. verify that calling refreshes the prompt files content properly. **skills provider** 1. use to dynamically contribute skills. 2. make sure the skills show up when you ask ""what skills do you know?"" 3. when clicking on settings wheel at top of chat window > skills, the skill.md should also appear and clicking it should also show an editor with the correct contents. 4. verify that calling refreshes the skills content properly. you can ask the agent to re-read the skill file to see if it changed.",3.8,Critical,1.0,crash-like behavior
kubernetes/kubernetes#136547,"[failing test][sig node] ""imagevolume subpath should succeed when using a valid subpathchanges""",### which jobs are failing? & ### which tests are failing? ci-cri-containerd-node-e2e-unlabelled & ci-crio-node-e2e-unlabelled ### since when has it been failing? 1/23 ### testgrid link ### reason for failure (if possible) ### anything else we need to know? _no response_ ### relevant sig(s) /sig node,[],['TESTING'],"['sig/node', 'kind/failing-test', 'needs-triage']",github,2026-01-26T19:11:51Z,,"[failing test][sig node] ""imagevolume subpath should succeed when using a valid subpathchanges"" ### which jobs are failing? & ### which tests are failing? ci-cri-containerd-node-e2e-unlabelled & ci-crio-node-e2e-unlabelled ### since when has it been failing? 1/23 ### testgrid link ### reason for failure (if possible) ### anything else we need to know? _no response_ ### relevant sig(s) /sig node",3.8,Critical,1.0,crash-like behavior
microsoft/vscode#290509,git: stash editor redirects to wrong stash when new stashes are created,"does this issue occur when all extensions are disabled?: yes/no report issue' dialog can assist with this. --> - vs code version: n/a - os version: n/a ## problem view stash relies on stash index, but if a new stash is created while the last one is shown in editor, trying to open the now recent one with index 0 will not work, it will redirect to the already opened now stash 1. ## how to reproduce 1. make some changes in a repo then them to create stash a (now at ) 2. open stash a using the source control stash menu ‚Üí it opens a diff editor 3. (optional) pin that editor tab (right-click tab ‚Üí ""pin"") - or just leave it open 4. make new changes and again to create stash b (b is now , a becomes ) 5. try to open the most recent stash b from the stash menu 6. **bug**: vs code redirects to the already open editor (showing a, not b) 7. **worse**: click ""pop stash"" or ""drop stash"" from that editor ‚Üí operates on the wrong stash! ## solution i've submitted a pr that fixes this by using the stash's commit hash (immutable) instead of index (shifts) as the editor identifier: ## side effect/known limitation when a stash editor is already open and the stash index shifts (due to new stashes being created), the editor title still shows the original index, even if you re-try to open the stash by view stash menu. the only way to get the updated index is by closing this editor and viewing the stash again. ## related issues i found this old issue that may be linked to this fix.",[],['BUG'],"['bug', 'git']",github,2026-01-26T19:16:05Z,,"git: stash editor redirects to wrong stash when new stashes are created does this issue occur when all extensions are disabled?: yes/no report issue' dialog can assist with this. --> - vs code version: n/a - os version: n/a ## problem view stash relies on stash index, but if a new stash is created while the last one is shown in editor, trying to open the now recent one with index 0 will not work, it will redirect to the already opened now stash 1. ## how to reproduce 1. make some changes in a repo then them to create stash a (now at ) 2. open stash a using the source control stash menu ‚Üí it opens a diff editor 3. (optional) pin that editor tab (right-click tab ‚Üí ""pin"") - or just leave it open 4. make new changes and again to create stash b (b is now , a becomes ) 5. try to open the most recent stash b from the stash menu 6. **bug**: vs code redirects to the already open editor (showing a, not b) 7. **worse**: click ""pop stash"" or ""drop stash"" from that editor ‚Üí operates on the wrong stash! ## solution i've submitted a pr that fixes this by using the stash's commit hash (immutable) instead of index (shifts) as the editor identifier: ## side effect/known limitation when a stash editor is already open and the stash index shifts (due to new stashes being created), the editor title still shows the original index, even if you re-try to open the stash by view stash menu. the only way to get the updated index is by closing this editor and viewing the stash again. ## related issues i found this old issue that may be linked to this fix.",6.4,Critical,1.0,crash-like behavior
envoyproxy/envoy#43164,how to build using new hermetic toolchain?,we build a custom version of envoy with filters. we recently attempted an upgrade to v1.37.0 and noticed our builds broke. file: previously our ci pipelines relied on which has since been removed. after following the paper trail it is now my understanding that the build pipeline should now setup llvm if it's not installed? i receive this error attempting to build our workspace without setting up llvm: our dockerfile looks something like the following: does v1.37.0 still require a local instance of the llvm? if so is there a new way provided to setup the installation similar to ?,[],['UI'],"['question', 'area/build']",github,2026-01-26T19:17:40Z,2026-01-27T15:30:01Z,how to build using new hermetic toolchain? we build a custom version of envoy with filters. we recently attempted an upgrade to v1.37.0 and noticed our builds broke. file: previously our ci pipelines relied on which has since been removed. after following the paper trail it is now my understanding that the build pipeline should now setup llvm if it's not installed? i receive this error attempting to build our workspace without setting up llvm: our dockerfile looks something like the following: does v1.37.0 still require a local instance of the llvm? if so is there a new way provided to setup the installation similar to ?,1.8,Low,0.629,user-visible issue
facebook/react#35633,[compiler bug]: some spaghetti code gives me todo: prunehoistedcontexts,"### what kind of issue is this? - [x] react compiler core (the js output is incorrect, or your app works incorrectly after optimization) - [ ] babel-plugin-react-compiler (build issue installing or using the babel plugin) - [ ] eslint-plugin-react-hooks (build issue installing or using the eslint plugin) - [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script) ### link to repro ### repro steps open playground, see error: > todo: [prunehoistedcontexts] rewrite hoisted function references changing to fixes it (among lots of other changes) ### how often does this bug happen? every time ### what version of react are you using? 19.2.3 ### what version of react compiler are you using? 1.0.0",[],['BUG'],"['Type: Bug', 'Status: Unconfirmed']",github,2026-01-26T19:19:55Z,2026-01-26T21:36:30Z,"[compiler bug]: some spaghetti code gives me todo: prunehoistedcontexts ### what kind of issue is this? - [x] react compiler core (the js output is incorrect, or your app works incorrectly after optimization) - [ ] babel-plugin-react-compiler (build issue installing or using the babel plugin) - [ ] eslint-plugin-react-hooks (build issue installing or using the eslint plugin) - [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script) ### link to repro ### repro steps open playground, see error: > todo: [prunehoistedcontexts] rewrite hoisted function references changing to fixes it (among lots of other changes) ### how often does this bug happen? every time ### what version of react are you using? 19.2.3 ### what version of react compiler are you using? 1.0.0",4.6,Critical,1.0,crash-like behavior
electron/electron#49530,linux: app.getgpuinfo throws error: gpu access not allowed. reason: gpu access is disabled in chrome://settings,"### preflight checklist - [x] i have read the [contributing guidelines]( for this project. - [x] i agree to follow the [code of conduct]( that this project adheres to. - [x] i have searched the [issue tracker]( for a bug report that matches the one i want to file, without success. ### electron version 40.0.0 ### what operating system(s) are you using? ubuntu ### operating system version 22,24 ### what arch are you using? x64 ### last known working electron version 39.3.0 ### does the issue also appear in chromium / google chrome? i don't know how to test ### expected behavior electron app starts and shows new . ### actual behavior after upgrade from v39.3.0 to v40.0.0 calling throws: the code is called before is created (by sentry.js initialization). we have tried in on 3 different machines (with different graphic cards) with ubuntu 22 and ubuntu 24, too. we use: and without this, the app starts ok. also when i've tried to add the switch: then the app starts ok, too. ### testcase gist url _no response_ ### additional information * switching from wayland to x11 by does not help. * same problem with v40.0.beta1. * v40.0.alpha.8 shows the window, but it's blank and console is flooded by ""network service crashed or was terminated, restarting service."" * => seems like the problem is in some commit between v40.0.alpha.8 and v40.0.beta1. * same problem with electron .0.0-alpha.2",[],"['BUG', 'UI']","['platform/linux', 'bug :beetle:', 'status/confirmed', '40-x-y']",github,2026-01-26T19:23:36Z,2026-01-27T18:32:43Z,"linux: app.getgpuinfo throws error: gpu access not allowed. reason: gpu access is disabled in chrome://settings ### preflight checklist - [x] i have read the [contributing guidelines]( for this project. - [x] i agree to follow the [code of conduct]( that this project adheres to. - [x] i have searched the [issue tracker]( for a bug report that matches the one i want to file, without success. ### electron version 40.0.0 ### what operating system(s) are you using? ubuntu ### operating system version 22,24 ### what arch are you using? x64 ### last known working electron version 39.3.0 ### does the issue also appear in chromium / google chrome? i don't know how to test ### expected behavior electron app starts and shows new . ### actual behavior after upgrade from v39.3.0 to v40.0.0 calling throws: the code is called before is created (by sentry.js initialization). we have tried in on 3 different machines (with different graphic cards) with ubuntu 22 and ubuntu 24, too. we use: and without this, the app starts ok. also when i've tried to add the switch: then the app starts ok, too. ### testcase gist url _no response_ ### additional information * switching from wayland to x11 by does not help. * same problem with v40.0.beta1. * v40.0.alpha.8 shows the window, but it's blank and console is flooded by ""network service crashed or was terminated, restarting service."" * => seems like the problem is in some commit between v40.0.alpha.8 and v40.0.beta1. * same problem with electron .0.0-alpha.2",3.499,High,1.0,"user-visible issue, crash-like behavior"
microsoft/vscode#290515,[testplan-item] chat subagent ux,"refs: - [ ] anyos - [x] anyos - [x] anyos complexity: 4 authors: [create issue]( --- we have made a bunch of improvements to subagents this iteration. please run through a bunch of subagent scenarios. first, set . this enables using custom agents for subagents. here are the things that are new - when the model makes multiple parallel tool calls, we actually execute the subagents in parallel. - the ux is redesigned- you should see a single line with the subagent name (or the default name subagent), the task description, and the latest tool call. example: - you can click the title to expand the container like a thinking block - when there is a confirmation in the subagent list, it should auto-expand so that you can see the confirmation, and then collapse again when you take action on it - the subagent input/output is shown when you expand the subagent block. see the first and last text parts below. i have clicked to expand the first one, the last one is collapsed. these are useful for debugging your subagents or just tracing how the parent agent communicated with them. example scenarios to try - plan mode uses a subagent to do research - an easy way to drive particular scenarios is just to tell the agent exactly what to do: - set up multiple custom agents with descriptions on how to orchestrate them. here's a simple example: agentwithreview.agent.md: code-reviewer.agent.md: then pick your agentwithreview in the picker and see if it uses the code-reviewer correctly. or set up a more complex multi-subagent scenario. ## debugging - calls from inside the subagent look like this",[],['TESTING'],['testplan-item'],github,2026-01-26T19:24:32Z,,"[testplan-item] chat subagent ux refs: - [ ] anyos - [x] anyos - [x] anyos complexity: 4 authors: [create issue]( --- we have made a bunch of improvements to subagents this iteration. please run through a bunch of subagent scenarios. first, set . this enables using custom agents for subagents. here are the things that are new - when the model makes multiple parallel tool calls, we actually execute the subagents in parallel. - the ux is redesigned- you should see a single line with the subagent name (or the default name subagent), the task description, and the latest tool call. example: - you can click the title to expand the container like a thinking block - when there is a confirmation in the subagent list, it should auto-expand so that you can see the confirmation, and then collapse again when you take action on it - the subagent input/output is shown when you expand the subagent block. see the first and last text parts below. i have clicked to expand the first one, the last one is collapsed. these are useful for debugging your subagents or just tracing how the parent agent communicated with them. example scenarios to try - plan mode uses a subagent to do research - an easy way to drive particular scenarios is just to tell the agent exactly what to do: - set up multiple custom agents with descriptions on how to orchestrate them. here's a simple example: agentwithreview.agent.md: code-reviewer.agent.md: then pick your agentwithreview in the picker and see if it uses the code-reviewer correctly. or set up a more complex multi-subagent scenario. ## debugging - calls from inside the subagent look like this",1.6,Low,0.584,localized low-impact
microsoft/vscode#290516,[testplan-item] long chat session perf issues,"refs: - [x] windows - [x] macos - [ ] linux complexity: 3 authors: [create issue]( --- we did a lot of work to improve the performance of long chats. this focused on two areas: saving chat sessions to disk, and rendering. for testing this, you will want to have a long chat session. you can find one in your history or create one. ""long"" can mean long individual agent responses or many short request/responses. ## chat persistence the chat session format on disk is totally rewritten. test that we migrate sessions forward correctly - find some old local chat sessions that you have sitting around, test that you can open them - or, create a chat session in vscode stable with some different types of content, then open it in insiders. you can run then saving very long sessions would have caused brief ui hangs at random times, and it could have delayed vscode shutdown with a popup dialog and a message about waiting for chat history to be saved. you should not see that happen now. ## chat rendering work with a very long chat session. scroll around. it should scroll more or less smoothly with few pauses in scrolling while rendering new content. there may still be some jank, especially if you have very long individual responses or many codeblocks. but it should be much smoother than in stable. test with the different settings. scroll while a response is in progress or not in progress. try using checkpoints and editing requests. it should not be so janky that it's annoying to work with or totally breaks. if it seems worse than it should, use the performance tab in devtools to take a perf trace while reproing the issue and share the log. also export your session with ""export chat session"" and share that with me. a good way to test all of this would be to just work with one single long chat session while working through other tpis or fixes this week. we usually don't work that way on this team but many of our users do.",[],['TESTING'],['testplan-item'],github,2026-01-26T19:26:03Z,,"[testplan-item] long chat session perf issues refs: - [x] windows - [x] macos - [ ] linux complexity: 3 authors: [create issue]( --- we did a lot of work to improve the performance of long chats. this focused on two areas: saving chat sessions to disk, and rendering. for testing this, you will want to have a long chat session. you can find one in your history or create one. ""long"" can mean long individual agent responses or many short request/responses. ## chat persistence the chat session format on disk is totally rewritten. test that we migrate sessions forward correctly - find some old local chat sessions that you have sitting around, test that you can open them - or, create a chat session in vscode stable with some different types of content, then open it in insiders. you can run then saving very long sessions would have caused brief ui hangs at random times, and it could have delayed vscode shutdown with a popup dialog and a message about waiting for chat history to be saved. you should not see that happen now. ## chat rendering work with a very long chat session. scroll around. it should scroll more or less smoothly with few pauses in scrolling while rendering new content. there may still be some jank, especially if you have very long individual responses or many codeblocks. but it should be much smoother than in stable. test with the different settings. scroll while a response is in progress or not in progress. try using checkpoints and editing requests. it should not be so janky that it's annoying to work with or totally breaks. if it seems worse than it should, use the performance tab in devtools to take a perf trace while reproing the issue and share the log. also export your session with ""export chat session"" and share that with me. a good way to test all of this would be to just work with one single long chat session while working through other tpis or fixes this week. we usually don't work that way on this team but many of our users do.",5.2,Critical,1.0,crash-like behavior
microsoft/vscode#290519,agent sessions: improve the stacked list view,"rather than showing the most recent 3 sessions and providing a link to expand to the full list, simply add a node ""more"" below the last session that can be expanded to reveal the other sessions:",[],['FEATURE'],"['feature-request', 'verified', 'verification-needed', 'insiders-released', 'chat-agents-view']",github,2026-01-26T19:40:13Z,2026-01-26T20:49:56Z,"agent sessions: improve the stacked list view rather than showing the most recent 3 sessions and providing a link to expand to the full list, simply add a node ""more"" below the last session that can be expanded to reveal the other sessions:",1.4,Low,0.538,localized low-impact
envoyproxy/envoy#43165,default filter chain metadata change is not handled as in-place listener update,"*title*: *default filter chain metadata change causes listener drain* *description*: when changing metadata for , envoy logs show `use full listener update path [...] draining listener`: the stat is not increased. however, when changing metadata of a filter chain in , an in-place update takes place: the stat is not increased. the stat is increased for duration of the drain. *config*: example (use dynamic lds and change metadata section):",[],['BUG'],"['area/listener', 'bug']",github,2026-01-26T19:40:35Z,,"default filter chain metadata change is not handled as in-place listener update *title*: *default filter chain metadata change causes listener drain* *description*: when changing metadata for , envoy logs show `use full listener update path [...] draining listener`: the stat is not increased. however, when changing metadata of a filter chain in , an in-place update takes place: the stat is not increased. the stat is increased for duration of the drain. *config*: example (use dynamic lds and change metadata section):",4.6,Critical,1.0,crash-like behavior
microsoft/vscode#290520,2026 theme sign in modal is transparent,sign in modals are missing blur background,[],['UI'],"['ux', 'polish', '2026-themes']",github,2026-01-26T19:41:16Z,,2026 theme sign in modal is transparent sign in modals are missing blur background,1.8,Low,0.629,user-visible issue
microsoft/vscode#290522,[testplan-item] support github's organization custom instructions,"refs - [x] anyos - [x] anyos complexity: 3 [create issue]( --- ### background github supports instructions at the organization level: we want to be able to load these in vs code as well. ### testing steps #### enterprise setup 1. check if you have access to 2. if not, reach out to for test enterprise access. make sure you have a spare/burner gh account ready and share that alias. 3. clone the test repository in the test enterprise #### github enterprise custom agents validation 1. open the cloned repository 2. sign out of your github account 3. sign in with the spare/burner gh account from previous step by clicking copilot icon on bottom right status bar 4. verify that when sending a chat message, the response is spoken like a mad scientist persona 5. clicking on the setting button in the top of the chat window > chat instructions you can find an organization instructions file (default.instructions.md)",[],['TESTING'],['testplan-item'],github,2026-01-26T19:43:45Z,2026-01-27T19:50:40Z,"[testplan-item] support github's organization custom instructions refs - [x] anyos - [x] anyos complexity: 3 [create issue]( --- ### background github supports instructions at the organization level: we want to be able to load these in vs code as well. ### testing steps #### enterprise setup 1. check if you have access to 2. if not, reach out to for test enterprise access. make sure you have a spare/burner gh account ready and share that alias. 3. clone the test repository in the test enterprise #### github enterprise custom agents validation 1. open the cloned repository 2. sign out of your github account 3. sign in with the spare/burner gh account from previous step by clicking copilot icon on bottom right status bar 4. verify that when sending a chat message, the response is spoken like a mad scientist persona 5. clicking on the setting button in the top of the chat window > chat instructions you can find an organization instructions file (default.instructions.md)",1.6,Low,0.584,localized low-impact
microsoft/vscode#290529,allow custom agents to be user-hidden by default,"some agents are only meant to be used as subagents, while other agents are not to be used as subageent, we want to extend the to a string enum with the values , , and . boolean and can still be read, they map to and . means the agent will be available as custom agent in the custom agent picker and can be used as subagent. means it is available as custom agent in the agent picker means it can only be used as subagent but the subagent tool means it is neither in the custom agent picker not can be picked by the subagent tool. if infer is not set, it defaults to .",[],['FEATURE'],"['feature-request', 'verified', 'verification-needed', 'insiders-released', 'chat-agent', 'chat-subagents']",github,2026-01-26T20:00:19Z,2026-01-26T22:11:21Z,"allow custom agents to be user-hidden by default some agents are only meant to be used as subagents, while other agents are not to be used as subageent, we want to extend the to a string enum with the values , , and . boolean and can still be read, they map to and . means the agent will be available as custom agent in the custom agent picker and can be used as subagent. means it is available as custom agent in the agent picker means it can only be used as subagent but the subagent tool means it is neither in the custom agent picker not can be picked by the subagent tool. if infer is not set, it defaults to .",1.4,Low,0.538,localized low-impact
jaegertracing/jaeger#7921,[bug]: error when no services returned from the backend,"### what happened? when i load ui against a just-started jaeger backend (which has not received any traces yet and does not have any services to return), the ui shows an error ### steps to reproduce 1. in the main repo 2. in the ui repo 3. open ui at ### expected behavior the ui should show in the services dropdown (and count 0 in the label), the error should not be displayed to the user ### relevant log output ### screenshot _no response_ ### additional context _no response_ ### jaeger backend version up to date branches as of jan 26 2026 ### sdk _no response_ ### pipeline _no response_ ### stogage backend _no response_ ### operating system _no response_ ### deployment model _no response_ ### deployment configs",[],"['BUG', 'UI']","['bug', 'help wanted', 'ui', 'good first issue']",github,2026-01-26T20:06:59Z,,"[bug]: error when no services returned from the backend ### what happened? when i load ui against a just-started jaeger backend (which has not received any traces yet and does not have any services to return), the ui shows an error ### steps to reproduce 1. in the main repo 2. in the ui repo 3. open ui at ### expected behavior the ui should show in the services dropdown (and count 0 in the label), the error should not be displayed to the user ### relevant log output ### screenshot _no response_ ### additional context _no response_ ### jaeger backend version up to date branches as of jan 26 2026 ### sdk _no response_ ### pipeline _no response_ ### stogage backend _no response_ ### operating system _no response_ ### deployment model _no response_ ### deployment configs",1.998,Low,0.674,user-visible issue
cockroachdb/cockroach#161811,sql/ttl/ttljob: testrowlevelttljobmultiplenodes failed,sql/ttl/ttljob.testrowlevelttljobmultiplenodes [failed]( with [artifacts]( on master @ [d3aa5a1d825b4b9cdb8bb701c32dd4484c96fca4]( help see also: [how to investigate a go test failure \(internal\)]( /cc /sql-foundations [this test on roachdash]( | [improve this report!]( jira issue: crdb-59091,[],['TESTING'],"['C-test-failure', 'O-robot', 'branch-master', 'T-sql-foundations', 'P-2', 's390x-test-failure']",github,2026-01-26T20:18:36Z,,sql/ttl/ttljob: testrowlevelttljobmultiplenodes failed sql/ttl/ttljob.testrowlevelttljobmultiplenodes [failed]( with [artifacts]( on master @ [d3aa5a1d825b4b9cdb8bb701c32dd4484c96fca4]( help see also: [how to investigate a go test failure \(internal\)]( /cc /sql-foundations [this test on roachdash]( | [improve this report!]( jira issue: crdb-59091,1.6,Low,0.584,localized low-impact
microsoft/vscode#290533,[testplan-item] add a setting to control whether are added or not,refs - [x] anyos complexity: 2 [create issue]( --- ## context new settings have been added to vs code that allows users to control the automatic addition of instruction files to the request context controls wether instructions with matching pattern are added to agent requests or not controls wether instructions referenced in other attached instructions are added added to agent requests or not ## test steps 1. create an instruction file that has an header and references some other file. (use command: ) 2. have to and to (default). run a prompt in agent mode: verify that your instruction is added but not the reference. you can see this in the 'used 1 references' node in the reply or the [request log]( 3. have to and to . run a prompt in agent mode: verify that your instruction is not added. 4. have to and to . run a prompt in agent mode: verify that instruction and reference are added to the context.,[],['TESTING'],['testplan-item'],github,2026-01-26T20:20:10Z,2026-01-27T18:51:46Z,[testplan-item] add a setting to control whether are added or not refs - [x] anyos complexity: 2 [create issue]( --- ## context new settings have been added to vs code that allows users to control the automatic addition of instruction files to the request context controls wether instructions with matching pattern are added to agent requests or not controls wether instructions referenced in other attached instructions are added added to agent requests or not ## test steps 1. create an instruction file that has an header and references some other file. (use command: ) 2. have to and to (default). run a prompt in agent mode: verify that your instruction is added but not the reference. you can see this in the 'used 1 references' node in the reply or the [request log]( 3. have to and to . run a prompt in agent mode: verify that your instruction is not added. 4. have to and to . run a prompt in agent mode: verify that instruction and reference are added to the context.,1.6,Low,0.584,localized low-impact
kubernetes/kubernetes#136549,failure cluster [e38cb0c2...],"### failure cluster [e38cb0c2711f512c7120]( ##### error text: #### recent failures: [1/26/2026, 1:31:12 pm ci-kubernetes-node-swap-ubuntu-serial]( [1/26/2026, 12:20:03 pm ci-kubernetes-node-kubelet-serial-containerd]( [1/26/2026, 11:25:59 am ci-containerd-node-arm64-e2e-serial-ec2]( [1/26/2026, 10:19:18 am ci-cos-containerd-node-e2e-serial]( [1/26/2026, 10:05:54 am ci-containerd-node-e2e-serial-ec2]( /kind failing-test /sig node",[],['TESTING'],"['priority/critical-urgent', 'sig/node', 'kind/failing-test', 'needs-triage']",github,2026-01-26T20:34:02Z,,"failure cluster [e38cb0c2...] ### failure cluster [e38cb0c2711f512c7120]( ##### error text: #### recent failures: [1/26/2026, 1:31:12 pm ci-kubernetes-node-swap-ubuntu-serial]( [1/26/2026, 12:20:03 pm ci-kubernetes-node-kubelet-serial-containerd]( [1/26/2026, 11:25:59 am ci-containerd-node-arm64-e2e-serial-ec2]( [1/26/2026, 10:19:18 am ci-cos-containerd-node-e2e-serial]( [1/26/2026, 10:05:54 am ci-containerd-node-e2e-serial-ec2]( /kind failing-test /sig node",1.6,Low,0.584,localized low-impact
python/cpython#144257,pymodule_setdocstring documentation doesn't describe the return value or success or failure conditions,# documentation see it should say it returns -1 on failure and 0 on success. failure can happen if creating a python string from the c string docstring fails or if setting the attribute on the module object fails. ### linked prs * gh-144258 * gh-144263 * gh-144286,[],['DOCUMENTATION'],"['docs', 'topic-C-API']",github,2026-01-26T20:36:51Z,2026-01-27T10:21:01Z,pymodule_setdocstring documentation doesn't describe the return value or success or failure conditions # documentation see it should say it returns -1 on failure and 0 on success. failure can happen if creating a python string from the c string docstring fails or if setting the attribute on the module object fails. ### linked prs * gh-144258 * gh-144263 * gh-144286,1.2,Low,0.493,localized low-impact
microsoft/vscode#290536,sanity test chat setup and entitlements,- [x] anyos complexity: 4 [create issue]( --- things to test: - pro sign in / sign out / setup - free sign in / sign out / setup / quota - no auth setup and sign in,[],['TESTING'],['testplan-item'],github,2026-01-26T20:37:41Z,2026-01-27T17:07:23Z,sanity test chat setup and entitlements - [x] anyos complexity: 4 [create issue]( --- things to test: - pro sign in / sign out / setup - free sign in / sign out / setup / quota - no auth setup and sign in,1.6,Low,0.584,localized low-impact
microsoft/vscode#290537,is not orchestrated across editors,"* submit the prompt: ""generate me a reply with 20 code blocks, they can be short"" * submit the prompt: ""generate me even more code blocks pls"" * this should end up creating two agent answers with many code blocks inside of them * reload window * scroll up * observe multiple sync layouts caused by",[],['PERFORMANCE'],"['freeze-slow-crash-leak', 'insiders-released']",github,2026-01-26T20:39:28Z,2026-01-26T21:27:34Z,"is not orchestrated across editors * submit the prompt: ""generate me a reply with 20 code blocks, they can be short"" * submit the prompt: ""generate me even more code blocks pls"" * this should end up creating two agent answers with many code blocks inside of them * reload window * scroll up * observe multiple sync layouts caused by",3.467,High,1.0,performance degradation
rust-lang/rust#151708,[ice]: generics_of: unexpected node kind crate(mod,"<!-- [31mice[0m: rustc ./18074ff2c0026a2cb1dae74699bc86dcf7076beed1e87dc391bfffb43561b701.rs '-zcrate-attr=feature(min_generic_const_args) -ooutputfile -zdump-mir-dir=dir' 'error: internal compiler error: compiler/rustc_hir_analysis/src/collect/generics_of.rs:212:14: generics_of: unexpected node kind crate(mod { spans: modspans { inner_span: /tmp/im/18074ff2c0026a2cb1dae74699bc86dcf7076beed1e87dc391bfffb43561b701.rs:4:1: 38:2 ( ), inject_use_span: /tmp/im/18074ff2c0026a2cb1dae74699bc86dcf7076beed1e87dc391bfffb43561b701.rs:4:1: 4:1 ( ) }, item_ids: [itemid { owner_id: defid(0:1 ~ 18074ff2c0026a2cb1dae74699bc86dcf7076beed1e87dc391bfffb43561b701[73ad]::std) }, itemid { owner_id: defid(0:2 ~ 18074ff2c0026a2cb1dae74699bc86dcf7076beed1e87dc391bfffb43561b701[73ad]::{use }) }, itemid { owner_id: defid(0:3 ~ 18074ff2c0026a2cb1dae74699bc86dcf7076beed1e87dc391bfffb43561b701[73ad]::{use }) }, itemid { owner_id: defid(0:4 ~ 18074ff2c0026a2cb1dae74699bc86dcf7076beed1e87dc391bfffb43561b701[73ad]::n) }, itemid { owner_id: defid(0:5 ~ 18074ff2c0026a2cb1dae74699bc86dcf7076beed1e87dc391bfffb43561b701[73ad]::collectarray) }, itemid { owner_id: defid(0:9 ~ 18074ff2c0026a2cb1dae74699bc86dcf7076beed1e87dc391bfffb43561b701[73ad]::{impl }) }, itemid { owner_id: defid(0:13 ~ 18074ff2c0026a2cb1dae74699bc86dcf7076beed1e87dc391bfffb43561b701[73ad]::main) }] })', 'error: internal compiler error: compiler/rustc_hir_analysis/src/collect/generics_of.rs:212:14: generics_of: unexpected node kind crate(mod { spans: modspans { inner_span: /tmp/im/18074ff2c0026a2cb1dae74699bc86dcf7076beed1e87dc391bfffb43561b701.rs:4:1: 38:2 ( ), inject_use_span: /tmp/im/18074ff2c0026a2cb1dae74699bc86dcf7076beed1e87dc391bfffb43561b701.rs:4:1: 4:1 ( ) }, item_ids: [itemid { owner_id: defid(0:1 ~ 18074ff2c0026a2cb1dae74699bc86dcf7076beed1e87dc391bfffb43561b701[73ad]::std) }, itemid { owner_id: defid(0:2 ~ 18074ff2c0026a2cb1dae74699bc86dcf7076beed1e87dc391bfffb43561b701[73ad]::{use }) }, itemid { owner_id: defid(0:3 ~ 18074ff2c0026a2cb1dae74699bc86dcf7076beed1e87dc391bfffb43561b701[73ad]::{use }) }, itemid { owner_id: defid(0:4 ~ 18074ff2c0026a2cb1dae74699bc86dcf7076beed1e87dc391bfffb43561b701[73ad]::n) }, itemid { owner_id: defid(0:5 ~ 18074ff2c0026a2cb1dae74699bc86dcf7076beed1e87dc391bfffb43561b701[73ad]::collectarray) }, itemid { owner_id: defid(0:9 ~ 18074ff2c0026a2cb1dae74699bc86dcf7076beed1e87dc391bfffb43561b701[73ad]::{impl }) }, itemid { owner_id: defid(0:13 ~ 18074ff2c0026a2cb1dae74699bc86dcf7076beed1e87dc391bfffb43561b701[73ad]::main) }] })' file: /tmp/im/18074ff2c0026a2cb1dae74699bc86dcf7076beed1e87dc391bfffb43561b701.rs --> auto-reduced (treereduce-rust): ` original: ` version information ` possibly related line of code: command: program output <!-- query stack: error: internal compiler error: /rustc-dev/474276961f48b0d05f4ea260ba400096b027584e/compiler/rustc_hir_analysis/src/collect/generics_of.rs:212:14: generics_of: unexpected node kind crate(mod { spans: modspans { inner_span: /tmp/icemaker_global_tempdir.gu4ncxaevxd3/rustc_testrunner_tmpdir_reporting.uvkmc2tj06t0/mvce.rs:1:1: 7:13 ( ), inject_use_span: no-location ( ) }, item_ids: [itemid { owner_id: defid(0:1 ~ mvce[a1f3]::std) }, itemid { owner_id: defid(0:2 ~ mvce[a1f3]::{use }) }, itemid { owner_id: defid(0:3 ~ mvce[a1f3]::n) }, itemid { owner_id: defid(0:4 ~ mvce[a1f3]::collectarray) }, itemid { owner_id: defid(0:7 ~ mvce[a1f3]::main) }] }) [generics_of] computing generics of [dyn_compatibility_violations] determining dyn-compatibility of trait [is_dyn_compatible] checking if trait is dyn-compatible [lint_mod] linting top-level module [analysis] running analysis passes on crate --> label +f-min_generic_const_args",[],['BUG'],"['I-ICE', 'T-compiler', 'C-bug', 'needs-triage', 'F-min_generic_const_args']",github,2026-01-26T20:39:51Z,,"[ice]: generics_of: unexpected node kind crate(mod <!-- [31mice[0m: rustc ./18074ff2c0026a2cb1dae74699bc86dcf7076beed1e87dc391bfffb43561b701.rs '-zcrate-attr=feature(min_generic_const_args) -ooutputfile -zdump-mir-dir=dir' 'error: internal compiler error: compiler/rustc_hir_analysis/src/collect/generics_of.rs:212:14: generics_of: unexpected node kind crate(mod { spans: modspans { inner_span: /tmp/im/18074ff2c0026a2cb1dae74699bc86dcf7076beed1e87dc391bfffb43561b701.rs:4:1: 38:2 ( ), inject_use_span: /tmp/im/18074ff2c0026a2cb1dae74699bc86dcf7076beed1e87dc391bfffb43561b701.rs:4:1: 4:1 ( ) }, item_ids: [itemid { owner_id: defid(0:1 ~ 18074ff2c0026a2cb1dae74699bc86dcf7076beed1e87dc391bfffb43561b701[73ad]::std) }, itemid { owner_id: defid(0:2 ~ 18074ff2c0026a2cb1dae74699bc86dcf7076beed1e87dc391bfffb43561b701[73ad]::{use }) }, itemid { owner_id: defid(0:3 ~ 18074ff2c0026a2cb1dae74699bc86dcf7076beed1e87dc391bfffb43561b701[73ad]::{use }) }, itemid { owner_id: defid(0:4 ~ 18074ff2c0026a2cb1dae74699bc86dcf7076beed1e87dc391bfffb43561b701[73ad]::n) }, itemid { owner_id: defid(0:5 ~ 18074ff2c0026a2cb1dae74699bc86dcf7076beed1e87dc391bfffb43561b701[73ad]::collectarray) }, itemid { owner_id: defid(0:9 ~ 18074ff2c0026a2cb1dae74699bc86dcf7076beed1e87dc391bfffb43561b701[73ad]::{impl }) }, itemid { owner_id: defid(0:13 ~ 18074ff2c0026a2cb1dae74699bc86dcf7076beed1e87dc391bfffb43561b701[73ad]::main) }] })', 'error: internal compiler error: compiler/rustc_hir_analysis/src/collect/generics_of.rs:212:14: generics_of: unexpected node kind crate(mod { spans: modspans { inner_span: /tmp/im/18074ff2c0026a2cb1dae74699bc86dcf7076beed1e87dc391bfffb43561b701.rs:4:1: 38:2 ( ), inject_use_span: /tmp/im/18074ff2c0026a2cb1dae74699bc86dcf7076beed1e87dc391bfffb43561b701.rs:4:1: 4:1 ( ) }, item_ids: [itemid { owner_id: defid(0:1 ~ 18074ff2c0026a2cb1dae74699bc86dcf7076beed1e87dc391bfffb43561b701[73ad]::std) }, itemid { owner_id: defid(0:2 ~ 18074ff2c0026a2cb1dae74699bc86dcf7076beed1e87dc391bfffb43561b701[73ad]::{use }) }, itemid { owner_id: defid(0:3 ~ 18074ff2c0026a2cb1dae74699bc86dcf7076beed1e87dc391bfffb43561b701[73ad]::{use }) }, itemid { owner_id: defid(0:4 ~ 18074ff2c0026a2cb1dae74699bc86dcf7076beed1e87dc391bfffb43561b701[73ad]::n) }, itemid { owner_id: defid(0:5 ~ 18074ff2c0026a2cb1dae74699bc86dcf7076beed1e87dc391bfffb43561b701[73ad]::collectarray) }, itemid { owner_id: defid(0:9 ~ 18074ff2c0026a2cb1dae74699bc86dcf7076beed1e87dc391bfffb43561b701[73ad]::{impl }) }, itemid { owner_id: defid(0:13 ~ 18074ff2c0026a2cb1dae74699bc86dcf7076beed1e87dc391bfffb43561b701[73ad]::main) }] })' file: /tmp/im/18074ff2c0026a2cb1dae74699bc86dcf7076beed1e87dc391bfffb43561b701.rs --> auto-reduced (treereduce-rust): ` original: ` version information ` possibly related line of code: command: program output <!-- query stack: error: internal compiler error: /rustc-dev/474276961f48b0d05f4ea260ba400096b027584e/compiler/rustc_hir_analysis/src/collect/generics_of.rs:212:14: generics_of: unexpected node kind crate(mod { spans: modspans { inner_span: /tmp/icemaker_global_tempdir.gu4ncxaevxd3/rustc_testrunner_tmpdir_reporting.uvkmc2tj06t0/mvce.rs:1:1: 7:13 ( ), inject_use_span: no-location ( ) }, item_ids: [itemid { owner_id: defid(0:1 ~ mvce[a1f3]::std) }, itemid { owner_id: defid(0:2 ~ mvce[a1f3]::{use }) }, itemid { owner_id: defid(0:3 ~ mvce[a1f3]::n) }, itemid { owner_id: defid(0:4 ~ mvce[a1f3]::collectarray) }, itemid { owner_id: defid(0:7 ~ mvce[a1f3]::main) }] }) [generics_of] computing generics of [dyn_compatibility_violations] determining dyn-compatibility of trait [is_dyn_compatible] checking if trait is dyn-compatible [lint_mod] linting top-level module [analysis] running analysis passes on crate --> label +f-min_generic_const_args",2.664,Medium,0.825,functional impact
microsoft/vscode#290538,integrated browser: session storage / isolation,"refs: - [x] anyos - [x] anyos complexity: 3 authors: , [create issue]( --- the integrated browser enables rich browser-like functionality within vs code desktop. this tpi only covers the session storage behavior of the browser such as saved logins, cookies, cache, localstorage, etc. **there is no need to test other browser behavior or commands as part of this tpi.** but of course feel free to open other issues if you find them. [list of all integrated browser tpis]( ## setup there are three types of session isolation, configurable via the setting. please test all three values: after modifying the setting, trigger the ""browser: open integrated browser"" command to open a new browser tab with that storage type. you can verify the storage type via the menu in the toolbar: - => storage - => storage - neither of the above => storage ### to test data / storage persistence you can sign into any website, or if you prefer you can use any site that utilizes local storage for persistence such as ## scenarios to cover ### global session 1. ensure that data is persisted and shared across browser tabs in the same workspace and across multiple workspaces / windows 2. ensure that data in the global storage scope is not accessible from an ephemeral or workspace session 3. ensure that ""clear storage (global)"" resets data across all tabs (after reloading) ### workspace session 1. ensure that data is persisted within a workspace (including after closing / opening vs code), but is isolated to each workspace 2. ensure that data in a workspace scope is not accessible from an ephemeral or global session 3. ensure that ""clear storage (workspace)"" resets data for the current workspace and only the current workspace ### ephemeral session (aka incognito) 1. ensure that data is not shared across multiple ephemeral tabs 3. ensure that data in an ephemeral scope is not accessible from a global or workspace session 2. ensure that data does not persist after closing and reopening a tab ### untrusted workspace 1. ensure that in an untrusted workspace, browser tabs are always ephemeral regardless of the setting",[],['TESTING'],"['testplan-item', 'browser-integration']",github,2026-01-26T20:41:07Z,2026-01-28T00:41:59Z,"integrated browser: session storage / isolation refs: - [x] anyos - [x] anyos complexity: 3 authors: , [create issue]( --- the integrated browser enables rich browser-like functionality within vs code desktop. this tpi only covers the session storage behavior of the browser such as saved logins, cookies, cache, localstorage, etc. **there is no need to test other browser behavior or commands as part of this tpi.** but of course feel free to open other issues if you find them. [list of all integrated browser tpis]( ## setup there are three types of session isolation, configurable via the setting. please test all three values: after modifying the setting, trigger the ""browser: open integrated browser"" command to open a new browser tab with that storage type. you can verify the storage type via the menu in the toolbar: - => storage - => storage - neither of the above => storage ### to test data / storage persistence you can sign into any website, or if you prefer you can use any site that utilizes local storage for persistence such as ## scenarios to cover ### global session 1. ensure that data is persisted and shared across browser tabs in the same workspace and across multiple workspaces / windows 2. ensure that data in the global storage scope is not accessible from an ephemeral or workspace session 3. ensure that ""clear storage (global)"" resets data across all tabs (after reloading) ### workspace session 1. ensure that data is persisted within a workspace (including after closing / opening vs code), but is isolated to each workspace 2. ensure that data in a workspace scope is not accessible from an ephemeral or global session 3. ensure that ""clear storage (workspace)"" resets data for the current workspace and only the current workspace ### ephemeral session (aka incognito) 1. ensure that data is not shared across multiple ephemeral tabs 3. ensure that data in an ephemeral scope is not accessible from a global or workspace session 2. ensure that data does not persist after closing and reopening a tab ### untrusted workspace 1. ensure that in an untrusted workspace, browser tabs are always ephemeral regardless of the setting",1.6,Low,0.584,localized low-impact
microsoft/vscode#290539,background - support workspaces with multiple git repositories,,[],"['TESTING', 'FEATURE']","['feature-request', 'on-testplan', 'chat-background-agent']",github,2026-01-26T20:47:23Z,2026-01-26T20:47:29Z,background - support workspaces with multiple git repositories,1.5,Low,0.561,localized low-impact
rust-lang/rust#151709,[ice]: applied to unexpected type: freshty(0),"<!-- [31mice[0m: rustc ./dyn-compat-self-const-projections-in-assoc-const-ty.rs '-znext-solver=globally -ooutputfile -zdump-mir-dir=dir' 'error: internal compiler error: compiler/rustc_middle/src/ty/sty.rs:1948:17: applied to unexpected type: freshty(0)', 'error: internal compiler error: compiler/rustc_middle/src/ty/sty.rs:1948:17: applied to unexpected type: freshty(0)' file: /tmp/im/dyn-compat-self-const-projections-in-assoc-const-ty.rs --> this is a reduction of which errors with the old solver but ices with the new solveer auto-reduced (treereduce-rust): ` original code original: ` version information ` possibly related line of code: command: program output <!-- query stack: [typeck] type-checking [used_trait_imports] finding used_trait_imports [check_unused_traits] checking unused trait imports in crate [analysis] running analysis passes on crate --> label +f-min_generic_const_args +wg-trait-system-refactor",[],"['CLEANUP', 'BUG']","['I-ICE', 'T-compiler', 'C-bug', 'WG-trait-system-refactor', 'F-min_generic_const_args']",github,2026-01-26T20:51:01Z,,"[ice]: applied to unexpected type: freshty(0) <!-- [31mice[0m: rustc ./dyn-compat-self-const-projections-in-assoc-const-ty.rs '-znext-solver=globally -ooutputfile -zdump-mir-dir=dir' 'error: internal compiler error: compiler/rustc_middle/src/ty/sty.rs:1948:17: applied to unexpected type: freshty(0)', 'error: internal compiler error: compiler/rustc_middle/src/ty/sty.rs:1948:17: applied to unexpected type: freshty(0)' file: /tmp/im/dyn-compat-self-const-projections-in-assoc-const-ty.rs --> this is a reduction of which errors with the old solver but ices with the new solveer auto-reduced (treereduce-rust): ` original code original: ` version information ` possibly related line of code: command: program output <!-- query stack: [typeck] type-checking [used_trait_imports] finding used_trait_imports [check_unused_traits] checking unused trait imports in crate [analysis] running analysis passes on crate --> label +f-min_generic_const_args +wg-trait-system-refactor",1.8,Low,0.629,localized low-impact
microsoft/vscode#290542,test configuring byok models,"refs: - [x] anyos - [x] anyos -d complexity: 5 [create issue]( --- in this milestone, i have changed the underlying approach of how byok models can be configured. this item is to test configuring them ## how to configure different byok models ### google / anthropic / cerebrase - open the management editor and click on add models - click on google / anthropic - provide the api key - if you do not have one please ping me in slack ### azure - open the management editor and click on add models - click on azure - provide the api key and following model config - please ping me in slack for api key ## test configuring and using models - configuring models from above model providers - make sure models are loaded in the management editor - test making them visible in the chat model picker - test using these models in the chat works - test you can create multiple groups for the same model provider - select adding models from the same provider and provide different name - make sure models are loaded and can be configured and used like above - test managing the models from the group using menu button on the gear action of the group - test removing the models group using menu action on the gear action of the group - test no management actions are shown on copilot models ## test migrating from old to new approach - install this version of vs code insiders - - open vs code insiders in a new user data directory and extensions directory - configure google / anthropic models - configure cerebras models - configure azure models - try to make some models visible - update vs code insiders and open in the same above used user data directory and extensions directory - make sure your models exist and visible selection remain as before and you can use them if you can think of other scenarios you are welcome to try and test. very much appreciated.",[],['TESTING'],['testplan-item'],github,2026-01-26T20:54:22Z,2026-01-28T08:01:40Z,"test configuring byok models refs: - [x] anyos - [x] anyos -d complexity: 5 [create issue]( --- in this milestone, i have changed the underlying approach of how byok models can be configured. this item is to test configuring them ## how to configure different byok models ### google / anthropic / cerebrase - open the management editor and click on add models - click on google / anthropic - provide the api key - if you do not have one please ping me in slack ### azure - open the management editor and click on add models - click on azure - provide the api key and following model config - please ping me in slack for api key ## test configuring and using models - configuring models from above model providers - make sure models are loaded in the management editor - test making them visible in the chat model picker - test using these models in the chat works - test you can create multiple groups for the same model provider - select adding models from the same provider and provide different name - make sure models are loaded and can be configured and used like above - test managing the models from the group using menu button on the gear action of the group - test removing the models group using menu action on the gear action of the group - test no management actions are shown on copilot models ## test migrating from old to new approach - install this version of vs code insiders - - open vs code insiders in a new user data directory and extensions directory - configure google / anthropic models - configure cerebras models - configure azure models - try to make some models visible - update vs code insiders and open in the same above used user data directory and extensions directory - make sure your models exist and visible selection remain as before and you can use them if you can think of other scenarios you are welcome to try and test. very much appreciated.",3.8,Critical,1.0,crash-like behavior
microsoft/vscode#290544,test language models editor improvements,refs: - [ ] anyos - [x] anyos complexity: 3 [create issue]( --- this is to sanity test following ui features of the language models management editor. you can open the editor using the command - - searching should filter the models as expected - test the filters in the filter dropdown in the search input - test that other actions in the search input work as expected - test the action updating the visibility of the model and make sure it is shown/hidden in the chat model picker - you can use bar on the model entry to toggle visibility - you can also multi select models and update the visibility from the context menu,[],['TESTING'],['testplan-item'],github,2026-01-26T20:55:32Z,,test language models editor improvements refs: - [ ] anyos - [x] anyos complexity: 3 [create issue]( --- this is to sanity test following ui features of the language models management editor. you can open the editor using the command - - searching should filter the models as expected - test the filters in the filter dropdown in the search input - test that other actions in the search input work as expected - test the action updating the visibility of the model and make sure it is shown/hidden in the chat model picker - you can use bar on the model entry to toggle visibility - you can also multi select models and update the visibility from the context menu,1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161814,opt: handle redundant unnest inside an any subquery,"it is not uncommon for users to write filters like this one: this results in a structure like this within the any subquery: when the query is first optimized with placeholders, we may hoist the subquery into a join like this: later, when placeholders are assigned concrete values, there's currently no way to go from this normalized plan to an efficient constrained scan. there are two ways we can improve this state: 1. a rule that inlines this complex into . this prevents the whole chain of events for this type of query, though might miss out on chance to improve other queries that result in a similar structure. 2. a rule that converts a partial (semi/anti) join with a values expression on the right input into an filter. jira issue: crdb-59092",[],['FEATURE'],"['C-enhancement', 'O-support', 'A-sql-optimizer', 'T-sql-queries']",github,2026-01-26T20:56:56Z,2026-01-27T19:42:31Z,"opt: handle redundant unnest inside an any subquery it is not uncommon for users to write filters like this one: this results in a structure like this within the any subquery: when the query is first optimized with placeholders, we may hoist the subquery into a join like this: later, when placeholders are assigned concrete values, there's currently no way to go from this normalized plan to an efficient constrained scan. there are two ways we can improve this state: 1. a rule that inlines this complex into . this prevents the whole chain of events for this type of query, though might miss out on chance to improve other queries that result in a similar structure. 2. a rule that converts a partial (semi/anti) join with a values expression on the right input into an filter. jira issue: crdb-59092",1.4,Low,0.538,localized low-impact
microsoft/vscode#290547,test: background - handle workspaces with multiple repositories,"refs: - [ ] anyos - [x] anyos complexity: 2 [create issue]( --- this milestone we have improved background agent sessions in a workspace that contains multiple git repositories. ### prerequisites * install the latest vs code insiders * install the latest pre-release of copilot chat extension * ensure that the following settings are set: * , * , ### testplan * launch vs code and open a multi-root workspace that contains multiple git repositories * open the repositories view, and confrim that all repositories are listed there * open the chat panel and switch to ""background"" * confirm that the chat input has a repository picker with all repositories * select one of the repositories in the picker, author a prompt and send it * confirm that the new worktree is being created in the selected repostiory * confirm that after the chat session started the repository picker is disabled * in the repositories view, expand the ""worktrees"" node for the repository * confirm that the newly created worktree is visible in the list (has a prefix)",[],['TESTING'],['testplan-item'],github,2026-01-26T21:01:17Z,,"test: background - handle workspaces with multiple repositories refs: - [ ] anyos - [x] anyos complexity: 2 [create issue]( --- this milestone we have improved background agent sessions in a workspace that contains multiple git repositories. ### prerequisites * install the latest vs code insiders * install the latest pre-release of copilot chat extension * ensure that the following settings are set: * , * , ### testplan * launch vs code and open a multi-root workspace that contains multiple git repositories * open the repositories view, and confrim that all repositories are listed there * open the chat panel and switch to ""background"" * confirm that the chat input has a repository picker with all repositories * select one of the repositories in the picker, author a prompt and send it * confirm that the new worktree is being created in the selected repostiory * confirm that after the chat session started the repository picker is disabled * in the repositories view, expand the ""worktrees"" node for the repository * confirm that the newly created worktree is visible in the list (has a prefix)",1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161815,ui: fix skipped timescaledropdown tests after antd 5.20.3 upgrade,## summary two tests in were skipped as part of the antd 5.20.3 upgrade ( ). these tests need to be fixed and re-enabled. ## skipped tests **file:** 1. 2. ## context these tests were skipped because they rely on antd timepicker behavior that changed in the upgrade. the tests use and need to properly interact with the updated antd 5.20.3 datepicker/timepicker components. there's a prior attempt at fixing similar issues referenced in the test comments: ## acceptance criteria - [ ] both tests are un-skipped and passing - [ ] tests properly interact with antd 5.20.3 timepicker components jira issue: crdb-59093 epic crdb-58145,[],['TESTING'],"['skipped-test', 'T-observability']",github,2026-01-26T21:01:59Z,,ui: fix skipped timescaledropdown tests after antd 5.20.3 upgrade ## summary two tests in were skipped as part of the antd 5.20.3 upgrade ( ). these tests need to be fixed and re-enabled. ## skipped tests **file:** 1. 2. ## context these tests were skipped because they rely on antd timepicker behavior that changed in the upgrade. the tests use and need to properly interact with the updated antd 5.20.3 datepicker/timepicker components. there's a prior attempt at fixing similar issues referenced in the test comments: ## acceptance criteria - [ ] both tests are un-skipped and passing - [ ] tests properly interact with antd 5.20.3 timepicker components jira issue: crdb-59093 epic crdb-58145,3.8,Critical,1.0,crash-like behavior
microsoft/vscode#290551,test: image support in background agents,"refs: - [x] anyos - [x] anyos -oikawa complexity: 2 authors: [create issue]( --- **image support in background agents** **steps** * open a regular chat (either in panel or chat editor) * select agent * test image support in background agents (e.g. ask model to explain the image) by either attaching an image, screenshot (via context picker) or ask model to read an image in current directory",[],['TESTING'],['testplan-item'],github,2026-01-26T21:10:25Z,2026-01-27T23:15:48Z,"test: image support in background agents refs: - [x] anyos - [x] anyos -oikawa complexity: 2 authors: [create issue]( --- **image support in background agents** **steps** * open a regular chat (either in panel or chat editor) * select agent * test image support in background agents (e.g. ask model to explain the image) by either attaching an image, screenshot (via context picker) or ask model to read an image in current directory",1.6,Low,0.584,localized low-impact
microsoft/vscode#290552,test support registrybaseurl for mcp packages,"refs - [x] windows - [ ] macos - [x] linux complexity: 3 [create issue]( --- mcp server package can define a custom registry url from which this package has to be installed. this tpi is to test installing such packages. ## steps - create a file with following content - run command - select the above file and install it - open install mcp servers view and check that the mcp server is installed and open in - it should have following configuration - command: - arguments: - right click on the server and try start it - it opens the output view for that server. starting server fails - expected because of no access to this registry, but you can verify that it finds the version from the registry. - try following the same above steps with following pypi package - validate for following configuration - command: - arguments: - make sure server runs successfully",[],['TESTING'],['testplan-item'],github,2026-01-26T21:11:13Z,,"test support registrybaseurl for mcp packages refs - [x] windows - [ ] macos - [x] linux complexity: 3 [create issue]( --- mcp server package can define a custom registry url from which this package has to be installed. this tpi is to test installing such packages. ## steps - create a file with following content - run command - select the above file and install it - open install mcp servers view and check that the mcp server is installed and open in - it should have following configuration - command: - arguments: - right click on the server and try start it - it opens the output view for that server. starting server fails - expected because of no access to this registry, but you can verify that it finds the version from the registry. - try following the same above steps with following pypi package - validate for following configuration - command: - arguments: - make sure server runs successfully",1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161817,"sentry: schema_changer.go:353: backfill query did not populate index √ó with expected number of rows (expected: 10012158, got: 14189216) (1) assertion failure wraps: (2) | (opaque error wrapper) |...",this issue was auto filed by sentry. it represents a crash or reported error on a live cluster with telemetry enabled. sentry link: [ panic message: stacktrace (expand for inline code snippets): [src/runtime/asm_amd64.s#l1699-l1701](src/runtime/asm_amd64.s#l1699-l1701) [pkg/util/stop/stopper.go#l439-l441](pkg/util/stop/stopper.go#l439-l441) [pkg/jobs/adopt.go#l266-l268](pkg/jobs/adopt.go#l266-l268) [pkg/jobs/adopt.go#l413-l415](pkg/jobs/adopt.go#l413-l415) [pkg/jobs/registry.go#l1645-l1647](pkg/jobs/registry.go#l1645-l1647) [pkg/jobs/registry.go#l1644-l1646](pkg/jobs/registry.go#l1644-l1646) [pkg/sql/schema_changer.go#l3129-l3131](pkg/sql/schema_changer.go#l3129-l3131) [pkg/sql/schema_changer.go#l2973-l2975](pkg/sql/schema_changer.go#l2973-l2975) [pkg/sql/schema_changer.go#l1036-l1038](pkg/sql/schema_changer.go#l1036-l1038) [pkg/sql/schema_changer.go#l2287-l2289](pkg/sql/schema_changer.go#l2287-l2289) [pkg/sql/backfill.go#l326-l328](pkg/sql/backfill.go#l326-l328) [pkg/sql/schema_changer.go#l306-l308](pkg/sql/schema_changer.go#l306-l308) [pkg/sql/schema_changer.go#l568-l570](pkg/sql/schema_changer.go#l568-l570) [pkg/sql/schema_changer.go#l352-l354](pkg/sql/schema_changer.go#l352-l354) ### tags | tag | value | | --- | --- | | command | server | | environment | v25.4.3 | | go version | go1.23.12 x:nocoverageredesign | | platform | linux amd64 | | distribution | ccl | | cockroach release | v25.4.3 | | cockroach sha | 71d853623f0d1f589fd5c727a1c4aec8a43e62e0 | | # of cpus | 8 | | # of goroutines | 866 | jira issue: crdb-59094,[],['BUG'],"['C-bug', 'O-sentry', 'X-blathers-triaged', 'T-sql-foundations', 'branch-release-25.4']",github,2026-01-26T21:12:00Z,2026-01-27T15:12:37Z,"sentry: schema_changer.go:353: backfill query did not populate index √ó with expected number of rows (expected: 10012158, got: 14189216) (1) assertion failure wraps: (2) | (opaque error wrapper) |... this issue was auto filed by sentry. it represents a crash or reported error on a live cluster with telemetry enabled. sentry link: [ panic message: stacktrace (expand for inline code snippets): [src/runtime/asm_amd64.s#l1699-l1701](src/runtime/asm_amd64.s#l1699-l1701) [pkg/util/stop/stopper.go#l439-l441](pkg/util/stop/stopper.go#l439-l441) [pkg/jobs/adopt.go#l266-l268](pkg/jobs/adopt.go#l266-l268) [pkg/jobs/adopt.go#l413-l415](pkg/jobs/adopt.go#l413-l415) [pkg/jobs/registry.go#l1645-l1647](pkg/jobs/registry.go#l1645-l1647) [pkg/jobs/registry.go#l1644-l1646](pkg/jobs/registry.go#l1644-l1646) [pkg/sql/schema_changer.go#l3129-l3131](pkg/sql/schema_changer.go#l3129-l3131) [pkg/sql/schema_changer.go#l2973-l2975](pkg/sql/schema_changer.go#l2973-l2975) [pkg/sql/schema_changer.go#l1036-l1038](pkg/sql/schema_changer.go#l1036-l1038) [pkg/sql/schema_changer.go#l2287-l2289](pkg/sql/schema_changer.go#l2287-l2289) [pkg/sql/backfill.go#l326-l328](pkg/sql/backfill.go#l326-l328) [pkg/sql/schema_changer.go#l306-l308](pkg/sql/schema_changer.go#l306-l308) [pkg/sql/schema_changer.go#l568-l570](pkg/sql/schema_changer.go#l568-l570) [pkg/sql/schema_changer.go#l352-l354](pkg/sql/schema_changer.go#l352-l354) ### tags | tag | value | | --- | --- | | command | server | | environment | v25.4.3 | | go version | go1.23.12 x:nocoverageredesign | | platform | linux amd64 | | distribution | ccl | | cockroach release | v25.4.3 | | cockroach sha | 71d853623f0d1f589fd5c727a1c4aec8a43e62e0 | | # of cpus | 8 | | # of goroutines | 866 | jira issue: crdb-59094",6.0,Critical,1.0,crash-like behavior
microsoft/vscode#290553,test: mcp support in background agents,refs: - [x] anyos - [x] anyos -oikawa complexity: 3 authors: [create issue]( --- **image support in background agents** **steps** * install mcp servers * open a regular chat (either in panel or chat editor) * select agent * test support for mcp in background agents **notes** * support for mcp servers that require authentication will not work * i.e. only local mcp servers without any user authentication will work,[],['TESTING'],['testplan-item'],github,2026-01-26T21:12:07Z,2026-01-28T00:11:21Z,test: mcp support in background agents refs: - [x] anyos - [x] anyos -oikawa complexity: 3 authors: [create issue]( --- **image support in background agents** **steps** * install mcp servers * open a regular chat (either in panel or chat editor) * select agent * test support for mcp in background agents **notes** * support for mcp servers that require authentication will not work * i.e. only local mcp servers without any user authentication will work,1.6,Low,0.584,localized low-impact
microsoft/vscode#290556,background - overall background agent session improvements,"- [x] do not open workspaces created by copilot - [x] commit the changes at the end of each turn - [x] working set does not have the option to keep/undo - [x] working set has actions to apply/view changes - [x] add ""view all changes"" action in the session list",[],"['TESTING', 'FEATURE']","['feature-request', 'on-testplan', 'chat-background-agent']",github,2026-01-26T21:20:28Z,2026-01-27T16:31:31Z,"background - overall background agent session improvements - [x] do not open workspaces created by copilot - [x] commit the changes at the end of each turn - [x] working set does not have the option to keep/undo - [x] working set has actions to apply/view changes - [x] add ""view all changes"" action in the session list",2.831,Medium,0.863,crash-like behavior
istio/istio#58903,allow templates in values.yaml for podannotations in gateway chart,"**describe the feature request** i'd like the [tpl() function]( to be applied to values used for . since pod annotations can contain lots of configuration values for the sidecars ( having templating is helpful. this is where the values are currently used and converted to yaml: **describe alternatives you've considered** the alternative is to require all values to be explicit, with the requirement to repeat one's self and also without the ability to make an annotation value simply depend on another value. **affected product area (please put an x in all that apply)** [ ] ambient [ ] docs [ ] dual stack [x] installation [ ] networking [ ] performance and scalability [ ] extensions and telemetry [ ] security [ ] test and release [x] user experience [ ] developer infrastructure **affected features (please put an x in all that apply)** [ ] multi cluster [ ] virtual machine [ ] multi control plane **additional context** there is a similar issue for affinity and topologyspreadconstraints: * with a pr attached",[],['FEATURE'],"['kind/enhancement', 'area/environments', 'area/user experience']",github,2026-01-26T21:25:46Z,,"allow templates in values.yaml for podannotations in gateway chart **describe the feature request** i'd like the [tpl() function]( to be applied to values used for . since pod annotations can contain lots of configuration values for the sidecars ( having templating is helpful. this is where the values are currently used and converted to yaml: **describe alternatives you've considered** the alternative is to require all values to be explicit, with the requirement to repeat one's self and also without the ability to make an annotation value simply depend on another value. **affected product area (please put an x in all that apply)** [ ] ambient [ ] docs [ ] dual stack [x] installation [ ] networking [ ] performance and scalability [ ] extensions and telemetry [ ] security [ ] test and release [x] user experience [ ] developer infrastructure **affected features (please put an x in all that apply)** [ ] multi cluster [ ] virtual machine [ ] multi control plane **additional context** there is a similar issue for affinity and topologyspreadconstraints: * with a pr attached",3.059,High,0.915,functional impact
microsoft/vscode#290562,[testplan-item] can a custom agent support a limited list of models?,"refs - [x] anyos -w-king - [x] anyos complexity: 3 [create issue]( --- ## context custom agents in vs code's chat feature can now specify a prioritized list of supported models, rather than just a single model. when multiple models are listed, the system will attempt to use each in order until a compatible model is found, improving agent compatibility and user experience. ## test steps 1. create a custom agent definition file with the 'model' field as a single string (e.g., 'gpt-4.1). - the agent should be recognized and usable as before (backward compatibility). 2. create a custom agent definition file with the 'model' field as a string array containing multiple valid model names - verify that you get code completion, hover and validation support 3. select the custom agent in the chat interface and start a conversation. - the chat should use the first available model from the agent's model list. 4. temporarily disable or remove the first model in the agent's list from the available models. - the system should automatically fall back to the next available model in the list when starting a chat. 5. add an invalid model name (e.g., 'not-a-real-model') to the array alongside valid models. - the agent should display a warning or error for the invalid model, but still use valid models if present.",[],['TESTING'],['testplan-item'],github,2026-01-26T21:34:09Z,2026-01-27T23:25:25Z,"[testplan-item] can a custom agent support a limited list of models? refs - [x] anyos -w-king - [x] anyos complexity: 3 [create issue]( --- ## context custom agents in vs code's chat feature can now specify a prioritized list of supported models, rather than just a single model. when multiple models are listed, the system will attempt to use each in order until a compatible model is found, improving agent compatibility and user experience. ## test steps 1. create a custom agent definition file with the 'model' field as a single string (e.g., 'gpt-4.1). - the agent should be recognized and usable as before (backward compatibility). 2. create a custom agent definition file with the 'model' field as a string array containing multiple valid model names - verify that you get code completion, hover and validation support 3. select the custom agent in the chat interface and start a conversation. - the chat should use the first available model from the agent's model list. 4. temporarily disable or remove the first model in the agent's list from the available models. - the system should automatically fall back to the next available model in the list when starting a chat. 5. add an invalid model name (e.g., 'not-a-real-model') to the array alongside valid models. - the agent should display a warning or error for the invalid model, but still use valid models if present.",1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161820,"sql,pgwire: investigate 2026-01-18 throughput drop in connection_latency/nodes=9/multiregion/password test","in our sli review, we noticed that the nightly test began to show a regression in throughput starting on january 18, 2026. see the [chart in roachperf]( use git bisect to determine what caused the regression, and determine if we can/need to fix this this. also, note that the certificate-based connectionlatency tests did not show a regression. i asked claude code to do an initial analysis of the commits around this time, but it didn't find anything solid. it only flagged two very general changes which don't seem related, but possibly could affect the test in a subtle way. - roachprod cpu platform change (7b367c940f5 on jan 14) - changed default from hardcoded ""intel ice lake"" to dynamic ""best"" selection. though for n2 machines this should still select ice lake, worth checking if the test is actually running on the expected cpu. - clusterversion move to 26.2 (5d1d647ab72) - could affect behavior in subtle ways jira issue: crdb-59095 epic crdb-58150",[],['PERFORMANCE'],"['C-investigation', 'C-performance', 'T-sql-foundations', 'A-authentication']",github,2026-01-26T21:41:55Z,,"sql,pgwire: investigate 2026-01-18 throughput drop in connection_latency/nodes=9/multiregion/password test in our sli review, we noticed that the nightly test began to show a regression in throughput starting on january 18, 2026. see the [chart in roachperf]( use git bisect to determine what caused the regression, and determine if we can/need to fix this this. also, note that the certificate-based connectionlatency tests did not show a regression. i asked claude code to do an initial analysis of the commits around this time, but it didn't find anything solid. it only flagged two very general changes which don't seem related, but possibly could affect the test in a subtle way. - roachprod cpu platform change (7b367c940f5 on jan 14) - changed default from hardcoded ""intel ice lake"" to dynamic ""best"" selection. though for n2 machines this should still select ice lake, worth checking if the test is actually running on the expected cpu. - clusterversion move to 26.2 (5d1d647ab72) - could affect behavior in subtle ways jira issue: crdb-59095 epic crdb-58150",6.8,Critical,1.0,"performance degradation, crash-like behavior"
microsoft/vscode#290566,[unhandled error] cannot read properties of undefined (reading 'getviewlinemincolumn'),"issue created from vs code errors analysis dashboard ## error bucket ## error message ## stack trace raw stack trace (minified) ## details | property | value | | --- | --- | | version | 1.108.1 | | commit | [585eba7c]( | | last seen | 2026-01-25t23:59:44.644z | | total hits | 29.4m | | affected users | 1.4m | | platforms | linux, mac, windows | | product | vscode | --- *this issue was automatically created from the vs code errors dashboard*",[],['BUG'],"['bug', 'error-telemetry']",github,2026-01-26T21:48:58Z,,"[unhandled error] cannot read properties of undefined (reading 'getviewlinemincolumn') issue created from vs code errors analysis dashboard ## error bucket ## error message ## stack trace raw stack trace (minified) ## details | property | value | | --- | --- | | version | 1.108.1 | | commit | [585eba7c]( | | last seen | 2026-01-25t23:59:44.644z | | total hits | 29.4m | | affected users | 1.4m | | platforms | linux, mac, windows | | product | vscode | --- *this issue was automatically created from the vs code errors dashboard*",2.323,Medium,0.748,functional impact
microsoft/vscode#290567,test: background session improvements,"refs: - [x] anyos -d - [x] anyos complexity: 5 [create issue]( --- this milestone we have done more work to simplify and polish the background sessions. ### prerequisites * install the latest vs code insiders * install the latest pre-release of copilot chat extension * ensure that the following settings are set: * , * , ### testplan * launch vs code insiders and open a folder/workspace that contains a git repository * open the chat panel and switch the target to be ""background"" * confirm that there is no more picker to pick between workspace/worktree * write a prompt (ex: add/edit readme file) and send it * notice from the chat output that a new worktree is being created * confirm that the newly created worktree appears in the ""worktrees"" node in the repositories view * confirm that the newly created worktree is not automatically opened * wait for the chat session to complete * assuming that the chat session produced file changes * confirm that the changed files appear in the working set * confirm that the working set does not have a keep/undo action * confirm that the working set has an action to apply, and view changes * click on the ""view changes"" action * confirm that the multi-file diff editor is opened with all changes * close the multi-file diff editor and go back to the agent sessions view * confirm that the agent sessions list contains the correct diff statistics that match the diff statistics from the working tree * click on the ""view changes"" action in the sessions list * confirm that the multi-file diff editor is opened with all the changes * click on the ""apply changes to workspace"" action in the toolbar in the lower right-hand corner * confirm that the changes are applied to the current workspace * confirm that the diff statistics for the session are being cleared * confirm that if you open the session, the working set is empty ### apply (working tree conflict resolution) * launch vs code insiders and open a folder/workspace that contains a git repository * open the chat panel and switch the target to be ""background"" * send a prompt that would result in a file being editor during the session * wait for the session to complete, and let's assume that is edited in the session * edit the same file ( ) in the workspace so that it appears in the working tree * click the ""apply"" button the chat session working set * confirm that the changes are applied and in case of a conflict there is a merge resolution experience * if you get an error, click to view the git command output and share it ### apply (index conflict resolution) * launch vs code insiders and open a folder/workspace that contains a git repository * open the chat panel and switch the target to be ""background"" * send a prompt that would result in a file being editor during the session * wait for the session to complete, and let's assume that is edited in the session * edit the same file ( ) in the workspace and stage the file * click the ""apply"" button the chat session working set * confirm that the changes are applied and in case of a conflict there is a merge resolution experience * if you get an error, click to view the git command output and share it",[],['TESTING'],['testplan-item'],github,2026-01-26T21:49:06Z,2026-01-27T20:40:46Z,"test: background session improvements refs: - [x] anyos -d - [x] anyos complexity: 5 [create issue]( --- this milestone we have done more work to simplify and polish the background sessions. ### prerequisites * install the latest vs code insiders * install the latest pre-release of copilot chat extension * ensure that the following settings are set: * , * , ### testplan * launch vs code insiders and open a folder/workspace that contains a git repository * open the chat panel and switch the target to be ""background"" * confirm that there is no more picker to pick between workspace/worktree * write a prompt (ex: add/edit readme file) and send it * notice from the chat output that a new worktree is being created * confirm that the newly created worktree appears in the ""worktrees"" node in the repositories view * confirm that the newly created worktree is not automatically opened * wait for the chat session to complete * assuming that the chat session produced file changes * confirm that the changed files appear in the working set * confirm that the working set does not have a keep/undo action * confirm that the working set has an action to apply, and view changes * click on the ""view changes"" action * confirm that the multi-file diff editor is opened with all changes * close the multi-file diff editor and go back to the agent sessions view * confirm that the agent sessions list contains the correct diff statistics that match the diff statistics from the working tree * click on the ""view changes"" action in the sessions list * confirm that the multi-file diff editor is opened with all the changes * click on the ""apply changes to workspace"" action in the toolbar in the lower right-hand corner * confirm that the changes are applied to the current workspace * confirm that the diff statistics for the session are being cleared * confirm that if you open the session, the working set is empty ### apply (working tree conflict resolution) * launch vs code insiders and open a folder/workspace that contains a git repository * open the chat panel and switch the target to be ""background"" * send a prompt that would result in a file being editor during the session * wait for the session to complete, and let's assume that is edited in the session * edit the same file ( ) in the workspace so that it appears in the working tree * click the ""apply"" button the chat session working set * confirm that the changes are applied and in case of a conflict there is a merge resolution experience * if you get an error, click to view the git command output and share it ### apply (index conflict resolution) * launch vs code insiders and open a folder/workspace that contains a git repository * open the chat panel and switch the target to be ""background"" * send a prompt that would result in a file being editor during the session * wait for the session to complete, and let's assume that is edited in the session * edit the same file ( ) in the workspace and stage the file * click the ""apply"" button the chat session working set * confirm that the changes are applied and in case of a conflict there is a merge resolution experience * if you get an error, click to view the git command output and share it",3.8,Critical,1.0,crash-like behavior
microsoft/vscode#290569,[testplan-item] allow agents to define which subagents can be used,"refs - [x] anyos - [x] anyos complexity: 3 [create issue]( --- ## context this feature allows agent configuration files in vs code to explicitly define which subagents are available for use under a top-level agent. previously, subagent availability was only controlled by the 'infer' property, and some subagents like 'plan' were always present; now, agent authors can specify a list of allowed subagents. ## test steps 1. create a a few custom agents. you can use the command ( ) for that. main agent can only do work with subagents: sub agents: edit.agent.md search.agent.md 2. verify that code completion, hovers and validation work well. - the list f agents can be empty (no subagents allowed) or contain - verify that there's a warning if you have but no tool 3. set to true 4. in the chat view, select the foo agent and ask it to search for something and use that to implement use the [request log]( to verify that sub agents are known: you should see a section like this: 5. in the request log you can check whether the subagents where used or not",[],['TESTING'],['testplan-item'],github,2026-01-26T21:52:06Z,2026-01-28T03:05:30Z,"[testplan-item] allow agents to define which subagents can be used refs - [x] anyos - [x] anyos complexity: 3 [create issue]( --- ## context this feature allows agent configuration files in vs code to explicitly define which subagents are available for use under a top-level agent. previously, subagent availability was only controlled by the 'infer' property, and some subagents like 'plan' were always present; now, agent authors can specify a list of allowed subagents. ## test steps 1. create a a few custom agents. you can use the command ( ) for that. main agent can only do work with subagents: sub agents: edit.agent.md search.agent.md 2. verify that code completion, hovers and validation work well. - the list f agents can be empty (no subagents allowed) or contain - verify that there's a warning if you have but no tool 3. set to true 4. in the chat view, select the foo agent and ask it to search for something and use that to implement use the [request log]( to verify that sub agents are known: you should see a section like this: 5. in the request log you can check whether the subagents where used or not",1.6,Low,0.584,localized low-impact
microsoft/vscode#290570,allow support for different editor types of the same file in goto/locationlink,"in our vsix plugin we have multiple different editor types that are linked to a single file. for example a tabular, textual and diagrammatic representation of the same file. we would like to be able to add goto (ctrl + click) navigation from the textual editor to both the diagrammatic and tabular representations separately. currently locationlink supplies the uri, however there is no way to say ""i want to open in this specific editor type"", it just picks the editor that is registered as the default.",[],['FEATURE'],['feature-request'],github,2026-01-26T21:54:08Z,,"allow support for different editor types of the same file in goto/locationlink in our vsix plugin we have multiple different editor types that are linked to a single file. for example a tabular, textual and diagrammatic representation of the same file. we would like to be able to add goto (ctrl + click) navigation from the textual editor to both the diagrammatic and tabular representations separately. currently locationlink supplies the uri, however there is no way to say ""i want to open in this specific editor type"", it just picks the editor that is registered as the default.",1.4,Low,0.538,localized low-impact
microsoft/vscode#290573,worktree trust prompts appear while working,"i now get trust prompts for worktrees while i'm working on vs code on the latest insiders. when i opened vs code, i got a dialog per worktree. i don't use any of these worktrees anymore, so i just pressed cancel on all of them. a few minutes later, i'm not sure which new process started, but three more worktree dialogs showed up.",[],['BUG'],"['bug', 'git', '*duplicate']",github,2026-01-26T22:02:47Z,2026-01-27T09:15:48Z,"worktree trust prompts appear while working i now get trust prompts for worktrees while i'm working on vs code on the latest insiders. when i opened vs code, i got a dialog per worktree. i don't use any of these worktrees anymore, so i just pressed cancel on all of them. a few minutes later, i'm not sure which new process started, but three more worktree dialogs showed up.",2.319,Medium,0.747,functional impact
microsoft/vscode#290576,test: run in terminal presenters,"refs: , , , - [x] macos - [x] linux - [x] windows complexity: 3 [create issue]( --- test that python, node and ruby commands run inline are presented with syntax highlighting and without the wrapping command. this should affect both the confirmation and the progress/finished widget. for example: there's some edge cases handled around quote escaping but they may not all be covered. note that you do not actually need python, node or ruby installed to get it to test this, that will only very whether the model gave a good and working suggestion. in addition to the language presentation there is also a new presentation for the working directory which is designed to make the command easier to review: this should play nicely with the language presenters.",[],['TESTING'],['testplan-item'],github,2026-01-26T22:22:57Z,2026-01-28T01:19:42Z,"test: run in terminal presenters refs: , , , - [x] macos - [x] linux - [x] windows complexity: 3 [create issue]( --- test that python, node and ruby commands run inline are presented with syntax highlighting and without the wrapping command. this should affect both the confirmation and the progress/finished widget. for example: there's some edge cases handled around quote escaping but they may not all be covered. note that you do not actually need python, node or ruby installed to get it to test this, that will only very whether the model gave a good and working suggestion. in addition to the language presentation there is also a new presentation for the working directory which is designed to make the command easier to review: this should play nicely with the language presenters.",1.6,Low,0.584,localized low-impact
microsoft/vscode#290580,2026 theme working set and to do hover states,hover states have added drop shadow (with double shadow around clear button) default light theme:,[],['UI'],"['ux', 'polish', '2026-themes']",github,2026-01-26T22:32:15Z,,2026 theme working set and to do hover states hover states have added drop shadow (with double shadow around clear button) default light theme:,1.8,Low,0.629,user-visible issue
openssl/openssl#29761,how to find size with evp_decryptupdate and no aad,"<!-- thank you for taking the time to report a documentation issue. please remember to tell us which openssl version you are using and then briefly describe the documentation error and where you encountered it (e.g., in which manual page). if you are missing the documentation for a certain command or api function, please tell us its name. --> with openssl 3.5.4 i see: does the number of authenticated bytes include the number of bytes of the clear text? if so how can i extract when no aad is present?",[],['DOCUMENTATION'],['issue: documentation'],github,2026-01-26T22:35:01Z,,"how to find size with evp_decryptupdate and no aad <!-- thank you for taking the time to report a documentation issue. please remember to tell us which openssl version you are using and then briefly describe the documentation error and where you encountered it (e.g., in which manual page). if you are missing the documentation for a certain command or api function, please tell us its name. --> with openssl 3.5.4 i see: does the number of authenticated bytes include the number of bytes of the clear text? if so how can i extract when no aad is present?",1.2,Low,0.493,localized low-impact
microsoft/vscode#290583,test: run in terminal lifecycle improvements,"refs: - [x] macos - [x] linux - [x] windows complexity: 4 [create issue]( --- the following new features were added to the terminal to help with their lifecycle and how the agent interacts with background/async terminals: - the user can manually push tool calls to the background via this button - terminal tool calls have a new property that will determine when to push the terminal to the background. this can be controlled by the user via - the agent has a new tool that will await for the command to finish, this also have a argument for how long to wait - a new tool that allows the agent to clean up old background terminals that are no longer needed - instructions around how cwd works are much more clear: - background terminals are always new terminals and start in the workspace dir - non-background terminals start as new terminals, starting in the workspace dir but retaining cwd across tool calls - when a non-background terminal becomes a background terminal (eg. by user pressing the continue in background button) the next non-background terminal will be in the workspace dir. some tips for testing: - test these new features using scenarios you come up with to trigger them - if you have scenarios that used to fail you should try that now, like launching servers or doing several commands in quick succession - adding or can nudge the agent in the direction you want - try with lesser models too, these have mostly only been tested with opus so far",[],['TESTING'],['testplan-item'],github,2026-01-26T22:35:53Z,2026-01-27T21:39:19Z,"test: run in terminal lifecycle improvements refs: - [x] macos - [x] linux - [x] windows complexity: 4 [create issue]( --- the following new features were added to the terminal to help with their lifecycle and how the agent interacts with background/async terminals: - the user can manually push tool calls to the background via this button - terminal tool calls have a new property that will determine when to push the terminal to the background. this can be controlled by the user via - the agent has a new tool that will await for the command to finish, this also have a argument for how long to wait - a new tool that allows the agent to clean up old background terminals that are no longer needed - instructions around how cwd works are much more clear: - background terminals are always new terminals and start in the workspace dir - non-background terminals start as new terminals, starting in the workspace dir but retaining cwd across tool calls - when a non-background terminal becomes a background terminal (eg. by user pressing the continue in background button) the next non-background terminal will be in the workspace dir. some tips for testing: - test these new features using scenarios you come up with to trigger them - if you have scenarios that used to fail you should try that now, like launching servers or doing several commands in quick succession - adding or can nudge the agent in the direction you want - try with lesser models too, these have mostly only been tested with opus so far",1.6,Low,0.584,localized low-impact
microsoft/vscode#290584,integrated browser: browsing behavior,"refs: - [ ] macos - [ ] linux - [ ] windows complexity: __________ authors: , --- the integrated browser enables rich browser-like functionality within vs code desktop. this tpi only covers __________. **there is no need to test the general browsing behavior or new commands as part of this tpi.** but of course feel free to open other issues if you find them. ### setup: ### scenarios to cover:",[],['TESTING'],"['testplan-item', 'browser-integration']",github,2026-01-26T22:36:41Z,2026-01-26T22:39:18Z,"integrated browser: browsing behavior refs: - [ ] macos - [ ] linux - [ ] windows complexity: __________ authors: , --- the integrated browser enables rich browser-like functionality within vs code desktop. this tpi only covers __________. **there is no need to test the general browsing behavior or new commands as part of this tpi.** but of course feel free to open other issues if you find them. ### setup: ### scenarios to cover:",1.6,Low,0.584,localized low-impact
microsoft/vscode#290585,integrated browser: accessibility / keyboard navigation,"refs: - [x] macos - [x] windows complexity: 3 authors: , [create issue]( --- the integrated browser enables rich browser-like functionality within vs code desktop. this tpi only covers accessibility and keyboard navigation. **there is no need to test the general browsing behavior or command functionality as part of this tpi.** but of course feel free to open other issues if you find them. [list of all integrated browser tpis]( ### setup: 1. use the ""browser: open integrated browser"" command to open a new browser tab ### scenarios to cover: scenarios should be performed via the keyboard alone as well as using a screen reader. validation is largely subjective. 1. navigate to a url (press enter in url bar to navigate) - focus should move to the browser page 2. back/forward, reload (alt+left/right, f5 when focused in the browser) 3. use the keyboard to move/interact within the browser page 4. within the browser page, when on a text-editing website, text editing keyboard shortcuts work - select all (ctrl/cmd+a), copy (...+c), cut, paste (...+v), undo (...+z), redo (mac: cmd+shift+z. windows: both ctrl+y and ctrl+shift+z should work) 5. page load failures 6. vs code commands (e.g. ctrl/cmd+shift+p, ctrl/cmd+shift+f) - focus should move back to the workbench 7. notification toast pops up (overlapping with the browser view) - this will disable the browser view. it should be reasonably simple to dismiss the notification and return to the browser.",[],['TESTING'],"['testplan-item', 'browser-integration']",github,2026-01-26T22:36:43Z,2026-01-27T17:15:35Z,"integrated browser: accessibility / keyboard navigation refs: - [x] macos - [x] windows complexity: 3 authors: , [create issue]( --- the integrated browser enables rich browser-like functionality within vs code desktop. this tpi only covers accessibility and keyboard navigation. **there is no need to test the general browsing behavior or command functionality as part of this tpi.** but of course feel free to open other issues if you find them. [list of all integrated browser tpis]( ### setup: 1. use the ""browser: open integrated browser"" command to open a new browser tab ### scenarios to cover: scenarios should be performed via the keyboard alone as well as using a screen reader. validation is largely subjective. 1. navigate to a url (press enter in url bar to navigate) - focus should move to the browser page 2. back/forward, reload (alt+left/right, f5 when focused in the browser) 3. use the keyboard to move/interact within the browser page 4. within the browser page, when on a text-editing website, text editing keyboard shortcuts work - select all (ctrl/cmd+a), copy (...+c), cut, paste (...+v), undo (...+z), redo (mac: cmd+shift+z. windows: both ctrl+y and ctrl+shift+z should work) 5. page load failures 6. vs code commands (e.g. ctrl/cmd+shift+p, ctrl/cmd+shift+f) - focus should move back to the workbench 7. notification toast pops up (overlapping with the browser view) - this will disable the browser view. it should be reasonably simple to dismiss the notification and return to the browser.",1.6,Low,0.584,localized low-impact
microsoft/vscode#290586,integrated browser: add element to chat feature,"refs: - [x] anyos - [x] anyos complexity: 2 authors: , [create issue]( --- the integrated browser enables rich browser-like functionality within vs code desktop. this tpi only covers the ""add element to chat"" behavior. **there is no need to test the general browsing behavior or other commands as part of this tpi.** but of course feel free to open other issues if you find them. [list of all integrated browser tpis]( ## setup 1. trigger the ""browser: open integrated browser"" command 2. navigate to any webpage -- either local or public. ## scenarios to cover ### basic usage trigger the command either via the menu item directly, or by pressing while focused anywhere within the browser tab: hover over an element in the page and click to select it: the chat panel should be opened if not already and two items should be added, with element information and a screenshot of the element: the element information should include the element html and css information. the agent should be able to use these attachments to answer questions about the element. ### cancellation when selection is active, the user should be able to cancel the selection by: - pressing a second time - activating the menu button a second time - pressing ### settings three settings can customize the behavior: - : when disabled, the command should not be available or visible - : when disabled, no css should be included in the attachment (but html still is) - : when disabled, no screenshot should be attached additionally, the feature should disappear (no button in ui, no keyboard shortcuts activating it) when the user sets the setting ""chat: disable ai features"" to true ( )",[],['TESTING'],"['testplan-item', 'browser-integration']",github,2026-01-26T22:36:45Z,2026-01-27T20:37:34Z,"integrated browser: add element to chat feature refs: - [x] anyos - [x] anyos complexity: 2 authors: , [create issue]( --- the integrated browser enables rich browser-like functionality within vs code desktop. this tpi only covers the ""add element to chat"" behavior. **there is no need to test the general browsing behavior or other commands as part of this tpi.** but of course feel free to open other issues if you find them. [list of all integrated browser tpis]( ## setup 1. trigger the ""browser: open integrated browser"" command 2. navigate to any webpage -- either local or public. ## scenarios to cover ### basic usage trigger the command either via the menu item directly, or by pressing while focused anywhere within the browser tab: hover over an element in the page and click to select it: the chat panel should be opened if not already and two items should be added, with element information and a screenshot of the element: the element information should include the element html and css information. the agent should be able to use these attachments to answer questions about the element. ### cancellation when selection is active, the user should be able to cancel the selection by: - pressing a second time - activating the menu button a second time - pressing ### settings three settings can customize the behavior: - : when disabled, the command should not be available or visible - : when disabled, no css should be included in the attachment (but html still is) - : when disabled, no screenshot should be attached additionally, the feature should disappear (no button in ui, no keyboard shortcuts activating it) when the user sets the setting ""chat: disable ai features"" to true ( )",1.6,Low,0.584,localized low-impact
microsoft/vscode#290587,integrated browser: find in page feature,"refs: feature pr - [ ] macos - [x] windows complexity: 1 authors: , [create issue]( --- the integrated browser enables rich browser-like functionality within vs code desktop. this tpi only covers the find in page feature. **there is no need to test the general browsing behavior or new commands as part of this tpi.** but of course feel free to open other issues if you find them. [list of all integrated browser tpis]( ### setup: * run the command ""browser: open integrated browser"" * load a url ### scenarios to cover: * entrypoints: * ""find in page"" in overflow menu to the right of url bar * ctrl/cmd + f * try features: * next/prev result (try buttons, try enter / shift+enter) * toggle ""match case"" * play around with the find in page feature and make sure that it works as you would expect of such a feature in a regular browser * exit: * click ""x"" or * press esc",[],['TESTING'],"['testplan-item', 'browser-integration']",github,2026-01-26T22:36:48Z,,"integrated browser: find in page feature refs: feature pr - [ ] macos - [x] windows complexity: 1 authors: , [create issue]( --- the integrated browser enables rich browser-like functionality within vs code desktop. this tpi only covers the find in page feature. **there is no need to test the general browsing behavior or new commands as part of this tpi.** but of course feel free to open other issues if you find them. [list of all integrated browser tpis]( ### setup: * run the command ""browser: open integrated browser"" * load a url ### scenarios to cover: * entrypoints: * ""find in page"" in overflow menu to the right of url bar * ctrl/cmd + f * try features: * next/prev result (try buttons, try enter / shift+enter) * toggle ""match case"" * play around with the find in page feature and make sure that it works as you would expect of such a feature in a regular browser * exit: * click ""x"" or * press esc",3.8,Critical,1.0,crash-like behavior
microsoft/vscode#290588,integrated browser: settings,"refs: - [ ] macos - [ ] linux - [ ] windows complexity: __________ authors: , --- the integrated browser enables rich browser-like functionality within vs code desktop. this tpi only covers __________. **there is no need to test the general browsing behavior or new commands as part of this tpi.** but of course feel free to open other issues if you find them. ### setup: ### scenarios to cover:",[],['TESTING'],"['testplan-item', 'browser-integration']",github,2026-01-26T22:36:51Z,2026-01-27T00:11:09Z,"integrated browser: settings refs: - [ ] macos - [ ] linux - [ ] windows complexity: __________ authors: , --- the integrated browser enables rich browser-like functionality within vs code desktop. this tpi only covers __________. **there is no need to test the general browsing behavior or new commands as part of this tpi.** but of course feel free to open other issues if you find them. ### setup: ### scenarios to cover:",1.6,Low,0.584,localized low-impact
microsoft/vscode#290589,integrated browser: entrypoint settings and commands,"refs: - [x] anyos - [x] anyos complexity: 3 authors: , [create issue]( --- the integrated browser enables rich browser-like functionality within vs code desktop. this tpi only covers scenarios related to integrated browser entrypoint settings and commands. **there is no need to test the general browsing behavior or new commands as part of this tpi.** but of course feel free to open other issues if you find them. [list of all integrated browser tpis]( ### setup: you need two vs codes as follows: on one side, insiders on vs code desktop: 1. open vs code insiders desktop 2. install [microsoft's live preview extension]( on another side, insiders on vs code web (with a compute backing it): 1. open a web instance of vs code insiders with backing compute. you can do this in two ways: - either run and open the url output from the console in a standard web browser - or open a github codespaces instance for a repo and connect to the instance from .github.dev and click on the settings icon and set ""switch to insiders version..."" 3. install [microsoft's live preview extension]( ### scenarios to cover: this tpi involves testing a command and some settings related to integrated browser. you should ensure those work on vs code insiders desktop, and should also ensure that they do not work on vs code insiders web (because integrated browser is not supported on web) * command ""browser: open integrated browser"" should open a built-in web browser on vs code desktop, but this command shouldn't even show as an option on vs code web * setting should work on vs code desktop and should not work on vs code web * set the setting in both vs code desktop and vs code web * open any html file in vs code * click on the preview button on the top right to open the live preview * on vs code desktop, we expect to have a ""..."" menu to the right of the url bar, indicating that the new integrated browser was opened. * * on vs code web, we to have a ""‚ò∞"" menu to the right of the url bar, meaning that it was live preview's own browser that opened. * * setting should work on vs code desktop and should not work on vs code web * set the setting in both vs code desktop and vs code web * run the command ""simple browser: show"" * on vs code desktop, you should see a built-in browser immediately, indicating that you're using the integrated browser * on vs code web, instead of immediately seeing a browser window, you should see a quick pick dialog that says ""enter url to visit"", which indicates that you're still using the old, simple browser. * * setting should work on vs code desktop and should not work on vs code web * verify that the setting doesn't even exist on vs code web * on vs code desktop only: * set the setting * in different parts of vs code, get some clickable links to localhost urls and to non-localhost urls. for example, echo a link in the built-in terminal or ask copilot to give you clickable links * clicking on **localhost links** should open them inside of vs code itself (inside of the integrated browser). clicking on **non-localhost links** should open them in your regular web browser.",[],['TESTING'],"['testplan-item', 'browser-integration']",github,2026-01-26T22:36:55Z,2026-01-28T01:26:37Z,"integrated browser: entrypoint settings and commands refs: - [x] anyos - [x] anyos complexity: 3 authors: , [create issue]( --- the integrated browser enables rich browser-like functionality within vs code desktop. this tpi only covers scenarios related to integrated browser entrypoint settings and commands. **there is no need to test the general browsing behavior or new commands as part of this tpi.** but of course feel free to open other issues if you find them. [list of all integrated browser tpis]( ### setup: you need two vs codes as follows: on one side, insiders on vs code desktop: 1. open vs code insiders desktop 2. install [microsoft's live preview extension]( on another side, insiders on vs code web (with a compute backing it): 1. open a web instance of vs code insiders with backing compute. you can do this in two ways: - either run and open the url output from the console in a standard web browser - or open a github codespaces instance for a repo and connect to the instance from .github.dev and click on the settings icon and set ""switch to insiders version..."" 3. install [microsoft's live preview extension]( ### scenarios to cover: this tpi involves testing a command and some settings related to integrated browser. you should ensure those work on vs code insiders desktop, and should also ensure that they do not work on vs code insiders web (because integrated browser is not supported on web) * command ""browser: open integrated browser"" should open a built-in web browser on vs code desktop, but this command shouldn't even show as an option on vs code web * setting should work on vs code desktop and should not work on vs code web * set the setting in both vs code desktop and vs code web * open any html file in vs code * click on the preview button on the top right to open the live preview * on vs code desktop, we expect to have a ""..."" menu to the right of the url bar, indicating that the new integrated browser was opened. * * on vs code web, we to have a ""‚ò∞"" menu to the right of the url bar, meaning that it was live preview's own browser that opened. * * setting should work on vs code desktop and should not work on vs code web * set the setting in both vs code desktop and vs code web * run the command ""simple browser: show"" * on vs code desktop, you should see a built-in browser immediately, indicating that you're using the integrated browser * on vs code web, instead of immediately seeing a browser window, you should see a quick pick dialog that says ""enter url to visit"", which indicates that you're still using the old, simple browser. * * setting should work on vs code desktop and should not work on vs code web * verify that the setting doesn't even exist on vs code web * on vs code desktop only: * set the setting * in different parts of vs code, get some clickable links to localhost urls and to non-localhost urls. for example, echo a link in the built-in terminal or ask copilot to give you clickable links * clicking on **localhost links** should open them inside of vs code itself (inside of the integrated browser). clicking on **non-localhost links** should open them in your regular web browser.",1.6,Low,0.584,localized low-impact
microsoft/vscode#290591,integrated browser: core browsing behavior,"refs: - [x] macos - [x] linux - [x] windows complexity: 4 roles: developer, engineering manager authors: , [create issue]( --- the integrated browser enables rich browser-like functionality within vs code desktop. this tpi only covers the core ""browsing"" functionality -- opening pages, links, sign-ins, permissions, etc. **there is no need to test other behaviors like workbench integration, data storage, commands, etc.** but of course feel free to open other issues if you find them. [list of all integrated browser tpis]( ## setup open a browser tab using the ""browser: open integrated browser"" command ## scenarios to cover 1. **empty state**: tab initially shows ""browser"" placeholder page prior to navigating 2. **error state**: load failures show an error page with the url and error code - page not found, failed to resolve hostname, 3. **urls**: can open schemas 4. **mime types**: can render various mime types like png, json, pdf, etc. - urls may be handy here 5. **links**: - normal navigation - anchor/in-page navigation - open in new tab (ctrl/cmd+click) 6. **auth flows**: major sign in flows work (e.g. sign in with google) - note that ms work account sign-in will not work due to device restrictions. this is expected 7. **popups**: new windows should be prevented - you can use a tool like to check this - new tabs are acceptable. new windows are not 8. **permissions**: most (e.g. camera access) should be automatically denied - you can use a tool like to check this - notifications, clipboard, and file selection should be allowed 9. **isolation**: page activity does not impact vs code -- page hangs, poor perf, slow network, etc. are isolated",[],['TESTING'],"['testplan-item', 'browser-integration']",github,2026-01-26T22:37:06Z,,"integrated browser: core browsing behavior refs: - [x] macos - [x] linux - [x] windows complexity: 4 roles: developer, engineering manager authors: , [create issue]( --- the integrated browser enables rich browser-like functionality within vs code desktop. this tpi only covers the core ""browsing"" functionality -- opening pages, links, sign-ins, permissions, etc. **there is no need to test other behaviors like workbench integration, data storage, commands, etc.** but of course feel free to open other issues if you find them. [list of all integrated browser tpis]( ## setup open a browser tab using the ""browser: open integrated browser"" command ## scenarios to cover 1. **empty state**: tab initially shows ""browser"" placeholder page prior to navigating 2. **error state**: load failures show an error page with the url and error code - page not found, failed to resolve hostname, 3. **urls**: can open schemas 4. **mime types**: can render various mime types like png, json, pdf, etc. - urls may be handy here 5. **links**: - normal navigation - anchor/in-page navigation - open in new tab (ctrl/cmd+click) 6. **auth flows**: major sign in flows work (e.g. sign in with google) - note that ms work account sign-in will not work due to device restrictions. this is expected 7. **popups**: new windows should be prevented - you can use a tool like to check this - new tabs are acceptable. new windows are not 8. **permissions**: most (e.g. camera access) should be automatically denied - you can use a tool like to check this - notifications, clipboard, and file selection should be allowed 9. **isolation**: page activity does not impact vs code -- page hangs, poor perf, slow network, etc. are isolated",5.2,Critical,1.0,crash-like behavior
microsoft/vscode#290601,[unhandled error] cannot read properties of undefined (reading 'getviewlinemincolumn'),"issue created from vs code errors analysis dashboard ## error bucket ## error message ## stack trace raw stack trace (minified) ## details | property | value | | --- | --- | | version | 1.108.2 | | commit | [c9d77990]( | | last seen | 2026-01-25t23:59:38.260z | | total hits | 20.4m | | affected users | 1.1m | | platforms | linux, mac, windows | | product | vscode | --- *this issue was automatically created from the vs code errors dashboard*",[],['BUG'],"['bug', 'error-telemetry']",github,2026-01-26T22:58:28Z,,"[unhandled error] cannot read properties of undefined (reading 'getviewlinemincolumn') issue created from vs code errors analysis dashboard ## error bucket ## error message ## stack trace raw stack trace (minified) ## details | property | value | | --- | --- | | version | 1.108.2 | | commit | [c9d77990]( | | last seen | 2026-01-25t23:59:38.260z | | total hits | 20.4m | | affected users | 1.1m | | platforms | linux, mac, windows | | product | vscode | --- *this issue was automatically created from the vs code errors dashboard*",2.481,Medium,0.784,functional impact
cockroachdb/cockroach#161822,pkg/sql/schemachanger/schemachanger_test: testpause_add_column_with_stored failed,pkg/sql/schemachanger/schemachanger_test.testpause_add_column_with_stored [failed]( with [artifacts]( on master @ [21df3c88d7427e992d59843e84e5d3f9ac456238]( help see also: [how to investigate a go test failure \(internal\)]( /cc /sql-foundations [this test on roachdash]( | [improve this report!]( jira issue: crdb-59096,[],"['TESTING', 'BUG']","['C-bug', 'C-test-failure', 'O-robot', 'branch-master', 'release-blocker', 'T-kv', 'A-kv-rangefeed', 'P-2']",github,2026-01-26T23:01:45Z,,pkg/sql/schemachanger/schemachanger_test: testpause_add_column_with_stored failed pkg/sql/schemachanger/schemachanger_test.testpause_add_column_with_stored [failed]( with [artifacts]( on master @ [21df3c88d7427e992d59843e84e5d3f9ac456238]( help see also: [how to investigate a go test failure \(internal\)]( /cc /sql-foundations [this test on roachdash]( | [improve this report!]( jira issue: crdb-59096,3.315,High,0.973,crash-like behavior
microsoft/vscode#290604,test: kitty keyboard protocol,"refs: - [x] macos - [ ] linux complexity: 4 [create issue]( --- ‚ö†Ô∏è do not test until this is merged and released: - we're rolling out kitty keyboard protocol to insiders right now, so set explicitly and create a new terminal to test this. this new feature enables richer encoding of keystrokes in the terminal and enables things like disambiguating keystrokes, press/release, modifier only keystrokes, repeat keystrokes. the docs for this feature are at check out the ""programs implementing this protocol"" and try some. cli (i think claude code too?) also uses this so you can test to make sure all inputs work properly there. if you use a non-us keyboard layout testing there would be extra valuable.",[],['TESTING'],['testplan-item'],github,2026-01-26T23:06:32Z,,"test: kitty keyboard protocol refs: - [x] macos - [ ] linux complexity: 4 [create issue]( --- ‚ö†Ô∏è do not test until this is merged and released: - we're rolling out kitty keyboard protocol to insiders right now, so set explicitly and create a new terminal to test this. this new feature enables richer encoding of keystrokes in the terminal and enables things like disambiguating keystrokes, press/release, modifier only keystrokes, repeat keystrokes. the docs for this feature are at check out the ""programs implementing this protocol"" and try some. cli (i think claude code too?) also uses this so you can test to make sure all inputs work properly there. if you use a non-us keyboard layout testing there would be extra valuable.",1.6,Low,0.584,localized low-impact
kubernetes/kubernetes#136553,failure cluster [119d47e5...] reading issue with ipv6 environments in test suite,"### failure cluster [119d47e5366d86e7133e]( ##### error text: #### recent failures: [1/26/2026, 7:01:46 pm ci-kubernetes-kind-network-parallel-ipv6]( [1/26/2026, 6:56:01 pm ci-kubernetes-e2e-kind-ipv6]( [1/26/2026, 5:56:01 pm ci-kubernetes-e2e-kind-ipv6]( [1/26/2026, 5:38:05 pm ci-kubernetes-e2e-kind-ipv6-canary]( [1/26/2026, 4:56:01 pm ci-kubernetes-e2e-kind-ipv6]( /kind failing-test /sig network",[],"['NETWORK', 'TESTING']","['sig/network', 'kind/failing-test', 'needs-triage']",github,2026-01-26T23:11:29Z,,"failure cluster [119d47e5...] reading issue with ipv6 environments in test suite ### failure cluster [119d47e5366d86e7133e]( ##### error text: #### recent failures: [1/26/2026, 7:01:46 pm ci-kubernetes-kind-network-parallel-ipv6]( [1/26/2026, 6:56:01 pm ci-kubernetes-e2e-kind-ipv6]( [1/26/2026, 5:56:01 pm ci-kubernetes-e2e-kind-ipv6]( [1/26/2026, 5:38:05 pm ci-kubernetes-e2e-kind-ipv6-canary]( [1/26/2026, 4:56:01 pm ci-kubernetes-e2e-kind-ipv6]( /kind failing-test /sig network",2.259,Medium,0.733,affects communication layer
microsoft/vscode#290611,add models not accessible in insiders,same account works fine on stable,[],['BUG'],"['bug', 'insiders-released', 'chat-lm-management']",github,2026-01-26T23:30:00Z,2026-01-27T14:33:15Z,add models not accessible in insiders same account works fine on stable,2.269,Medium,0.736,functional impact
microsoft/vscode#290615,integrated browser: pause when there are overlays,"refs: - [x] macos - [x] windows complexity: 2 authors: , [create issue]( --- the integrated browser enables rich browser-like functionality within vs code desktop. due to technical constraints, components in the workbench cannot be rendered on top of the browser contents. to mitigate this issue, we dynamically ""pause"" the browser view and show a greyed out screenshot of the page as a placeholder until the overlapping element is hidden. note that this tpi only covers this auto-pausing behavior. **there is no need to test the general browsing behavior or command functionality as part of this tpi.** but of course feel free to open other issues if you find them. [list of all integrated browser tpis]( ### setup: 1. trigger the ""browser: open integrated browser"" command 2. navigate to any valid url -- local or public. this must be a valid page and not error out. ### to validate ensure that contents in the workbench are never hidden behind the browser view. for example: | correct ‚úÖ | incorrect ‚ùå | | - | - | | | | when a workbench element overlaps with the browser, it may grey out as in the above example. it should resume when there are no more overlapping elements. ### scenarios to cover: > [!note] > note for macos users: several of the scenarios below do not apply on mac os as we use native views for e.g. context menus, so they can overlap without needing to pause. please cover the following scenarios: 1. context menus 2. tooltips 3. quick picks / command palette 4. notifications - since notifications can appear without any user interaction, we display a message in the middle of the greyed out browser view to encourage the user to handle / dismiss them. 5. any other popups you might be able to think of",[],['TESTING'],"['testplan-item', 'browser-integration']",github,2026-01-26T23:39:09Z,2026-01-28T01:36:41Z,"integrated browser: pause when there are overlays refs: - [x] macos - [x] windows complexity: 2 authors: , [create issue]( --- the integrated browser enables rich browser-like functionality within vs code desktop. due to technical constraints, components in the workbench cannot be rendered on top of the browser contents. to mitigate this issue, we dynamically ""pause"" the browser view and show a greyed out screenshot of the page as a placeholder until the overlapping element is hidden. note that this tpi only covers this auto-pausing behavior. **there is no need to test the general browsing behavior or command functionality as part of this tpi.** but of course feel free to open other issues if you find them. [list of all integrated browser tpis]( ### setup: 1. trigger the ""browser: open integrated browser"" command 2. navigate to any valid url -- local or public. this must be a valid page and not error out. ### to validate ensure that contents in the workbench are never hidden behind the browser view. for example: | correct ‚úÖ | incorrect ‚ùå | | - | - | | | | when a workbench element overlaps with the browser, it may grey out as in the above example. it should resume when there are no more overlapping elements. ### scenarios to cover: > [!note] > note for macos users: several of the scenarios below do not apply on mac os as we use native views for e.g. context menus, so they can overlap without needing to pause. please cover the following scenarios: 1. context menus 2. tooltips 3. quick picks / command palette 4. notifications - since notifications can appear without any user interaction, we display a message in the middle of the greyed out browser view to encourage the user to handle / dismiss them. 5. any other popups you might be able to think of",1.6,Low,0.584,localized low-impact
microsoft/vscode#290620,sandboxing terminal commands executed from copilot chat,"refs: - [x] mac - [x] linux complexity: 4 [create issue]( --- ## summary this change enables sandboxing for terminal commands executed through the tool from the copilot chat window. sandboxing allows users to manage files and domains that can be accessed during command execution. when is enabled (macos/linux only): - by default, terminal commands have read and write access to the working directory. - network access is blocked by default for all domains. sandbox configuration is controlled by: - - (linux) - (macos) ## prerequisites - ensure you can use copilot chat and the tool. ## testing ### 1) baseline behavior (sandbox disabled) 1. set . 2. ask chat to run a simple command (e.g. ). 3. verify the tool runs the command successfully. 4. open the executed command in the terminal and confirm the command is not wrapped with sandbox parameters (does not include ). ### 2) sandboxed execution rewrites the executed command 1. set (macos/linux). 2. ask chat to run or any command that uses the tool. 3. check if the command completed execution: - the displayed/confirmation command remains the original and should not show . - open the executed command in the terminal and confirm the command is wrapped with sandbox parameters (includes ). ### 3) auto-approval behavior in sandboxed mode 1. enable sandboxing. 3. ask chat to run a command that would normally require confirmation. 4. verify the command is auto-approved (no confirmation ui), and the tool proceeds to execution. ### 4) file system restrictions (macos/linux) 1. enable sandboxing. 2. configure file system rules for your os: - linux: - macos: 3. verify: - writes are blocked outside of . - blocks writes even under allowed locations (takes precedence over ). - blocks reads for configured paths. #### deny-write precedence 1. configure your os file system setting so the working directory is writable, but a specific sub-path is denied: - set to include - set to include a subfolder such as 2. in the workspace, create two folders: - 3. run a write into the allowed folder: - - verify this succeeds. 4. run a write into the denied folder: - - verify this is blocked by sandboxing. ### 5) network restrictions (macos/linux) 1. enable sandboxing. 2. set . 3. run a command that needs network access (e.g. `curl 4. verify network access is blocked. 5. add (or a suitable wildcard) to and re-run. 6. verify network access is allowed. ### 6) windows behavior (n/a) 1. on windows, set . 2. verify sandboxing does not activate (commands execute normally, no wrapping), and there are no errors.",[],['TESTING'],['testplan-item'],github,2026-01-26T23:52:46Z,2026-01-27T22:25:39Z,"sandboxing terminal commands executed from copilot chat refs: - [x] mac - [x] linux complexity: 4 [create issue]( --- ## summary this change enables sandboxing for terminal commands executed through the tool from the copilot chat window. sandboxing allows users to manage files and domains that can be accessed during command execution. when is enabled (macos/linux only): - by default, terminal commands have read and write access to the working directory. - network access is blocked by default for all domains. sandbox configuration is controlled by: - - (linux) - (macos) ## prerequisites - ensure you can use copilot chat and the tool. ## testing ### 1) baseline behavior (sandbox disabled) 1. set . 2. ask chat to run a simple command (e.g. ). 3. verify the tool runs the command successfully. 4. open the executed command in the terminal and confirm the command is not wrapped with sandbox parameters (does not include ). ### 2) sandboxed execution rewrites the executed command 1. set (macos/linux). 2. ask chat to run or any command that uses the tool. 3. check if the command completed execution: - the displayed/confirmation command remains the original and should not show . - open the executed command in the terminal and confirm the command is wrapped with sandbox parameters (includes ). ### 3) auto-approval behavior in sandboxed mode 1. enable sandboxing. 3. ask chat to run a command that would normally require confirmation. 4. verify the command is auto-approved (no confirmation ui), and the tool proceeds to execution. ### 4) file system restrictions (macos/linux) 1. enable sandboxing. 2. configure file system rules for your os: - linux: - macos: 3. verify: - writes are blocked outside of . - blocks writes even under allowed locations (takes precedence over ). - blocks reads for configured paths. #### deny-write precedence 1. configure your os file system setting so the working directory is writable, but a specific sub-path is denied: - set to include - set to include a subfolder such as 2. in the workspace, create two folders: - 3. run a write into the allowed folder: - - verify this succeeds. 4. run a write into the denied folder: - - verify this is blocked by sandboxing. ### 5) network restrictions (macos/linux) 1. enable sandboxing. 2. set . 3. run a command that needs network access (e.g. `curl 4. verify network access is blocked. 5. add (or a suitable wildcard) to and re-run. 6. verify network access is allowed. ### 6) windows behavior (n/a) 1. on windows, set . 2. verify sandboxing does not activate (commands execute normally, no wrapping), and there are no errors.",3.8,Critical,1.0,crash-like behavior
microsoft/vscode#290623,test: mcp apps,"refs: - [x] anyos - [x] anyos - [x] anyos complexity: 5 [create issue]( --- we added support for [mcp apps]( in chat this iteration. this was _just_ released so in-the-wild support for apps is still quite nascent. please: 1. take a few of their [examples]( to test out. comment what you're going to test below so we don't duplicate effort too much. only one person needs to test any of the 's. 2. clone the repo, , open an example server's folder, start it, and install your chosen server in your workspace. (mcp: add server > http, generally) 3. try it out! you could also modify the sample to try adding more features to excercise the features of apps. there are a variety of features in the apps spec, some notes on them: - pretty much all of the app features are hooked up! - apps can support multiple display modes, but we only support mode in chat currently - 'sending a message' from the app will just fill in the input box if there is no other text there, not auto-send it. - context from an app will appear as attachments - tool calling and logging should work. - we give apps clipboard write access, but don't support camera/mic/geolocation",[],['TESTING'],"['verified', 'testplan-item']",github,2026-01-27T00:05:17Z,2026-01-27T21:50:51Z,"test: mcp apps refs: - [x] anyos - [x] anyos - [x] anyos complexity: 5 [create issue]( --- we added support for [mcp apps]( in chat this iteration. this was _just_ released so in-the-wild support for apps is still quite nascent. please: 1. take a few of their [examples]( to test out. comment what you're going to test below so we don't duplicate effort too much. only one person needs to test any of the 's. 2. clone the repo, , open an example server's folder, start it, and install your chosen server in your workspace. (mcp: add server > http, generally) 3. try it out! you could also modify the sample to try adding more features to excercise the features of apps. there are a variety of features in the apps spec, some notes on them: - pretty much all of the app features are hooked up! - apps can support multiple display modes, but we only support mode in chat currently - 'sending a message' from the app will just fill in the input box if there is no other text there, not auto-send it. - context from an app will appear as attachments - tool calling and logging should work. - we give apps clipboard write access, but don't support camera/mic/geolocation",1.6,Low,0.584,localized low-impact
microsoft/vscode#290625,test: model-specific tools (memory),"refs: - [x] anyos - [x] anyos complexity: 2 authors: , [create issue]( --- we've done work to support tools only available for certain models and have adopted it for the anthropic tool. please: 1. on modern anthropic models, you should be able to enable and disable the ""memory"" tool (under the ""vscode"" toolset) 2. enable/disabling should work as you would expect to enable and disable the tool in requests (verify with the chat debug log, or ask the model if it has a memory tool available) 3. when switching to a different model, you should no longer see the memory 4. you should be able to see and reference model-specific tools in prompt and instruction files",[],['TESTING'],['testplan-item'],github,2026-01-27T00:07:34Z,2026-01-28T09:28:47Z,"test: model-specific tools (memory) refs: - [x] anyos - [x] anyos complexity: 2 authors: , [create issue]( --- we've done work to support tools only available for certain models and have adopted it for the anthropic tool. please: 1. on modern anthropic models, you should be able to enable and disable the ""memory"" tool (under the ""vscode"" toolset) 2. enable/disabling should work as you would expect to enable and disable the tool in requests (verify with the chat debug log, or ask the model if it has a memory tool available) 3. when switching to a different model, you should no longer see the memory 4. you should be able to see and reference model-specific tools in prompt and instruction files",1.6,Low,0.584,localized low-impact
electron/electron#49539,running electron tests should not create folder in home directory,"### preflight checklist - [x] i have read the [contributing guidelines]( for this project. - [x] i agree to follow the [code of conduct]( that this project adheres to. - [x] i have searched the [issue tracker]( for a bug report that matches the one i want to file, without success. ### electron version 42, 41 ### what operating system(s) are you using? macos ### operating system version tahoe 26.2 ### what arch are you using? arm64 (including apple silicon) ### last known working electron version _no response_ ### does the issue also appear in chromium / google chrome? no ### expected behavior running does not create folders in my home directory. ### actual behavior i found a folder called in my home directory from the last time i ran . it contained files related to single instance lock. this is where that folder name is referenced in our source code: ### testcase gist url _no response_ ### additional information fix ideas: - create the folder in - delete the folder at the end of that test",[],['BUG'],"['platform/macOS', 'bug :beetle:', 'status/confirmed', 'has-repro-comment', '41-x-y']",github,2026-01-27T00:10:46Z,,"running electron tests should not create folder in home directory ### preflight checklist - [x] i have read the [contributing guidelines]( for this project. - [x] i agree to follow the [code of conduct]( that this project adheres to. - [x] i have searched the [issue tracker]( for a bug report that matches the one i want to file, without success. ### electron version 42, 41 ### what operating system(s) are you using? macos ### operating system version tahoe 26.2 ### what arch are you using? arm64 (including apple silicon) ### last known working electron version _no response_ ### does the issue also appear in chromium / google chrome? no ### expected behavior running does not create folders in my home directory. ### actual behavior i found a folder called in my home directory from the last time i ran . it contained files related to single instance lock. this is where that folder name is referenced in our source code: ### testcase gist url _no response_ ### additional information fix ideas: - create the folder in - delete the folder at the end of that test",4.2,Critical,1.0,system-wide impact
microsoft/vscode#290628,allow pinning chat sessions in copilot chat,"i would love the ability to pin specific chat sessions in copilot chat to keep important conversations easily accessible. this would help users quickly return to valuable chat sessions without having to search through their history or risk losing track of important conversations. my personal use case is pinning specific chats that i want to demo at a later date i envision there being a pin button next to the archive button, and a pin option in the context menu for a chat",[],"['FEATURE', 'UI']","['feature-request', 'ux', 'chat-agents-view', 'copilot-chat']",github,2026-01-27T00:11:59Z,,"allow pinning chat sessions in copilot chat i would love the ability to pin specific chat sessions in copilot chat to keep important conversations easily accessible. this would help users quickly return to valuable chat sessions without having to search through their history or risk losing track of important conversations. my personal use case is pinning specific chats that i want to demo at a later date i envision there being a pin button next to the archive button, and a pin option in the context menu for a chat",1.6,Low,0.584,user-visible issue
microsoft/vscode#290631,test: claude agent in preview,"refs: complexity: 5 - [x] anyos - [x] anyos [create issue]( ------- claude agent sessions are now enabled by default as a preview feature. this allows users to start and resume agentic coding sessions powered by anthropic's claude agent sdk directly in vs code using their existing copilot subscription. in chat, you can now hit the session dropdown and choose claude. **use it instead of local and let me know how it goes and what you liked or didn't like.** fyi, there are some slash commands (similar slash commands are in claude code) to play with: * - custom agents, go through the wizard and then you'll be able to just ask to use them in chat and it'll work - * - these are fun * - helps you author claude.md files * and a few more, all of which have info also, you can get yolo mode enabled via: and then there's a new option in the drop down in chat. claude code was made for not really looking at the code, try to build agents and hooks that help you not open the code, ideally. ### random stuff there's a setting to disable it:",[],['TESTING'],['testplan-item'],github,2026-01-27T00:15:49Z,2026-01-27T19:40:02Z,"test: claude agent in preview refs: complexity: 5 - [x] anyos - [x] anyos [create issue]( ------- claude agent sessions are now enabled by default as a preview feature. this allows users to start and resume agentic coding sessions powered by anthropic's claude agent sdk directly in vs code using their existing copilot subscription. in chat, you can now hit the session dropdown and choose claude. **use it instead of local and let me know how it goes and what you liked or didn't like.** fyi, there are some slash commands (similar slash commands are in claude code) to play with: * - custom agents, go through the wizard and then you'll be able to just ask to use them in chat and it'll work - * - these are fun * - helps you author claude.md files * and a few more, all of which have info also, you can get yolo mode enabled via: and then there's a new option in the drop down in chat. claude code was made for not really looking at the code, try to build agents and hooks that help you not open the code, ideally. ### random stuff there's a setting to disable it:",1.6,Low,0.584,localized low-impact
microsoft/vscode#290632,test: plan agent's iterative workflow with ask questions,"refs: - [x] anyos - [ ] anyos complexity: 3 authors: [create issue]( --- ## context the plan agent now uses a 4-phase iterative workflow (discovery ‚Üí alignment ‚Üí design ‚Üí refinement) that integrates the ask questions tool after initial research. this prevents the agent from drafting complete plans before clarifying ambiguities with the user. ## prerequisites - enable the ask questions setting: - open a codebase with some complexity (multiple files, dependencies) ## test steps ### 1) verify ask questions tool is available 1. open chat and switch to plan mode (or invoke ). 2. ask for a plan on a non-trivial task, e.g., ""plan how to add a dark mode toggle to this app."" 3. confirm the agent runs a subagent for discovery/research first. 4. confirm after discovery, the agent uses the ask questions tool to clarify requirements before drafting a full plan. - you should see a question prompt appear, not immediately a complete plan. ### 2) verify iterative refinement 1. answer the clarifying question(s) from step 1. 2. confirm the agent proceeds to design phase and presents a draft plan. 3. request a change (e.g., ""can we also support system preference detection?""). 4. confirm the agent loops back appropriately: - may run additional discovery if needed. - may ask follow-up questions. - updates the plan based on feedback. ### 3) verify plan format includes new sections 1. review the final plan output. 2. confirm it includes: - a **verification** section describing how to test. - a **decisions** section (if applicable) documenting choices made. 3. confirm there are no trailing questions in the plan body (questions should have been asked via the tool, not embedded in the plan). ### 4) verify behavior with ask questions disabled 1. set . 2. repeat step 1 with a new planning task. 3. confirm the agent does not use the ask questions tool. 4. confirm the workflow still produces a coherent plan without the alignment phase. **bonus) get creative on scenarios**: - is it fast and seamless for a _simple renaming or few-line change_ - is the plan going deep enough to capture the complexity of a _massive large refactoring_",[],['TESTING'],['testplan-item'],github,2026-01-27T00:20:06Z,,"test: plan agent's iterative workflow with ask questions refs: - [x] anyos - [ ] anyos complexity: 3 authors: [create issue]( --- ## context the plan agent now uses a 4-phase iterative workflow (discovery ‚Üí alignment ‚Üí design ‚Üí refinement) that integrates the ask questions tool after initial research. this prevents the agent from drafting complete plans before clarifying ambiguities with the user. ## prerequisites - enable the ask questions setting: - open a codebase with some complexity (multiple files, dependencies) ## test steps ### 1) verify ask questions tool is available 1. open chat and switch to plan mode (or invoke ). 2. ask for a plan on a non-trivial task, e.g., ""plan how to add a dark mode toggle to this app."" 3. confirm the agent runs a subagent for discovery/research first. 4. confirm after discovery, the agent uses the ask questions tool to clarify requirements before drafting a full plan. - you should see a question prompt appear, not immediately a complete plan. ### 2) verify iterative refinement 1. answer the clarifying question(s) from step 1. 2. confirm the agent proceeds to design phase and presents a draft plan. 3. request a change (e.g., ""can we also support system preference detection?""). 4. confirm the agent loops back appropriately: - may run additional discovery if needed. - may ask follow-up questions. - updates the plan based on feedback. ### 3) verify plan format includes new sections 1. review the final plan output. 2. confirm it includes: - a **verification** section describing how to test. - a **decisions** section (if applicable) documenting choices made. 3. confirm there are no trailing questions in the plan body (questions should have been asked via the tool, not embedded in the plan). ### 4) verify behavior with ask questions disabled 1. set . 2. repeat step 1 with a new planning task. 3. confirm the agent does not use the ask questions tool. 4. confirm the workflow still produces a coherent plan without the alignment phase. **bonus) get creative on scenarios**: - is it fast and seamless for a _simple renaming or few-line change_ - is the plan going deep enough to capture the complexity of a _massive large refactoring_",3.8,Critical,1.0,crash-like behavior
microsoft/vscode#290633,test: chat tool,"refs: - [x] anyos - [x] anyos complexity: 2 authors: [create issue]( --- ## overview the tool allows chat participants to present a series of questions to users inline within chat responses. questions appear in a carousel format with navigation controls, supporting text input, single-select (radio), and multi-select (checkbox) question types. ## setup 1. install latest insiders github copilot chat extension 2. open the chat panel 3. ensure tool is enabled. 4. have the model ask you a question (e.g, ""quiz me on this repo. 4 questions max"". ) ## testing - [ ] trigger a chat interaction that uses the tool to display a question carousel - [ ] explore navigating between questions using the arrow buttons - [ ] try answering different question types: text inputs, radio buttons, and checkboxes - [ ] submit answers and verify the chat participant receives and processes them correctly - [ ] if available, try the ""skip all"" button to dismiss the carousel without answering - [ ] verify answers are preserved when navigating back and forth between questions - [ ] test keyboard navigation (tab, enter) through the carousel",[],['TESTING'],['testplan-item'],github,2026-01-27T00:20:16Z,2026-01-27T23:21:24Z,"test: chat tool refs: - [x] anyos - [x] anyos complexity: 2 authors: [create issue]( --- ## overview the tool allows chat participants to present a series of questions to users inline within chat responses. questions appear in a carousel format with navigation controls, supporting text input, single-select (radio), and multi-select (checkbox) question types. ## setup 1. install latest insiders github copilot chat extension 2. open the chat panel 3. ensure tool is enabled. 4. have the model ask you a question (e.g, ""quiz me on this repo. 4 questions max"". ) ## testing - [ ] trigger a chat interaction that uses the tool to display a question carousel - [ ] explore navigating between questions using the arrow buttons - [ ] try answering different question types: text inputs, radio buttons, and checkboxes - [ ] submit answers and verify the chat participant receives and processes them correctly - [ ] if available, try the ""skip all"" button to dismiss the carousel without answering - [ ] verify answers are preserved when navigating back and forth between questions - [ ] test keyboard navigation (tab, enter) through the carousel",1.6,Low,0.584,localized low-impact
microsoft/vscode#290636,test: welcome view with background agents,"refs: - [x] anyos - [x] anyos - [x] anyos complexity: 5 authors: [create issue]( --- **background agents in welcome view** **pre-requisites** * configure user setting as follows * basic understanding of background sessions (the fact that worktrees are created) **steps** * open an empty workspace folder * open the welcome view (verify the chat input is visble in welcome view) * [ ] verify you can select as below # recently used repos/folders * [ ] verify the picker on far right hand side display the last used repos/folders (see below) * [ ] select an item and send a chat and verify it works as expected # pick a new repos * [ ] click the item * [ ] select a repo such as or or any other git repo * [ ] send a prompt such as , and verify the model completes and display the path to the worktree. what we're looking for is the fact that a worktree should be created for this background session and the prompt will help us verify that a work tree was created. # pick a new folder * [ ] click the item * [ ] select a folder that doesn't have git initialized (e.g. create a new folder with some files in it and open that) * [ ] send a prompt such as , and verify the model completes and display the path to the folder. what we're looking for is the fact that a worktree should not be created for this background session and the prompt will help us verify that a work tree was created. # testing trusted folders * [ ] untrust a previously trusted folder such as source folder * [ ] click the item * [ ] open vs code * [ ] send a prompt and verify you are prompted to trust this * [ ] verify the prompt works if you trust this * [ ] verify the prompt ends if you do not trust this folder # testing copy/move changes * select repo which has some pending changes (e.g. open vscode repo and make some changes to that & then select in the folder picker or last used repos) * send a prompt and verify you are prompted to copy/migrate changes to the new background session",[],['TESTING'],['testplan-item'],github,2026-01-27T00:26:51Z,2026-01-28T00:37:47Z,"test: welcome view with background agents refs: - [x] anyos - [x] anyos - [x] anyos complexity: 5 authors: [create issue]( --- **background agents in welcome view** **pre-requisites** * configure user setting as follows * basic understanding of background sessions (the fact that worktrees are created) **steps** * open an empty workspace folder * open the welcome view (verify the chat input is visble in welcome view) * [ ] verify you can select as below # recently used repos/folders * [ ] verify the picker on far right hand side display the last used repos/folders (see below) * [ ] select an item and send a chat and verify it works as expected # pick a new repos * [ ] click the item * [ ] select a repo such as or or any other git repo * [ ] send a prompt such as , and verify the model completes and display the path to the worktree. what we're looking for is the fact that a worktree should be created for this background session and the prompt will help us verify that a work tree was created. # pick a new folder * [ ] click the item * [ ] select a folder that doesn't have git initialized (e.g. create a new folder with some files in it and open that) * [ ] send a prompt such as , and verify the model completes and display the path to the folder. what we're looking for is the fact that a worktree should not be created for this background session and the prompt will help us verify that a work tree was created. # testing trusted folders * [ ] untrust a previously trusted folder such as source folder * [ ] click the item * [ ] open vs code * [ ] send a prompt and verify you are prompted to trust this * [ ] verify the prompt works if you trust this * [ ] verify the prompt ends if you do not trust this folder # testing copy/move changes * select repo which has some pending changes (e.g. open vscode repo and make some changes to that & then select in the folder picker or last used repos) * send a prompt and verify you are prompted to copy/migrate changes to the new background session",5.6,Critical,1.0,crash-like behavior
microsoft/vscode#290638,test: edits and terminals inside reasoning dropdown,"todo - [ ] anyos - [x] anyos - [x] anyos complexity: 4 [create issue]( --- in the pursuit of making chat more simple and consistent, we're allowing nearly all tool calls, including edits and terminal tool calls, to be added inside the reasoning dropdown. ## settings : collapsed, collapsedpreview, fixedscrolling (default) : none, always (default), withthinking : true (default), false ## setup - for anthropic models, make sure you are using messages api for anthropic: ## test steps 1. set to , the to ., and to true. these are the current defaults in insiders. 2. ask chat a few questions, forcing some kind of edit. 3. make sure to try the built in a good variety of models, including opus, codex, gemini, etc. 4. verify that edits are inside the reasoning section. 5. try making some terminal tool calls. 6. make sure you are able to click thru and access the terminal, or see the output in failures. 7. also try again with various (try and ) ## things to note/known bugs - when terminal output is scrolling, sometimes it gets cut off at the bottom as the thinking scrollbar doesn't scroll all the way down. - sometimes terminal content will say , due to some mismatch in how rendering worked.",[],['TESTING'],['testplan-item'],github,2026-01-27T00:32:24Z,,"test: edits and terminals inside reasoning dropdown todo - [ ] anyos - [x] anyos - [x] anyos complexity: 4 [create issue]( --- in the pursuit of making chat more simple and consistent, we're allowing nearly all tool calls, including edits and terminal tool calls, to be added inside the reasoning dropdown. ## settings : collapsed, collapsedpreview, fixedscrolling (default) : none, always (default), withthinking : true (default), false ## setup - for anthropic models, make sure you are using messages api for anthropic: ## test steps 1. set to , the to ., and to true. these are the current defaults in insiders. 2. ask chat a few questions, forcing some kind of edit. 3. make sure to try the built in a good variety of models, including opus, codex, gemini, etc. 4. verify that edits are inside the reasoning section. 5. try making some terminal tool calls. 6. make sure you are able to click thru and access the terminal, or see the output in failures. 7. also try again with various (try and ) ## things to note/known bugs - when terminal output is scrolling, sometimes it gets cut off at the bottom as the thinking scrollbar doesn't scroll all the way down. - sometimes terminal content will say , due to some mismatch in how rendering worked.",1.6,Low,0.584,localized low-impact
microsoft/vscode#290642,running chat not marked as 'in-progress' if currently viewed,does this issue occur when all extensions are disabled?: yes/no report issue' dialog can assist with this. --> - vs code version: - os version: 1. start a chat 2. üêõ note that you need to move focus to another chat for the 'agent status' indicator to show +1,"[""Running chat not marked as 'in-progress' if currently viewed (fix #290642) (#291199)""]",['BUG'],"['bug', 'unreleased', 'chat-agents-view']",github,2026-01-27T00:38:09Z,2026-01-28T08:54:36Z,running chat not marked as 'in-progress' if currently viewed does this issue occur when all extensions are disabled?: yes/no report issue' dialog can assist with this. --> - vs code version: - os version: 1. start a chat 2. üêõ note that you need to move focus to another chat for the 'agent status' indicator to show +1 Running chat not marked as 'in-progress' if currently viewed (fix #290642) (#291199),2.593,Medium,0.809,functional impact
microsoft/vscode#290644,browser: ctrl+t to open new browsing tab from an existing one,,[],['FEATURE'],"['feature-request', 'verification-needed', 'insiders-released', 'browser-integration']",github,2026-01-27T00:43:11Z,2026-01-27T20:30:31Z,browser: ctrl+t to open new browsing tab from an existing one,1.4,Low,0.538,localized low-impact
microsoft/vscode#290645,browser: ctrl+l to focus url input,,[],['FEATURE'],"['feature-request', 'verification-needed', 'insiders-released', 'browser-integration']",github,2026-01-27T00:43:47Z,2026-01-27T20:30:30Z,browser: ctrl+l to focus url input,1.4,Low,0.538,localized low-impact
grpc/grpc#41492,onwritedone(ok=false) blocked after client disconnect in serverwritereactor,"<!-- please do not post a question here. this form is for bug reports and feature requests only! for general questions and troubleshooting, please ask/look for answers at stackoverflow, with ""grpc"" tag: for questions that specifically need to be answered by grpc team members, please ask/look for answers at grpc.io mailing list: issues specific to *grpc-java*, *grpc-go*, *grpc-node*, *grpc-dart*, *grpc-web* should be created in the repository they belong to (e.g. --> ### what version of grpc and what language are you using? version: commit 0de88768ee3b0a6c75d940668c4fc7ee89dddd3b (version 1.50.2) language: c++ ### what operating system (linux, windows,...) and version? ubuntu 24.04.2 lts (noble numbat) ### what runtime / compiler are you using (e.g. python version or version of gcc) gcc 13.3.0 ### what did you do? please provide either 1) a unit test for reproducing the bug or 2) specific steps for us to follow to reproduce the bug. if there‚Äôs not enough information to debug the problem, grpc team may close the issue at their discretion. you‚Äôre welcome to re-open the issue once you have a reproduction. setup: - server-side streaming rpc using serverwritereactor - server side has a built up message queue, our onwritedone callback pulls 1 message from the queue and calls startwrite in a loop - client connects via envoy proxy (grpc-web) with default stream_idle_timeout=300s - server streams large amounts of data (~13,000+ responses per stream) - client can cancel stream mid-flight by aborting the http/2 connection ### what did you expect to see? - when the frontend client aborts the http2 stream, there is always 1 pending write for which startwrite has been called but onwritedone has not, and i tested this with 100s of cancels and this is always the case - every onwritedone for the 1000s of messages before the stream cancels, each one takes 10s of ms from its accompanying startwrite with a ok=true return val - i expect this final ondonewrite to return with ok=false in this same 10ms period after its startwrite once the client is aborted as theres no where for grpc to write this message to ### what did you see instead? - the currently in-flight startwrite() does not fail immediately - i observed onwritedone(ok=false) is not called for ~300 seconds - after exactly 300 seconds (envoy's stream_idle_timeout), onwritedone(ok=false) finally returns - this happens in every one of the 100s of frontend client aborts i tested - even when the client doesn't explicitly abort but starts a new streaming rpc on the same http/2 connection, the old reactor's pending startwrite() still blocks for 300s before onwritedone(ok=false). this makes it seem like grpc is prioritizing the new streams over the other still alive ones, as they share the same underlying tcp channel? - this above case is weird as the startwrite() blocks for the old reactor as soon as the new stream readreactor is made server side which begins furiously sending its writes to the client, then again envoy times out the connection resulting in onwritedone(ok=false) after 300s. - **the issue**: grpc c++ doesn't detect client disconnect. the pending startwrite() waits indefinitely until the proxy times out the idle stream. - **question**: - is there a way to make onwritedone(ok=false) return immediately when the client disconnects? - or an api to cancel a pending startwrite()? make sure you include information that can help us debug (full error message, exception listing, stack trace, logs). ### log evidence here's the complete timeline for 5 cancelled streams showing the consistent behavior: **request 1:** | step | timestamp | log | |-----------------------|--------------|-----| | 1. startwrite() | 20:26:49.687 | | | 2. fe abort | ~20:27:21.9 | *(frontend calls abortcontroller.abort() ‚Üí triggers cancel rpc)* | | 3. cancel rpc | 20:27:21.969 | | | 4. onwritedone(false) | 20:31:49.688 | | ‚è±Ô∏è startwrite ‚Üí **300s** ‚Üí onwritedone(false). cancel arrived 32s after startwrite. --- **request 2:** | step | timestamp | log | |-----------------------|--------------|-----| | 1. startwrite() | 20:27:25.706 | | | 2. fe abort | ~20:29:28.4 | *(frontend calls abortcontroller.abort())* | | 3. cancel rpc | 20:29:28.410 | | | 4. onwritedone(false) | 20:32:25.707 | | ‚è±Ô∏è startwrite ‚Üí **300s** ‚Üí onwritedone(false). cancel arrived 123s after startwrite. --- **request 3:** | step | timestamp | log | |-----------------------|--------------|-----| | 1. startwrite() | 20:29:30.935 | | | 2. fe abort | ~20:32:05.7 | *(frontend calls abortcontroller.abort())* | | 3. cancel rpc | 20:32:05.787 | | | 4. onwritedone(false) | 20:34:30.935 | | ‚è±Ô∏è startwrite ‚Üí **300s** ‚Üí onwritedone(false). cancel arrived 155s after startwrite. --- **request 4:** | step | timestamp | log | |-----------------------|--------------|-----| | 1. startwrite() | 20:32:08.361 | | | 2. fe abort | ~20:35:11.6 | *(frontend calls abortcontroller.abort())* | | 3. cancel rpc | 20:35:11.621 | | | 4. onwritedone(false) | 20:37:08.361 | | ‚è±Ô∏è startwrite ‚Üí **300s** ‚Üí onwritedone(false). cancel arrived 183s after startwrite. --- **request 5:** | step | timestamp | log | |-----------------------|--------------|-----| | 1. startwrite() | 20:35:14.035 | | | 2. fe abort | ~20:37:31.1 | *(frontend calls abortcontroller.abort())* | | 3. cancel rpc | 20:37:31.121 | | | 4. onwritedone(false) | 20:40:14.036 | | ‚è±Ô∏è startwrite ‚Üí **300s** ‚Üí onwritedone(false). cancel arrived 137s after startwrite. --- **summary:** | req | startwrite | cancel | onwritedone | startwrite‚Üíonwritedone | |-----|------------|------------------|-------------|------------------------| | 1 | 20:26:49 | 20:27:21 (+32s) | 20:31:49 | **300s** | | 2 | 20:27:25 | 20:29:28 (+123s) | 20:32:25 | **300s** | | 3 | 20:29:30 | 20:32:05 (+155s) | 20:34:30 | **300s** | | 4 | 20:32:08 | 20:35:11 (+183s) | 20:37:08 | **300s** | | 5 | 20:35:14 | 20:37:31 (+137s) | 20:40:14 | **300s** | **key finding:** always returns exactly 300s after was called, regardless of when the client aborted. grpc does not detect the client disconnect - it waits for the proxy timeout to kill the connection. see [troubleshooting.md]( for how to diagnose problems better. i turned on verbose grpc logging but didnt see anything helpful. ### anything else we should know about your project / environment? - proxy: envoy (grpc-web) - oncancel() is never called - known issue with envoy/grpc-web not propagating cancellation, so we use an out of band cancel() rpc",[],['BUG'],"['kind/bug', 'lang/c++', 'priority/P2', 'untriaged']",github,2026-01-27T00:49:15Z,,"onwritedone(ok=false) blocked after client disconnect in serverwritereactor <!-- please do not post a question here. this form is for bug reports and feature requests only! for general questions and troubleshooting, please ask/look for answers at stackoverflow, with ""grpc"" tag: for questions that specifically need to be answered by grpc team members, please ask/look for answers at grpc.io mailing list: issues specific to *grpc-java*, *grpc-go*, *grpc-node*, *grpc-dart*, *grpc-web* should be created in the repository they belong to (e.g. --> ### what version of grpc and what language are you using? version: commit 0de88768ee3b0a6c75d940668c4fc7ee89dddd3b (version 1.50.2) language: c++ ### what operating system (linux, windows,...) and version? ubuntu 24.04.2 lts (noble numbat) ### what runtime / compiler are you using (e.g. python version or version of gcc) gcc 13.3.0 ### what did you do? please provide either 1) a unit test for reproducing the bug or 2) specific steps for us to follow to reproduce the bug. if there‚Äôs not enough information to debug the problem, grpc team may close the issue at their discretion. you‚Äôre welcome to re-open the issue once you have a reproduction. setup: - server-side streaming rpc using serverwritereactor - server side has a built up message queue, our onwritedone callback pulls 1 message from the queue and calls startwrite in a loop - client connects via envoy proxy (grpc-web) with default stream_idle_timeout=300s - server streams large amounts of data (~13,000+ responses per stream) - client can cancel stream mid-flight by aborting the http/2 connection ### what did you expect to see? - when the frontend client aborts the http2 stream, there is always 1 pending write for which startwrite has been called but onwritedone has not, and i tested this with 100s of cancels and this is always the case - every onwritedone for the 1000s of messages before the stream cancels, each one takes 10s of ms from its accompanying startwrite with a ok=true return val - i expect this final ondonewrite to return with ok=false in this same 10ms period after its startwrite once the client is aborted as theres no where for grpc to write this message to ### what did you see instead? - the currently in-flight startwrite() does not fail immediately - i observed onwritedone(ok=false) is not called for ~300 seconds - after exactly 300 seconds (envoy's stream_idle_timeout), onwritedone(ok=false) finally returns - this happens in every one of the 100s of frontend client aborts i tested - even when the client doesn't explicitly abort but starts a new streaming rpc on the same http/2 connection, the old reactor's pending startwrite() still blocks for 300s before onwritedone(ok=false). this makes it seem like grpc is prioritizing the new streams over the other still alive ones, as they share the same underlying tcp channel? - this above case is weird as the startwrite() blocks for the old reactor as soon as the new stream readreactor is made server side which begins furiously sending its writes to the client, then again envoy times out the connection resulting in onwritedone(ok=false) after 300s. - **the issue**: grpc c++ doesn't detect client disconnect. the pending startwrite() waits indefinitely until the proxy times out the idle stream. - **question**: - is there a way to make onwritedone(ok=false) return immediately when the client disconnects? - or an api to cancel a pending startwrite()? make sure you include information that can help us debug (full error message, exception listing, stack trace, logs). ### log evidence here's the complete timeline for 5 cancelled streams showing the consistent behavior: **request 1:** | step | timestamp | log | |-----------------------|--------------|-----| | 1. startwrite() | 20:26:49.687 | | | 2. fe abort | ~20:27:21.9 | *(frontend calls abortcontroller.abort() ‚Üí triggers cancel rpc)* | | 3. cancel rpc | 20:27:21.969 | | | 4. onwritedone(false) | 20:31:49.688 | | ‚è±Ô∏è startwrite ‚Üí **300s** ‚Üí onwritedone(false). cancel arrived 32s after startwrite. --- **request 2:** | step | timestamp | log | |-----------------------|--------------|-----| | 1. startwrite() | 20:27:25.706 | | | 2. fe abort | ~20:29:28.4 | *(frontend calls abortcontroller.abort())* | | 3. cancel rpc | 20:29:28.410 | | | 4. onwritedone(false) | 20:32:25.707 | | ‚è±Ô∏è startwrite ‚Üí **300s** ‚Üí onwritedone(false). cancel arrived 123s after startwrite. --- **request 3:** | step | timestamp | log | |-----------------------|--------------|-----| | 1. startwrite() | 20:29:30.935 | | | 2. fe abort | ~20:32:05.7 | *(frontend calls abortcontroller.abort())* | | 3. cancel rpc | 20:32:05.787 | | | 4. onwritedone(false) | 20:34:30.935 | | ‚è±Ô∏è startwrite ‚Üí **300s** ‚Üí onwritedone(false). cancel arrived 155s after startwrite. --- **request 4:** | step | timestamp | log | |-----------------------|--------------|-----| | 1. startwrite() | 20:32:08.361 | | | 2. fe abort | ~20:35:11.6 | *(frontend calls abortcontroller.abort())* | | 3. cancel rpc | 20:35:11.621 | | | 4. onwritedone(false) | 20:37:08.361 | | ‚è±Ô∏è startwrite ‚Üí **300s** ‚Üí onwritedone(false). cancel arrived 183s after startwrite. --- **request 5:** | step | timestamp | log | |-----------------------|--------------|-----| | 1. startwrite() | 20:35:14.035 | | | 2. fe abort | ~20:37:31.1 | *(frontend calls abortcontroller.abort())* | | 3. cancel rpc | 20:37:31.121 | | | 4. onwritedone(false) | 20:40:14.036 | | ‚è±Ô∏è startwrite ‚Üí **300s** ‚Üí onwritedone(false). cancel arrived 137s after startwrite. --- **summary:** | req | startwrite | cancel | onwritedone | startwrite‚Üíonwritedone | |-----|------------|------------------|-------------|------------------------| | 1 | 20:26:49 | 20:27:21 (+32s) | 20:31:49 | **300s** | | 2 | 20:27:25 | 20:29:28 (+123s) | 20:32:25 | **300s** | | 3 | 20:29:30 | 20:32:05 (+155s) | 20:34:30 | **300s** | | 4 | 20:32:08 | 20:35:11 (+183s) | 20:37:08 | **300s** | | 5 | 20:35:14 | 20:37:31 (+137s) | 20:40:14 | **300s** | **key finding:** always returns exactly 300s after was called, regardless of when the client aborted. grpc does not detect the client disconnect - it waits for the proxy timeout to kill the connection. see [troubleshooting.md]( for how to diagnose problems better. i turned on verbose grpc logging but didnt see anything helpful. ### anything else we should know about your project / environment? - proxy: envoy (grpc-web) - oncancel() is never called - known issue with envoy/grpc-web not propagating cancellation, so we use an out of band cancel() rpc",4.6,Critical,1.0,crash-like behavior
microsoft/vscode#290647,test: [cloud agent] model and partner agents,"ref: - [x] anyos - [x] anyos complexity: 4 [create issue]( --- ### pre-requisites - _for partner agents_: must feature flagged into the github. contact josh, who will contact github to get you flagged in. - once ff'd in, you need to self-enable here: ### steps 1. open a personal github repo 2. open chat 3. swap to 'cloud' 4. observe options selectable for the cloud agent note: the three pickers are . these pickers will dynamically hide if there are no options to pick from. for example, if you don't have a [custom agent ( for me)]( checked into your github repo, it wouldn't show up here: selecting a 'partner agent' other than 'copilot' will hide the custom agent and model pickers. those are not suppported. some examples of expected views: please test out this feature by sending cloud agent jobs for your personal repo. ### reporting issues please try to include (or dm to josh) your _trace_ logs from .",[],['TESTING'],['testplan-item'],github,2026-01-27T00:56:08Z,2026-01-27T23:33:47Z,"test: [cloud agent] model and partner agents ref: - [x] anyos - [x] anyos complexity: 4 [create issue]( --- ### pre-requisites - _for partner agents_: must feature flagged into the github. contact josh, who will contact github to get you flagged in. - once ff'd in, you need to self-enable here: ### steps 1. open a personal github repo 2. open chat 3. swap to 'cloud' 4. observe options selectable for the cloud agent note: the three pickers are . these pickers will dynamically hide if there are no options to pick from. for example, if you don't have a [custom agent ( for me)]( checked into your github repo, it wouldn't show up here: selecting a 'partner agent' other than 'copilot' will hide the custom agent and model pickers. those are not suppported. some examples of expected views: please test out this feature by sending cloud agent jobs for your personal repo. ### reporting issues please try to include (or dm to josh) your _trace_ logs from .",1.6,Low,0.584,localized low-impact
microsoft/vscode#290648,[unhandled error] cannot read properties of undefined (reading 'normalizeposition'),"issue created from vs code errors analysis dashboard ## error bucket ## error message ## stack trace raw stack trace (minified) ## details | property | value | | --- | --- | | version | 1.108.2 | | commit | [c9d77990]( | | last seen | 2026-01-25t23:59:38.260z | | total hits | 12.7m | | affected users | 857.7k | | platforms | linux, mac, windows | | product | vscode | --- *this issue was automatically created from the vs code errors dashboard*",[],['BUG'],"['bug', 'error-telemetry']",github,2026-01-27T00:58:10Z,,"[unhandled error] cannot read properties of undefined (reading 'normalizeposition') issue created from vs code errors analysis dashboard ## error bucket ## error message ## stack trace raw stack trace (minified) ## details | property | value | | --- | --- | | version | 1.108.2 | | commit | [c9d77990]( | | last seen | 2026-01-25t23:59:38.260z | | total hits | 12.7m | | affected users | 857.7k | | platforms | linux, mac, windows | | product | vscode | --- *this issue was automatically created from the vs code errors dashboard*",2.337,Medium,0.751,functional impact
microsoft/vscode#290649,test: quickinputbutton toggles & location properties,"refs: - [x] anyos - [x] anyos complexity: 4 authors: , [create issue]( --- this iteration we finalized two pieces to : * - lets you decide where the button should be (title bar, inline, input) * - turns that button into a toggle you can use these properties on both: * quickpick/inputbox objects returned with / * items! ### prereqs * have an extension (don't have one? [create one]( * run to get the api file ### testing give the finalized api a try. play around with the different areas to put buttons and toggles.",[],['TESTING'],['testplan-item'],github,2026-01-27T00:58:23Z,2026-01-28T08:02:01Z,"test: quickinputbutton toggles & location properties refs: - [x] anyos - [x] anyos complexity: 4 authors: , [create issue]( --- this iteration we finalized two pieces to : * - lets you decide where the button should be (title bar, inline, input) * - turns that button into a toggle you can use these properties on both: * quickpick/inputbox objects returned with / * items! ### prereqs * have an extension (don't have one? [create one]( * run to get the api file ### testing give the finalized api a try. play around with the different areas to put buttons and toggles.",1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161828,operation/failureinjection: ood (out of disk),"similar to oom [1], we need to regularly test for out of disk failures. in some cases, pebble will panic the node, thus failing fast. in other cases, the node may remain partially available for an arbitrary duration. a recent outage in [2] resulted from an online-restore job filling up disks on two different nodes. while no (replicated) data was corrupted, the cluster exhibited (partial) unavailability. further, while the nodes continued to run, the logs were spammed with the error of the form, upon restarting a node, would panic, the above suggests left pebble in an inconsistent state. there might be multiple root causes, hence the impetus for this story. [1] [2] jira issue: crdb-59097",[],"['TESTING', 'FEATURE']","['C-enhancement', 'T-testeng', 'A-testeng-foundations']",github,2026-01-27T01:04:02Z,,"operation/failureinjection: ood (out of disk) similar to oom [1], we need to regularly test for out of disk failures. in some cases, pebble will panic the node, thus failing fast. in other cases, the node may remain partially available for an arbitrary duration. a recent outage in [2] resulted from an online-restore job filling up disks on two different nodes. while no (replicated) data was corrupted, the cluster exhibited (partial) unavailability. further, while the nodes continued to run, the logs were spammed with the error of the form, upon restarting a node, would panic, the above suggests left pebble in an inconsistent state. there might be multiple root causes, hence the impetus for this story. [1] [2] jira issue: crdb-59097",2.747,Medium,0.844,crash-like behavior
microsoft/vscode#290650,test: 'agent status' indicator,"- [x] macos - [x] windows - [x] linux complexity: 4 [create issue]( --- test the 'agent status' indicator in the 'command center' section of the editor, to the right of the search box. ## features - has a button to toggle viewing/hiding the chat sidebar (configurable with ) - when there are 'unread' chats, shows that count - clicking on this will toggle filter to just show unread - when there are 'in progress' chats, shows that count - clicking on this will toggle filter to just show in progress please keep an eye on it throughout the day as you use agents, and share any feedback. ## known issues - - project board:",[],['TESTING'],['testplan-item'],github,2026-01-27T01:06:06Z,2026-01-27T20:04:33Z,"test: 'agent status' indicator - [x] macos - [x] windows - [x] linux complexity: 4 [create issue]( --- test the 'agent status' indicator in the 'command center' section of the editor, to the right of the search box. ## features - has a button to toggle viewing/hiding the chat sidebar (configurable with ) - when there are 'unread' chats, shows that count - clicking on this will toggle filter to just show unread - when there are 'in progress' chats, shows that count - clicking on this will toggle filter to just show in progress please keep an eye on it throughout the day as you use agents, and share any feedback. ## known issues - - project board:",1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161829,sql: add statement hints to statement diagnostics bundles,"in order to recreate the plan of a statement that used a statement hint, we need to add that statement hint to the statement diagnostics bundle. probably the simplest thing to do would be to add statements to schema.sql calling . jira issue: crdb-59098",[],"['BUG', 'FEATURE']","['C-enhancement', 'T-sql-queries', 'A-sql-debug-bundle', 'A-plan-management', 'A-statement-hint']",github,2026-01-27T01:15:10Z,,"sql: add statement hints to statement diagnostics bundles in order to recreate the plan of a statement that used a statement hint, we need to add that statement hint to the statement diagnostics bundle. probably the simplest thing to do would be to add statements to schema.sql calling . jira issue: crdb-59098",1.9,Low,0.652,localized low-impact
cockroachdb/cockroach#161830,"sentry: datum.go:5374: comparison of two different versions of enum user defined enum: √ó oid 100125: versions 19 and 18, gist """" (1) assertion failure wraps: (2) attached stack trace -- stack trace...",this issue was auto filed by sentry. it represents a crash or reported error on a live cluster with telemetry enabled. sentry link: [ panic message: stacktrace (expand for inline code snippets): [pkg/sql/plan_opt.go#l259-l261](pkg/sql/plan_opt.go#l259-l261) [pkg/sql/plan_opt.go#l859-l861](pkg/sql/plan_opt.go#l859-l861) [pkg/sql/opt/xform/optimizer.go#l265-l267](pkg/sql/opt/xform/optimizer.go#l265-l267) [pkg/sql/opt/xform/optimizer.go#l545-l547](pkg/sql/opt/xform/optimizer.go#l545-l547) [pkg/sql/opt/xform/optimizer.go#l588-l590](pkg/sql/opt/xform/optimizer.go#l588-l590) [pkg/sql/opt/xform/optimizer.go#l697-l699](pkg/sql/opt/xform/optimizer.go#l697-l699) [pkg/sql/opt/xform/optimizer.go#l745-l747](pkg/sql/opt/xform/optimizer.go#l745-l747) [pkg/sql/opt/xform/optimizer.go#l545-l547](pkg/sql/opt/xform/optimizer.go#l545-l547) [pkg/sql/opt/xform/optimizer.go#l603-l605](pkg/sql/opt/xform/optimizer.go#l603-l605) [pkg/sql/opt/xform/optimizer.go#l295-l297](pkg/sql/opt/xform/optimizer.go#l295-l297) [pkg/sql/opt/xform/optimizer.go#l545-l547](pkg/sql/opt/xform/optimizer.go#l545-l547) [pkg/sql/opt/xform/optimizer.go#l603-l605](pkg/sql/opt/xform/optimizer.go#l603-l605) [pkg/sql/opt/xform/optimizer.go#l307-l309](pkg/sql/opt/xform/optimizer.go#l307-l309) [pkg/sql/opt/xform/optimizer.go#l647-l649](pkg/sql/opt/xform/optimizer.go#l647-l649) [pkg/sql/opt/xform/optimizer.go#l307-l309](pkg/sql/opt/xform/optimizer.go#l307-l309) [pkg/sql/opt/xform/optimizer.go#l647-l649](pkg/sql/opt/xform/optimizer.go#l647-l649) [pkg/sql/opt/xform/optimizer.go#l295-l297](pkg/sql/opt/xform/optimizer.go#l295-l297) [pkg/sql/opt/xform/optimizer.go#l545-l547](pkg/sql/opt/xform/optimizer.go#l545-l547) [pkg/sql/opt/xform/optimizer.go#l588-l590](pkg/sql/opt/xform/optimizer.go#l588-l590) [pkg/sql/opt/xform/optimizer.go#l697-l699](pkg/sql/opt/xform/optimizer.go#l697-l699) [pkg/sql/opt/xform/optimizer.go#l745-l747](pkg/sql/opt/xform/optimizer.go#l745-l747) [pkg/sql/opt/xform/optimizer.go#l558-l560](pkg/sql/opt/xform/optimizer.go#l558-l560) [pkg/sql/opt/xform/explorer.go#l179-l181](pkg/sql/opt/xform/explorer.go#l179-l181) [bazel-out/k8-opt/bin/pkg/sql/opt/xform/explorer.og.go#l41-l43](bazel-out/k8-opt/bin/pkg/sql/opt/xform/explorer.og.go#l41-l43) [bazel-out/k8-opt/bin/pkg/sql/opt/xform/explorer.og.go#l2779-l2781](bazel-out/k8-opt/bin/pkg/sql/opt/xform/explorer.og.go#l2779-l2781) [pkg/sql/opt/xform/join_funcs.go#l1418-l1420](pkg/sql/opt/xform/join_funcs.go#l1418-l1420) [pkg/sql/opt/xform/join_funcs.go#l1460-l1462](pkg/sql/opt/xform/join_funcs.go#l1460-l1462) [pkg/sql/opt/constraint/locality.go#l131-l133](pkg/sql/opt/constraint/locality.go#l131-l133) [pkg/sql/opt/constraint/locality.go#l98-l100](pkg/sql/opt/constraint/locality.go#l98-l100) [goroot/src/sort/search.go#l64-l66](goroot/src/sort/search.go#l64-l66) [pkg/sql/opt/constraint/locality.go#l79-l81](pkg/sql/opt/constraint/locality.go#l79-l81) [pkg/sql/sem/tree/datum.go#l5373-l5375](pkg/sql/sem/tree/datum.go#l5373-l5375) ### tags | tag | value | | --- | --- | | command | server | | environment | v24.3.25 | | go version | go1.22.12 x:nocoverageredesign | | platform | linux arm64 | | distribution | ccl | | cockroach release | v24.3.25 | | cockroach sha | eecd7c2c316ed213daab1c664e0d854fc937a746 | | # of cpus | 8 | | # of goroutines | 612 | jira issue: crdb-59099,[],['BUG'],"['C-bug', 'O-sentry', 'X-blathers-triaged', 'T-sql-foundations', 'branch-release-24.3']",github,2026-01-27T01:22:58Z,2026-01-27T01:44:09Z,"sentry: datum.go:5374: comparison of two different versions of enum user defined enum: √ó oid 100125: versions 19 and 18, gist """" (1) assertion failure wraps: (2) attached stack trace -- stack trace... this issue was auto filed by sentry. it represents a crash or reported error on a live cluster with telemetry enabled. sentry link: [ panic message: stacktrace (expand for inline code snippets): [pkg/sql/plan_opt.go#l259-l261](pkg/sql/plan_opt.go#l259-l261) [pkg/sql/plan_opt.go#l859-l861](pkg/sql/plan_opt.go#l859-l861) [pkg/sql/opt/xform/optimizer.go#l265-l267](pkg/sql/opt/xform/optimizer.go#l265-l267) [pkg/sql/opt/xform/optimizer.go#l545-l547](pkg/sql/opt/xform/optimizer.go#l545-l547) [pkg/sql/opt/xform/optimizer.go#l588-l590](pkg/sql/opt/xform/optimizer.go#l588-l590) [pkg/sql/opt/xform/optimizer.go#l697-l699](pkg/sql/opt/xform/optimizer.go#l697-l699) [pkg/sql/opt/xform/optimizer.go#l745-l747](pkg/sql/opt/xform/optimizer.go#l745-l747) [pkg/sql/opt/xform/optimizer.go#l545-l547](pkg/sql/opt/xform/optimizer.go#l545-l547) [pkg/sql/opt/xform/optimizer.go#l603-l605](pkg/sql/opt/xform/optimizer.go#l603-l605) [pkg/sql/opt/xform/optimizer.go#l295-l297](pkg/sql/opt/xform/optimizer.go#l295-l297) [pkg/sql/opt/xform/optimizer.go#l545-l547](pkg/sql/opt/xform/optimizer.go#l545-l547) [pkg/sql/opt/xform/optimizer.go#l603-l605](pkg/sql/opt/xform/optimizer.go#l603-l605) [pkg/sql/opt/xform/optimizer.go#l307-l309](pkg/sql/opt/xform/optimizer.go#l307-l309) [pkg/sql/opt/xform/optimizer.go#l647-l649](pkg/sql/opt/xform/optimizer.go#l647-l649) [pkg/sql/opt/xform/optimizer.go#l307-l309](pkg/sql/opt/xform/optimizer.go#l307-l309) [pkg/sql/opt/xform/optimizer.go#l647-l649](pkg/sql/opt/xform/optimizer.go#l647-l649) [pkg/sql/opt/xform/optimizer.go#l295-l297](pkg/sql/opt/xform/optimizer.go#l295-l297) [pkg/sql/opt/xform/optimizer.go#l545-l547](pkg/sql/opt/xform/optimizer.go#l545-l547) [pkg/sql/opt/xform/optimizer.go#l588-l590](pkg/sql/opt/xform/optimizer.go#l588-l590) [pkg/sql/opt/xform/optimizer.go#l697-l699](pkg/sql/opt/xform/optimizer.go#l697-l699) [pkg/sql/opt/xform/optimizer.go#l745-l747](pkg/sql/opt/xform/optimizer.go#l745-l747) [pkg/sql/opt/xform/optimizer.go#l558-l560](pkg/sql/opt/xform/optimizer.go#l558-l560) [pkg/sql/opt/xform/explorer.go#l179-l181](pkg/sql/opt/xform/explorer.go#l179-l181) [bazel-out/k8-opt/bin/pkg/sql/opt/xform/explorer.og.go#l41-l43](bazel-out/k8-opt/bin/pkg/sql/opt/xform/explorer.og.go#l41-l43) [bazel-out/k8-opt/bin/pkg/sql/opt/xform/explorer.og.go#l2779-l2781](bazel-out/k8-opt/bin/pkg/sql/opt/xform/explorer.og.go#l2779-l2781) [pkg/sql/opt/xform/join_funcs.go#l1418-l1420](pkg/sql/opt/xform/join_funcs.go#l1418-l1420) [pkg/sql/opt/xform/join_funcs.go#l1460-l1462](pkg/sql/opt/xform/join_funcs.go#l1460-l1462) [pkg/sql/opt/constraint/locality.go#l131-l133](pkg/sql/opt/constraint/locality.go#l131-l133) [pkg/sql/opt/constraint/locality.go#l98-l100](pkg/sql/opt/constraint/locality.go#l98-l100) [goroot/src/sort/search.go#l64-l66](goroot/src/sort/search.go#l64-l66) [pkg/sql/opt/constraint/locality.go#l79-l81](pkg/sql/opt/constraint/locality.go#l79-l81) [pkg/sql/sem/tree/datum.go#l5373-l5375](pkg/sql/sem/tree/datum.go#l5373-l5375) ### tags | tag | value | | --- | --- | | command | server | | environment | v24.3.25 | | go version | go1.22.12 x:nocoverageredesign | | platform | linux arm64 | | distribution | ccl | | cockroach release | v24.3.25 | | cockroach sha | eecd7c2c316ed213daab1c664e0d854fc937a746 | | # of cpus | 8 | | # of goroutines | 612 | jira issue: crdb-59099",6.0,Critical,1.0,crash-like behavior
nodejs/node#61539,expose sqlite virtual table api,### what is the problem this feature will solve? the current module does not expose an api to create virtual tables. ### what is the feature you are proposing to solve the problem? expose bindings for sqlite virtual tables ([vtab]( allowing users to create and use virtual tables. ### what alternatives have you considered? _no response_,[],['FEATURE'],"['feature request', 'sqlite']",github,2026-01-27T01:40:50Z,,expose sqlite virtual table api ### what is the problem this feature will solve? the current module does not expose an api to create virtual tables. ### what is the feature you are proposing to solve the problem? expose bindings for sqlite virtual tables ([vtab]( allowing users to create and use virtual tables. ### what alternatives have you considered? _no response_,1.4,Low,0.538,localized low-impact
cockroachdb/cockroach#161832,sentry: error.go:77: unexpected error from the vectorized engine: √ó (1) wraps: (2) wraps: (3) assertion failure wraps: (4) attached stack trace -- stack trace: | github.com/cockroachdb/cockroach/...,this issue was auto filed by sentry. it represents a crash or reported error on a live cluster with telemetry enabled. sentry link: [ panic message: stacktrace (expand for inline code snippets): [pkg/sql/apply_join.go#l359-l361](pkg/sql/apply_join.go#l359-l361) [pkg/sql/apply_join.go#l356-l358](pkg/sql/apply_join.go#l356-l358) [pkg/sql/distsql_running.go#l2016-l2018](pkg/sql/distsql_running.go#l2016-l2018) [pkg/sql/distsql_running.go#l923-l925](pkg/sql/distsql_running.go#l923-l925) [pkg/sql/colflow/vectorized_flow.go#l299-l301](pkg/sql/colflow/vectorized_flow.go#l299-l301) [pkg/sql/flowinfra/flow.go#l573-l575](pkg/sql/flowinfra/flow.go#l573-l575) [pkg/sql/execinfra/processorsbase.go#l726-l728](pkg/sql/execinfra/processorsbase.go#l726-l728) [pkg/sql/execinfra/base.go#l192-l194](pkg/sql/execinfra/base.go#l192-l194) [pkg/sql/colflow/flow_coordinator.go#l140-l142](pkg/sql/colflow/flow_coordinator.go#l140-l142) [pkg/sql/colexecerror/error.go#l161-l163](pkg/sql/colexecerror/error.go#l161-l163) [pkg/sql/colflow/flow_coordinator.go#l135-l137](pkg/sql/colflow/flow_coordinator.go#l135-l137) [pkg/sql/colflow/flow_coordinator.go#l118-l120](pkg/sql/colflow/flow_coordinator.go#l118-l120) [pkg/sql/colexec/materializer.go#l276-l278](pkg/sql/colexec/materializer.go#l276-l278) [pkg/sql/colexecerror/error.go#l161-l163](pkg/sql/colexecerror/error.go#l161-l163) [pkg/sql/colexec/materializer.go#l270-l272](pkg/sql/colexec/materializer.go#l270-l272) [pkg/sql/colexec/materializer.go#l245-l247](pkg/sql/colexec/materializer.go#l245-l247) [pkg/sql/colexec/colexecdisk/disk_spiller.go#l204-l206](pkg/sql/colexec/colexecdisk/disk_spiller.go#l204-l206) [pkg/sql/colexecerror/error.go#l161-l163](pkg/sql/colexecerror/error.go#l161-l163) [pkg/sql/colexec/colexecdisk/disk_spiller.go#l206-l208](pkg/sql/colexec/colexecdisk/disk_spiller.go#l206-l208) [pkg/sql/colexec/unordered_distinct.go#l94-l96](pkg/sql/colexec/unordered_distinct.go#l94-l96) [pkg/sql/colexec/colexecdisk/disk_spiller.go#l204-l206](pkg/sql/colexec/colexecdisk/disk_spiller.go#l204-l206) [pkg/sql/colexecerror/error.go#l161-l163](pkg/sql/colexecerror/error.go#l161-l163) [pkg/sql/colexec/colexecdisk/disk_spiller.go#l206-l208](pkg/sql/colexec/colexecdisk/disk_spiller.go#l206-l208) [pkg/sql/colexec/unordered_distinct.go#l94-l96](pkg/sql/colexec/unordered_distinct.go#l94-l96) [pkg/sql/colexec/colexecbase/simple_project.go#l118-l120](pkg/sql/colexec/colexecbase/simple_project.go#l118-l120) [pkg/sql/colexec/colexecdisk/disk_spiller.go#l227-l229](pkg/sql/colexec/colexecdisk/disk_spiller.go#l227-l229) [pkg/sql/colexec/colexecdisk/external_sort.go#l436-l438](pkg/sql/colexec/colexecdisk/external_sort.go#l436-l438) [bazel-out/k8-opt/bin/pkg/sql/colexec/ordered_synchronizer.eg.go#l131-l133](bazel-out/k8-opt/bin/pkg/sql/colexec/ordered_synchronizer.eg.go#l131-l133) [pkg/sql/colexec/colexecdisk/utils.go#l73-l75](pkg/sql/colexec/colexecdisk/utils.go#l73-l75) [pkg/sql/colexecerror/error.go#l300-l302](pkg/sql/colexecerror/error.go#l300-l302) [goroot/src/runtime/panic.go#l769-l771](goroot/src/runtime/panic.go#l769-l771) [pkg/sql/colexecerror/error.go#l76-l78](pkg/sql/colexecerror/error.go#l76-l78) ### tags | tag | value | | --- | --- | | command | server | | environment | v24.3.4 | | go version | go1.22.8 x:nocoverageredesign | | platform | linux amd64 | | distribution | ccl | | cockroach release | v24.3.4 | | cockroach sha | 781c478ed1d879da1ab1a331529990bfbd9bc46c | | # of cpus | 12 | | # of goroutines | 645 | jira issue: crdb-59100,[],['BUG'],"['C-bug', 'O-sentry', 'X-blathers-triaged', 'T-sql-queries', 'branch-release-24.3']",github,2026-01-27T01:42:28Z,2026-01-27T01:43:57Z,sentry: error.go:77: unexpected error from the vectorized engine: √ó (1) wraps: (2) wraps: (3) assertion failure wraps: (4) attached stack trace -- stack trace: | github.com/cockroachdb/cockroach/... this issue was auto filed by sentry. it represents a crash or reported error on a live cluster with telemetry enabled. sentry link: [ panic message: stacktrace (expand for inline code snippets): [pkg/sql/apply_join.go#l359-l361](pkg/sql/apply_join.go#l359-l361) [pkg/sql/apply_join.go#l356-l358](pkg/sql/apply_join.go#l356-l358) [pkg/sql/distsql_running.go#l2016-l2018](pkg/sql/distsql_running.go#l2016-l2018) [pkg/sql/distsql_running.go#l923-l925](pkg/sql/distsql_running.go#l923-l925) [pkg/sql/colflow/vectorized_flow.go#l299-l301](pkg/sql/colflow/vectorized_flow.go#l299-l301) [pkg/sql/flowinfra/flow.go#l573-l575](pkg/sql/flowinfra/flow.go#l573-l575) [pkg/sql/execinfra/processorsbase.go#l726-l728](pkg/sql/execinfra/processorsbase.go#l726-l728) [pkg/sql/execinfra/base.go#l192-l194](pkg/sql/execinfra/base.go#l192-l194) [pkg/sql/colflow/flow_coordinator.go#l140-l142](pkg/sql/colflow/flow_coordinator.go#l140-l142) [pkg/sql/colexecerror/error.go#l161-l163](pkg/sql/colexecerror/error.go#l161-l163) [pkg/sql/colflow/flow_coordinator.go#l135-l137](pkg/sql/colflow/flow_coordinator.go#l135-l137) [pkg/sql/colflow/flow_coordinator.go#l118-l120](pkg/sql/colflow/flow_coordinator.go#l118-l120) [pkg/sql/colexec/materializer.go#l276-l278](pkg/sql/colexec/materializer.go#l276-l278) [pkg/sql/colexecerror/error.go#l161-l163](pkg/sql/colexecerror/error.go#l161-l163) [pkg/sql/colexec/materializer.go#l270-l272](pkg/sql/colexec/materializer.go#l270-l272) [pkg/sql/colexec/materializer.go#l245-l247](pkg/sql/colexec/materializer.go#l245-l247) [pkg/sql/colexec/colexecdisk/disk_spiller.go#l204-l206](pkg/sql/colexec/colexecdisk/disk_spiller.go#l204-l206) [pkg/sql/colexecerror/error.go#l161-l163](pkg/sql/colexecerror/error.go#l161-l163) [pkg/sql/colexec/colexecdisk/disk_spiller.go#l206-l208](pkg/sql/colexec/colexecdisk/disk_spiller.go#l206-l208) [pkg/sql/colexec/unordered_distinct.go#l94-l96](pkg/sql/colexec/unordered_distinct.go#l94-l96) [pkg/sql/colexec/colexecdisk/disk_spiller.go#l204-l206](pkg/sql/colexec/colexecdisk/disk_spiller.go#l204-l206) [pkg/sql/colexecerror/error.go#l161-l163](pkg/sql/colexecerror/error.go#l161-l163) [pkg/sql/colexec/colexecdisk/disk_spiller.go#l206-l208](pkg/sql/colexec/colexecdisk/disk_spiller.go#l206-l208) [pkg/sql/colexec/unordered_distinct.go#l94-l96](pkg/sql/colexec/unordered_distinct.go#l94-l96) [pkg/sql/colexec/colexecbase/simple_project.go#l118-l120](pkg/sql/colexec/colexecbase/simple_project.go#l118-l120) [pkg/sql/colexec/colexecdisk/disk_spiller.go#l227-l229](pkg/sql/colexec/colexecdisk/disk_spiller.go#l227-l229) [pkg/sql/colexec/colexecdisk/external_sort.go#l436-l438](pkg/sql/colexec/colexecdisk/external_sort.go#l436-l438) [bazel-out/k8-opt/bin/pkg/sql/colexec/ordered_synchronizer.eg.go#l131-l133](bazel-out/k8-opt/bin/pkg/sql/colexec/ordered_synchronizer.eg.go#l131-l133) [pkg/sql/colexec/colexecdisk/utils.go#l73-l75](pkg/sql/colexec/colexecdisk/utils.go#l73-l75) [pkg/sql/colexecerror/error.go#l300-l302](pkg/sql/colexecerror/error.go#l300-l302) [goroot/src/runtime/panic.go#l769-l771](goroot/src/runtime/panic.go#l769-l771) [pkg/sql/colexecerror/error.go#l76-l78](pkg/sql/colexecerror/error.go#l76-l78) ### tags | tag | value | | --- | --- | | command | server | | environment | v24.3.4 | | go version | go1.22.8 x:nocoverageredesign | | platform | linux amd64 | | distribution | ccl | | cockroach release | v24.3.4 | | cockroach sha | 781c478ed1d879da1ab1a331529990bfbd9bc46c | | # of cpus | 12 | | # of goroutines | 645 | jira issue: crdb-59100,6.0,Critical,1.0,crash-like behavior
cockroachdb/cockroach#161833,sql: make explain (fingerprint) execute match hint injection behavior,"currently, just shows the execute statement itself. but this doesn't match the behavior of hint injection (or other statement hint applications) which use the prepared statement. we should change the output of to show the statement fingerprint of the prepared statement. jira issue: crdb-59101",[],['FEATURE'],"['C-enhancement', 'T-sql-queries', 'A-plan-management', 'A-statement-hint']",github,2026-01-27T01:43:04Z,,"sql: make explain (fingerprint) execute match hint injection behavior currently, just shows the execute statement itself. but this doesn't match the behavior of hint injection (or other statement hint applications) which use the prepared statement. we should change the output of to show the statement fingerprint of the prepared statement. jira issue: crdb-59101",3.6,Critical,1.0,crash-like behavior
cockroachdb/cockroach#161834,drt/operations: implement heuristics to prevent ood (out-of-disk) during long running operations,"### background recently, experienced an outage due to an online-restore job filling up disk on two different nodes. after troubleshooting, it became clear there are at least two different issues, - an online-restore is not robust wrt ood - concurrent import/restore operations are not robust wrt ood the latter invariant is effectively broken by which periodically imports out-of-band wrt operations. that is, an operation such as (online) restore or import may be executing concurrently with . even if a single import/restore is completely robust wrt ood, concurrent ones aren't. the former might be improved/fixed by mma [1]. there is potentially another issues, as discussed in [2], which may or may not be directly addressed by mma. since runs multiple stores (four), after a series of imports and restores, there was a significant disk utilization imbalance wherein only one store was getting (restored) data, while the other three had plenty of capacity. ### plan first, we need to ensure there is sufficient disk capacity before selecting an (online) restore or import operation. for safety, we could disallow concurrent import/restore and have additional safeguards around online restore. second, we need to revisit the spec for this cluster [3]. currently, it uses az localities with two nodes per az. if an entire az is low on disk capacity, decommissioning a node in that az can prove challenging/infeasible. [1] [2] [3] jira issue: crdb-59102",[],"['TESTING', 'FEATURE']","['C-enhancement', 'T-testeng', 'A-testeng-foundations']",github,2026-01-27T02:00:17Z,,"drt/operations: implement heuristics to prevent ood (out-of-disk) during long running operations ### background recently, experienced an outage due to an online-restore job filling up disk on two different nodes. after troubleshooting, it became clear there are at least two different issues, - an online-restore is not robust wrt ood - concurrent import/restore operations are not robust wrt ood the latter invariant is effectively broken by which periodically imports out-of-band wrt operations. that is, an operation such as (online) restore or import may be executing concurrently with . even if a single import/restore is completely robust wrt ood, concurrent ones aren't. the former might be improved/fixed by mma [1]. there is potentially another issues, as discussed in [2], which may or may not be directly addressed by mma. since runs multiple stores (four), after a series of imports and restores, there was a significant disk utilization imbalance wherein only one store was getting (restored) data, while the other three had plenty of capacity. ### plan first, we need to ensure there is sufficient disk capacity before selecting an (online) restore or import operation. for safety, we could disallow concurrent import/restore and have additional safeguards around online restore. second, we need to revisit the spec for this cluster [3]. currently, it uses az localities with two nodes per az. if an entire az is low on disk capacity, decommissioning a node in that az can prove challenging/infeasible. [1] [2] [3] jira issue: crdb-59102",1.5,Low,0.561,localized low-impact
microsoft/vscode#290655,test: external indexing,"test for - [x] mac - [x] linux - [ ] windows complexity: 4 [create issue]( --- # overview external indexing allows non-github workspaces to be indexed and quickly searched by agents. the results it providers are exactly like the existing code search skill. we are independently working to improve the results from these tools ## testing 1. enable this feature using . this special setting turns on external indexing for all repos, including those backed by github you can also set: this disables faster search methods that we use for smaller workspaces. this makes it more likely that external ingest code search tool is used instead 2. trigger a question in a repo. you want to make sure the tool is used. the index is built on the first request. this may take a few minutes depending on the repo size and your network connection. generally something like the vs code repo (8000 files) will take 1-2 minutes to index. 1. modify a few files and save them to disk. try to make sure the index is updated. the simplestway to do this is by asking for something that would only be in the modified text and inspecting the tool call result 3. inspect the results of the tool call. make sure they look relatively reasonable to the question 4. ask another question. make sure the response is faster this time 5. try manually managing the index. you can use the command to manually create the index 6. for debugging, you can also use . this lets you fully delete the index and then rebuild it. a few other notes on testing: - not all files are expected to be indexed. for example, files under and binary files are skipped. this is expected. however please report cases where you expect a file to be indexed but it is not - be sure to try this out in a few different sized repos. we still have some limits in place so don't test this on repos with more than 100,000 files just yet. we plan on relaxing these limits soon",[],['TESTING'],['testplan-item'],github,2026-01-27T02:03:28Z,,"test: external indexing test for - [x] mac - [x] linux - [ ] windows complexity: 4 [create issue]( --- # overview external indexing allows non-github workspaces to be indexed and quickly searched by agents. the results it providers are exactly like the existing code search skill. we are independently working to improve the results from these tools ## testing 1. enable this feature using . this special setting turns on external indexing for all repos, including those backed by github you can also set: this disables faster search methods that we use for smaller workspaces. this makes it more likely that external ingest code search tool is used instead 2. trigger a question in a repo. you want to make sure the tool is used. the index is built on the first request. this may take a few minutes depending on the repo size and your network connection. generally something like the vs code repo (8000 files) will take 1-2 minutes to index. 1. modify a few files and save them to disk. try to make sure the index is updated. the simplestway to do this is by asking for something that would only be in the modified text and inspecting the tool call result 3. inspect the results of the tool call. make sure they look relatively reasonable to the question 4. ask another question. make sure the response is faster this time 5. try manually managing the index. you can use the command to manually create the index 6. for debugging, you can also use . this lets you fully delete the index and then rebuild it. a few other notes on testing: - not all files are expected to be indexed. for example, files under and binary files are skipped. this is expected. however please report cases where you expect a file to be indexed but it is not - be sure to try this out in a few different sized repos. we still have some limits in place so don't test this on repos with more than 100,000 files just yet. we plan on relaxing these limits soon",1.6,Low,0.584,localized low-impact
microsoft/vscode#290656,test: agent sessions welcome view,"refs: - [x] macos - [x] windows - [x] linux complexity: 5 authors: , , [create issue]( --- ## pre-requisites - latest vs code insiders - latest copilot chat pre-release installed - ensure you have the required settings enabled: ## context we added a new agent sessions welcome page that serves as the entry point for users working with copilot agent sessions. this page shows recent sessions, provides quick actions, and includes an embedded chat widget for starting new tasks. --- ## test cases ### basic welcome page 1. open vs code to the welcome page (set ) 2. verify the page displays the product name header 3. verify the quick action buttons work: ""open recent..."", ""new file..."", ""clone git repository..."" 4. verify the chat input widget is displayed and functional 5. try switching between chat modes (ask, edit, agent) using the mode picker 6. verify the ""show welcome page on startup"" checkbox toggles the setting correctly ### sessions grid 1. create several agent sessions (at least 6+) 2. re-open the welcome page and verify sessions are displayed in a grid (max 6 shown) 3. verify archived sessions are not shown in the grid 4. click on a session item and verify it opens correctly 5. click ""view all sessions"" and verify it opens the full sessions view ### empty state / walkthroughs 1. clear all agent sessions (or use a fresh profile) 2. open the welcome page and verify walkthroughs are shown instead of sessions grid 3. use the navigation arrows to browse through walkthroughs 4. click on a walkthrough card and verify it opens the getting started editor ### empty workspace scenario 1. open vs code without any folder/workspace open 2. open the welcome page and verify the workspace picker appears, select local 3. select a recent workspace from the picker 4. type a message in the chat input and submit 5. verify vs code opens the selected workspace and prefills the chat with your message ### privacy notice (anonymous users) 1. sign out of github or use a fresh profile without authentication 2. open the welcome page 3. verify the privacy notice card appears with terms and privacy links 4. dismiss the notice and verify it doesn't reappear on subsequent opens 5. alternatively, send a chat message and verify the notice auto-dismisses ### layout & responsiveness 1. resize the editor pane to various widths 2. verify the chat widget and sessions grid resize appropriately 3. verify scrolling works correctly when content overflows",[],['TESTING'],['testplan-item'],github,2026-01-27T02:10:43Z,2026-01-27T23:29:28Z,"test: agent sessions welcome view refs: - [x] macos - [x] windows - [x] linux complexity: 5 authors: , , [create issue]( --- ## pre-requisites - latest vs code insiders - latest copilot chat pre-release installed - ensure you have the required settings enabled: ## context we added a new agent sessions welcome page that serves as the entry point for users working with copilot agent sessions. this page shows recent sessions, provides quick actions, and includes an embedded chat widget for starting new tasks. --- ## test cases ### basic welcome page 1. open vs code to the welcome page (set ) 2. verify the page displays the product name header 3. verify the quick action buttons work: ""open recent..."", ""new file..."", ""clone git repository..."" 4. verify the chat input widget is displayed and functional 5. try switching between chat modes (ask, edit, agent) using the mode picker 6. verify the ""show welcome page on startup"" checkbox toggles the setting correctly ### sessions grid 1. create several agent sessions (at least 6+) 2. re-open the welcome page and verify sessions are displayed in a grid (max 6 shown) 3. verify archived sessions are not shown in the grid 4. click on a session item and verify it opens correctly 5. click ""view all sessions"" and verify it opens the full sessions view ### empty state / walkthroughs 1. clear all agent sessions (or use a fresh profile) 2. open the welcome page and verify walkthroughs are shown instead of sessions grid 3. use the navigation arrows to browse through walkthroughs 4. click on a walkthrough card and verify it opens the getting started editor ### empty workspace scenario 1. open vs code without any folder/workspace open 2. open the welcome page and verify the workspace picker appears, select local 3. select a recent workspace from the picker 4. type a message in the chat input and submit 5. verify vs code opens the selected workspace and prefills the chat with your message ### privacy notice (anonymous users) 1. sign out of github or use a fresh profile without authentication 2. open the welcome page 3. verify the privacy notice card appears with terms and privacy links 4. dismiss the notice and verify it doesn't reappear on subsequent opens 5. alternatively, send a chat message and verify the notice auto-dismisses ### layout & responsiveness 1. resize the editor pane to various widths 2. verify the chat widget and sessions grid resize appropriately 3. verify scrolling works correctly when content overflows",3.8,Critical,1.0,crash-like behavior
python/cpython#144259,inconsistent display of long multiline pasted content in the repl,"# bug report ### bug description: pasting multiline content with long lines (more columns than the current terminal width) causes an unusual behavior on the repl display, this happens on cmd and powershell. pasting this snippet in the repl will cause this: the interpreter actually handles the content correctly, but the editing of this is weird and leads to confusion. ## video example ## more info i believe this issue is windows-only, as far as i have tested, this works perfectly on ubuntu 22. python 3.13 didn't have this issue, while 3.14 introduced this and 3.15 still has it. ### cpython versions tested on: 3.15 ### operating systems tested on: windows ### linked prs * gh-144297",[],['BUG'],"['type-bug', 'OS-windows', '3.14', 'topic-repl', '3.15']",github,2026-01-27T03:00:24Z,,"inconsistent display of long multiline pasted content in the repl # bug report ### bug description: pasting multiline content with long lines (more columns than the current terminal width) causes an unusual behavior on the repl display, this happens on cmd and powershell. pasting this snippet in the repl will cause this: the interpreter actually handles the content correctly, but the editing of this is weird and leads to confusion. ## video example ## more info i believe this issue is windows-only, as far as i have tested, this works perfectly on ubuntu 22. python 3.13 didn't have this issue, while 3.14 introduced this and 3.15 still has it. ### cpython versions tested on: 3.15 ### operating systems tested on: windows ### linked prs * gh-144297",2.387,Medium,0.763,functional impact
microsoft/vscode#290664,list of edited files displayed for background session is wrong,"**steps to repro** * create a plan to make some changes to a proposed api with gpt 5.2, claude 4.5, gpt 5 * i tried 3 plans with 3 different models * start implementing them all in 3 different background sessions * when they completed the list of edited files is completely wrong. assume we have one background session with worktree . looking at the logs, all files and paths are related to this worktree. however at the end of the session, the list of edited files is from a whole different worktree. the files are listed from a worktree that was created 7 days ago, **note: all 3 sessions have the edited files coming from the same worktree of .** note: this is latest insiders and copilot main branch. version: 1.109.0-insider commit: b074b32e5a00d95510b5471393a62c9be38c127e date: 2026-01-26t17:05:31.575z electron: 39.2.7 electronbuildid: 13098910 chromium: 142.0.7444.235 node.js: 22.21.1 v8: 14.2.231.21-electron.0 os: darwin arm64 25.2.0",[],['BUG'],"['bug', 'chat-background-agent']",github,2026-01-27T03:06:25Z,,"list of edited files displayed for background session is wrong **steps to repro** * create a plan to make some changes to a proposed api with gpt 5.2, claude 4.5, gpt 5 * i tried 3 plans with 3 different models * start implementing them all in 3 different background sessions * when they completed the list of edited files is completely wrong. assume we have one background session with worktree . looking at the logs, all files and paths are related to this worktree. however at the end of the session, the list of edited files is from a whole different worktree. the files are listed from a worktree that was created 7 days ago, **note: all 3 sessions have the edited files coming from the same worktree of .** note: this is latest insiders and copilot main branch. version: 1.109.0-insider commit: b074b32e5a00d95510b5471393a62c9be38c127e date: 2026-01-26t17:05:31.575z electron: 39.2.7 electronbuildid: 13098910 chromium: 142.0.7444.235 node.js: 22.21.1 v8: 14.2.231.21-electron.0 os: darwin arm64 25.2.0",4.6,Critical,1.0,crash-like behavior
microsoft/vscode#290671,[test plan item] double click after curly brace to select the content,"refs - [x] anyos - [x] anyos - [x] anyos complexity: 3 [create issue]( --- ### feature summary double-clicking immediately after an opening bracket/quote or immediately before a closing bracket/quote now selects the content inside. ### setup - create test files with different languages (javascript, typescript, json, etc.) --- ### bracket selection tests | test | steps | expected result | |------|-------|-----------------| | **curly braces - after opening** | in double-click right after (position 10) | is selected | | **curly braces - before closing** | in double-click right before (position 17) | is selected | | **parentheses - after opening** | in double-click right after | is selected | | **parentheses - before closing** | in double-click right before | is selected | | **square brackets - after opening** | in double-click right after | is selected | | **square brackets - before closing** | in double-click right before | is selected | | **nested brackets - inner** | in double-click right after inner | is selected (innermost content) | | **empty brackets** | in double-click right after | empty selection is created | | **multi-line brackets** | in a multi-line function body, double-click after opening | all content between and matching is selected | --- ### string selection tests | test | steps | expected result | |------|-------|-----------------| | **double quotes - after opening** | in double-click right after (at start of string content) | is selected | | **double quotes - before closing** | in double-click right before closing | is selected | | **single quotes - after opening** | in double-click after opening | is selected | | **single quotes - before closing** | in double-click before closing | is selected | | **empty string** | in double-click after opening quote | empty selection is created | --- ### negative tests (no change in behavior) | test | steps | expected result | |------|-------|-----------------| | **middle of word** | double-click on a word like | word is selected (default behavior) | | **not adjacent to bracket** | in double-click on the space between and | space or word is selected (default behavior) | | **unmatched bracket** | double-click after with no matching | default word/character selection | | **interpolated strings** | double-click in near | default behavior (not supported per pr) | | **multi-line strings** | double-click in a template literal spanning multiple lines | default behavior (not supported per pr) | | **comments** | double-click after in a comment | default behavior (not supported per pr) | | **regex literals** | double-click after in | default behavior (not supported per pr) | --- ### language-specific tests | test | steps | expected result | |------|-------|-----------------| | **typescript** | test with typescript type annotations: | works with configured brackets | | **json** | test with json object: | brackets and strings work | | **html** | test with attributes: | string selection works | | **python** | test with brackets and strings | works as expected | --- ### edge cases | test | steps | expected result | |------|-------|-----------------| | **very long content** | brackets containing 1000+ characters | content is selected without performance issues | | **unicode content** | - double-click after | unicode content is correctly selected | | **special characters** | - double-click after | special characters are selected | | **line with only brackets** | on a line - double-click after | empty selection | | **cursor position preserved** | after selection, check cursor is at selection end | cursor at end of selection |",[],['TESTING'],['testplan-item'],github,2026-01-27T04:15:36Z,2026-01-27T22:05:21Z,"[test plan item] double click after curly brace to select the content refs - [x] anyos - [x] anyos - [x] anyos complexity: 3 [create issue]( --- ### feature summary double-clicking immediately after an opening bracket/quote or immediately before a closing bracket/quote now selects the content inside. ### setup - create test files with different languages (javascript, typescript, json, etc.) --- ### bracket selection tests | test | steps | expected result | |------|-------|-----------------| | **curly braces - after opening** | in double-click right after (position 10) | is selected | | **curly braces - before closing** | in double-click right before (position 17) | is selected | | **parentheses - after opening** | in double-click right after | is selected | | **parentheses - before closing** | in double-click right before | is selected | | **square brackets - after opening** | in double-click right after | is selected | | **square brackets - before closing** | in double-click right before | is selected | | **nested brackets - inner** | in double-click right after inner | is selected (innermost content) | | **empty brackets** | in double-click right after | empty selection is created | | **multi-line brackets** | in a multi-line function body, double-click after opening | all content between and matching is selected | --- ### string selection tests | test | steps | expected result | |------|-------|-----------------| | **double quotes - after opening** | in double-click right after (at start of string content) | is selected | | **double quotes - before closing** | in double-click right before closing | is selected | | **single quotes - after opening** | in double-click after opening | is selected | | **single quotes - before closing** | in double-click before closing | is selected | | **empty string** | in double-click after opening quote | empty selection is created | --- ### negative tests (no change in behavior) | test | steps | expected result | |------|-------|-----------------| | **middle of word** | double-click on a word like | word is selected (default behavior) | | **not adjacent to bracket** | in double-click on the space between and | space or word is selected (default behavior) | | **unmatched bracket** | double-click after with no matching | default word/character selection | | **interpolated strings** | double-click in near | default behavior (not supported per pr) | | **multi-line strings** | double-click in a template literal spanning multiple lines | default behavior (not supported per pr) | | **comments** | double-click after in a comment | default behavior (not supported per pr) | | **regex literals** | double-click after in | default behavior (not supported per pr) | --- ### language-specific tests | test | steps | expected result | |------|-------|-----------------| | **typescript** | test with typescript type annotations: | works with configured brackets | | **json** | test with json object: | brackets and strings work | | **html** | test with attributes: | string selection works | | **python** | test with brackets and strings | works as expected | --- ### edge cases | test | steps | expected result | |------|-------|-----------------| | **very long content** | brackets containing 1000+ characters | content is selected without performance issues | | **unicode content** | - double-click after | unicode content is correctly selected | | **special characters** | - double-click after | special characters are selected | | **line with only brackets** | on a line - double-click after | empty selection | | **cursor position preserved** | after selection, check cursor is at selection end | cursor at end of selection |",5.2,Critical,1.0,crash-like behavior
microsoft/vscode#290672,[test plan item] snippets: scope by specific file or pattern,"refs - [x] windows - [x] macos - [x] linux complexity: 3 [create issue]( --- ## context this feature adds the ability to scope user-defined snippets to specific files or file patterns using new 'include' and 'exclude' glob pattern settings in the snippet schema. this allows snippets to be suggested only in files matching specified patterns, improving relevance in autocomplete suggestions. semantics similar to . ### setup create test snippets in a global snippet file (snippets: configure user snippets ‚Üí new global snippets file): --- ### test cases #### 1. include patterns with path ( ) | step | expected | |------|----------| | open , trigger intellisense, type | snippet appears | | open , trigger intellisense, type | snippet appears | | open (non-test), trigger intellisense, type | snippet does not appear | #### 2. include patterns without path (filename-only: ) | step | expected | |------|----------| | open , trigger intellisense, type | snippet appears | | open , trigger intellisense, type | snippet appears | | open , trigger intellisense, type | snippet does not appear | #### 3. exclude patterns | step | expected | |------|----------| | open , trigger intellisense, type | snippet appears | | open , trigger intellisense, type | snippet does not appear | | open , trigger intellisense, type | snippet does not appear | #### 4. include + exclude combined (exclude takes precedence) | step | expected | |------|----------| | open , trigger intellisense, type | snippet appears | | open , trigger intellisense, type | snippet does not appear (excluded) | #### 5. baseline (no include/exclude) | step | expected | |------|----------| | open any , , or file, type | snippet appears in all files with matching scope | --- ### snippet surface coverage #### 6. intellisense completion - covered in tests 1-5 above #### 7. quick fix / code actions (file templates) | step | expected | |------|----------| | create empty file , open it | file template code action should show snippet | | create empty file , open it | file template code action should not show snippet | #### 8. insert snippet command ( ‚Üí ""insert snippet"") | step | expected | |------|----------| | open , run insert snippet command | and snippets appear in picker | | open , run insert snippet command | and snippets do not appear in picker | #### 9. surround with snippet create a snippet for surround: | step | expected | |------|----------| | open , select text, run ""surround with snippet"" | snippet appears | | open , select text, run ""surround with snippet"" | snippet does not appear | #### 10. tab completion | step | expected | |------|----------| | open , type , press | snippet expands | | open , type , press | nothing happens (snippet filtered out) | --- ### edge cases #### 11. untitled files | step | expected | |------|----------| | create new untitled typescript file, trigger intellisense | snippets with path-based patterns should reasonably handle untitled uris (no crash) | #### 12. remote workspaces | step | expected | |------|----------| | open a file over ssh/wsl matching an include pattern | snippet filtering works correctly | #### 13. pattern formats | pattern | match behavior | |---------|----------------| | | matches filename only | | | matches full path | | | matches exact filename | | | multiple patterns (or logic) | --- ### authoring experience #### 14. snippet file creation | step | expected | |------|----------| | run ""snippets: configure user snippets"", create new global snippets file | template includes example with / | | run ""snippets: configure user snippets"", select a language | template includes example with / | #### 15. schema validation | step | expected | |------|----------| | in snippet file, add | intellisense shows property, no validation errors | | in snippet file, add (string) | accepted (string or array both valid) | | in snippet file, add | validation error shown |",[],['TESTING'],['testplan-item'],github,2026-01-27T04:16:43Z,2026-01-28T06:30:10Z,"[test plan item] snippets: scope by specific file or pattern refs - [x] windows - [x] macos - [x] linux complexity: 3 [create issue]( --- ## context this feature adds the ability to scope user-defined snippets to specific files or file patterns using new 'include' and 'exclude' glob pattern settings in the snippet schema. this allows snippets to be suggested only in files matching specified patterns, improving relevance in autocomplete suggestions. semantics similar to . ### setup create test snippets in a global snippet file (snippets: configure user snippets ‚Üí new global snippets file): --- ### test cases #### 1. include patterns with path ( ) | step | expected | |------|----------| | open , trigger intellisense, type | snippet appears | | open , trigger intellisense, type | snippet appears | | open (non-test), trigger intellisense, type | snippet does not appear | #### 2. include patterns without path (filename-only: ) | step | expected | |------|----------| | open , trigger intellisense, type | snippet appears | | open , trigger intellisense, type | snippet appears | | open , trigger intellisense, type | snippet does not appear | #### 3. exclude patterns | step | expected | |------|----------| | open , trigger intellisense, type | snippet appears | | open , trigger intellisense, type | snippet does not appear | | open , trigger intellisense, type | snippet does not appear | #### 4. include + exclude combined (exclude takes precedence) | step | expected | |------|----------| | open , trigger intellisense, type | snippet appears | | open , trigger intellisense, type | snippet does not appear (excluded) | #### 5. baseline (no include/exclude) | step | expected | |------|----------| | open any , , or file, type | snippet appears in all files with matching scope | --- ### snippet surface coverage #### 6. intellisense completion - covered in tests 1-5 above #### 7. quick fix / code actions (file templates) | step | expected | |------|----------| | create empty file , open it | file template code action should show snippet | | create empty file , open it | file template code action should not show snippet | #### 8. insert snippet command ( ‚Üí ""insert snippet"") | step | expected | |------|----------| | open , run insert snippet command | and snippets appear in picker | | open , run insert snippet command | and snippets do not appear in picker | #### 9. surround with snippet create a snippet for surround: | step | expected | |------|----------| | open , select text, run ""surround with snippet"" | snippet appears | | open , select text, run ""surround with snippet"" | snippet does not appear | #### 10. tab completion | step | expected | |------|----------| | open , type , press | snippet expands | | open , type , press | nothing happens (snippet filtered out) | --- ### edge cases #### 11. untitled files | step | expected | |------|----------| | create new untitled typescript file, trigger intellisense | snippets with path-based patterns should reasonably handle untitled uris (no crash) | #### 12. remote workspaces | step | expected | |------|----------| | open a file over ssh/wsl matching an include pattern | snippet filtering works correctly | #### 13. pattern formats | pattern | match behavior | |---------|----------------| | | matches filename only | | | matches full path | | | matches exact filename | | | multiple patterns (or logic) | --- ### authoring experience #### 14. snippet file creation | step | expected | |------|----------| | run ""snippets: configure user snippets"", create new global snippets file | template includes example with / | | run ""snippets: configure user snippets"", select a language | template includes example with / | #### 15. schema validation | step | expected | |------|----------| | in snippet file, add | intellisense shows property, no validation errors | | in snippet file, add (string) | accepted (string or array both valid) | | in snippet file, add | validation error shown |",3.8,Critical,1.0,crash-like behavior
microsoft/vscode#290673,[test plan item] can theme colour be added please?,"refs - [x] windows - [x] macos - [x] linux complexity: 2 [create issue]( --- ### summary new setting allows theming the text color of matching brackets when cursor is positioned next to a bracket. ### test setup any file with nested bracket structures, e.g., typescript/javascript: --- ### core functionality - [ ] **new foreground color applies**: configure in . position cursor next to a bracket , , , etc. verify both matching brackets display in red. - [ ] **default behavior unchanged**: without set, verify matching brackets use their normal/syntax-highlighted color (no regression). - [ ] **multiple bracket types**: test with parentheses , curly braces , and square brackets - all should respect the foreground color. --- ### combination with existing settings - [ ] **with **: set both and colors. verify both apply simultaneously (colored text on colored background). - [ ] **with **: set both and colors. verify text color and border box both appear correctly. - [ ] **all three settings together**: set , , and . confirm all three customizations render together. - [ ] **with **: enable bracket pair colorization. when cursor is on a bracket, the should override the bracket pair colorization color for the matched brackets. - [ ] **with **: disable bracket pair colorization. verify still works independently. - [ ] **with **: when bracket pair colorization is enabled with custom colors, verify takes precedence when cursor is on bracket. --- ### no text shifting / layout stability - [ ] **no horizontal text jumping**: position cursor at various positions around brackets. as bracket matching activates/deactivates, verify surrounding text does not shift horizontally. - [ ] **no vertical text jumping**: move cursor through multiple lines with brackets. verify no vertical layout shifts occur. - [ ] **rapid cursor movement**: move cursor quickly through code with many brackets using arrow keys. verify no text flashing or jumping. - [ ] **typing near brackets**: type characters next to brackets. verify layout remains stable as matching highlights appear/disappear. --- ### edge cases - [ ] **nested brackets**: position cursor at deeply nested brackets . verify correct pair is highlighted with foreground color. - [ ] **mismatched/unbalanced brackets**: place cursor next to unmatched bracket. verify no crash and appropriate highlighting behavior. --- ### settings discovery - [ ] **settings autocomplete**: in , typing should suggest alongside and . - [ ] **color picker**: when editing , the color picker should appear.",[],['TESTING'],['testplan-item'],github,2026-01-27T04:17:10Z,2026-01-27T22:58:36Z,"[test plan item] can theme colour be added please? refs - [x] windows - [x] macos - [x] linux complexity: 2 [create issue]( --- ### summary new setting allows theming the text color of matching brackets when cursor is positioned next to a bracket. ### test setup any file with nested bracket structures, e.g., typescript/javascript: --- ### core functionality - [ ] **new foreground color applies**: configure in . position cursor next to a bracket , , , etc. verify both matching brackets display in red. - [ ] **default behavior unchanged**: without set, verify matching brackets use their normal/syntax-highlighted color (no regression). - [ ] **multiple bracket types**: test with parentheses , curly braces , and square brackets - all should respect the foreground color. --- ### combination with existing settings - [ ] **with **: set both and colors. verify both apply simultaneously (colored text on colored background). - [ ] **with **: set both and colors. verify text color and border box both appear correctly. - [ ] **all three settings together**: set , , and . confirm all three customizations render together. - [ ] **with **: enable bracket pair colorization. when cursor is on a bracket, the should override the bracket pair colorization color for the matched brackets. - [ ] **with **: disable bracket pair colorization. verify still works independently. - [ ] **with **: when bracket pair colorization is enabled with custom colors, verify takes precedence when cursor is on bracket. --- ### no text shifting / layout stability - [ ] **no horizontal text jumping**: position cursor at various positions around brackets. as bracket matching activates/deactivates, verify surrounding text does not shift horizontally. - [ ] **no vertical text jumping**: move cursor through multiple lines with brackets. verify no vertical layout shifts occur. - [ ] **rapid cursor movement**: move cursor quickly through code with many brackets using arrow keys. verify no text flashing or jumping. - [ ] **typing near brackets**: type characters next to brackets. verify layout remains stable as matching highlights appear/disappear. --- ### edge cases - [ ] **nested brackets**: position cursor at deeply nested brackets . verify correct pair is highlighted with foreground color. - [ ] **mismatched/unbalanced brackets**: place cursor next to unmatched bracket. verify no crash and appropriate highlighting behavior. --- ### settings discovery - [ ] **settings autocomplete**: in , typing should suggest alongside and . - [ ] **color picker**: when editing , the color picker should appear.",3.8,Critical,1.0,crash-like behavior
microsoft/vscode#290674,[test plan item] git - add a command palette option for git rm,"refs - [x] anyos - [ ] anyos complexity: 2 [create issue]( --- ### feature summary a new command ""git: delete"" has been added that runs on the current file. the command: - operates on the currently opened editor file - only works on committed files without uncommitted changes - closes the editor after deletion (if not dirty) --- ### test cases #### basic functionality | # | test | steps | expected result | |---|------|-------|-----------------| | 1 | delete committed file via command palette | 1. open a git repo with committed files 2. open a committed file in editor 3. run **git: delete** from command palette | file is deleted from disk and git index, editor closes | | 2 | verify was executed | after test , check | file appears as ""deleted"" in staged changes | | 3 | verify file removed from file system | after test , check file system | file no longer exists on disk | #### edge cases - uncommitted changes | # | test | steps | expected result | |---|------|-------|-----------------| | 4 | attempt delete on modified (unstaged) file | 1. modify a committed file (don't stage) 2. run **git: delete** | shows info message: ""git: delete can only be performed on committed files without uncommitted changes."" | | 5 | attempt delete on staged file | 1. stage a file (new or modified) 2. run **git: delete** | shows info message about uncommitted changes | | 6 | attempt delete on untracked file | 1. create a new file (not committed) 2. run **git: delete** | shows info message about uncommitted changes | | 7 | attempt delete during merge conflict | 1. create a merge conflict on a file 2. run **git: delete** on that file | shows info message about uncommitted changes | #### edge cases - file/editor states | # | test | steps | expected result | |---|------|-------|-----------------| | 8 | delete with dirty editor | 1. open committed file, make unsaved change 2. run **git: delete** | file deleted but editor stays open (dirty) | | 9 | no file open | 1. close all editors 2. run **git: delete** | command does nothing (silently returns) | | 10 | file outside git repository | 1. open a file not in any git repo 2. run **git: delete** | command does nothing | | 11 | delete file in nested folder | 1. open committed file in subfolder 2. run **git: delete** | file deleted correctly, including path | #### command availability | # | test | steps | expected result | |---|------|-------|-----------------| | 12 | command visible in palette | open command palette and search ""git: delete"" | command appears in list | | 13 | command disabled when git disabled | 1. set 2. search for **git: delete** | command should not appear or be disabled | | 14 | command disabled when no repos | 1. open folder without git repo 2. search for **git: delete** | command should not appear or be disabled | #### multi-repo workspace | # | test | steps | expected result | |---|------|-------|-----------------| | 15 | multiple git repos in workspace | 1. open workspace with multiple git repos 2. open file from repo a 3. run **git: delete** | correctly identifies file's repo, deletes from correct repo |",[],['TESTING'],['testplan-item'],github,2026-01-27T04:17:42Z,,"[test plan item] git - add a command palette option for git rm refs - [x] anyos - [ ] anyos complexity: 2 [create issue]( --- ### feature summary a new command ""git: delete"" has been added that runs on the current file. the command: - operates on the currently opened editor file - only works on committed files without uncommitted changes - closes the editor after deletion (if not dirty) --- ### test cases #### basic functionality | # | test | steps | expected result | |---|------|-------|-----------------| | 1 | delete committed file via command palette | 1. open a git repo with committed files 2. open a committed file in editor 3. run **git: delete** from command palette | file is deleted from disk and git index, editor closes | | 2 | verify was executed | after test , check | file appears as ""deleted"" in staged changes | | 3 | verify file removed from file system | after test , check file system | file no longer exists on disk | #### edge cases - uncommitted changes | # | test | steps | expected result | |---|------|-------|-----------------| | 4 | attempt delete on modified (unstaged) file | 1. modify a committed file (don't stage) 2. run **git: delete** | shows info message: ""git: delete can only be performed on committed files without uncommitted changes."" | | 5 | attempt delete on staged file | 1. stage a file (new or modified) 2. run **git: delete** | shows info message about uncommitted changes | | 6 | attempt delete on untracked file | 1. create a new file (not committed) 2. run **git: delete** | shows info message about uncommitted changes | | 7 | attempt delete during merge conflict | 1. create a merge conflict on a file 2. run **git: delete** on that file | shows info message about uncommitted changes | #### edge cases - file/editor states | # | test | steps | expected result | |---|------|-------|-----------------| | 8 | delete with dirty editor | 1. open committed file, make unsaved change 2. run **git: delete** | file deleted but editor stays open (dirty) | | 9 | no file open | 1. close all editors 2. run **git: delete** | command does nothing (silently returns) | | 10 | file outside git repository | 1. open a file not in any git repo 2. run **git: delete** | command does nothing | | 11 | delete file in nested folder | 1. open committed file in subfolder 2. run **git: delete** | file deleted correctly, including path | #### command availability | # | test | steps | expected result | |---|------|-------|-----------------| | 12 | command visible in palette | open command palette and search ""git: delete"" | command appears in list | | 13 | command disabled when git disabled | 1. set 2. search for **git: delete** | command should not appear or be disabled | | 14 | command disabled when no repos | 1. open folder without git repo 2. search for **git: delete** | command should not appear or be disabled | #### multi-repo workspace | # | test | steps | expected result | |---|------|-------|-----------------| | 15 | multiple git repos in workspace | 1. open workspace with multiple git repos 2. open file from repo a 3. run **git: delete** | correctly identifies file's repo, deletes from correct repo |",3.8,Critical,1.0,crash-like behavior
microsoft/vscode#290675,[test plan item] add language configuration file for visual basic,"refs - [x] anyos - [x] anyos complexity: 3 [create issue]( --- vb language spec: use various language constructs and verify auto-indentation works as expected. verify both formatting and enter behavior after a line. **notes** - there will be edge cases as it's still regex based and not semantic - colorization is vb is broken in many cases due to very stale textmate grammar more detailed test plan is below. ### setup 1. create/open a file 2. set to ### test 1: block statement indentation (increase) type a block-opening statement and press enter: verify indent increases after statements like: - control flow: , , , , - loops: , , - declarations: , , , , - others: , , , , ### test 2: block statement indentation (decrease) type a closing statement and verify automatic de-indent: verify de-indent for: - , , , , , , etc. - , , , , - loop terminators: , , , ### test 3: single-line statements (no extra indent) verify no indent increase when block opens and closes on same line: ### test 4: onenter after end statements press enter on a line after an end statement: ### test 5: onenter on blank line after block terminators after an end statement, press enter on a blank line: ### test 6: case insensitivity verify indentation works regardless of casing: ### test 7: comments after statements block-opening keywords followed by comments should still indent:",[],['TESTING'],['testplan-item'],github,2026-01-27T04:18:21Z,2026-01-27T23:24:53Z,"[test plan item] add language configuration file for visual basic refs - [x] anyos - [x] anyos complexity: 3 [create issue]( --- vb language spec: use various language constructs and verify auto-indentation works as expected. verify both formatting and enter behavior after a line. **notes** - there will be edge cases as it's still regex based and not semantic - colorization is vb is broken in many cases due to very stale textmate grammar more detailed test plan is below. ### setup 1. create/open a file 2. set to ### test 1: block statement indentation (increase) type a block-opening statement and press enter: verify indent increases after statements like: - control flow: , , , , - loops: , , - declarations: , , , , - others: , , , , ### test 2: block statement indentation (decrease) type a closing statement and verify automatic de-indent: verify de-indent for: - , , , , , , etc. - , , , , - loop terminators: , , , ### test 3: single-line statements (no extra indent) verify no indent increase when block opens and closes on same line: ### test 4: onenter after end statements press enter on a line after an end statement: ### test 5: onenter on blank line after block terminators after an end statement, press enter on a blank line: ### test 6: case insensitivity verify indentation works regardless of casing: ### test 7: comments after statements block-opening keywords followed by comments should still indent:",1.6,Low,0.584,localized low-impact
tensorflow/tensorflow#108891,crashes ( ) with specific int64 overflow in shape,"### issue type bug ### have you reproduced the bug with tensorflow nightly? no ### source binary ### tensorflow version 2.20.0 ### custom code yes ### os platform and distribution linux ubuntu 24.04 ### mobile device _no response_ ### python version 3.12 ### bazel version _no response_ ### gcc/compiler version _no response_ ### cuda/cudnn version _no response_ ### gpu model and memory _no response_ ### current behavior? running on an empty tensor with a dimension size close to triggers a caused by a check failed assertion in c++. the tensor has 0 elements (one dimension is 0), but another dimension is set to 9223372036854775806. here is a [gist]( ### standalone code to reproduce the issue ### relevant log output",[],['BUG'],"['type:bug', 'comp:apis', 'TF 2.19']",github,2026-01-27T04:18:35Z,,"crashes ( ) with specific int64 overflow in shape ### issue type bug ### have you reproduced the bug with tensorflow nightly? no ### source binary ### tensorflow version 2.20.0 ### custom code yes ### os platform and distribution linux ubuntu 24.04 ### mobile device _no response_ ### python version 3.12 ### bazel version _no response_ ### gcc/compiler version _no response_ ### cuda/cudnn version _no response_ ### gpu model and memory _no response_ ### current behavior? running on an empty tensor with a dimension size close to triggers a caused by a check failed assertion in c++. the tensor has 0 elements (one dimension is 0), but another dimension is set to 9223372036854775806. here is a [gist]( ### standalone code to reproduce the issue ### relevant log output",6.4,Critical,1.0,crash-like behavior
microsoft/vscode#290676,[test plan item] scm - add action to collapse all resource groups for a repository,"refs - [x] windows - [x] macos complexity: 2 [create issue]( --- ### feature summary adds a ""collapse all"" inline action button to scm resource groups (e.g., ""changes"", ""staged changes"") that collapses all expanded folder nodes within that group when ""view as tree"" mode is enabled. --- ### prerequisites - a git repository with multiple files changed across different nested directories (to have a tree structure) - changes in multiple groups (e.g., both staged and unstaged changes) --- ### test cases #### 1. basic functionality - collapse all button visibility | step | action | expected result | |------|--------|-----------------| | 1 | open a git repository with file changes | scm view shows changed files | | 2 | set view mode to ""view as list"" (via context menu or menu) | changed files shown as flat list | | 3 | verify the resource group header (e.g., ""changes"") | **no** ""collapse all"" button should be visible | | 4 | set view mode to ""view as tree"" | changed files shown in tree structure with folders | | 5 | verify the resource group header | ""collapse all"" button (‚äü icon) should be visible inline | #### 2. basic functionality - collapse all action | step | action | expected result | |------|--------|-----------------| | 1 | open a repo with changes in nested folders (e.g., , ) | files show in tree view under expanded folders | | 2 | expand all folders in the ""changes"" group | all folders are expanded | | 3 | click the ""collapse all"" button on the ""changes"" group | all folders within ""changes"" group are collapsed recursively | | 4 | verify other groups (e.g., ""staged changes"") | other groups remain unaffected | #### 3. multiple resource groups | step | action | expected result | |------|--------|-----------------| | 1 | stage some files so you have both ""staged changes"" and ""changes"" groups | both groups visible in scm view | | 2 | expand folders in both groups | folders expanded in both groups | | 3 | click ""collapse all"" on the ""changes"" group | only ""changes"" group folders are collapsed; ""staged changes"" remains expanded | | 4 | click ""collapse all"" on the ""staged changes"" group | only ""staged changes"" group folders are collapsed | #### 4. edge cases - empty or single file groups | step | action | expected result | |------|--------|-----------------| | 1 | have a group with only a single file (no folders) | ""collapse all"" button visible but clicking it has no visible effect (no error) | | 2 | have a group with files only at root level (no nested folders) | clicking ""collapse all"" completes without error | #### 5. multiple repositories | step | action | expected result | |------|--------|-----------------| | 1 | open a multi-root workspace with multiple git repositories | multiple scm providers shown | | 2 | expand folders in different repository sections | folders expanded across repos | | 3 | click ""collapse all"" on a group in repository a | only that group's folders in repository a collapse; repository b is unaffected | #### 6. view mode switching | step | action | expected result | |------|--------|-----------------| | 1 | with ""view as tree"" enabled, note the ""collapse all"" button is visible | button visible | | 2 | switch to ""view as list"" mode | ""collapse all"" button should disappear | | 3 | switch back to ""view as tree"" mode | ""collapse all"" button should reappear | #### 7. keyboard/screen reader accessibility | step | action | expected result | |------|--------|-----------------| | 1 | focus the ""collapse all"" button using keyboard navigation | button receives focus | | 2 | press enter or space | action triggers and folders collapse |",[],['TESTING'],['testplan-item'],github,2026-01-27T04:19:29Z,2026-01-27T21:52:57Z,"[test plan item] scm - add action to collapse all resource groups for a repository refs - [x] windows - [x] macos complexity: 2 [create issue]( --- ### feature summary adds a ""collapse all"" inline action button to scm resource groups (e.g., ""changes"", ""staged changes"") that collapses all expanded folder nodes within that group when ""view as tree"" mode is enabled. --- ### prerequisites - a git repository with multiple files changed across different nested directories (to have a tree structure) - changes in multiple groups (e.g., both staged and unstaged changes) --- ### test cases #### 1. basic functionality - collapse all button visibility | step | action | expected result | |------|--------|-----------------| | 1 | open a git repository with file changes | scm view shows changed files | | 2 | set view mode to ""view as list"" (via context menu or menu) | changed files shown as flat list | | 3 | verify the resource group header (e.g., ""changes"") | **no** ""collapse all"" button should be visible | | 4 | set view mode to ""view as tree"" | changed files shown in tree structure with folders | | 5 | verify the resource group header | ""collapse all"" button (‚äü icon) should be visible inline | #### 2. basic functionality - collapse all action | step | action | expected result | |------|--------|-----------------| | 1 | open a repo with changes in nested folders (e.g., , ) | files show in tree view under expanded folders | | 2 | expand all folders in the ""changes"" group | all folders are expanded | | 3 | click the ""collapse all"" button on the ""changes"" group | all folders within ""changes"" group are collapsed recursively | | 4 | verify other groups (e.g., ""staged changes"") | other groups remain unaffected | #### 3. multiple resource groups | step | action | expected result | |------|--------|-----------------| | 1 | stage some files so you have both ""staged changes"" and ""changes"" groups | both groups visible in scm view | | 2 | expand folders in both groups | folders expanded in both groups | | 3 | click ""collapse all"" on the ""changes"" group | only ""changes"" group folders are collapsed; ""staged changes"" remains expanded | | 4 | click ""collapse all"" on the ""staged changes"" group | only ""staged changes"" group folders are collapsed | #### 4. edge cases - empty or single file groups | step | action | expected result | |------|--------|-----------------| | 1 | have a group with only a single file (no folders) | ""collapse all"" button visible but clicking it has no visible effect (no error) | | 2 | have a group with files only at root level (no nested folders) | clicking ""collapse all"" completes without error | #### 5. multiple repositories | step | action | expected result | |------|--------|-----------------| | 1 | open a multi-root workspace with multiple git repositories | multiple scm providers shown | | 2 | expand folders in different repository sections | folders expanded across repos | | 3 | click ""collapse all"" on a group in repository a | only that group's folders in repository a collapse; repository b is unaffected | #### 6. view mode switching | step | action | expected result | |------|--------|-----------------| | 1 | with ""view as tree"" enabled, note the ""collapse all"" button is visible | button visible | | 2 | switch to ""view as list"" mode | ""collapse all"" button should disappear | | 3 | switch back to ""view as tree"" mode | ""collapse all"" button should reappear | #### 7. keyboard/screen reader accessibility | step | action | expected result | |------|--------|-----------------| | 1 | focus the ""collapse all"" button using keyboard navigation | button receives focus | | 2 | press enter or space | action triggers and folders collapse |",5.6,Critical,1.0,crash-like behavior
microsoft/vscode#290677,[test plan item] glob matching should be case insensitive,"refs - [x] windows - [ ] macos - [x] linux complexity: 3 [create issue]( --- ## overview this test plan covers the case-insensitive glob matching feature for file paths on windows and macos. on these platforms, glob patterns in search include/exclude fields and settings should match files regardless of case. **platform behavior:** - **windows/macos**: glob patterns are case-insensitive by default - **linux**: glob patterns remain case-sensitive (no change) --- ## 1. search view - ""files to include"" field ### test 1.1: basic case-insensitive include pattern **steps:** 1. create files: , , 2. open search view (ctrl+shift+f) 3. expand ""files to include"" (click ""..."") 4. enter in ""files to include"" 5. search for a string known to exist in these files **expected (windows/macos):** all three files are searched and results shown **expected (linux):** only (if exists) is searched ### test 1.2: mixed-case folder pattern **steps:** 1. create folder structure: , , 2. search for ""test"" with include pattern **expected (windows/macos):** all variations of the folder are searched **expected (linux):** only exact match is searched ### test 1.3: extension case sensitivity **steps:** 1. create files: , , 2. search with include pattern **expected (windows/macos):** all three files are searched **expected (linux):** only is searched ### test 1.4: glob with braces **steps:** 1. create files: , , 2. search with include pattern **expected (windows/macos):** all files are searched **expected (linux):** case-sensitive matching applies --- ## 2. search view - ""files to exclude"" field ### test 2.1: case-insensitive exclude pattern **steps:** 1. create files: , , 2. search for a common string with exclude pattern **expected (windows/macos):** no files in any case variation of src folder appear in results **expected (linux):** only exact src folder is excluded ### test 2.2: mixed include/exclude **steps:** 1. create: , , 2. search with: - include: - exclude: **expected (windows/macos):** in all folder case variations is excluded **expected (linux):** only exact path match is excluded --- ## 3. settings: ### test 3.1: setting with mixed-case pattern **steps:** 1. add to settings.json: 2. create folder node_modules with files inside 3. perform search **expected (windows/macos):** node_modules folder is excluded despite casing mismatch in setting **expected (linux):** node_modules is not excluded (pattern doesn't match) ### test 3.2: sibling clause pattern ( ) **steps:** 1. add: 2. create and in same folder 3. search **expected (windows/macos):** is excluded (sibling exists, case-insensitive) **expected (linux):** is not excluded (no sibling with that exact casing) --- ## 4. settings: ### test 4.1: explorer exclusion **steps:** 1. add: 2. create 3. check explorer view **expected (windows/macos):** build folder is hidden **expected (linux):** build folder is visible ### test 4.2: affects search **steps:** 1. same setup as 4.1 2. perform search with ""use exclude settings"" enabled **expected (windows/macos):** files in build folder excluded from search **expected (linux):** files in build folder included in search --- ## 6. .gitignore / files ### test 6.1: gitignore pattern case **steps:** 1. create .gitignore with pattern build 2. create folder build with files 3. search with ""use ignore files"" enabled **expected (windows/macos):** build is excluded (case-insensitive match) **expected (linux):** build is not excluded (no exact match) ### test 6.2: nested ignore files **steps:** 1. create .gitignore in root with 2. create with 3. create , folders 4. search with parent ignore files enabled **expected (windows/macos):** both folders excluded **expected (linux):** neither folder excluded --- ## 8. search editor ### test 8.1: search editor include/exclude **steps:** 1. create new search editor 2. set include pattern with different casing than actual files 3. run search **expected:** same case-insensitive behavior as search view --- ## 9. find in folder (context menu) ### test 9.1: right-click search in folder **steps:** 1. right-click on in explorer 2. select ""find in folder..."" 3. check the pre-filled ""files to include"" pattern **expected:** search works regardless of folder name casing in pattern --- ## 12. remote/wsl scenarios ### test 12.1: windows client ‚Üí linux remote **steps:** 1. connect to linux remote (wsl/ssh) 2. perform search with mixed-case patterns **expected:** linux case-sensitivity applies (remote os determines behavior) ### test 12.2: macos client ‚Üí linux remote **steps:** 1. same as above from macos **expected:** linux case-sensitivity applies",[],['TESTING'],['testplan-item'],github,2026-01-27T04:25:05Z,,"[test plan item] glob matching should be case insensitive refs - [x] windows - [ ] macos - [x] linux complexity: 3 [create issue]( --- ## overview this test plan covers the case-insensitive glob matching feature for file paths on windows and macos. on these platforms, glob patterns in search include/exclude fields and settings should match files regardless of case. **platform behavior:** - **windows/macos**: glob patterns are case-insensitive by default - **linux**: glob patterns remain case-sensitive (no change) --- ## 1. search view - ""files to include"" field ### test 1.1: basic case-insensitive include pattern **steps:** 1. create files: , , 2. open search view (ctrl+shift+f) 3. expand ""files to include"" (click ""..."") 4. enter in ""files to include"" 5. search for a string known to exist in these files **expected (windows/macos):** all three files are searched and results shown **expected (linux):** only (if exists) is searched ### test 1.2: mixed-case folder pattern **steps:** 1. create folder structure: , , 2. search for ""test"" with include pattern **expected (windows/macos):** all variations of the folder are searched **expected (linux):** only exact match is searched ### test 1.3: extension case sensitivity **steps:** 1. create files: , , 2. search with include pattern **expected (windows/macos):** all three files are searched **expected (linux):** only is searched ### test 1.4: glob with braces **steps:** 1. create files: , , 2. search with include pattern **expected (windows/macos):** all files are searched **expected (linux):** case-sensitive matching applies --- ## 2. search view - ""files to exclude"" field ### test 2.1: case-insensitive exclude pattern **steps:** 1. create files: , , 2. search for a common string with exclude pattern **expected (windows/macos):** no files in any case variation of src folder appear in results **expected (linux):** only exact src folder is excluded ### test 2.2: mixed include/exclude **steps:** 1. create: , , 2. search with: - include: - exclude: **expected (windows/macos):** in all folder case variations is excluded **expected (linux):** only exact path match is excluded --- ## 3. settings: ### test 3.1: setting with mixed-case pattern **steps:** 1. add to settings.json: 2. create folder node_modules with files inside 3. perform search **expected (windows/macos):** node_modules folder is excluded despite casing mismatch in setting **expected (linux):** node_modules is not excluded (pattern doesn't match) ### test 3.2: sibling clause pattern ( ) **steps:** 1. add: 2. create and in same folder 3. search **expected (windows/macos):** is excluded (sibling exists, case-insensitive) **expected (linux):** is not excluded (no sibling with that exact casing) --- ## 4. settings: ### test 4.1: explorer exclusion **steps:** 1. add: 2. create 3. check explorer view **expected (windows/macos):** build folder is hidden **expected (linux):** build folder is visible ### test 4.2: affects search **steps:** 1. same setup as 4.1 2. perform search with ""use exclude settings"" enabled **expected (windows/macos):** files in build folder excluded from search **expected (linux):** files in build folder included in search --- ## 6. .gitignore / files ### test 6.1: gitignore pattern case **steps:** 1. create .gitignore with pattern build 2. create folder build with files 3. search with ""use ignore files"" enabled **expected (windows/macos):** build is excluded (case-insensitive match) **expected (linux):** build is not excluded (no exact match) ### test 6.2: nested ignore files **steps:** 1. create .gitignore in root with 2. create with 3. create , folders 4. search with parent ignore files enabled **expected (windows/macos):** both folders excluded **expected (linux):** neither folder excluded --- ## 8. search editor ### test 8.1: search editor include/exclude **steps:** 1. create new search editor 2. set include pattern with different casing than actual files 3. run search **expected:** same case-insensitive behavior as search view --- ## 9. find in folder (context menu) ### test 9.1: right-click search in folder **steps:** 1. right-click on in explorer 2. select ""find in folder..."" 3. check the pre-filled ""files to include"" pattern **expected:** search works regardless of folder name casing in pattern --- ## 12. remote/wsl scenarios ### test 12.1: windows client ‚Üí linux remote **steps:** 1. connect to linux remote (wsl/ssh) 2. perform search with mixed-case patterns **expected:** linux case-sensitivity applies (remote os determines behavior) ### test 12.2: macos client ‚Üí linux remote **steps:** 1. same as above from macos **expected:** linux case-sensitivity applies",3.8,Critical,1.0,crash-like behavior
rust-lang/rust#151720,unexpected follow-up errors after proc-macro attribute panic,"i tried this code: i expected to see this happen: the compilation fails only with custom attribute panicked. instead, this happened: ### meta : this error was discovered when running in the ui test for .",[],"['BUG', 'UI']","['A-testsuite', 'C-bug', 'needs-triage']",github,2026-01-27T04:32:02Z,,"unexpected follow-up errors after proc-macro attribute panic i tried this code: i expected to see this happen: the compilation fails only with custom attribute panicked. instead, this happened: ### meta : this error was discovered when running in the ui test for .",3.368,High,0.985,"user-visible issue, crash-like behavior"
tensorflow/tensorflow#108904,crashes (segfault/abort) when int64_max is passed as shape,"### issue type bug ### have you reproduced the bug with tensorflow nightly? no ### source binary ### tensorflow version 2.20.0 ### custom code yes ### os platform and distribution linux ubuntu 24.04 ### mobile device _no response_ ### python version 3.12 ### bazel version _no response_ ### gcc/compiler version _no response_ ### cuda/cudnn version _no response_ ### gpu model and memory _no response_ ### current behavior? when calling with a shape equivalent to int64_max ( ), the python process terminates immediately. the behavior is unstable and manifests in two ways depending on execution: - segmentation fault ( ): occurs inside . - aborted (core dumped): occurs in the bfc allocator during deallocation with a error, indicating heap metadata corruption. this likely indicates an integer overflow in the size calculation that leads to heap corruption or an unchecked memory access in the c++ backend. here is a [gist]( ### standalone code to reproduce the issue ### relevant log output",[],['BUG'],"['type:bug', 'comp:apis', 'TF 2.19']",github,2026-01-27T04:54:00Z,,"crashes (segfault/abort) when int64_max is passed as shape ### issue type bug ### have you reproduced the bug with tensorflow nightly? no ### source binary ### tensorflow version 2.20.0 ### custom code yes ### os platform and distribution linux ubuntu 24.04 ### mobile device _no response_ ### python version 3.12 ### bazel version _no response_ ### gcc/compiler version _no response_ ### cuda/cudnn version _no response_ ### gpu model and memory _no response_ ### current behavior? when calling with a shape equivalent to int64_max ( ), the python process terminates immediately. the behavior is unstable and manifests in two ways depending on execution: - segmentation fault ( ): occurs inside . - aborted (core dumped): occurs in the bfc allocator during deallocation with a error, indicating heap metadata corruption. this likely indicates an integer overflow in the size calculation that leads to heap corruption or an unchecked memory access in the c++ backend. here is a [gist]( ### standalone code to reproduce the issue ### relevant log output",6.4,Critical,1.0,crash-like behavior
cockroachdb/cockroach#161836,sql: make statement_hints match statement_statistics,"in anticipation of both (a) friendlier ui for hint injection and (b) the upcoming statement fingerprint normalization project, we might consider making more closely match , , and other related tables. this might entail - adding the current database as a matching criteria - adding whether the statement is in an implicit transaction as a matching criteria - including the current database and implicittxn in the hash of the statement fingerprint, like in jira issue: crdb-59103",[],['FEATURE'],"['C-enhancement', 'T-sql-queries', 'A-plan-management', 'A-statement-hint']",github,2026-01-27T04:57:29Z,,"sql: make statement_hints match statement_statistics in anticipation of both (a) friendlier ui for hint injection and (b) the upcoming statement fingerprint normalization project, we might consider making more closely match , , and other related tables. this might entail - adding the current database as a matching criteria - adding whether the statement is in an implicit transaction as a matching criteria - including the current database and implicittxn in the hash of the statement fingerprint, like in jira issue: crdb-59103",1.4,Low,0.538,localized low-impact
rust-lang/rust#151723,hang in next-solver: infinite recursion in,"<!-- thank you for filing a bug report! üêõ please provide a short summary of the bug, along with any information you feel relevant to replicating the bug. --> i tried this code: (auto-reduced) | release channel | result | |-----------------------------------|--------| | current stable | error | | current nightly (default solver) | error | | current nightly (+ | hang | ### meta <!-- if you're using the stable version of the compiler, you should also check if the bug also exists in the beta or nightly versions. --> : <!-- include a backtrace in the code block by setting in your environment. e.g. . --> backtrace",[],"['CLEANUP', 'BUG']","['A-trait-system', 'T-compiler', 'C-bug', 'WG-trait-system-refactor', 'needs-triage']",github,2026-01-27T05:19:15Z,,"hang in next-solver: infinite recursion in <!-- thank you for filing a bug report! üêõ please provide a short summary of the bug, along with any information you feel relevant to replicating the bug. --> i tried this code: (auto-reduced) | release channel | result | |-----------------------------------|--------| | current stable | error | | current nightly (default solver) | error | | current nightly (+ | hang | ### meta <!-- if you're using the stable version of the compiler, you should also check if the bug also exists in the beta or nightly versions. --> : <!-- include a backtrace in the code block by setting in your environment. e.g. . --> backtrace",3.155,High,0.937,crash-like behavior
tensorflow/tensorflow#108916,crashes process ( ) with failure when input has 0-dimension,"### issue type bug ### have you reproduced the bug with tensorflow nightly? no ### source binary ### tensorflow version 2.20 ### custom code yes ### os platform and distribution linux ubutnu 24.04 ### mobile device linux ubuntu 24.04 ### python version 3.12 ### bazel version _no response_ ### gcc/compiler version _no response_ ### cuda/cudnn version _no response_ ### gpu model and memory _no response_ ### current behavior? when calling with a tensor that has a valid rank but one dimension set to 0 (e.g., shape ), the python interpreter aborts immediately with a core dump. it does not raise a python-level exception. instead, it hits a check in the c++ backend: here is a [gist]( ### standalone code to reproduce the issue ### relevant log output",[],['BUG'],"['type:bug', 'comp:core', 'TF 2.19']",github,2026-01-27T05:21:27Z,,"crashes process ( ) with failure when input has 0-dimension ### issue type bug ### have you reproduced the bug with tensorflow nightly? no ### source binary ### tensorflow version 2.20 ### custom code yes ### os platform and distribution linux ubutnu 24.04 ### mobile device linux ubuntu 24.04 ### python version 3.12 ### bazel version _no response_ ### gcc/compiler version _no response_ ### cuda/cudnn version _no response_ ### gpu model and memory _no response_ ### current behavior? when calling with a tensor that has a valid rank but one dimension set to 0 (e.g., shape ), the python interpreter aborts immediately with a core dump. it does not raise a python-level exception. instead, it hits a check in the c++ backend: here is a [gist]( ### standalone code to reproduce the issue ### relevant log output",6.4,Critical,1.0,crash-like behavior
microsoft/vscode#290683,test: dmg installation on macos,"refs: - [x] macos x64 - [x] macos arm64 - [x] macos universal complexity: 3 authors: [create issue]( --- ## context vs code on macos currently downloads as a zip, requiring users to move it to the right location which is often skipped. this feature implements dmg based installation to improve this flow and promote installation into the folder. ## testing ### dmg download and mount 1. download the vs code dmg installer from [here]( artifacts with suffix. 2. verify the dmg file downloads successfully 3. double-click the dmg to mount it 4. verify the dmg mounts and opens a finder window with the vs code app, applications folder shortcut 5. drag the vs code app icon to the applications folder shortcut in the dmg window 6. verify vs code is copied to folder, if there is an existing installation it allows replacing it 7. open vs code from the applications folder 8. verify the app launches correctly (no ""damaged app"" or security warnings should appear on properly signed builds) ### cleanup 1. eject the mounted dmg 2. delete the downloaded dmg file 3. verify vs code continues to run from applications folder",[],['TESTING'],['testplan-item'],github,2026-01-27T05:51:38Z,2026-01-27T22:54:56Z,"test: dmg installation on macos refs: - [x] macos x64 - [x] macos arm64 - [x] macos universal complexity: 3 authors: [create issue]( --- ## context vs code on macos currently downloads as a zip, requiring users to move it to the right location which is often skipped. this feature implements dmg based installation to improve this flow and promote installation into the folder. ## testing ### dmg download and mount 1. download the vs code dmg installer from [here]( artifacts with suffix. 2. verify the dmg file downloads successfully 3. double-click the dmg to mount it 4. verify the dmg mounts and opens a finder window with the vs code app, applications folder shortcut 5. drag the vs code app icon to the applications folder shortcut in the dmg window 6. verify vs code is copied to folder, if there is an existing installation it allows replacing it 7. open vs code from the applications folder 8. verify the app launches correctly (no ""damaged app"" or security warnings should appear on properly signed builds) ### cleanup 1. eject the mounted dmg 2. delete the downloaded dmg file 3. verify vs code continues to run from applications folder",1.6,Low,0.584,localized low-impact
tensorflow/tensorflow#108921,causes on empty tensors with large dimensions,"### issue type bug ### have you reproduced the bug with tensorflow nightly? no ### source binary ### tensorflow version 2.20.0 ### custom code yes ### os platform and distribution linux ubuntu 24.04 ### mobile device _no response_ ### python version 3.12 ### bazel version _no response_ ### gcc/compiler version _no response_ ### cuda/cudnn version _no response_ ### gpu model and memory _no response_ ### current behavior? a failure ( ) occurs in when performing a reduction operation (specifically ) on a tensor that contains both a 0 dimension and a dimension close to int64_max. although the tensor is logically empty (0 elements), the internal shape inference logic appears to calculate the product of dimensions before accounting for the zero dimension. this calculation overflows int64, resulting in a negative value for the number of elements, which triggers the assertion . here is a [gist]( ### standalone code to reproduce the issue ### relevant log output",[],['BUG'],"['type:bug', 'comp:apis', 'TF 2.19']",github,2026-01-27T05:55:11Z,,"causes on empty tensors with large dimensions ### issue type bug ### have you reproduced the bug with tensorflow nightly? no ### source binary ### tensorflow version 2.20.0 ### custom code yes ### os platform and distribution linux ubuntu 24.04 ### mobile device _no response_ ### python version 3.12 ### bazel version _no response_ ### gcc/compiler version _no response_ ### cuda/cudnn version _no response_ ### gpu model and memory _no response_ ### current behavior? a failure ( ) occurs in when performing a reduction operation (specifically ) on a tensor that contains both a 0 dimension and a dimension close to int64_max. although the tensor is logically empty (0 elements), the internal shape inference logic appears to calculate the product of dimensions before accounting for the zero dimension. this calculation overflows int64, resulting in a negative value for the number of elements, which triggers the assertion . here is a [gist]( ### standalone code to reproduce the issue ### relevant log output",6.4,Critical,1.0,crash-like behavior
python/cpython#144261,syntaxwarning for await expession,"# feature or enhancement ### proposal: now python has warned for const expression that impossible to successfully executed: this feature want to enable the warning for await expressions: as i have known, there is no literal constant type that support for await (int, bool, float, complex, str, list, tuple, dict, set, ellipsis) so if there is only a constant after await we can safely warn it. ### has this already been discussed elsewhere? this is a minor feature, which does not need previous discussion elsewhere ### links to previous discussion of this feature: _no response_ ### linked prs * gh-144271",[],['FEATURE'],"['type-feature', 'topic-asyncio', 'stdlib']",github,2026-01-27T06:03:19Z,,"syntaxwarning for await expession # feature or enhancement ### proposal: now python has warned for const expression that impossible to successfully executed: this feature want to enable the warning for await expressions: as i have known, there is no literal constant type that support for await (int, bool, float, complex, str, list, tuple, dict, set, ellipsis) so if there is only a constant after await we can safely warn it. ### has this already been discussed elsewhere? this is a minor feature, which does not need previous discussion elsewhere ### links to previous discussion of this feature: _no response_ ### linked prs * gh-144271",1.4,Low,0.538,localized low-impact
microsoft/vscode#290686,command center foreground theme colors not applied correctly.,"does this issue occur when all extensions are disabled?: yes report issue' dialog can assist with this. --> - vs code version: 1.108.2, 1.109.0-insider - os version: macos 26.2 tahoe steps to reproduce: 1. add the following to and save the file: 2. click any other window to make the vsc window inactive 3. hover cursor to the command center 4. observe the command center text and icon colors --- ### expected behavior when the window is inactive, command center elements should use in this case, blue ( ff). according to the documentation, [theme color documentation]( > : foreground color of the command center when the window is inactive. ### actual behavior when the window is inactive, all command center elements use red (#ff0000) instead. the is never applied to any text or icon. ### screenshots - active - inactive ### related issue on hover, is only applied to text, not to the search icon: - text: yellow (#ffff00) - search icon: stays cyan ( ffff) - this should also be yellow ![image]( ### additional the variable is correctly defined in [theme.ts]( however, in [titlebarpart.css]( is never used. all inactive colors use instead. and also, is used correctly for border color. --- ### workaround theme authors must set to achieve the desired inactive appearance for command center, or use similar colors to avoid color jumping.",[],['UI'],['ux'],github,2026-01-27T06:06:23Z,,"command center foreground theme colors not applied correctly. does this issue occur when all extensions are disabled?: yes report issue' dialog can assist with this. --> - vs code version: 1.108.2, 1.109.0-insider - os version: macos 26.2 tahoe steps to reproduce: 1. add the following to and save the file: 2. click any other window to make the vsc window inactive 3. hover cursor to the command center 4. observe the command center text and icon colors --- ### expected behavior when the window is inactive, command center elements should use in this case, blue ( ff). according to the documentation, [theme color documentation]( > : foreground color of the command center when the window is inactive. ### actual behavior when the window is inactive, all command center elements use red (#ff0000) instead. the is never applied to any text or icon. ### screenshots - active - inactive ### related issue on hover, is only applied to text, not to the search icon: - text: yellow (#ffff00) - search icon: stays cyan ( ffff) - this should also be yellow ![image]( ### additional the variable is correctly defined in [theme.ts]( however, in [titlebarpart.css]( is never used. all inactive colors use instead. and also, is used correctly for border color. --- ### workaround theme authors must set to achieve the desired inactive appearance for command center, or use similar colors to avoid color jumping.",1.8,Low,0.629,user-visible issue
microsoft/vscode#290687,test: windows 11 context menu,"refs: - [x] windows x64 - [x] windows arm64 complexity: 4 authors: [create issue]( --- ## context vs code on windows 11 now integrates with the modern context menu (right-click menu) in file explorer. this feature enables the ""open with code"" option to appear directly in the top level of windows 11 context menu without requiring users to click ""show more options"" to access the legacy context menu. ## prerequisite 1. download the user setup from [here]( 2. run the installer and check the **add 'open with code' action to windows explorer file context menu** option 3. complete the installation ## testing ### opening files 1. right-click on any file from the windows file explorer 2. select ""open with code"" 3. verify vs code opens with the selected file ### opening folders 1. right-click on a folder from the windows file explorer 2. select ""open with code"" 3. verify vs code opens with the folder as the workspace ### run as administrator scenarios > refs: 1. navigate to vs code installation folder (e.g., or ) 2. right-click and select ""properties"" 3. click on the ""compatibility"" tab 4. check ""run this program as an administrator"" 5. click ""ok"" to save 6. navigate to a folder from the windows file explorer 7. right-click on the folder and select ""open with code"" 8. verify a uac (user account control) elevation prompt appears 9. click ""yes"" on the uac prompt 10. verify vs code opens correctly with administrator privileges, you should see in the window title 11. right-click on a file and select ""open with code"" 12. verify a uac prompt appears and vs code opens correctly after approval ### run as administrator - uac denied 1. configure vs code to run as administrator (as above) 2. right-click on a folder and select ""open with code"" 3. when the uac prompt appears, click ""no"" 4. verify vs code does not open (expected behavior - access denied) ### uninstallation 1. uninstall vs code 2. open the windows file explorer and right-click on any file 3. verify ""open with code"" no longer appears in the modern context menu 4. right-click on any folder 5. verify ""open with code"" no longer appears in the modern context menu ### upgrade scenarios 1. install a previous version of vs code from [here]( 2. verify context menu entry appears under ""show more options"" menu 3. upgrade to the new version, use command from within vs code 4. verify ""open with code"" now appears in the top level context menu 5. verify old context menu entries are cleaned up (no duplicates)",[],['TESTING'],['testplan-item'],github,2026-01-27T06:17:53Z,2026-01-27T22:40:30Z,"test: windows 11 context menu refs: - [x] windows x64 - [x] windows arm64 complexity: 4 authors: [create issue]( --- ## context vs code on windows 11 now integrates with the modern context menu (right-click menu) in file explorer. this feature enables the ""open with code"" option to appear directly in the top level of windows 11 context menu without requiring users to click ""show more options"" to access the legacy context menu. ## prerequisite 1. download the user setup from [here]( 2. run the installer and check the **add 'open with code' action to windows explorer file context menu** option 3. complete the installation ## testing ### opening files 1. right-click on any file from the windows file explorer 2. select ""open with code"" 3. verify vs code opens with the selected file ### opening folders 1. right-click on a folder from the windows file explorer 2. select ""open with code"" 3. verify vs code opens with the folder as the workspace ### run as administrator scenarios > refs: 1. navigate to vs code installation folder (e.g., or ) 2. right-click and select ""properties"" 3. click on the ""compatibility"" tab 4. check ""run this program as an administrator"" 5. click ""ok"" to save 6. navigate to a folder from the windows file explorer 7. right-click on the folder and select ""open with code"" 8. verify a uac (user account control) elevation prompt appears 9. click ""yes"" on the uac prompt 10. verify vs code opens correctly with administrator privileges, you should see in the window title 11. right-click on a file and select ""open with code"" 12. verify a uac prompt appears and vs code opens correctly after approval ### run as administrator - uac denied 1. configure vs code to run as administrator (as above) 2. right-click on a folder and select ""open with code"" 3. when the uac prompt appears, click ""no"" 4. verify vs code does not open (expected behavior - access denied) ### uninstallation 1. uninstall vs code 2. open the windows file explorer and right-click on any file 3. verify ""open with code"" no longer appears in the modern context menu 4. right-click on any folder 5. verify ""open with code"" no longer appears in the modern context menu ### upgrade scenarios 1. install a previous version of vs code from [here]( 2. verify context menu entry appears under ""show more options"" menu 3. upgrade to the new version, use command from within vs code 4. verify ""open with code"" now appears in the top level context menu 5. verify old context menu entries are cleaned up (no duplicates)",1.6,Low,0.584,localized low-impact
microsoft/vscode#290688,pre-release toggle doesn't install pre-release version when switching from release,"> **note**: this bug report was generated with assistance from ai (github copilot cli). ## summary when a user unchecks the pre-release checkbox (installing the release version), then re-checks it, vs code does not install the pre-release version. the extension stays on the release version with a pre-release badge. this was the root cause of a confusing user experience where i was seeing errors in copilot chat - i was unknowingly on an older release version (0.36.2) that was incompatible with my vs code insiders build, even though the ui showed the pre-release badge. ## steps to reproduce 1. install an extension with pre-release enabled (e.g., version ) 2. uncheck the ""pre-release"" checkbox in the extensions view 3. extension updates to the release version (e.g., ) 4. re-check the ""pre-release"" checkbox 5. **expected**: extension should update to the latest pre-release version ( ) 6. **actual**: extension stays on release version ( ) but displays the pre-release badge ## root cause analysis the bug is in . when (line 2799) calls with , the install method at line 2458-2464 takes the branch: the problem: 1. is set to - the **cached** gallery extension (the release version just installed) 2. is only created when is set 3. since doesn't set a version, remains 4. at line 2471-2473, the code only fetches a new gallery extension if is set 5. the install proceeds with the **wrong (release) version** from the cache ## suggested fix add a check for when differs from the cached gallery version's pre-release status: ## environment - vs code: 1.109.0-insider - extension: github.copilot-chat - release version: 0.36.2 - pre-release version: 0.37.2026012701 ## related code locations - - : lines 2799-2808 - : lines 2439-2530 (specifically 2458-2474)",[],['BUG'],"['bug', 'extensions']",github,2026-01-27T06:47:30Z,,"pre-release toggle doesn't install pre-release version when switching from release > **note**: this bug report was generated with assistance from ai (github copilot cli). ## summary when a user unchecks the pre-release checkbox (installing the release version), then re-checks it, vs code does not install the pre-release version. the extension stays on the release version with a pre-release badge. this was the root cause of a confusing user experience where i was seeing errors in copilot chat - i was unknowingly on an older release version (0.36.2) that was incompatible with my vs code insiders build, even though the ui showed the pre-release badge. ## steps to reproduce 1. install an extension with pre-release enabled (e.g., version ) 2. uncheck the ""pre-release"" checkbox in the extensions view 3. extension updates to the release version (e.g., ) 4. re-check the ""pre-release"" checkbox 5. **expected**: extension should update to the latest pre-release version ( ) 6. **actual**: extension stays on release version ( ) but displays the pre-release badge ## root cause analysis the bug is in . when (line 2799) calls with , the install method at line 2458-2464 takes the branch: the problem: 1. is set to - the **cached** gallery extension (the release version just installed) 2. is only created when is set 3. since doesn't set a version, remains 4. at line 2471-2473, the code only fetches a new gallery extension if is set 5. the install proceeds with the **wrong (release) version** from the cache ## suggested fix add a check for when differs from the cached gallery version's pre-release status: ## environment - vs code: 1.109.0-insider - extension: github.copilot-chat - release version: 0.36.2 - pre-release version: 0.37.2026012701 ## related code locations - - : lines 2799-2808 - : lines 2439-2530 (specifically 2458-2474)",2.383,Medium,0.762,functional impact
cilium/cilium#44026,egress gateway is broken,"### is there an existing issue for this? - [x] i have searched the existing issues ### version equal or higher than v1.18.6 and lower than v1.19.0 ### what happened? we updated our cilium helm chart from 1.17.6 to 1.18.6 and cilium egress gateway seems to be broken. we also tested on 1.18.5 and it's still broken. we're working on a gke cluster. ### how can we reproduce the issue? 1. install cilium version 1.18.6 with helm with the following values 2. create relevant forwarding rules and target instances for egress gateway to work. 3. create a like the following: 4. create a namespace with label and run a pod in it. 5. from the pod, run the following: in version 1.17.6, the response is the egress ip, as expecrted. in version 1.18.6, this is the response: ### cilium version v1.18.6 ### kernel version 6.6.113+ ### kubernetes version v1.33.5 ### regression it worked fine on 1.17.6 ### sysdump _no response_ ### relevant log output ### anything else? hubble ui shows traffic as (unlike the hubble cli), with traffic direction ### cilium users document - [x] are you a user of cilium? please add yourself to the [users doc]( ### code of conduct - [x] i agree to follow this project's code of conduct",[],"['BUG', 'FEATURE']","['kind/bug', 'kind/community-report', 'kind/regression', 'feature/egress-gateway', 'info-completed']",github,2026-01-27T06:48:03Z,,"egress gateway is broken ### is there an existing issue for this? - [x] i have searched the existing issues ### version equal or higher than v1.18.6 and lower than v1.19.0 ### what happened? we updated our cilium helm chart from 1.17.6 to 1.18.6 and cilium egress gateway seems to be broken. we also tested on 1.18.5 and it's still broken. we're working on a gke cluster. ### how can we reproduce the issue? 1. install cilium version 1.18.6 with helm with the following values 2. create relevant forwarding rules and target instances for egress gateway to work. 3. create a like the following: 4. create a namespace with label and run a pod in it. 5. from the pod, run the following: in version 1.17.6, the response is the egress ip, as expecrted. in version 1.18.6, this is the response: ### cilium version v1.18.6 ### kernel version 6.6.113+ ### kubernetes version v1.33.5 ### regression it worked fine on 1.17.6 ### sysdump _no response_ ### relevant log output ### anything else? hubble ui shows traffic as (unlike the hubble cli), with traffic direction ### cilium users document - [x] are you a user of cilium? please add yourself to the [users doc]( ### code of conduct - [x] i agree to follow this project's code of conduct",3.037,High,0.91,crash-like behavior
kubernetes/kubernetes#136555,support to kill pod by lastest when changed.,"### what would you like to be added? support to kill pod by lastest when changed. ### why is this needed? support to change with per deletion request. such as follow case: 1. t0: pod is deleted with , it may set 3000s. 2. t1: hope to exit quicky with 10s, then delete pod with . the of pod would be changed to 10, but kubelet do not care this, pod still would exit with 3000s. this feature hopes that support kill pod by lastest when changed.",[],['FEATURE'],"['sig/node', 'kind/feature', 'needs-triage']",github,2026-01-27T07:08:25Z,2026-01-27T14:29:58Z,"support to kill pod by lastest when changed. ### what would you like to be added? support to kill pod by lastest when changed. ### why is this needed? support to change with per deletion request. such as follow case: 1. t0: pod is deleted with , it may set 3000s. 2. t1: hope to exit quicky with 10s, then delete pod with . the of pod would be changed to 10, but kubelet do not care this, pod still would exit with 3000s. this feature hopes that support kill pod by lastest when changed.",3.6,Critical,1.0,crash-like behavior
cockroachdb/cockroach#161840,roachtest: disagg-rebalance/aws/n4cpu4 failed,roachtest.disagg-rebalance/aws/n4cpu4 [failed]( with [artifacts]( on release-26.1.0-rc @ [6c9b2f778bd29895383d493a2fa4490323499ecf]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21027402-1769497959-01-n4cpu4-0001 | 3.148.184.211 | 10.12.10.222 | | teamcity-21027402-1769497959-01-n4cpu4-0002 | 3.147.126.123 | 10.12.9.235 | | teamcity-21027402-1769497959-01-n4cpu4-0003 | 3.149.254.236 | 10.12.0.49 | | teamcity-21027402-1769497959-01-n4cpu4-0004 | 3.14.131.93 | 10.12.4.243 | parameters: - arch=arm64 - cloud=aws - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - metamorphicwritebuffering=true - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( _grafana is not yet available for aws clusters_ same failure on other branches - roachtest: disagg-rebalance/aws/n4cpu4 failed [a-storage c-test-failure o-roachtest o-robot t-storage branch-release-26.1] /cc /storage [this test on roachdash]( | [improve this report!]( jira issue: crdb-59106,[],['TESTING'],"['C-test-failure', 'O-robot', 'A-storage', 'O-roachtest', 'T-storage', 'branch-release-26.1.0-rc']",github,2026-01-27T07:16:21Z,,roachtest: disagg-rebalance/aws/n4cpu4 failed roachtest.disagg-rebalance/aws/n4cpu4 [failed]( with [artifacts]( on release-26.1.0-rc @ [6c9b2f778bd29895383d493a2fa4490323499ecf]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21027402-1769497959-01-n4cpu4-0001 | 3.148.184.211 | 10.12.10.222 | | teamcity-21027402-1769497959-01-n4cpu4-0002 | 3.147.126.123 | 10.12.9.235 | | teamcity-21027402-1769497959-01-n4cpu4-0003 | 3.149.254.236 | 10.12.0.49 | | teamcity-21027402-1769497959-01-n4cpu4-0004 | 3.14.131.93 | 10.12.4.243 | parameters: - arch=arm64 - cloud=aws - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - metamorphicwritebuffering=true - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( _grafana is not yet available for aws clusters_ same failure on other branches - roachtest: disagg-rebalance/aws/n4cpu4 failed [a-storage c-test-failure o-roachtest o-robot t-storage branch-release-26.1] /cc /storage [this test on roachdash]( | [improve this report!]( jira issue: crdb-59106,2.862,Medium,0.871,functional impact
cockroachdb/cockroach#161841,roachtest: db-console/cypress-pages failed,"**note:** this build has runtime assertions enabled. if the same failure was hit in a run without assertions enabled, there should be a similar failure without this message. if there isn& ;t one, then this failure is likely due to an assertion violation or (assertion) timeout. roachtest.db-console/cypress-pages [failed]( with [artifacts]( on master @ [5b0bfce304d2475a3ede5c8f02cfc381347fcab9]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21027247-1769496440-13-n4cpu4-0001 | 20.106.179.171 | 10.1.0.100 | | teamcity-21027247-1769496440-13-n4cpu4-0002 | 20.55.80.120 | 10.1.0.10 | | teamcity-21027247-1769496440-13-n4cpu4-0003 | 40.76.240.24 | 10.1.0.101 | | teamcity-21027247-1769496440-13-n4cpu4-0004 | 52.188.226.206 | 10.1.0.132 | parameters: - arch=amd64 - cloud=azure - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - metamorphicwritebuffering=true - runtimeassertionsbuild=true help see: [roachtest readme]( see: [how to investigate \(internal\)]( _grafana is not yet available for azure clusters_ /cc /obs-prs [this test on roachdash]( | [improve this report!]( jira issue: crdb-59107",[],['TESTING'],"['C-test-failure', 'O-robot', 'O-roachtest', 'branch-master', 'T-observability', 'B-runtime-assertions-enabled']",github,2026-01-27T07:16:38Z,,"roachtest: db-console/cypress-pages failed **note:** this build has runtime assertions enabled. if the same failure was hit in a run without assertions enabled, there should be a similar failure without this message. if there isn& ;t one, then this failure is likely due to an assertion violation or (assertion) timeout. roachtest.db-console/cypress-pages [failed]( with [artifacts]( on master @ [5b0bfce304d2475a3ede5c8f02cfc381347fcab9]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21027247-1769496440-13-n4cpu4-0001 | 20.106.179.171 | 10.1.0.100 | | teamcity-21027247-1769496440-13-n4cpu4-0002 | 20.55.80.120 | 10.1.0.10 | | teamcity-21027247-1769496440-13-n4cpu4-0003 | 40.76.240.24 | 10.1.0.101 | | teamcity-21027247-1769496440-13-n4cpu4-0004 | 52.188.226.206 | 10.1.0.132 | parameters: - arch=amd64 - cloud=azure - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - metamorphicwritebuffering=true - runtimeassertionsbuild=true help see: [roachtest readme]( see: [how to investigate \(internal\)]( _grafana is not yet available for azure clusters_ /cc /obs-prs [this test on roachdash]( | [improve this report!]( jira issue: crdb-59107",2.994,Medium,0.901,functional impact
cockroachdb/cockroach#161842,kv/kvserver: teststorerangemergestats failed,kv/kvserver.teststorerangemergestats [failed]( on release-25.3 @ [0f522d4f3ba1b8b1129df47e1c38ae3fbc9e4e64]( parameters: - attempt=1 - run=10 - shard=45 help see also: [how to investigate a go test failure \(internal\)]( same failure on other branches - kv/kvserver: teststorerangemergestats failed [a-testing c-bug c-test-failure o-robot p-3 t-kv branch-release-25.3.7-rc] /cc /kv-triage [this test on roachdash]( | [improve this report!]( jira issue: crdb-59108,[],"['TESTING', 'BUG']","['C-bug', 'C-test-failure', 'O-robot', 'X-duplicate', 'A-testing', 'T-kv', 'branch-release-25.3']",github,2026-01-27T07:28:13Z,2026-01-27T15:28:27Z,kv/kvserver: teststorerangemergestats failed kv/kvserver.teststorerangemergestats [failed]( on release-25.3 @ [0f522d4f3ba1b8b1129df47e1c38ae3fbc9e4e64]( parameters: - attempt=1 - run=10 - shard=45 help see also: [how to investigate a go test failure \(internal\)]( same failure on other branches - kv/kvserver: teststorerangemergestats failed [a-testing c-bug c-test-failure o-robot p-3 t-kv branch-release-25.3.7-rc] /cc /kv-triage [this test on roachdash]( | [improve this report!]( jira issue: crdb-59108,2.0,Medium,0.675,localized low-impact
cockroachdb/cockroach#161843,roachtest: backup/ceph/squid/plain failed,roachtest.backup/ceph/squid/plain [failed]( with [artifacts]( on release-26.1.0-rc @ [6c9b2f778bd29895383d493a2fa4490323499ecf]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21027401-1769498110-12-n4cpu4-0001 | 35.196.164.101 | 10.142.3.21 | | teamcity-21027401-1769498110-12-n4cpu4-0002 | 35.227.8.32 | 10.142.3.22 | | teamcity-21027401-1769498110-12-n4cpu4-0003 | 34.148.24.236 | 10.142.3.14 | | teamcity-21027401-1769498110-12-n4cpu4-0004 | 34.148.74.107 | 10.142.3.47 | parameters: - arch=fips - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=true - metamorphicleases=default - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( same failure on other branches - roachtest: backup/ceph/squid/plain failed [b-runtime-assertions-enabled c-test-failure o-roachtest o-robot t-field-eng branch-master] /cc /field-engineering [this test on roachdash]( | [improve this report!]( jira issue: crdb-59109,[],['TESTING'],"['C-test-failure', 'O-robot', 'O-roachtest', 'T-field-eng', 'branch-release-26.1.0-rc']",github,2026-01-27T07:30:03Z,,roachtest: backup/ceph/squid/plain failed roachtest.backup/ceph/squid/plain [failed]( with [artifacts]( on release-26.1.0-rc @ [6c9b2f778bd29895383d493a2fa4490323499ecf]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21027401-1769498110-12-n4cpu4-0001 | 35.196.164.101 | 10.142.3.21 | | teamcity-21027401-1769498110-12-n4cpu4-0002 | 35.227.8.32 | 10.142.3.22 | | teamcity-21027401-1769498110-12-n4cpu4-0003 | 34.148.24.236 | 10.142.3.14 | | teamcity-21027401-1769498110-12-n4cpu4-0004 | 34.148.74.107 | 10.142.3.47 | parameters: - arch=fips - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=true - metamorphicleases=default - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( same failure on other branches - roachtest: backup/ceph/squid/plain failed [b-runtime-assertions-enabled c-test-failure o-roachtest o-robot t-field-eng branch-master] /cc /field-engineering [this test on roachdash]( | [improve this report!]( jira issue: crdb-59109,3.132,High,0.932,functional impact
cockroachdb/cockroach#161844,ccl/multiregionccl: testmultiregiondatadriven_regional_by_table failed,ccl/multiregionccl.testmultiregiondatadriven_regional_by_table [failed]( on release-26.1 @ [be336920639819feab95e8fc2620ba50e2c71bbf]( parameters: - attempt=1 - run=4 - shard=4 help see also: [how to investigate a go test failure \(internal\)]( /cc /sql-foundations [this test on roachdash]( | [improve this report!]( jira issue: crdb-59110,[],['TESTING'],"['C-test-failure', 'O-robot', 'T-sql-foundations', 'P-2', 'branch-release-26.1']",github,2026-01-27T07:36:27Z,,ccl/multiregionccl: testmultiregiondatadriven_regional_by_table failed ccl/multiregionccl.testmultiregiondatadriven_regional_by_table [failed]( on release-26.1 @ [be336920639819feab95e8fc2620ba50e2c71bbf]( parameters: - attempt=1 - run=4 - shard=4 help see also: [how to investigate a go test failure \(internal\)]( /cc /sql-foundations [this test on roachdash]( | [improve this report!]( jira issue: crdb-59110,1.6,Low,0.584,localized low-impact
kubernetes/kubernetes#136556,"after the node reboots, many pods enter the containerstatusunknown state","### what happened? after a node reboot, many pods managed by the deployment enter the containerstatusunknown state across the cluster. ### what did you expect to happen? we want to avoid a large number of pods entering the containerstatusunknown state after a node reboot. ### how can we reproduce it (as minimally and precisely as possible)? powering off all cluster nodes and then powering them back on may reproduce this issue. ### anything else we need to know? _no response_ ### kubernetes version client version: v1.28.13 server version: v1.28.13 ### cloud provider ### os version ### install tools ### container runtime (cri) and version (if applicable) ### related plugins (cni, csi, ...) and versions (if applicable)",[],['BUG'],"['kind/bug', 'needs-sig', 'needs-triage']",github,2026-01-27T07:43:11Z,,"after the node reboots, many pods enter the containerstatusunknown state ### what happened? after a node reboot, many pods managed by the deployment enter the containerstatusunknown state across the cluster. ### what did you expect to happen? we want to avoid a large number of pods entering the containerstatusunknown state after a node reboot. ### how can we reproduce it (as minimally and precisely as possible)? powering off all cluster nodes and then powering them back on may reproduce this issue. ### anything else we need to know? _no response_ ### kubernetes version client version: v1.28.13 server version: v1.28.13 ### cloud provider ### os version ### install tools ### container runtime (cri) and version (if applicable) ### related plugins (cni, csi, ...) and versions (if applicable)",2.281,Medium,0.738,functional impact
cockroachdb/cockroach#161845,sql/ttl/ttljob: testrowlevelttljobcancelprivileges failed,sql/ttl/ttljob.testrowlevelttljobcancelprivileges [failed]( on master @ [1278b9dd33172c2e6b23089bb1212684187f0299]( parameters: - attempt=1 - run=1 - shard=1 help see also: [how to investigate a go test failure \(internal\)]( /cc /sql-foundations [this test on roachdash]( | [improve this report!]( jira issue: crdb-59111,[],['TESTING'],"['C-test-failure', 'O-robot', 'branch-master', 'T-sql-foundations', 'P-2']",github,2026-01-27T07:43:26Z,,sql/ttl/ttljob: testrowlevelttljobcancelprivileges failed sql/ttl/ttljob.testrowlevelttljobcancelprivileges [failed]( on master @ [1278b9dd33172c2e6b23089bb1212684187f0299]( parameters: - attempt=1 - run=1 - shard=1 help see also: [how to investigate a go test failure \(internal\)]( /cc /sql-foundations [this test on roachdash]( | [improve this report!]( jira issue: crdb-59111,1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161846,roachtest: import/nodeshutdown/worker/distmerge=true/nodes=4 failed,roachtest.import/nodeshutdown/worker/distmerge=true/nodes=4 [failed]( with [artifacts]( on master @ [5b0bfce304d2475a3ede5c8f02cfc381347fcab9]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21027236-1769498019-03-n4cpu4-0001 | 35.227.114.226 | 10.142.2.45 | | teamcity-21027236-1769498019-03-n4cpu4-0002 | 34.138.119.224 | 10.142.2.41 | | teamcity-21027236-1769498019-03-n4cpu4-0003 | 35.190.178.12 | 10.142.2.52 | | teamcity-21027236-1769498019-03-n4cpu4-0004 | 34.74.191.73 | 10.142.2.48 | parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - metamorphicleases=default - metamorphicwritebuffering=true - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( [this test on roachdash]( | [improve this report!]( jira issue: crdb-59112,[],['TESTING'],"['C-test-failure', 'O-robot', 'O-roachtest', 'branch-master', 'release-blocker', 'T-sql-queries']",github,2026-01-27T07:49:41Z,2026-01-27T19:44:02Z,roachtest: import/nodeshutdown/worker/distmerge=true/nodes=4 failed roachtest.import/nodeshutdown/worker/distmerge=true/nodes=4 [failed]( with [artifacts]( on master @ [5b0bfce304d2475a3ede5c8f02cfc381347fcab9]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21027236-1769498019-03-n4cpu4-0001 | 35.227.114.226 | 10.142.2.45 | | teamcity-21027236-1769498019-03-n4cpu4-0002 | 34.138.119.224 | 10.142.2.41 | | teamcity-21027236-1769498019-03-n4cpu4-0003 | 35.190.178.12 | 10.142.2.52 | | teamcity-21027236-1769498019-03-n4cpu4-0004 | 34.74.191.73 | 10.142.2.48 | parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - metamorphicleases=default - metamorphicwritebuffering=true - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( [this test on roachdash]( | [improve this report!]( jira issue: crdb-59112,3.134,High,0.932,functional impact
docker/docker#51942,does not expose mig devices in docker 29.2.0,"### description after upgrading from docker 29.1.5 to 29.2.0, no longer exposes mig devices to containers. only the physical gpu is visible. ### reproduce #### environment - docker version: 29.2.0 (rootless mode) - os: ubuntu 24.04.3 lts - kernel: 6.14.0-37-generic - nvidia driver: 580.95.05 - gpu: nvidia rtx pro 6000 blackwell workstation edition - mig configuration: 2x 2g.48gb instances enabled - nvidia container toolkit: 1.18.2 #### steps to reproduce 1. enable mig: 2. verify mig devices on host: 3. run container with : only physical gpu is visible, mig devices are missing. ### expected behavior mig devices are visible inside the container: rolling back to docker 29.1.5 resolves the issue. ### docker version ### docker info ### additional info explicitly specifying mig uuid works correctly:",[],['BUG'],"['status/0-triage', 'kind/bug', 'area/cdi', 'version/29.2']",github,2026-01-27T07:50:40Z,,"does not expose mig devices in docker 29.2.0 ### description after upgrading from docker 29.1.5 to 29.2.0, no longer exposes mig devices to containers. only the physical gpu is visible. ### reproduce #### environment - docker version: 29.2.0 (rootless mode) - os: ubuntu 24.04.3 lts - kernel: 6.14.0-37-generic - nvidia driver: 580.95.05 - gpu: nvidia rtx pro 6000 blackwell workstation edition - mig configuration: 2x 2g.48gb instances enabled - nvidia container toolkit: 1.18.2 #### steps to reproduce 1. enable mig: 2. verify mig devices on host: 3. run container with : only physical gpu is visible, mig devices are missing. ### expected behavior mig devices are visible inside the container: rolling back to docker 29.1.5 resolves the issue. ### docker version ### docker info ### additional info explicitly specifying mig uuid works correctly:",4.6,Critical,1.0,crash-like behavior
python/cpython#144264,optimize base64 decoding with ignored characters,"base64 decoding was optimized in . but the optimization only worked to the first ignored character. the proposed pr makes it used even if there are some ignored characters (e.g. multiline data). examples: for comparison, base64 decoding without ignored characters: ### linked prs * gh-144265",[],['PERFORMANCE'],"['performance', 'extension-modules', '3.15']",github,2026-01-27T07:53:19Z,,"optimize base64 decoding with ignored characters base64 decoding was optimized in . but the optimization only worked to the first ignored character. the proposed pr makes it used even if there are some ignored characters (e.g. multiline data). examples: for comparison, base64 decoding without ignored characters: ### linked prs * gh-144265",3.183,High,0.943,performance degradation
microsoft/vscode#290699,vscode insiders - windows 11 context menu still missing (system installer),"does this issue occur when all extensions are disabled?: yes/no report issue' dialog can assist with this. --> - vs code version: 1.109.0-insider (system setup) - os version: windows_nt x64 10.0.26100 steps to reproduce: 1. update to latest vscode insiders version within vscode insiders. 2. restart windows after update. 3. right-click in a folder, notice there is no context menu option still. i also tried reinstalling the vscode insiders (system) installation, i ensured the options to add the context menu options to explorer were enabled and completed the installation. to give the best chance of working, i rebooted again and then started vscode. again, right-clicking in a folder shows no context menu option for 'open with vscode' still. i am making a new issue as the previous tracked ones are closed/locked for some reason. - ref: - ref: _**note:** the old context menu entry is also missing. i have no means to open vscode from any right-click menu in windows currently._ ## full about info ## workaround at this point in time, this feature has been broken for nearly 3 years. for those that do not want to continue to wait for a fix, this post contains a workaround that is currently working: i just installed this and it restored the menu option without issue.",[],['BUG'],"['bug', 'windows', 'system-context-menu']",github,2026-01-27T07:55:16Z,,"vscode insiders - windows 11 context menu still missing (system installer) does this issue occur when all extensions are disabled?: yes/no report issue' dialog can assist with this. --> - vs code version: 1.109.0-insider (system setup) - os version: windows_nt x64 10.0.26100 steps to reproduce: 1. update to latest vscode insiders version within vscode insiders. 2. restart windows after update. 3. right-click in a folder, notice there is no context menu option still. i also tried reinstalling the vscode insiders (system) installation, i ensured the options to add the context menu options to explorer were enabled and completed the installation. to give the best chance of working, i rebooted again and then started vscode. again, right-clicking in a folder shows no context menu option for 'open with vscode' still. i am making a new issue as the previous tracked ones are closed/locked for some reason. - ref: - ref: _**note:** the old context menu entry is also missing. i have no means to open vscode from any right-click menu in windows currently._ ## full about info ## workaround at this point in time, this feature has been broken for nearly 3 years. for those that do not want to continue to wait for a fix, this post contains a workaround that is currently working: i just installed this and it restored the menu option without issue.",2.684,Medium,0.83,functional impact
cockroachdb/cockroach#161847,roachtest: import/pause/distmerge=true/nodes=4 failed,roachtest.import/pause/distmerge=true/nodes=4 [failed]( with [artifacts]( on master @ [5b0bfce304d2475a3ede5c8f02cfc381347fcab9]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21027236-1769498019-08-n4cpu4-0001 | 34.73.85.62 | 10.142.2.38 | | teamcity-21027236-1769498019-08-n4cpu4-0002 | 34.74.175.80 | 10.142.2.35 | | teamcity-21027236-1769498019-08-n4cpu4-0003 | 34.26.246.169 | 10.142.2.33 | | teamcity-21027236-1769498019-08-n4cpu4-0004 | 35.237.14.113 | 10.142.2.34 | parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=true - metamorphicleases=leader - metamorphicwritebuffering=true - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( [this test on roachdash]( | [improve this report!]( jira issue: crdb-59113,[],['TESTING'],"['C-test-failure', 'O-robot', 'O-roachtest', 'branch-master', 'release-blocker', 'T-sql-queries']",github,2026-01-27T08:04:25Z,2026-01-27T19:49:14Z,roachtest: import/pause/distmerge=true/nodes=4 failed roachtest.import/pause/distmerge=true/nodes=4 [failed]( with [artifacts]( on master @ [5b0bfce304d2475a3ede5c8f02cfc381347fcab9]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21027236-1769498019-08-n4cpu4-0001 | 34.73.85.62 | 10.142.2.38 | | teamcity-21027236-1769498019-08-n4cpu4-0002 | 34.74.175.80 | 10.142.2.35 | | teamcity-21027236-1769498019-08-n4cpu4-0003 | 34.26.246.169 | 10.142.2.33 | | teamcity-21027236-1769498019-08-n4cpu4-0004 | 35.237.14.113 | 10.142.2.34 | parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=true - metamorphicleases=leader - metamorphicwritebuffering=true - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( [this test on roachdash]( | [improve this report!]( jira issue: crdb-59113,3.041,High,0.911,functional impact
microsoft/vscode#290702,"empty window: picking ""background"" adds a repository right away","steps to reproduce: 1. new empty window 2. pick ""background"" from the new chat dropdown => üêõ a repo is added to my scm i think we just decide to pick the first repository when opening a new background session: maybe a better solution would be to ask the user to pick a repo first?",[],['BUG'],"['bug', 'chat-background-agent']",github,2026-01-27T08:04:46Z,2026-01-28T03:13:39Z,"empty window: picking ""background"" adds a repository right away steps to reproduce: 1. new empty window 2. pick ""background"" from the new chat dropdown => üêõ a repo is added to my scm i think we just decide to pick the first repository when opening a new background session: maybe a better solution would be to ask the user to pick a repo first?",2.552,Medium,0.8,functional impact
rust-lang/rust#151728,sorting boxes causes miri to report ub,this issue was discovered by on [zulip]( the following code causes miri to report ub: miri error ### meta reproducible on the playground with miri version,[],['BUG'],"['regression-from-stable-to-stable', 'I-unsound', 'C-bug', 'I-prioritize', 'T-libs', 'T-opsem', 'A-box']",github,2026-01-27T08:11:11Z,,sorting boxes causes miri to report ub this issue was discovered by on [zulip]( the following code causes miri to report ub: miri error ### meta reproducible on the playground with miri version,2.589,Medium,0.809,functional impact
microsoft/vscode#290706,git - multiple resource trust dialogs when opening a repository with worktrees,steps to reproduce: 1. have a git repository that has multiple worktrees 2. ensure that the repository folder is trusted 3. ensure that the folder containing the worktrees is not trusted 4. open the git repository folder in vs code expected: no resource trust dialog is shown actual: a resource trust dialog is shown for each worktree,[],['BUG'],"['bug', 'git', 'insiders-released']",github,2026-01-27T08:11:41Z,2026-01-27T08:32:20Z,git - multiple resource trust dialogs when opening a repository with worktrees steps to reproduce: 1. have a git repository that has multiple worktrees 2. ensure that the repository folder is trusted 3. ensure that the folder containing the worktrees is not trusted 4. open the git repository folder in vs code expected: no resource trust dialog is shown actual: a resource trust dialog is shown for each worktree,4.2,Critical,1.0,system-wide impact
microsoft/vscode#290708,"if multiple instances of vscode are opened for different git worktrees, the git editor is opened in the main worktree vscode instance","does this issue occur when all extensions are disabled?: yes report issue' dialog can assist with this. --> - vs code version: - os version: if multiple instances of vscode are opened for different git worktrees, the git editor is opened in the main worktree vscode instance instead. steps to reproduce: 1. have a local git repository with multiple worktrees, e.g. and 2. have a vscode instance opened in each worktree directory 3. try to or in the vscode instance of the not main worktree, e.g. 4. **expected**: the git editor for git commit message or git rebase todo is opened in that worktrees vscode instance. **actual**: the git editor is opened in the main worktree's vscode recorded misbehaviour (no extensions enabled) as gifs: commit ![image]( rebase ![image](",[],['BUG'],"['bug', 'git']",github,2026-01-27T08:27:24Z,,"if multiple instances of vscode are opened for different git worktrees, the git editor is opened in the main worktree vscode instance does this issue occur when all extensions are disabled?: yes report issue' dialog can assist with this. --> - vs code version: - os version: if multiple instances of vscode are opened for different git worktrees, the git editor is opened in the main worktree vscode instance instead. steps to reproduce: 1. have a local git repository with multiple worktrees, e.g. and 2. have a vscode instance opened in each worktree directory 3. try to or in the vscode instance of the not main worktree, e.g. 4. **expected**: the git editor for git commit message or git rebase todo is opened in that worktrees vscode instance. **actual**: the git editor is opened in the main worktree's vscode recorded misbehaviour (no extensions enabled) as gifs: commit ![image]( rebase ![image](",2.409,Medium,0.767,functional impact
openssl/openssl#29781,provider compatibility tests against 3.0/3.3 should enable announcing weak curves in tls.,provider compatibility tests against 3.0/3.3 should enable announcing weak curves in tls. see as example,[],['BUG'],"['branch: master', 'triaged: bug', 'severity: urgent']",github,2026-01-27T08:41:10Z,,provider compatibility tests against 3.0/3.3 should enable announcing weak curves in tls. provider compatibility tests against 3.0/3.3 should enable announcing weak curves in tls. see as example,2.429,Medium,0.772,functional impact
rust-lang/rust#151730,[ice]: builtin derive created an unaligned reference,"<!-- thank you for finding an internal compiler error! üßä if possible, try to provide a minimal verifiable example. you can read ""rust bug minimization patterns"" for how to create smaller examples. --> ### code [playground]( ### meta <!-- if you're using the stable version of the compiler, you should also check if the bug also exists in the beta or nightly versions. --> : but on linux and the current most recent nightly too. ### error output <!-- include a backtrace in the code block by setting in your environment. e.g. . --> backtrace from the playground:",[],['BUG'],"['I-ICE', 'T-compiler', 'C-bug']",github,2026-01-27T08:42:00Z,2026-01-27T10:43:29Z,"[ice]: builtin derive created an unaligned reference <!-- thank you for finding an internal compiler error! üßä if possible, try to provide a minimal verifiable example. you can read ""rust bug minimization patterns"" for how to create smaller examples. --> ### code [playground]( ### meta <!-- if you're using the stable version of the compiler, you should also check if the bug also exists in the beta or nightly versions. --> : but on linux and the current most recent nightly too. ### error output <!-- include a backtrace in the code block by setting in your environment. e.g. . --> backtrace from the playground:",2.595,Medium,0.81,functional impact
cilium/cilium#44027,cfp: scale-to-zero for cilium services,"## cilium feature proposal **is your proposed feature related to a problem?** kubernetes clusters often run workloads that sit idle most of the time. development and staging environments are only active during work hours. internal tools like admin dashboards get used sporadically. preview environments exist for every pr but rarely receive traffic. these workloads consume resources 24/7 but handle traffic for a fraction of that time. in cloud environments this translates directly to wasted cost. existing solutions fall short. keda requires external infrastructure and polling-based detection adds latency. knative has a heavy footprint and only works with http. hpa cannot scale below one replica. cronjobs work on schedules, not actual traffic patterns. **describe the feature you'd like** i would like cilium to automatically scale down idle deployments to zero replicas and scale them back up when traffic arrives. the feature should be opt-in through service annotations like: when a service has no traffic for the configured idle period, cilium should scale its backing deployment to zero. when new traffic arrives, cilium should scale the deployment back up and route traffic once the pod is ready. **notify relevant community channels** - /sig-lb - /sig-datapath - /operator **(optional) describe your proposed solution** cilium is in a unique position to implement this. bpf programs sit in the kernel data path and see every packet before it reaches any pod. this enables instant traffic detection with no polling, works with tcp/udp/grpc and other protocols, and requires no sidecars or external dependencies. #### the high-level approach: the operator would make all scaling decisions since it already has leader election and cluster-wide visibility. it watches services with the annotation enabled, tracks idle time, and scales deployments accordingly. when scaling down, it updates a state annotation on the service. agents on each node watch for the state annotation and sync a local bpf map. when traffic arrives for a scaled-down service, bpf intercepts the packet, emits a perf event, and drops the packet so the client retries. the agent notifies the operator which then scales the deployment back up. once the pod passes readiness, the operator updates the annotation and agents remove the service from the bpf map. this approach avoids new crds by using service annotations for state. it avoids kvstore dependencies since most deployments do not use one. it centralizes decisions in the operator to handle the multi-node case correctly, where different nodes might see different connection counts and reach wrong conclusions about idle state. i put together a rough proof of concept that runs on a single node. happy to discuss this further and if the design gets approved, i would be glad to work on the implementation.",[],['FEATURE'],"['kind/feature', 'kind/cfp']",github,2026-01-27T08:45:42Z,,"cfp: scale-to-zero for cilium services ## cilium feature proposal **is your proposed feature related to a problem?** kubernetes clusters often run workloads that sit idle most of the time. development and staging environments are only active during work hours. internal tools like admin dashboards get used sporadically. preview environments exist for every pr but rarely receive traffic. these workloads consume resources 24/7 but handle traffic for a fraction of that time. in cloud environments this translates directly to wasted cost. existing solutions fall short. keda requires external infrastructure and polling-based detection adds latency. knative has a heavy footprint and only works with http. hpa cannot scale below one replica. cronjobs work on schedules, not actual traffic patterns. **describe the feature you'd like** i would like cilium to automatically scale down idle deployments to zero replicas and scale them back up when traffic arrives. the feature should be opt-in through service annotations like: when a service has no traffic for the configured idle period, cilium should scale its backing deployment to zero. when new traffic arrives, cilium should scale the deployment back up and route traffic once the pod is ready. **notify relevant community channels** - /sig-lb - /sig-datapath - /operator **(optional) describe your proposed solution** cilium is in a unique position to implement this. bpf programs sit in the kernel data path and see every packet before it reaches any pod. this enables instant traffic detection with no polling, works with tcp/udp/grpc and other protocols, and requires no sidecars or external dependencies. #### the high-level approach: the operator would make all scaling decisions since it already has leader election and cluster-wide visibility. it watches services with the annotation enabled, tracks idle time, and scales deployments accordingly. when scaling down, it updates a state annotation on the service. agents on each node watch for the state annotation and sync a local bpf map. when traffic arrives for a scaled-down service, bpf intercepts the packet, emits a perf event, and drops the packet so the client retries. the agent notifies the operator which then scales the deployment back up. once the pod passes readiness, the operator updates the annotation and agents remove the service from the bpf map. this approach avoids new crds by using service annotations for state. it avoids kvstore dependencies since most deployments do not use one. it centralizes decisions in the operator to handle the multi-node case correctly, where different nodes might see different connection counts and reach wrong conclusions about idle state. i put together a rough proof of concept that runs on a single node. happy to discuss this further and if the design gets approved, i would be glad to work on the implementation.",6.8,Critical,1.0,crash-like behavior
prometheus/prometheus#17941,[meta] appendv2 flaky tests,"collection of flaky tests to fix, potentially introduced (or elevated) by appenderv2 work: * [ ] (windows) and * [ ] * [ ]",[],['BUG'],"['help wanted', 'kind/bug']",github,2026-01-27T08:51:35Z,,"[meta] appendv2 flaky tests collection of flaky tests to fix, potentially introduced (or elevated) by appenderv2 work: * [ ] (windows) and * [ ] * [ ]",2.633,Medium,0.818,functional impact
cockroachdb/cockroach#161848,kv/kvserver: testwriteloadstatsaccounting failed,kv/kvserver.testwriteloadstatsaccounting [failed]( on release-25.4.4-rc @ [688cda9084f2924ec4e22044c861aa0a3f34e956]( parameters: - attempt=1 - deadlock=true - run=3 - shard=48 help see also: [how to investigate a go test failure \(internal\)]( same failure on other branches - kv/kvserver: testwriteloadstatsaccounting failed [a-testing c-bug c-test-failure o-robot t-kv x-nostale branch-release-25.4] /cc /kv-triage [this test on roachdash]( | [improve this report!]( jira issue: crdb-59114,[],"['TESTING', 'BUG']","['C-bug', 'C-test-failure', 'O-robot', 'X-duplicate', 'A-testing', 'T-kv', 'X-nostale', 'branch-release-25.4.4-rc']",github,2026-01-27T09:11:29Z,,kv/kvserver: testwriteloadstatsaccounting failed kv/kvserver.testwriteloadstatsaccounting [failed]( on release-25.4.4-rc @ [688cda9084f2924ec4e22044c861aa0a3f34e956]( parameters: - attempt=1 - deadlock=true - run=3 - shard=48 help see also: [how to investigate a go test failure \(internal\)]( same failure on other branches - kv/kvserver: testwriteloadstatsaccounting failed [a-testing c-bug c-test-failure o-robot t-kv x-nostale branch-release-25.4] /cc /kv-triage [this test on roachdash]( | [improve this report!]( jira issue: crdb-59114,3.301,High,0.97,crash-like behavior
microsoft/vscode#290715,zen mode is not using space well while center mode.,"i often switch between, mac 14"" screen and 27 monitor screen. on monitor i use center mode on 14"" i need as much space as i can. and then thinks like isn't well used space and i can't just push the space away and deactivate on this way center mode. a button which appears if zen mode is active, to deactivate center mode, may an option. and if i then go in side bar the space is also not well used. instead that the sidebar is using the empty space, it's just pushing the empty space away. that problem i not just have on 14"" also on 27"", its a general problem. so if i am on mac the center mode is stealing me more then 10% of working space.",[],['UI'],['workbench-zen'],github,2026-01-27T09:12:23Z,,"zen mode is not using space well while center mode. i often switch between, mac 14"" screen and 27 monitor screen. on monitor i use center mode on 14"" i need as much space as i can. and then thinks like isn't well used space and i can't just push the space away and deactivate on this way center mode. a button which appears if zen mode is active, to deactivate center mode, may an option. and if i then go in side bar the space is also not well used. instead that the sidebar is using the empty space, it's just pushing the empty space away. that problem i not just have on 14"" also on 27"", its a general problem. so if i am on mac the center mode is stealing me more then 10% of working space.",1.8,Low,0.629,user-visible issue
vercel/next.js#89090,zlib memory leak node.js 24,"### link to the code that reproduces this issue ### to reproduce 1. use node.js version 24.13.0 2. npm run build 3. node_options=--inspect ./node_modules/.bin/next start 4. 5. call gc & make memory snapshot 6. run snippet in browser console 7. call gc & make memory snapshot 8. compare snapshots and see positive delta ### current vs. expected behavior expect memory to still more or less on the same level ### provide environment information ### which area(s) are affected? (select all that apply) performance ### which stage(s) are affected? (select all that apply) next start (local) ### additional context i tested on 24.0.0, the issue is still there. i tested on lates 22.22.0 and there is no issue with zlib.",[],['PERFORMANCE'],"['Performance', 'invalid link']",github,2026-01-27T09:14:46Z,2026-01-27T09:15:00Z,"zlib memory leak node.js 24 ### link to the code that reproduces this issue ### to reproduce 1. use node.js version 24.13.0 2. npm run build 3. node_options=--inspect ./node_modules/.bin/next start 4. 5. call gc & make memory snapshot 6. run snippet in browser console 7. call gc & make memory snapshot 8. compare snapshots and see positive delta ### current vs. expected behavior expect memory to still more or less on the same level ### provide environment information ### which area(s) are affected? (select all that apply) performance ### which stage(s) are affected? (select all that apply) next start (local) ### additional context i tested on 24.0.0, the issue is still there. i tested on lates 22.22.0 and there is no issue with zlib.",4.6,Critical,1.0,performance degradation
cockroachdb/cockroach#161849,cli: testzipincludeandexcludefilesdatadriven failed,cli.testzipincludeandexcludefilesdatadriven [failed]( on release-25.2.12-rc @ [7fc1e95228ad6610c2ea21d8ba55d0d6513c319f]( parameters: - attempt=1 - deadlock=true - run=2 - shard=8 help see also: [how to investigate a go test failure \(internal\)]( same failure on other branches - cli: testzipincludeandexcludefilesdatadriven failed [c-test-failure o-robot t-observability branch-release-26.1 s390x-test-failure] /cc /obs-prs /server /obs-india-prs [this test on roachdash]( | [improve this report!]( jira issue: crdb-59115,[],['TESTING'],"['C-test-failure', 'O-robot', 'release-blocker', 'T-observability', 'branch-release-25.2.12-rc']",github,2026-01-27T09:14:51Z,,cli: testzipincludeandexcludefilesdatadriven failed cli.testzipincludeandexcludefilesdatadriven [failed]( on release-25.2.12-rc @ [7fc1e95228ad6610c2ea21d8ba55d0d6513c319f]( parameters: - attempt=1 - deadlock=true - run=2 - shard=8 help see also: [how to investigate a go test failure \(internal\)]( same failure on other branches - cli: testzipincludeandexcludefilesdatadriven failed [c-test-failure o-robot t-observability branch-release-26.1 s390x-test-failure] /cc /obs-prs /server /obs-india-prs [this test on roachdash]( | [improve this report!]( jira issue: crdb-59115,3.8,Critical,1.0,crash-like behavior
microsoft/vscode#290716,updating vs code insiders causes insiders to crash while restarting,"on windows when i update vs code it closes all windows but never reopens them, looking at the logs i see the following:",[],['BUG'],"['bug', 'important', 'windows', 'insiders-released', 'papercut :drop_of_blood:']",github,2026-01-27T09:16:31Z,2026-01-27T11:29:39Z,"updating vs code insiders causes insiders to crash while restarting on windows when i update vs code it closes all windows but never reopens them, looking at the logs i see the following:",4.6,Critical,1.0,crash-like behavior
microsoft/vscode#290718,refactoring git branch naming,"type: feature request when i click at branch name on task bar select create new branch and add branch name with spaces then it shows dashes instead of spaces. but if there are mutiple spaces then it shows multiple dashes. maybe we can combine combination of space and dashes and replace it with single dash vs code version: code 1.108.2 (c9d77990917f3102ada88be140d28b038d1dd7c7, 2026-01-21t13:52:09.270z) os version: windows_nt x64 10.0.26200 modes:",[],['FEATURE'],"['feature-request', 'git']",github,2026-01-27T09:18:15Z,,"refactoring git branch naming type: feature request when i click at branch name on task bar select create new branch and add branch name with spaces then it shows dashes instead of spaces. but if there are mutiple spaces then it shows multiple dashes. maybe we can combine combination of space and dashes and replace it with single dash vs code version: code 1.108.2 (c9d77990917f3102ada88be140d28b038d1dd7c7, 2026-01-21t13:52:09.270z) os version: windows_nt x64 10.0.26200 modes:",1.4,Low,0.538,localized low-impact
cockroachdb/cockroach#161850,roachtest: jepsen/multi-register/strobe-skews failed,"**note:** this build has runtime assertions enabled. if the same failure was hit in a run without assertions enabled, there should be a similar failure without this message. if there isn& ;t one, then this failure is likely due to an assertion violation or (assertion) timeout. roachtest.jepsen/multi-register/strobe-skews [failed]( with [artifacts]( on master @ [5b0bfce304d2475a3ede5c8f02cfc381347fcab9]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21027247-1769496440-77-n6cpu4-0001 | 40.76.176.231 | 10.1.0.183 | | teamcity-21027247-1769496440-77-n6cpu4-0002 | 20.127.204.40 | 10.1.0.185 | | teamcity-21027247-1769496440-77-n6cpu4-0003 | 57.151.98.132 | 10.1.0.184 | | teamcity-21027247-1769496440-77-n6cpu4-0004 | 20.106.174.112 | 10.1.0.181 | | teamcity-21027247-1769496440-77-n6cpu4-0005 | 57.151.107.146 | 10.1.0.186 | | teamcity-21027247-1769496440-77-n6cpu4-0006 | 40.90.252.126 | 10.1.0.182 | parameters: - arch=amd64 - cloud=azure - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - fs=ext4 - localssd=true - metamorphicleases=expiration - runtimeassertionsbuild=true help see: [roachtest readme]( see: [how to investigate \(internal\)]( _grafana is not yet available for azure clusters_ /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-59116",[],['TESTING'],"['C-test-failure', 'O-robot', 'O-roachtest', 'branch-master', 'release-blocker', 'T-testeng', 'B-runtime-assertions-enabled']",github,2026-01-27T09:22:52Z,,"roachtest: jepsen/multi-register/strobe-skews failed **note:** this build has runtime assertions enabled. if the same failure was hit in a run without assertions enabled, there should be a similar failure without this message. if there isn& ;t one, then this failure is likely due to an assertion violation or (assertion) timeout. roachtest.jepsen/multi-register/strobe-skews [failed]( with [artifacts]( on master @ [5b0bfce304d2475a3ede5c8f02cfc381347fcab9]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21027247-1769496440-77-n6cpu4-0001 | 40.76.176.231 | 10.1.0.183 | | teamcity-21027247-1769496440-77-n6cpu4-0002 | 20.127.204.40 | 10.1.0.185 | | teamcity-21027247-1769496440-77-n6cpu4-0003 | 57.151.98.132 | 10.1.0.184 | | teamcity-21027247-1769496440-77-n6cpu4-0004 | 20.106.174.112 | 10.1.0.181 | | teamcity-21027247-1769496440-77-n6cpu4-0005 | 57.151.107.146 | 10.1.0.186 | | teamcity-21027247-1769496440-77-n6cpu4-0006 | 40.90.252.126 | 10.1.0.182 | parameters: - arch=amd64 - cloud=azure - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - fs=ext4 - localssd=true - metamorphicleases=expiration - runtimeassertionsbuild=true help see: [roachtest readme]( see: [how to investigate \(internal\)]( _grafana is not yet available for azure clusters_ /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-59116",2.966,Medium,0.894,functional impact
vercel/next.js#89091,zlib memory leak node.js 24,"### link to the code that reproduces this issue ### to reproduce 1. use node.js version 24.13.0 2. npm run build 3. node_options=--inspect ./node_modules/.bin/next start 4. follow 5. call gc & make memory snapshot 6. run snippet in browser console: 7. call gc & make memory snapshot 8. compare snapshots and see node / zlib_memory positive delta ### current vs. expected behavior expect memory to still more or less on the same level ### provide environment information ### which area(s) are affected? (select all that apply) performance ### which stage(s) are affected? (select all that apply) next start (local) ### additional context i tested on 24.0.0 also, the issue is still there. i tested on lates 22.22.0 and there is no issue with zlib.",[],['PERFORMANCE'],['Performance'],github,2026-01-27T09:24:50Z,,"zlib memory leak node.js 24 ### link to the code that reproduces this issue ### to reproduce 1. use node.js version 24.13.0 2. npm run build 3. node_options=--inspect ./node_modules/.bin/next start 4. follow 5. call gc & make memory snapshot 6. run snippet in browser console: 7. call gc & make memory snapshot 8. compare snapshots and see node / zlib_memory positive delta ### current vs. expected behavior expect memory to still more or less on the same level ### provide environment information ### which area(s) are affected? (select all that apply) performance ### which stage(s) are affected? (select all that apply) next start (local) ### additional context i tested on 24.0.0 also, the issue is still there. i tested on lates 22.22.0 and there is no issue with zlib.",4.6,Critical,1.0,performance degradation
containerd/containerd#12828,kernel: overlayfs: idmapped layers are currently not supported (moby-containerd amd64 2.2.1-ubuntu22.04u1),"### description when we run the moby-containerd version 1.7.30-ubuntu22.04u1, it does not produce any errors. however, after updating to version 2.2.1, we encounter the error: ""kernel: overlayfs: idmapped layers are currently not supported."" ### steps to reproduce the issue 1. 2. 3. ### describe the results you received and expected when we run the moby-containerd version 1.7.30-ubuntu22.04u1, it does not produce any errors. however, after updating to version 2.2.1, we encounter the error: ""kernel: overlayfs: idmapped layers are currently not supported."" ### what version of containerd are you using? containerd github.com/containerd/containerd/v2 2.2.1-1 dea7da592f5d1d2b7755e3a161be07f43fad8f75 ### any other relevant information _no response_ ### show configuration if it is related to cri plugin. _no response_",[],['BUG'],"['kind/bug', 'area/runtime']",github,2026-01-27T09:29:09Z,,"kernel: overlayfs: idmapped layers are currently not supported (moby-containerd amd64 2.2.1-ubuntu22.04u1) ### description when we run the moby-containerd version 1.7.30-ubuntu22.04u1, it does not produce any errors. however, after updating to version 2.2.1, we encounter the error: ""kernel: overlayfs: idmapped layers are currently not supported."" ### steps to reproduce the issue 1. 2. 3. ### describe the results you received and expected when we run the moby-containerd version 1.7.30-ubuntu22.04u1, it does not produce any errors. however, after updating to version 2.2.1, we encounter the error: ""kernel: overlayfs: idmapped layers are currently not supported."" ### what version of containerd are you using? containerd github.com/containerd/containerd/v2 2.2.1-1 dea7da592f5d1d2b7755e3a161be07f43fad8f75 ### any other relevant information _no response_ ### show configuration if it is related to cri plugin. _no response_",4.6,Critical,1.0,crash-like behavior
kubernetes/kubernetes#136559,[failing test][sig-network] kubeproxy should set tcp close_wait timeout,"### which jobs are failing? * sig-release-master-blocking * kind-ipv6-master ### which tests are failing? * [kubernetes e2e suite.[it] [sig-network] kubeproxy should set tcp close_wait timeout [privileged]]( ### since when has it been failing? * first failure: sun, 25 jan 2026 03:33:59 utc * latest failure: tue, 27 jan 2026 09:00:01 utc ### testgrid link * [ * [ ### reason for failure (if possible) ### anything else we need to know? _no response_ ### relevant sig(s) /sig network /kind failing-test cc /release-team-release-signal",[],"['NETWORK', 'TESTING']","['sig/network', 'kind/failing-test', 'triage/accepted']",github,2026-01-27T09:32:40Z,,"[failing test][sig-network] kubeproxy should set tcp close_wait timeout ### which jobs are failing? * sig-release-master-blocking * kind-ipv6-master ### which tests are failing? * [kubernetes e2e suite.[it] [sig-network] kubeproxy should set tcp close_wait timeout [privileged]]( ### since when has it been failing? * first failure: sun, 25 jan 2026 03:33:59 utc * latest failure: tue, 27 jan 2026 09:00:01 utc ### testgrid link * [ * [ ### reason for failure (if possible) ### anything else we need to know? _no response_ ### relevant sig(s) /sig network /kind failing-test cc /release-team-release-signal",2.345,Medium,0.753,affects communication layer
microsoft/vscode#290724,agent sessions profile is still around in code,we need some alternative for the splash at least.,[],['BUG'],"['bug', 'chat-agents-window']",github,2026-01-27T09:34:20Z,,agent sessions profile is still around in code we need some alternative for the splash at least.,2.444,Medium,0.775,functional impact
microsoft/vscode#290725,modal dialogs on shutdown for running chat sessions when none is running,"type: bug seeing 3 every time i exit vs code (having several windows open): vs code version: code - insiders 1.109.0-insider (e7a06c8eabf2915e2c383b1ce6d2b993d90e2e92, 2026-01-27t07:16:53.085z) os version: darwin arm64 24.6.0 modes: connection to 'dev-container+7b22686f737450617468223a222f55736572732f6368726d617274692f446576656c6f706d656e742f7265706f732f736d6b747374222c226c6f63616c446f636b6572223a66616c73652c2273657474696e6773223a7b22636f6e74657874223a226465736b746f702d6c696e7578227d2c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a222f55736572732f6368726d617274692f446576656c6f706d656e742f7265706f732f736d6b7473742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f55736572732f6368726d617274692f446576656c6f706d656e742f7265706f732f736d6b7473742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d' could not be established system info |item|value| |---|---| |cpus|apple m1 max (10 x 2400)| |gpu status|2d_canvas: enabled gpu0: vendor= 0x106b [google inc. (apple)], device=0x0000 [angle (apple, angle metal renderer: apple m1 max, version 15.7.3 (build 24g419))], driver_vendor=apple, driver_version=15.7.3 *active* machine model name: macbookpro machine model version: 18.2 direct_rendering_display_compositor: disabled_off_ok gpu_compositing: enabled multiple_raster_threads: enabled_on opengl: enabled_on rasterization: enabled raw_draw: disabled_off_ok skia_graphite: disabled_off trees_in_viz: disabled_off video_decode: enabled video_encode: enabled webgl: enabled webgl2: enabled webgpu: enabled webnn: disabled_off| |load (avg)|197, 152, 114| |memory (system)|64.00gb (2.65gb free)| |process argv|--log trace --log github.copilot-chat=debug --crash-reporter-id 7aad610c-0e6b-48aa-ba87-b704b1b543cf --log trace --log github.copilot-chat=debug --crash-reporter-id 7aad610c-0e6b-48aa-ba87-b704b1b543cf| |screen reader|no| |vm|0%| connection to 'dev-container+7b22686f737450617468223a222f55736572732f6368726d617274692f446576656c6f706d656e742f7265706f732f736d6b747374222c226c6f63616c446f636b6572223a66616c73652c2273657474696e6773223a7b22636f6e74657874223a226465736b746f702d6c696e7578227d2c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a222f55736572732f6368726d617274692f446576656c6f706d656e742f7265706f732f736d6b7473742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f55736572732f6368726d617274692f446576656c6f706d656e742f7265706f732f736d6b7473742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d' could not be established extensions (75) extension|author (truncated)|version ---|---|--- tsl-problem-matcher|amo|0.6.2 claude-code|ant|2.1.20 openmatchingfiles|bca|0.5.4 unique-lines|bib|1.0.0 devcontainer-image-convert|bri|0.0.1 ruff|cha|2026.34.0 network-proxy-test|chr|0.0.22 regex|chr|0.6.0 esbuild-problem-matchers|con|0.0.3 vscode-eslint|dba|3.0.20 docker|doc|0.18.0 kusto|don|0.5.4 prettier-vscode|esb|12.3.0 codespaces|git|1.18.5 copilot-chat|git|0.37.2026012701 remotehub|git|0.64.0 vscode-pull-request-github|git|0.127.2026012704 gitlab-workflow|git|6.67.3 vscode-test-explorer|hbe|2.22.1 vscode-drawio|hed|1.9.0 rest-client|hum|0.25.1 template-string-converter|meg|0.6.1 regexsnippets|mon|1.0.2 vscode-azurefunctions|ms-|1.20.3 vscode-azureresourcegroups|ms-|0.11.7 vscode-containers|ms-|2.4.0 vscode-docker|ms-|2.0.0 vscode-language-pack-de|ms-|1.108.2026011409 vscode-language-pack-qps-ploc|ms-|1.108.2026012109 debugpy|ms-|2025.19.2026012601 python|ms-|2026.1.2026012301 vscode-pylance|ms-|2025.12.101 vscode-python-envs|ms-|1.17.10261013 jupyter|ms-|2025.10.2026010601 jupyter-hub|ms-|2024.10.1002831100 jupyter-keymap|ms-|1.1.2 jupyter-renderers|ms-|1.3.2025062701 vscode-ai|ms-|1.5.2026012208 vscode-ai-remote|ms-|1.5.2026012009 vscode-jupyter-cell-tags|ms-|0.1.9 vscode-jupyter-slideshow|ms-|0.1.6 remote-containers|ms-|0.440.0 remote-ssh|ms-|0.123.2026012215 remote-ssh-edit|ms-|0.87.0 remote-wsl|ms-|0.104.3 vscode-remote-extensionpack|ms-|0.26.0 azure-repos|ms-|0.40.0 azurecli|ms-|0.6.0 debug-value-editor|ms-|0.2.2 extension-test-runner|ms-|0.0.14 hexeditor|ms-|1.11.1 live-server|ms-|0.5.2026012601 remote-explorer|ms-|0.6.2025081809 remote-repositories|ms-|0.42.0 remote-server|ms-|1.6.2026011209 test-adapter-converter|ms-|0.2.1 ts-file-path-support|ms-|1.0.0 vscode-github-issue-notebooks|ms-|0.0.134 vscode-selfhost-test-provider|ms-|0.3.25 vscode-speech|ms-|0.16.0 vscode-speech-language-pack-de-de|ms-|0.5.0 vscode-websearchforcopilot|ms-|0.2.2026012701 web-editors|ms-|0.3.0 vscode-xml|red|0.29.2025112508 vscode-yaml|red|1.19.1 vscode-dall-toys|rob|0.5.0 rust-analyzer|rus|0.4.2768 eval|sto|0.0.6 vscode-open-in-github|sys|1.18.0 even-better-toml|tam|0.21.2 shellcheck|tim|0.38.6 native-preview|typ|0.20260127.1 vscode-lldb|vad|1.12.0 explorer|vit|1.38.1 autocomplete-english-word|wus|0.1.7 (1 theme extensions excluded) a/b experiments",[],['BUG'],"['bug', 'confirmation-pending', 'chat-agent']",github,2026-01-27T09:35:25Z,,"modal dialogs on shutdown for running chat sessions when none is running type: bug seeing 3 every time i exit vs code (having several windows open): vs code version: code - insiders 1.109.0-insider (e7a06c8eabf2915e2c383b1ce6d2b993d90e2e92, 2026-01-27t07:16:53.085z) os version: darwin arm64 24.6.0 modes: connection to 'dev-container+7b22686f737450617468223a222f55736572732f6368726d617274692f446576656c6f706d656e742f7265706f732f736d6b747374222c226c6f63616c446f636b6572223a66616c73652c2273657474696e6773223a7b22636f6e74657874223a226465736b746f702d6c696e7578227d2c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a222f55736572732f6368726d617274692f446576656c6f706d656e742f7265706f732f736d6b7473742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f55736572732f6368726d617274692f446576656c6f706d656e742f7265706f732f736d6b7473742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d' could not be established system info |item|value| |---|---| |cpus|apple m1 max (10 x 2400)| |gpu status|2d_canvas: enabled gpu0: vendor= 0x106b [google inc. (apple)], device=0x0000 [angle (apple, angle metal renderer: apple m1 max, version 15.7.3 (build 24g419))], driver_vendor=apple, driver_version=15.7.3 *active* machine model name: macbookpro machine model version: 18.2 direct_rendering_display_compositor: disabled_off_ok gpu_compositing: enabled multiple_raster_threads: enabled_on opengl: enabled_on rasterization: enabled raw_draw: disabled_off_ok skia_graphite: disabled_off trees_in_viz: disabled_off video_decode: enabled video_encode: enabled webgl: enabled webgl2: enabled webgpu: enabled webnn: disabled_off| |load (avg)|197, 152, 114| |memory (system)|64.00gb (2.65gb free)| |process argv|--log trace --log github.copilot-chat=debug --crash-reporter-id 7aad610c-0e6b-48aa-ba87-b704b1b543cf --log trace --log github.copilot-chat=debug --crash-reporter-id 7aad610c-0e6b-48aa-ba87-b704b1b543cf| |screen reader|no| |vm|0%| connection to 'dev-container+7b22686f737450617468223a222f55736572732f6368726d617274692f446576656c6f706d656e742f7265706f732f736d6b747374222c226c6f63616c446f636b6572223a66616c73652c2273657474696e6773223a7b22636f6e74657874223a226465736b746f702d6c696e7578227d2c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a222f55736572732f6368726d617274692f446576656c6f706d656e742f7265706f732f736d6b7473742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f55736572732f6368726d617274692f446576656c6f706d656e742f7265706f732f736d6b7473742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d' could not be established extensions (75) extension|author (truncated)|version ---|---|--- tsl-problem-matcher|amo|0.6.2 claude-code|ant|2.1.20 openmatchingfiles|bca|0.5.4 unique-lines|bib|1.0.0 devcontainer-image-convert|bri|0.0.1 ruff|cha|2026.34.0 network-proxy-test|chr|0.0.22 regex|chr|0.6.0 esbuild-problem-matchers|con|0.0.3 vscode-eslint|dba|3.0.20 docker|doc|0.18.0 kusto|don|0.5.4 prettier-vscode|esb|12.3.0 codespaces|git|1.18.5 copilot-chat|git|0.37.2026012701 remotehub|git|0.64.0 vscode-pull-request-github|git|0.127.2026012704 gitlab-workflow|git|6.67.3 vscode-test-explorer|hbe|2.22.1 vscode-drawio|hed|1.9.0 rest-client|hum|0.25.1 template-string-converter|meg|0.6.1 regexsnippets|mon|1.0.2 vscode-azurefunctions|ms-|1.20.3 vscode-azureresourcegroups|ms-|0.11.7 vscode-containers|ms-|2.4.0 vscode-docker|ms-|2.0.0 vscode-language-pack-de|ms-|1.108.2026011409 vscode-language-pack-qps-ploc|ms-|1.108.2026012109 debugpy|ms-|2025.19.2026012601 python|ms-|2026.1.2026012301 vscode-pylance|ms-|2025.12.101 vscode-python-envs|ms-|1.17.10261013 jupyter|ms-|2025.10.2026010601 jupyter-hub|ms-|2024.10.1002831100 jupyter-keymap|ms-|1.1.2 jupyter-renderers|ms-|1.3.2025062701 vscode-ai|ms-|1.5.2026012208 vscode-ai-remote|ms-|1.5.2026012009 vscode-jupyter-cell-tags|ms-|0.1.9 vscode-jupyter-slideshow|ms-|0.1.6 remote-containers|ms-|0.440.0 remote-ssh|ms-|0.123.2026012215 remote-ssh-edit|ms-|0.87.0 remote-wsl|ms-|0.104.3 vscode-remote-extensionpack|ms-|0.26.0 azure-repos|ms-|0.40.0 azurecli|ms-|0.6.0 debug-value-editor|ms-|0.2.2 extension-test-runner|ms-|0.0.14 hexeditor|ms-|1.11.1 live-server|ms-|0.5.2026012601 remote-explorer|ms-|0.6.2025081809 remote-repositories|ms-|0.42.0 remote-server|ms-|1.6.2026011209 test-adapter-converter|ms-|0.2.1 ts-file-path-support|ms-|1.0.0 vscode-github-issue-notebooks|ms-|0.0.134 vscode-selfhost-test-provider|ms-|0.3.25 vscode-speech|ms-|0.16.0 vscode-speech-language-pack-de-de|ms-|0.5.0 vscode-websearchforcopilot|ms-|0.2.2026012701 web-editors|ms-|0.3.0 vscode-xml|red|0.29.2025112508 vscode-yaml|red|1.19.1 vscode-dall-toys|rob|0.5.0 rust-analyzer|rus|0.4.2768 eval|sto|0.0.6 vscode-open-in-github|sys|1.18.0 even-better-toml|tam|0.21.2 shellcheck|tim|0.38.6 native-preview|typ|0.20260127.1 vscode-lldb|vad|1.12.0 explorer|vit|1.38.1 autocomplete-english-word|wus|0.1.7 (1 theme extensions excluded) a/b experiments",7.8,Critical,1.0,crash-like behavior
microsoft/vscode#290727,[testplan-item] tools picker should honour setting,refs - [x] anyos complexity: 1 [create issue]( --- - open the chat tool picker - click somewhere outside the picker. verify the picker closes,[],['TESTING'],['testplan-item'],github,2026-01-27T09:38:09Z,2026-01-28T03:53:50Z,[testplan-item] tools picker should honour setting refs - [x] anyos complexity: 1 [create issue]( --- - open the chat tool picker - click somewhere outside the picker. verify the picker closes,1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161851,kv/kvserver: testleasesdontthrashwhennodebecomessuspect failed,kv/kvserver.testleasesdontthrashwhennodebecomessuspect [failed]( on release-26.1 @ [be336920639819feab95e8fc2620ba50e2c71bbf]( parameters: - attempt=1 - deadlock=true - run=2 - shard=27 help see also: [how to investigate a go test failure \(internal\)]( /cc /kv-triage [this test on roachdash]( | [improve this report!]( jira issue: crdb-59117,[],['TESTING'],"['C-test-failure', 'O-robot', 'release-blocker', 'T-kv', 'branch-release-26.1']",github,2026-01-27T09:49:20Z,,kv/kvserver: testleasesdontthrashwhennodebecomessuspect failed kv/kvserver.testleasesdontthrashwhennodebecomessuspect [failed]( on release-26.1 @ [be336920639819feab95e8fc2620ba50e2c71bbf]( parameters: - attempt=1 - deadlock=true - run=2 - shard=27 help see also: [how to investigate a go test failure \(internal\)]( /cc /kv-triage [this test on roachdash]( | [improve this report!]( jira issue: crdb-59117,3.8,Critical,1.0,crash-like behavior
microsoft/vscode#290731,[test plan item] portable vs code should avoid changes to os and handle auth correctly,"refs - [ ] windows - [x] macos complexity: 4 [create issue]( [create issue]( --- in order to test portable mode, download the archive target from build and follow os-specific instructions below for creating a user data folder. when validating that no changes to os is made, either clean out the registry or use a clean vm for testing. for authentication you will need a github account and a microsoft account and a feature to use it for (e.g. settings sync). ### summary changes made to improve portable mode behavior: 1. **windows**: skips protocol handler registration ( ) in portable mode 2. **macos**: continue listening to events but authentication flows should avoid using protocol redirect 3. **github authentication**: in portable mode, the local server flow avoids redirecting back to vs code via protocol handler 4. **microsoft authentication**: the protocol handler flow is excluded when running in portable mode 5. **new api**: added (proposed api) for extensions to detect portable mode --- ### test cases #### 1. windows - protocol handler registration | step | expected | |------|----------| | 1. run vs code insiders in **non-portable mode** on windows | protocol handler is registered. open and verify exists with the correct path | | 2. run vs code insiders in **portable mode** (create a folder next to the app) | protocol handler is not registered in the registry. verify key is not created/updated | #### 2. macos - oauth authentication flow | step | expected | |------|----------| | 1. run vs code insiders in **non-portable mode** on macos | oauth flows (github, microsoft) complete via protocol handler redirect back to vs code | | 2. run vs code insiders in **portable mode** on macos (create a folder next to the bundle) | oauth flows complete using local server or device code flow without protocol handler redirect | #### 3. github authentication in portable mode | step | expected | |------|----------| | 1. in **non-portable mode**, sign in with github via ""github: sign in"" | after browser auth, redirects to protocol and closes browser tab showing ""you are now signed in..."" with redirect link | | 2. in **portable mode**, sign in with github via ""github: sign in"" | after browser auth, stays on local server page showing ""you are now signed in..."" message without redirect link (just param). authentication completes successfully. token is stored in portable data folder | #### 4. microsoft authentication in portable mode | step | expected | |------|----------| | 1. in **non-portable mode**, sign in with microsoft via ""microsoft: sign in"" | authentication uses default or protocol handler flow (based on client support) | | 2. in **portable mode**, sign in with microsoft via ""microsoft: sign in"" | protocol handler flow is excluded from available flows. only (loopback) or flows are available. authentication completes successfully | #### 5. authentication without protocol handler (fallback) | step | expected | |------|----------| | 1. in portable mode, trigger github oauth that would normally use protocol redirect | local server flow is used; auth server does not include in success response | | 2. in portable mode, trigger microsoft oauth | default loopback flow or device code flow is used (not url handler flow) | #### 6. public api - | step | expected | |------|----------| | 1. in an extension that has in , access in **non-portable mode** | returns | | 2. in an extension that has in , access in **portable mode** | returns | | 3. in an extension without proposal, try to access | throws an error about proposed api not enabled | | 4. in the api test extension ( ), verify is accessible | api is available and returns correct boolean value based on mode | #### 7. regression - non-portable mode works as before | step | expected | |------|----------| | 1. in non-portable mode on windows, click link in browser | vs code opens and handles the url | | 2. in non-portable mode, complete github oauth flow | redirects back to vs code via protocol handler | | 3. in non-portable mode, complete microsoft oauth flow | all authentication flows (including protocol handler) work correctly | --- ### setup for portable mode testing **windows:** **macos:**",[],['TESTING'],['testplan-item'],github,2026-01-27T09:50:34Z,,"[test plan item] portable vs code should avoid changes to os and handle auth correctly refs - [ ] windows - [x] macos complexity: 4 [create issue]( [create issue]( --- in order to test portable mode, download the archive target from build and follow os-specific instructions below for creating a user data folder. when validating that no changes to os is made, either clean out the registry or use a clean vm for testing. for authentication you will need a github account and a microsoft account and a feature to use it for (e.g. settings sync). ### summary changes made to improve portable mode behavior: 1. **windows**: skips protocol handler registration ( ) in portable mode 2. **macos**: continue listening to events but authentication flows should avoid using protocol redirect 3. **github authentication**: in portable mode, the local server flow avoids redirecting back to vs code via protocol handler 4. **microsoft authentication**: the protocol handler flow is excluded when running in portable mode 5. **new api**: added (proposed api) for extensions to detect portable mode --- ### test cases #### 1. windows - protocol handler registration | step | expected | |------|----------| | 1. run vs code insiders in **non-portable mode** on windows | protocol handler is registered. open and verify exists with the correct path | | 2. run vs code insiders in **portable mode** (create a folder next to the app) | protocol handler is not registered in the registry. verify key is not created/updated | #### 2. macos - oauth authentication flow | step | expected | |------|----------| | 1. run vs code insiders in **non-portable mode** on macos | oauth flows (github, microsoft) complete via protocol handler redirect back to vs code | | 2. run vs code insiders in **portable mode** on macos (create a folder next to the bundle) | oauth flows complete using local server or device code flow without protocol handler redirect | #### 3. github authentication in portable mode | step | expected | |------|----------| | 1. in **non-portable mode**, sign in with github via ""github: sign in"" | after browser auth, redirects to protocol and closes browser tab showing ""you are now signed in..."" with redirect link | | 2. in **portable mode**, sign in with github via ""github: sign in"" | after browser auth, stays on local server page showing ""you are now signed in..."" message without redirect link (just param). authentication completes successfully. token is stored in portable data folder | #### 4. microsoft authentication in portable mode | step | expected | |------|----------| | 1. in **non-portable mode**, sign in with microsoft via ""microsoft: sign in"" | authentication uses default or protocol handler flow (based on client support) | | 2. in **portable mode**, sign in with microsoft via ""microsoft: sign in"" | protocol handler flow is excluded from available flows. only (loopback) or flows are available. authentication completes successfully | #### 5. authentication without protocol handler (fallback) | step | expected | |------|----------| | 1. in portable mode, trigger github oauth that would normally use protocol redirect | local server flow is used; auth server does not include in success response | | 2. in portable mode, trigger microsoft oauth | default loopback flow or device code flow is used (not url handler flow) | #### 6. public api - | step | expected | |------|----------| | 1. in an extension that has in , access in **non-portable mode** | returns | | 2. in an extension that has in , access in **portable mode** | returns | | 3. in an extension without proposal, try to access | throws an error about proposed api not enabled | | 4. in the api test extension ( ), verify is accessible | api is available and returns correct boolean value based on mode | #### 7. regression - non-portable mode works as before | step | expected | |------|----------| | 1. in non-portable mode on windows, click link in browser | vs code opens and handles the url | | 2. in non-portable mode, complete github oauth flow | redirects back to vs code via protocol handler | | 3. in non-portable mode, complete microsoft oauth flow | all authentication flows (including protocol handler) work correctly | --- ### setup for portable mode testing **windows:** **macos:**",3.8,Critical,1.0,crash-like behavior
microsoft/vscode#290732,processes hang when trying exit insiders,"type: bug vs code version: code - insiders 1.109.0-insider (e7a06c8eabf2915e2c383b1ce6d2b993d90e2e92, 2026-01-27t07:16:53.085z) os version: darwin arm64 24.6.0 modes: connection to 'dev-container+7b22686f737450617468223a222f55736572732f6368726d617274692f446576656c6f706d656e742f7265706f732f736d6b747374222c226c6f63616c446f636b6572223a66616c73652c2273657474696e6773223a7b22636f6e74657874223a226465736b746f702d6c696e7578227d2c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a222f55736572732f6368726d617274692f446576656c6f706d656e742f7265706f732f736d6b7473742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f55736572732f6368726d617274692f446576656c6f706d656e742f7265706f732f736d6b7473742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d' could not be established system info |item|value| |---|---| |cpus|apple m1 max (10 x 2400)| |gpu status|2d_canvas: enabled gpu0: vendor= 0x106b [google inc. (apple)], device=0x0000 [angle (apple, angle metal renderer: apple m1 max, version 15.7.3 (build 24g419))], driver_vendor=apple, driver_version=15.7.3 *active* machine model name: macbookpro machine model version: 18.2 direct_rendering_display_compositor: disabled_off_ok gpu_compositing: enabled multiple_raster_threads: enabled_on opengl: enabled_on rasterization: enabled raw_draw: disabled_off_ok skia_graphite: disabled_off trees_in_viz: disabled_off video_decode: enabled video_encode: enabled webgl: enabled webgl2: enabled webgpu: enabled webnn: disabled_off| |load (avg)|256, 230, 194| |memory (system)|64.00gb (0.17gb free)| |process argv|--log trace --log github.copilot-chat=debug --crash-reporter-id 7aad610c-0e6b-48aa-ba87-b704b1b543cf| |screen reader|no| |vm|0%| connection to 'dev-container+7b22686f737450617468223a222f55736572732f6368726d617274692f446576656c6f706d656e742f7265706f732f736d6b747374222c226c6f63616c446f636b6572223a66616c73652c2273657474696e6773223a7b22636f6e74657874223a226465736b746f702d6c696e7578227d2c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a222f55736572732f6368726d617274692f446576656c6f706d656e742f7265706f732f736d6b7473742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f55736572732f6368726d617274692f446576656c6f706d656e742f7265706f732f736d6b7473742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d' could not be established extensions (75) extension|author (truncated)|version ---|---|--- tsl-problem-matcher|amo|0.6.2 claude-code|ant|2.1.20 openmatchingfiles|bca|0.5.4 unique-lines|bib|1.0.0 devcontainer-image-convert|bri|0.0.1 ruff|cha|2026.34.0 network-proxy-test|chr|0.0.22 regex|chr|0.6.0 esbuild-problem-matchers|con|0.0.3 vscode-eslint|dba|3.0.20 docker|doc|0.18.0 kusto|don|0.5.4 prettier-vscode|esb|12.3.0 codespaces|git|1.18.5 copilot-chat|git|0.37.2026012701 remotehub|git|0.64.0 vscode-pull-request-github|git|0.127.2026012704 gitlab-workflow|git|6.67.3 vscode-test-explorer|hbe|2.22.1 vscode-drawio|hed|1.9.0 rest-client|hum|0.25.1 template-string-converter|meg|0.6.1 regexsnippets|mon|1.0.2 vscode-azurefunctions|ms-|1.20.3 vscode-azureresourcegroups|ms-|0.11.7 vscode-containers|ms-|2.4.0 vscode-docker|ms-|2.0.0 vscode-language-pack-de|ms-|1.108.2026011409 vscode-language-pack-qps-ploc|ms-|1.108.2026012109 debugpy|ms-|2025.19.2026012601 python|ms-|2026.1.2026012301 vscode-pylance|ms-|2025.12.101 vscode-python-envs|ms-|1.17.10261013 jupyter|ms-|2025.10.2026010601 jupyter-hub|ms-|2024.10.1002831100 jupyter-keymap|ms-|1.1.2 jupyter-renderers|ms-|1.3.2025062701 vscode-ai|ms-|1.5.2026012208 vscode-ai-remote|ms-|1.5.2026012009 vscode-jupyter-cell-tags|ms-|0.1.9 vscode-jupyter-slideshow|ms-|0.1.6 remote-containers|ms-|0.440.0 remote-ssh|ms-|0.123.2026012215 remote-ssh-edit|ms-|0.87.0 remote-wsl|ms-|0.104.3 vscode-remote-extensionpack|ms-|0.26.0 azure-repos|ms-|0.40.0 azurecli|ms-|0.6.0 debug-value-editor|ms-|0.2.2 extension-test-runner|ms-|0.0.14 hexeditor|ms-|1.11.1 live-server|ms-|0.5.2026012601 remote-explorer|ms-|0.6.2025081809 remote-repositories|ms-|0.42.0 remote-server|ms-|1.6.2026011209 test-adapter-converter|ms-|0.2.1 ts-file-path-support|ms-|1.0.0 vscode-github-issue-notebooks|ms-|0.0.134 vscode-selfhost-test-provider|ms-|0.3.25 vscode-speech|ms-|0.16.0 vscode-speech-language-pack-de-de|ms-|0.5.0 vscode-websearchforcopilot|ms-|0.2.2026012701 web-editors|ms-|0.3.0 vscode-xml|red|0.29.2025112508 vscode-yaml|red|1.19.1 vscode-dall-toys|rob|0.5.0 rust-analyzer|rus|0.4.2768 eval|sto|0.0.6 vscode-open-in-github|sys|1.18.0 even-better-toml|tam|0.21.2 shellcheck|tim|0.38.6 native-preview|typ|0.20260127.1 vscode-lldb|vad|1.12.0 explorer|vit|1.38.1 autocomplete-english-word|wus|0.1.7 (1 theme extensions excluded) a/b experiments",[],['PERFORMANCE'],"['freeze-slow-crash-leak', 'macos', 'under-discussion']",github,2026-01-27T09:52:21Z,,"processes hang when trying exit insiders type: bug vs code version: code - insiders 1.109.0-insider (e7a06c8eabf2915e2c383b1ce6d2b993d90e2e92, 2026-01-27t07:16:53.085z) os version: darwin arm64 24.6.0 modes: connection to 'dev-container+7b22686f737450617468223a222f55736572732f6368726d617274692f446576656c6f706d656e742f7265706f732f736d6b747374222c226c6f63616c446f636b6572223a66616c73652c2273657474696e6773223a7b22636f6e74657874223a226465736b746f702d6c696e7578227d2c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a222f55736572732f6368726d617274692f446576656c6f706d656e742f7265706f732f736d6b7473742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f55736572732f6368726d617274692f446576656c6f706d656e742f7265706f732f736d6b7473742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d' could not be established system info |item|value| |---|---| |cpus|apple m1 max (10 x 2400)| |gpu status|2d_canvas: enabled gpu0: vendor= 0x106b [google inc. (apple)], device=0x0000 [angle (apple, angle metal renderer: apple m1 max, version 15.7.3 (build 24g419))], driver_vendor=apple, driver_version=15.7.3 *active* machine model name: macbookpro machine model version: 18.2 direct_rendering_display_compositor: disabled_off_ok gpu_compositing: enabled multiple_raster_threads: enabled_on opengl: enabled_on rasterization: enabled raw_draw: disabled_off_ok skia_graphite: disabled_off trees_in_viz: disabled_off video_decode: enabled video_encode: enabled webgl: enabled webgl2: enabled webgpu: enabled webnn: disabled_off| |load (avg)|256, 230, 194| |memory (system)|64.00gb (0.17gb free)| |process argv|--log trace --log github.copilot-chat=debug --crash-reporter-id 7aad610c-0e6b-48aa-ba87-b704b1b543cf| |screen reader|no| |vm|0%| connection to 'dev-container+7b22686f737450617468223a222f55736572732f6368726d617274692f446576656c6f706d656e742f7265706f732f736d6b747374222c226c6f63616c446f636b6572223a66616c73652c2273657474696e6773223a7b22636f6e74657874223a226465736b746f702d6c696e7578227d2c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a222f55736572732f6368726d617274692f446576656c6f706d656e742f7265706f732f736d6b7473742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f55736572732f6368726d617274692f446576656c6f706d656e742f7265706f732f736d6b7473742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d' could not be established extensions (75) extension|author (truncated)|version ---|---|--- tsl-problem-matcher|amo|0.6.2 claude-code|ant|2.1.20 openmatchingfiles|bca|0.5.4 unique-lines|bib|1.0.0 devcontainer-image-convert|bri|0.0.1 ruff|cha|2026.34.0 network-proxy-test|chr|0.0.22 regex|chr|0.6.0 esbuild-problem-matchers|con|0.0.3 vscode-eslint|dba|3.0.20 docker|doc|0.18.0 kusto|don|0.5.4 prettier-vscode|esb|12.3.0 codespaces|git|1.18.5 copilot-chat|git|0.37.2026012701 remotehub|git|0.64.0 vscode-pull-request-github|git|0.127.2026012704 gitlab-workflow|git|6.67.3 vscode-test-explorer|hbe|2.22.1 vscode-drawio|hed|1.9.0 rest-client|hum|0.25.1 template-string-converter|meg|0.6.1 regexsnippets|mon|1.0.2 vscode-azurefunctions|ms-|1.20.3 vscode-azureresourcegroups|ms-|0.11.7 vscode-containers|ms-|2.4.0 vscode-docker|ms-|2.0.0 vscode-language-pack-de|ms-|1.108.2026011409 vscode-language-pack-qps-ploc|ms-|1.108.2026012109 debugpy|ms-|2025.19.2026012601 python|ms-|2026.1.2026012301 vscode-pylance|ms-|2025.12.101 vscode-python-envs|ms-|1.17.10261013 jupyter|ms-|2025.10.2026010601 jupyter-hub|ms-|2024.10.1002831100 jupyter-keymap|ms-|1.1.2 jupyter-renderers|ms-|1.3.2025062701 vscode-ai|ms-|1.5.2026012208 vscode-ai-remote|ms-|1.5.2026012009 vscode-jupyter-cell-tags|ms-|0.1.9 vscode-jupyter-slideshow|ms-|0.1.6 remote-containers|ms-|0.440.0 remote-ssh|ms-|0.123.2026012215 remote-ssh-edit|ms-|0.87.0 remote-wsl|ms-|0.104.3 vscode-remote-extensionpack|ms-|0.26.0 azure-repos|ms-|0.40.0 azurecli|ms-|0.6.0 debug-value-editor|ms-|0.2.2 extension-test-runner|ms-|0.0.14 hexeditor|ms-|1.11.1 live-server|ms-|0.5.2026012601 remote-explorer|ms-|0.6.2025081809 remote-repositories|ms-|0.42.0 remote-server|ms-|1.6.2026011209 test-adapter-converter|ms-|0.2.1 ts-file-path-support|ms-|1.0.0 vscode-github-issue-notebooks|ms-|0.0.134 vscode-selfhost-test-provider|ms-|0.3.25 vscode-speech|ms-|0.16.0 vscode-speech-language-pack-de-de|ms-|0.5.0 vscode-websearchforcopilot|ms-|0.2.2026012701 web-editors|ms-|0.3.0 vscode-xml|red|0.29.2025112508 vscode-yaml|red|1.19.1 vscode-dall-toys|rob|0.5.0 rust-analyzer|rus|0.4.2768 eval|sto|0.0.6 vscode-open-in-github|sys|1.18.0 even-better-toml|tam|0.21.2 shellcheck|tim|0.38.6 native-preview|typ|0.20260127.1 vscode-lldb|vad|1.12.0 explorer|vit|1.38.1 autocomplete-english-word|wus|0.1.7 (1 theme extensions excluded) a/b experiments",8.6,Critical,1.0,"performance degradation, crash-like behavior"
microsoft/vscode#290737,integrated browser - find in page feedback,"testing * open the integrated browser and navigate to * open find, and look for ""ai"", and toggle match case (you should have 6 hits) * while the find widget is visible, to to the ""..."" menu and click on ""find in page"" ### expected: * since the find widget is already visible, nothing should happen * if the find widget would not be visible i would expect it to be revealed ### actual * we are navigating to the next hit on the page",[],['BUG'],"['bug', 'insiders-released', 'browser-integration']",github,2026-01-27T10:05:06Z,2026-01-27T17:52:31Z,"integrated browser - find in page feedback testing * open the integrated browser and navigate to * open find, and look for ""ai"", and toggle match case (you should have 6 hits) * while the find widget is visible, to to the ""..."" menu and click on ""find in page"" ### expected: * since the find widget is already visible, nothing should happen * if the find widget would not be visible i would expect it to be revealed ### actual * we are navigating to the next hit on the page",2.577,Medium,0.806,functional impact
microsoft/vscode#290739,[test plan item] ignorefile compares file paths in a case-sensitive way on windows/macos,"refs - [x] windows - [x] macos -d - [x] linux complexity: 3 [create issue]( [create issue]( --- ### summary the change makes .gitignore file pattern matching case-insensitive on windows and macos, where file systems are case-insensitive. this fixes the issue where files were not properly excluded when their paths differed in case from what was specified in .gitignore. for linux, verify no change no behavior. if possible, try vscode.dev scenario as well on the target os. ### prerequisites - vs code insiders on windows or macos - enable setting ( ) --- ### test cases (windows/macos) #### 1. basic case-insensitive file exclusion 1. create a new folder and open it in vs code 2. create a .gitignore file with content: 3. create a file named (or , etc.) 4. enable in settings 5. **expected**: should be hidden from explorer #### 2. basic case-insensitive folder exclusion 1. create a new folder and open it in vs code 2. create a .gitignore file with content: node_modules 3. create a folder named node_modules or node_modules 4. enable in settings 5. **expected**: the folder should be hidden from explorer #### 3. glob pattern case-insensitivity 1. create a .gitignore with: 2. create files: , , 3. **expected**: all three files should be hidden #### 4. path-based exclusion with different cases 1. create a .gitignore with: 2. create folder structure: with some files inside 3. **expected**: folder should be hidden #### 5. nested .gitignore with case differences 1. create root .gitignore with: 2. create a subfolder src (case mismatch) 3. in src, create .gitignore with: (negation) 4. create and 5. **expected**: hidden, visible #### 6. mixed case in workspace path 1. open a folder with a mixed-case path (e.g., ) 2. create a .gitignore with entries 3. **expected**: excludes should work regardless of the workspace path casing ### linux tests (no changes expected) #### 7. case-sensitive behavior preserved on linux 1. on linux, create .gitignore with: 2. create both and 3. **expected**: only is hidden; remains visible (linux is case-sensitive) --- ### edge cases #### 8. file becomes visible/hidden after toggling setting 1. have files that would be excluded by case-insensitive matching 2. toggle off then on 3. **expected**: files hide/show correctly after toggle #### 9. .gitignore update with case-different patterns 1. start with .gitignore containing: 2. files like should be hidden 3. update .gitignore to: (negation) 4. **expected**: becomes visible without restart",[],['TESTING'],['testplan-item'],github,2026-01-27T10:10:08Z,2026-01-28T02:39:33Z,"[test plan item] ignorefile compares file paths in a case-sensitive way on windows/macos refs - [x] windows - [x] macos -d - [x] linux complexity: 3 [create issue]( [create issue]( --- ### summary the change makes .gitignore file pattern matching case-insensitive on windows and macos, where file systems are case-insensitive. this fixes the issue where files were not properly excluded when their paths differed in case from what was specified in .gitignore. for linux, verify no change no behavior. if possible, try vscode.dev scenario as well on the target os. ### prerequisites - vs code insiders on windows or macos - enable setting ( ) --- ### test cases (windows/macos) #### 1. basic case-insensitive file exclusion 1. create a new folder and open it in vs code 2. create a .gitignore file with content: 3. create a file named (or , etc.) 4. enable in settings 5. **expected**: should be hidden from explorer #### 2. basic case-insensitive folder exclusion 1. create a new folder and open it in vs code 2. create a .gitignore file with content: node_modules 3. create a folder named node_modules or node_modules 4. enable in settings 5. **expected**: the folder should be hidden from explorer #### 3. glob pattern case-insensitivity 1. create a .gitignore with: 2. create files: , , 3. **expected**: all three files should be hidden #### 4. path-based exclusion with different cases 1. create a .gitignore with: 2. create folder structure: with some files inside 3. **expected**: folder should be hidden #### 5. nested .gitignore with case differences 1. create root .gitignore with: 2. create a subfolder src (case mismatch) 3. in src, create .gitignore with: (negation) 4. create and 5. **expected**: hidden, visible #### 6. mixed case in workspace path 1. open a folder with a mixed-case path (e.g., ) 2. create a .gitignore with entries 3. **expected**: excludes should work regardless of the workspace path casing ### linux tests (no changes expected) #### 7. case-sensitive behavior preserved on linux 1. on linux, create .gitignore with: 2. create both and 3. **expected**: only is hidden; remains visible (linux is case-sensitive) --- ### edge cases #### 8. file becomes visible/hidden after toggling setting 1. have files that would be excluded by case-insensitive matching 2. toggle off then on 3. **expected**: files hide/show correctly after toggle #### 9. .gitignore update with case-different patterns 1. start with .gitignore containing: 2. files like should be hidden 3. update .gitignore to: (negation) 4. **expected**: becomes visible without restart",3.8,Critical,1.0,crash-like behavior
microsoft/vscode#290741,suggestions show behind browser,testing * open code and browser in a side by side view * in the editor trigger suggestions * :bug: the editor suggestions show behind the browser,[],['BUG'],"['bug', 'insiders-released', 'browser-integration']",github,2026-01-27T10:12:33Z,2026-01-27T19:51:56Z,suggestions show behind browser testing * open code and browser in a side by side view * in the editor trigger suggestions * :bug: the editor suggestions show behind the browser,2.241,Medium,0.729,functional impact
microsoft/vscode#290742,ux: agent sessions title control needs polish,"related issues: - there's no hover feedback on the unread sessions button - the active state on the unread sessions is represented by a grey background color on the twistie next to it - the mouse cursor shows the üëÜ style a good 30px to the right of the entire control. pressing mouse down in that area will show the same active state as the issue above - when clicking the unread sessions button, you'll get a selected state background which has rounded corners on the right and is flat on the left: awesome. the button+dropdown control on the left has rounded corners all around; it should also be flat on the right side. - resizing the window makes the controls overflow the layout buttons. - padding is uneven in the unread sessions button: there's more left padding than right.",[],"['BUG', 'UI']","['bug', 'ux', 'polish', 'insiders-released']",github,2026-01-27T10:13:39Z,2026-01-27T19:46:22Z,"ux: agent sessions title control needs polish related issues: - there's no hover feedback on the unread sessions button - the active state on the unread sessions is represented by a grey background color on the twistie next to it - the mouse cursor shows the üëÜ style a good 30px to the right of the entire control. pressing mouse down in that area will show the same active state as the issue above - when clicking the unread sessions button, you'll get a selected state background which has rounded corners on the right and is flat on the left: awesome. the button+dropdown control on the left has rounded corners all around; it should also be flat on the right side. - resizing the window makes the controls overflow the layout buttons. - padding is uneven in the unread sessions button: there's more left padding than right.",3.198,High,0.947,"user-visible issue, crash-like behavior"
microsoft/vscode#290744,screen cheese with extensions suggest and integrated browser,testing * have extensions view and browser open * trigger suggestion in the extension input * :bug: the suggest widget weirdly over/underlays with the browser,[],['BUG'],"['bug', 'insiders-released', 'browser-integration']",github,2026-01-27T10:15:06Z,2026-01-27T19:51:55Z,screen cheese with extensions suggest and integrated browser testing * have extensions view and browser open * trigger suggestion in the extension input * :bug: the suggest widget weirdly over/underlays with the browser,2.668,Medium,0.826,functional impact
microsoft/vscode#290750,drop target too small when using integrated browser,testing * open only the integrated browser * drag-and-drop a file from the explorer into the browser area * the drag-and-drop target is only available at the top of the screen,[],['BUG'],"['bug', 'browser-integration']",github,2026-01-27T10:23:45Z,,drop target too small when using integrated browser testing * open only the integrated browser * drag-and-drop a file from the explorer into the browser area * the drag-and-drop target is only available at the top of the screen,2.584,Medium,0.807,functional impact
cockroachdb/cockroach#161852,sentry: panic.go:770: runtime error: invalid memory address or nil pointer dereference (1) attached stack trace -- stack trace: | runtime.gopanic | goroot/src/runtime/panic.go:770 | runtime.p...,this issue was auto filed by sentry. it represents a crash or reported error on a live cluster with telemetry enabled. sentry link: [ panic message: stacktrace (expand for inline code snippets): [src/runtime/asm_amd64.s#l1694-l1696](src/runtime/asm_amd64.s#l1694-l1696) [pkg/util/stop/stopper.go#l497-l499](pkg/util/stop/stopper.go#l497-l499) [pkg/kv/kvserver/scheduler.go#l318-l320](pkg/kv/kvserver/scheduler.go#l318-l320) [pkg/kv/kvserver/scheduler.go#l418-l420](pkg/kv/kvserver/scheduler.go#l418-l420) [pkg/kv/kvserver/store_raft.go#l681-l683](pkg/kv/kvserver/store_raft.go#l681-l683) [pkg/kv/kvserver/replica_raft.go#l845-l847](pkg/kv/kvserver/replica_raft.go#l845-l847) [pkg/kv/kvserver/replica_raft.go#l1268-l1270](pkg/kv/kvserver/replica_raft.go#l1268-l1270) [pkg/kv/kvserver/apply/task.go#l245-l247](pkg/kv/kvserver/apply/task.go#l245-l247) [pkg/kv/kvserver/apply/task.go#l295-l297](pkg/kv/kvserver/apply/task.go#l295-l297) [pkg/kv/kvserver/apply/cmd.go#l262-l264](pkg/kv/kvserver/apply/cmd.go#l262-l264) [pkg/kv/kvserver/replica_application_cmd.go#l150-l152](pkg/kv/kvserver/replica_application_cmd.go#l150-l152) [pkg/kv/kvserver/replica_proposal.go#l253-l255](pkg/kv/kvserver/replica_proposal.go#l253-l255) [pkg/kv/kvserver/replica_send.go#l1355-l1357](pkg/kv/kvserver/replica_send.go#l1355-l1357) [pkg/kv/kvserver/concurrency/concurrency_manager.go#l447-l449](pkg/kv/kvserver/concurrency/concurrency_manager.go#l447-l449) [pkg/kv/kvserver/concurrency/latch_manager.go#l64-l66](pkg/kv/kvserver/concurrency/latch_manager.go#l64-l66) [pkg/kv/kvserver/spanlatch/manager.go#l638-l640](pkg/kv/kvserver/spanlatch/manager.go#l638-l640) [pkg/kv/kvserver/spanlatch/manager.go#l668-l670](pkg/kv/kvserver/spanlatch/manager.go#l668-l670) [bazel-out/k8-opt/bin/pkg/kv/kvserver/spanlatch/latch_interval_btree.go#l721-l723](bazel-out/k8-opt/bin/pkg/kv/kvserver/spanlatch/latch_interval_btree.go#l721-l723) [bazel-out/k8-opt/bin/pkg/kv/kvserver/spanlatch/latch_interval_btree.go#l477-l479](bazel-out/k8-opt/bin/pkg/kv/kvserver/spanlatch/latch_interval_btree.go#l477-l479) [bazel-out/k8-opt/bin/pkg/kv/kvserver/spanlatch/latch_interval_btree.go#l460-l462](bazel-out/k8-opt/bin/pkg/kv/kvserver/spanlatch/latch_interval_btree.go#l460-l462) [bazel-out/k8-opt/bin/pkg/kv/kvserver/spanlatch/latch_interval_btree.go#l656-l658](bazel-out/k8-opt/bin/pkg/kv/kvserver/spanlatch/latch_interval_btree.go#l656-l658) [bazel-out/k8-opt/bin/pkg/kv/kvserver/spanlatch/latch_interval_btree.go#l67-l69](bazel-out/k8-opt/bin/pkg/kv/kvserver/spanlatch/latch_interval_btree.go#l67-l69) [goroot/src/bytes/bytes.go#l26-l28](goroot/src/bytes/bytes.go#l26-l28) [src/internal/bytealg/compare_amd64.s#l99-l101](src/internal/bytealg/compare_amd64.s#l99-l101) [goroot/src/runtime/signal_unix.go#l880-l882](goroot/src/runtime/signal_unix.go#l880-l882) [goroot/src/runtime/panic.go#l260-l262](goroot/src/runtime/panic.go#l260-l262) [goroot/src/runtime/panic.go#l769-l771](goroot/src/runtime/panic.go#l769-l771) ### tags | tag | value | | --- | --- | | command | start-single-node | | environment | v24.3.3 | | go version | go1.22.8 x:nocoverageredesign | | platform | linux amd64 | | distribution | ccl | | cockroach release | v24.3.3 | | cockroach sha | 343b4202e553d58a76a36c60018588615bd4c30a | | # of cpus | 24 | | # of goroutines | 1327 | jira issue: crdb-59118,[],['BUG'],"['C-bug', 'O-sentry', 'X-blathers-triaged', 'T-kv', 'branch-release-24.3']",github,2026-01-27T10:30:48Z,,sentry: panic.go:770: runtime error: invalid memory address or nil pointer dereference (1) attached stack trace -- stack trace: | runtime.gopanic | goroot/src/runtime/panic.go:770 | runtime.p... this issue was auto filed by sentry. it represents a crash or reported error on a live cluster with telemetry enabled. sentry link: [ panic message: stacktrace (expand for inline code snippets): [src/runtime/asm_amd64.s#l1694-l1696](src/runtime/asm_amd64.s#l1694-l1696) [pkg/util/stop/stopper.go#l497-l499](pkg/util/stop/stopper.go#l497-l499) [pkg/kv/kvserver/scheduler.go#l318-l320](pkg/kv/kvserver/scheduler.go#l318-l320) [pkg/kv/kvserver/scheduler.go#l418-l420](pkg/kv/kvserver/scheduler.go#l418-l420) [pkg/kv/kvserver/store_raft.go#l681-l683](pkg/kv/kvserver/store_raft.go#l681-l683) [pkg/kv/kvserver/replica_raft.go#l845-l847](pkg/kv/kvserver/replica_raft.go#l845-l847) [pkg/kv/kvserver/replica_raft.go#l1268-l1270](pkg/kv/kvserver/replica_raft.go#l1268-l1270) [pkg/kv/kvserver/apply/task.go#l245-l247](pkg/kv/kvserver/apply/task.go#l245-l247) [pkg/kv/kvserver/apply/task.go#l295-l297](pkg/kv/kvserver/apply/task.go#l295-l297) [pkg/kv/kvserver/apply/cmd.go#l262-l264](pkg/kv/kvserver/apply/cmd.go#l262-l264) [pkg/kv/kvserver/replica_application_cmd.go#l150-l152](pkg/kv/kvserver/replica_application_cmd.go#l150-l152) [pkg/kv/kvserver/replica_proposal.go#l253-l255](pkg/kv/kvserver/replica_proposal.go#l253-l255) [pkg/kv/kvserver/replica_send.go#l1355-l1357](pkg/kv/kvserver/replica_send.go#l1355-l1357) [pkg/kv/kvserver/concurrency/concurrency_manager.go#l447-l449](pkg/kv/kvserver/concurrency/concurrency_manager.go#l447-l449) [pkg/kv/kvserver/concurrency/latch_manager.go#l64-l66](pkg/kv/kvserver/concurrency/latch_manager.go#l64-l66) [pkg/kv/kvserver/spanlatch/manager.go#l638-l640](pkg/kv/kvserver/spanlatch/manager.go#l638-l640) [pkg/kv/kvserver/spanlatch/manager.go#l668-l670](pkg/kv/kvserver/spanlatch/manager.go#l668-l670) [bazel-out/k8-opt/bin/pkg/kv/kvserver/spanlatch/latch_interval_btree.go#l721-l723](bazel-out/k8-opt/bin/pkg/kv/kvserver/spanlatch/latch_interval_btree.go#l721-l723) [bazel-out/k8-opt/bin/pkg/kv/kvserver/spanlatch/latch_interval_btree.go#l477-l479](bazel-out/k8-opt/bin/pkg/kv/kvserver/spanlatch/latch_interval_btree.go#l477-l479) [bazel-out/k8-opt/bin/pkg/kv/kvserver/spanlatch/latch_interval_btree.go#l460-l462](bazel-out/k8-opt/bin/pkg/kv/kvserver/spanlatch/latch_interval_btree.go#l460-l462) [bazel-out/k8-opt/bin/pkg/kv/kvserver/spanlatch/latch_interval_btree.go#l656-l658](bazel-out/k8-opt/bin/pkg/kv/kvserver/spanlatch/latch_interval_btree.go#l656-l658) [bazel-out/k8-opt/bin/pkg/kv/kvserver/spanlatch/latch_interval_btree.go#l67-l69](bazel-out/k8-opt/bin/pkg/kv/kvserver/spanlatch/latch_interval_btree.go#l67-l69) [goroot/src/bytes/bytes.go#l26-l28](goroot/src/bytes/bytes.go#l26-l28) [src/internal/bytealg/compare_amd64.s#l99-l101](src/internal/bytealg/compare_amd64.s#l99-l101) [goroot/src/runtime/signal_unix.go#l880-l882](goroot/src/runtime/signal_unix.go#l880-l882) [goroot/src/runtime/panic.go#l260-l262](goroot/src/runtime/panic.go#l260-l262) [goroot/src/runtime/panic.go#l769-l771](goroot/src/runtime/panic.go#l769-l771) ### tags | tag | value | | --- | --- | | command | start-single-node | | environment | v24.3.3 | | go version | go1.22.8 x:nocoverageredesign | | platform | linux amd64 | | distribution | ccl | | cockroach release | v24.3.3 | | cockroach sha | 343b4202e553d58a76a36c60018588615bd4c30a | | # of cpus | 24 | | # of goroutines | 1327 | jira issue: crdb-59118,6.0,Critical,1.0,crash-like behavior
envoyproxy/envoy#43182,remove python-based kafka integration tests and migrate kafka coverage to examples repo,"**title**: remove python-based kafka integration tests and migrate kafka coverage to examples repo **description:** the python-based kafka integration tests located in and are problematic outliers in the codebase: - they build and exec a local envoy binary, starting kafka, zookeeper, and related infra within the test run, which is at odds with envoy's integration testing practices and general bazel/ci hygiene. - keeping this approach introduces dependencies, slows ci, impedes developer experience, and adds tech debt. there is no architectural justification for this anti-pattern to remain for the kafka filter alone. no other filters test in this way and for good reason. all protocol, filter, and serialization logic is already well-covered by c++ unit and integration tests. **proposal/plan:** - remove the legacy python-based kafka integration tests and their dependencies (e.g., , bespoke infra) - enhance the existing example in envoyproxy/examples to robustly cover broker filter test scenarios: topic ops, admin, produce/consume, metrics, consumer groups - create a new example to cover mesh filter's multi-cluster routing logic the examples repo already runs as part of ci in both projects; moving this coverage over aligns with maintainability best-practices and will unblock bzlmod migration. *relevant links:* - [ ]( - [ ]( - [ ](",[],['FEATURE'],"['enhancement', 'area/kafka']",github,2026-01-27T10:32:02Z,,"remove python-based kafka integration tests and migrate kafka coverage to examples repo **title**: remove python-based kafka integration tests and migrate kafka coverage to examples repo **description:** the python-based kafka integration tests located in and are problematic outliers in the codebase: - they build and exec a local envoy binary, starting kafka, zookeeper, and related infra within the test run, which is at odds with envoy's integration testing practices and general bazel/ci hygiene. - keeping this approach introduces dependencies, slows ci, impedes developer experience, and adds tech debt. there is no architectural justification for this anti-pattern to remain for the kafka filter alone. no other filters test in this way and for good reason. all protocol, filter, and serialization logic is already well-covered by c++ unit and integration tests. **proposal/plan:** - remove the legacy python-based kafka integration tests and their dependencies (e.g., , bespoke infra) - enhance the existing example in envoyproxy/examples to robustly cover broker filter test scenarios: topic ops, admin, produce/consume, metrics, consumer groups - create a new example to cover mesh filter's multi-cluster routing logic the examples repo already runs as part of ci in both projects; moving this coverage over aligns with maintainability best-practices and will unblock bzlmod migration. *relevant links:* - [ ]( - [ ]( - [ ](",2.724,Medium,0.839,functional impact
microsoft/vscode#290754,clear storage shouldn't be hidden but disabled when using ephemeral storage,"testing * configure to be ephemeral * check the overflow menu of the integrated browser * üòï there is no more entry about storage/cache, imo and for explaining what happens it shows as disabled as ""clear storage (ephemeral)""",[],['FEATURE'],"['feature-request', 'verification-needed', 'insiders-released', 'browser-integration']",github,2026-01-27T10:34:15Z,2026-01-27T20:13:07Z,"clear storage shouldn't be hidden but disabled when using ephemeral storage testing * configure to be ephemeral * check the overflow menu of the integrated browser * üòï there is no more entry about storage/cache, imo and for explaining what happens it shows as disabled as ""clear storage (ephemeral)""",3.6,Critical,1.0,crash-like behavior
cockroachdb/cockroach#161853,roachtest: tpcc-nowait/isolation-level=mixed/nodes=3/w=1 failed,roachtest.tpcc-nowait/isolation-level=mixed/nodes=3/w=1 [failed]( with [artifacts]( on release-26.1 @ [be336920639819feab95e8fc2620ba50e2c71bbf]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21027386-1769497947-51-n4cpu16-0001 | 34.26.249.95 | 10.142.0.207 | | teamcity-21027386-1769497947-51-n4cpu16-0002 | 35.237.118.180 | 10.142.0.236 | | teamcity-21027386-1769497947-51-n4cpu16-0003 | 35.190.154.74 | 10.142.1.173 | | teamcity-21027386-1769497947-51-n4cpu16-0004 | 35.227.11.109 | 10.142.0.30 | parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=16 - diskcount=0 - encrypted=false - metamorphicleases=default - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( same failure on other branches - roachtest: tpcc-nowait/isolation-level=mixed/nodes=3/w=1 failed [c-test-failure o-roachtest o-robot t-testeng branch-release-25.3.8-rc release-blocker] - roachtest: tpcc-nowait/isolation-level=mixed/nodes=3/w=1 failed [a-kv-transactions c-test-failure o-roachtest o-robot p-3 t-kv x-duplicate branch-master] /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-59120,[],['TESTING'],"['C-test-failure', 'O-robot', 'X-duplicate', 'O-roachtest', 'release-blocker', 'T-testeng', 'X-infra-flake', 'branch-release-26.1']",github,2026-01-27T10:34:59Z,2026-01-28T06:11:37Z,roachtest: tpcc-nowait/isolation-level=mixed/nodes=3/w=1 failed roachtest.tpcc-nowait/isolation-level=mixed/nodes=3/w=1 [failed]( with [artifacts]( on release-26.1 @ [be336920639819feab95e8fc2620ba50e2c71bbf]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21027386-1769497947-51-n4cpu16-0001 | 34.26.249.95 | 10.142.0.207 | | teamcity-21027386-1769497947-51-n4cpu16-0002 | 35.237.118.180 | 10.142.0.236 | | teamcity-21027386-1769497947-51-n4cpu16-0003 | 35.190.154.74 | 10.142.1.173 | | teamcity-21027386-1769497947-51-n4cpu16-0004 | 35.227.11.109 | 10.142.0.30 | parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=16 - diskcount=0 - encrypted=false - metamorphicleases=default - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( same failure on other branches - roachtest: tpcc-nowait/isolation-level=mixed/nodes=3/w=1 failed [c-test-failure o-roachtest o-robot t-testeng branch-release-25.3.8-rc release-blocker] - roachtest: tpcc-nowait/isolation-level=mixed/nodes=3/w=1 failed [a-kv-transactions c-test-failure o-roachtest o-robot p-3 t-kv x-duplicate branch-master] /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-59120,2.888,Medium,0.876,functional impact
tensorflow/tensorflow#108936,"gradienttape.jacobian (hessian) fails in graph mode for tf.cond / autograph if with experimental_use_pfor=true, while eager mode is correct (tf 2.20.0, ubuntu 24.04.3, rtx 3090)","### issue type bug ### have you reproduced the bug with tensorflow nightly? no ### source binary ### tensorflow version 2.20.0 ### custom code yes ### os platform and distribution ubuntu 24.04.3 lts (noble numbat) ### mobile device n/a ### python version python 3.13.5 (anaconda build) ### bazel version n/a ### gcc/compiler version n/a ### cuda/cudnn version nvidia driver reports: cuda version 12.4 (via nvidia-smi) ### gpu model and memory 4√ó nvidia geforce rtx 3090. each shows 24576 mib (24gb) in nvidia-smi. tf device creation log shows ~22446 mb per gpu. ### current behavior? computing a second derivative (hessian) by nesting tf.gradienttape and calling: `t2.jacobian(g, x, experimental_use_pfor=true) ` fails in graph mode (inside .function / tf.config.run_functions_eagerly(false)) with: > encountered an exception while vectorizing the jacobian computation but the same code works in eager mode, returning the correct scalar hessian value 2.0 for y = x*x at x=0.5. this failure occurs for: - autograph-converted python if inside .function - explicit tf.cond inside .function ### standalone code to reproduce the issue ### relevant log output",[],['BUG'],['type:bug'],github,2026-01-27T10:39:18Z,,"gradienttape.jacobian (hessian) fails in graph mode for tf.cond / autograph if with experimental_use_pfor=true, while eager mode is correct (tf 2.20.0, ubuntu 24.04.3, rtx 3090) ### issue type bug ### have you reproduced the bug with tensorflow nightly? no ### source binary ### tensorflow version 2.20.0 ### custom code yes ### os platform and distribution ubuntu 24.04.3 lts (noble numbat) ### mobile device n/a ### python version python 3.13.5 (anaconda build) ### bazel version n/a ### gcc/compiler version n/a ### cuda/cudnn version nvidia driver reports: cuda version 12.4 (via nvidia-smi) ### gpu model and memory 4√ó nvidia geforce rtx 3090. each shows 24576 mib (24gb) in nvidia-smi. tf device creation log shows ~22446 mb per gpu. ### current behavior? computing a second derivative (hessian) by nesting tf.gradienttape and calling: `t2.jacobian(g, x, experimental_use_pfor=true) ` fails in graph mode (inside .function / tf.config.run_functions_eagerly(false)) with: > encountered an exception while vectorizing the jacobian computation but the same code works in eager mode, returning the correct scalar hessian value 2.0 for y = x*x at x=0.5. this failure occurs for: - autograph-converted python if inside .function - explicit tf.cond inside .function ### standalone code to reproduce the issue ### relevant log output",4.2,Critical,1.0,system-wide impact
microsoft/vscode#290759,integrated browser should reload on cmd+r,testing * open integrated browser * notice that the reload btn is cound * this should be because that's the default reload gesture,[],['FEATURE'],"['feature-request', 'verification-needed', 'insiders-released', 'browser-integration']",github,2026-01-27T10:40:32Z,2026-01-27T20:30:30Z,integrated browser should reload on cmd+r testing * open integrated browser * notice that the reload btn is cound * this should be because that's the default reload gesture,1.4,Low,0.538,localized low-impact
microsoft/vscode#290761,make it that -storage doesn't work for workspace-less windows,"testing * configure to be * open an empty new window * open integrated browser * :bug: no caching, we should either spell this out in the description or use a fallback non-workspace storage target",[],['BUG'],"['bug', 'insiders-released', 'browser-integration']",github,2026-01-27T10:43:20Z,2026-01-27T20:13:06Z,"make it that -storage doesn't work for workspace-less windows testing * configure to be * open an empty new window * open integrated browser * :bug: no caching, we should either spell this out in the description or use a fallback non-workspace storage target",2.422,Medium,0.77,functional impact
python/cpython#144270,xml.etree.elementtree.subelement definition is incorrect / inconsistent with c,"# bug report ### bug description: this code runs fine on python 3.13.7: however, pyright *correctly* detects a type error in the call. has this python signature: the parameter has already been assigned, so this *should* fail. i believe the fact that it doesn't is because it's actually using a c implementation, and the c implementation has *this* type signature: which *does* allow specifying . so i think the solution is just to add that into the python code. ### cpython versions tested on: 3.13 ### operating systems tested on: linux",[],['BUG'],"['type-bug', 'stdlib', 'topic-XML']",github,2026-01-27T10:49:21Z,,"xml.etree.elementtree.subelement definition is incorrect / inconsistent with c # bug report ### bug description: this code runs fine on python 3.13.7: however, pyright *correctly* detects a type error in the call. has this python signature: the parameter has already been assigned, so this *should* fail. i believe the fact that it doesn't is because it's actually using a c implementation, and the c implementation has *this* type signature: which *does* allow specifying . so i think the solution is just to add that into the python code. ### cpython versions tested on: 3.13 ### operating systems tested on: linux",2.609,Medium,0.813,functional impact
kubernetes/kubernetes#136562,[cleanup] default kube_proxy_daemonset to true in gce cluster scripts,"### what happened? while auditing stale todo comments in the gce configuration scripts (following my recent merged cleanup pr [ ]( i identified a 9-year-old todo at line 525 of cluster/gce/config-test.sh. the environment variable kube_proxy_daemonset still defaults to false, causing kube-proxy to be deployed as static pods in certain gce test configurations. ### what did you expect to happen? i expect gce cluster configurations to align with modern kubernetes standards (like kubeadm), where kube-proxy is managed as a daemonset by default. this change will allow for the future removal of legacy static-pod logic in cluster/gce/gci/configure-helper.sh, significantly reducing technical debt. ### how can we reproduce it (as minimally and precisely as possible)? 1. inspect cluster/gce/config-test.sh at line 527. 2. observe kube_proxy_daemonset=${kube_proxy_daemonset:-false}. 3. this stems from commit 1102656 (pr ) from 2017, where the variable was introduced as an experimental option but never flipped to the default. ### anything else we need to know? i am a senior devops engineer and am happy to take ownership of this transition. my plan is to: 1. flip the default to true. 2. monitor ci signal (pull-kubernetes-e2e-gce) to ensure no regressions. 3. submit a follow-up pr to deprecate the false code path once verified. ### kubernetes version v1.36-alpha ### cloud provider gce ### os version ### install tools ### container runtime (cri) and version (if applicable) ### related plugins (cni, csi, ...) and versions (if applicable)",[],['BUG'],"['kind/bug', 'needs-sig', 'needs-triage']",github,2026-01-27T11:08:10Z,2026-01-27T11:08:28Z,"[cleanup] default kube_proxy_daemonset to true in gce cluster scripts ### what happened? while auditing stale todo comments in the gce configuration scripts (following my recent merged cleanup pr [ ]( i identified a 9-year-old todo at line 525 of cluster/gce/config-test.sh. the environment variable kube_proxy_daemonset still defaults to false, causing kube-proxy to be deployed as static pods in certain gce test configurations. ### what did you expect to happen? i expect gce cluster configurations to align with modern kubernetes standards (like kubeadm), where kube-proxy is managed as a daemonset by default. this change will allow for the future removal of legacy static-pod logic in cluster/gce/gci/configure-helper.sh, significantly reducing technical debt. ### how can we reproduce it (as minimally and precisely as possible)? 1. inspect cluster/gce/config-test.sh at line 527. 2. observe kube_proxy_daemonset=${kube_proxy_daemonset:-false}. 3. this stems from commit 1102656 (pr ) from 2017, where the variable was introduced as an experimental option but never flipped to the default. ### anything else we need to know? i am a senior devops engineer and am happy to take ownership of this transition. my plan is to: 1. flip the default to true. 2. monitor ci signal (pull-kubernetes-e2e-gce) to ensure no regressions. 3. submit a follow-up pr to deprecate the false code path once verified. ### kubernetes version v1.36-alpha ### cloud provider gce ### os version ### install tools ### container runtime (cri) and version (if applicable) ### related plugins (cni, csi, ...) and versions (if applicable)",4.6,Critical,1.0,crash-like behavior
kubernetes/kubernetes#136563,[cleanup] default kube_proxy_daemonset to true in gce cluster scripts,"### what happened? while auditing stale todo comments in the gce configuration scripts (following my recent merged cleanup pr ), i identified a 9-year-old todo at line 525 of cluster/gce/config-test.sh. the environment variable kube_proxy_daemonset still defaults to false, causing kube-proxy to be deployed as static pods in certain gce test configurations. ### what did you expect to happen? i expect gce cluster configurations to align with modern kubernetes standards (like kubeadm), where kube-proxy is managed as a daemonset by default. this change will allow for the future removal of legacy static-pod logic in cluster/gce/gci/configure-helper.sh, significantly reducing technical debt. ### how can we reproduce it (as minimally and precisely as possible)? 1. inspect cluster/gce/config-test.sh at line 527. 2. observe kube_proxy_daemonset=${kube_proxy_daemonset:-false}. 3. this stems from commit 1102656 (pr ) from 2017, where the variable was introduced as an experimental option but never flipped to the default. ### anything else we need to know? i am a devops engineer and am happy to take ownership of this transition. my plan is to: 1. flip the default to true. 2. monitor ci signal (pull-kubernetes-e2e-gce) to ensure no regressions. 3. submit a follow-up pr to deprecate the false code path once verified. ### kubernetes version v1.36-alpha ### cloud provider gcp / gce ### os version n/a - script logic audit ### install tools kube-up.sh ### container runtime (cri) and version (if applicable) containerd ### related plugins (cni, csi, ...) and versions (if applicable) kube-proxy",[],"['CLEANUP', 'TESTING']","['kind/cleanup', 'sig/testing', 'needs-triage']",github,2026-01-27T11:14:37Z,2026-01-27T21:57:57Z,"[cleanup] default kube_proxy_daemonset to true in gce cluster scripts ### what happened? while auditing stale todo comments in the gce configuration scripts (following my recent merged cleanup pr ), i identified a 9-year-old todo at line 525 of cluster/gce/config-test.sh. the environment variable kube_proxy_daemonset still defaults to false, causing kube-proxy to be deployed as static pods in certain gce test configurations. ### what did you expect to happen? i expect gce cluster configurations to align with modern kubernetes standards (like kubeadm), where kube-proxy is managed as a daemonset by default. this change will allow for the future removal of legacy static-pod logic in cluster/gce/gci/configure-helper.sh, significantly reducing technical debt. ### how can we reproduce it (as minimally and precisely as possible)? 1. inspect cluster/gce/config-test.sh at line 527. 2. observe kube_proxy_daemonset=${kube_proxy_daemonset:-false}. 3. this stems from commit 1102656 (pr ) from 2017, where the variable was introduced as an experimental option but never flipped to the default. ### anything else we need to know? i am a devops engineer and am happy to take ownership of this transition. my plan is to: 1. flip the default to true. 2. monitor ci signal (pull-kubernetes-e2e-gce) to ensure no regressions. 3. submit a follow-up pr to deprecate the false code path once verified. ### kubernetes version v1.36-alpha ### cloud provider gcp / gce ### os version n/a - script logic audit ### install tools kube-up.sh ### container runtime (cri) and version (if applicable) containerd ### related plugins (cni, csi, ...) and versions (if applicable) kube-proxy",2.526,Medium,0.794,crash-like behavior
cockroachdb/cockroach#161854,roachtest: unoptimized-query-oracle/disable-rules=half/rand-tables failed,roachtest.unoptimized-query-oracle/disable-rules=half/rand-tables [failed]( with [artifacts]( on release-24.3 @ [054227773dd341b03dd8128064bfbb0b37bd6a36]( parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - encrypted=false - runtimeassertionsbuild=false - ssd=0 help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( [this test on roachdash]( | [improve this report!]( jira issue: crdb-59121,[],['TESTING'],"['C-test-failure', 'O-robot', 'O-roachtest', 'O-rsg', 'release-blocker', 'T-sql-queries', 'branch-release-24.3']",github,2026-01-27T11:16:49Z,,roachtest: unoptimized-query-oracle/disable-rules=half/rand-tables failed roachtest.unoptimized-query-oracle/disable-rules=half/rand-tables [failed]( with [artifacts]( on release-24.3 @ [054227773dd341b03dd8128064bfbb0b37bd6a36]( parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - encrypted=false - runtimeassertionsbuild=false - ssd=0 help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( [this test on roachdash]( | [improve this report!]( jira issue: crdb-59121,3.09,High,0.922,functional impact
microsoft/vscode#290771,support cmd+]/[ to move forth/back between pages in integrated browser,"testing currently cmd+arrow left/right are supported but it would be great if the browser supported cmd+]/[, which i think are also standard keybindings for this behavior across mac apps, eg finder",[],['FEATURE'],"['feature-request', 'verification-needed', 'insiders-released', 'browser-integration']",github,2026-01-27T11:18:09Z,2026-01-27T20:30:29Z,"support cmd+]/[ to move forth/back between pages in integrated browser testing currently cmd+arrow left/right are supported but it would be great if the browser supported cmd+]/[, which i think are also standard keybindings for this behavior across mac apps, eg finder",1.4,Low,0.538,localized low-impact
microsoft/vscode#290774,changing inline chat model does not persist in session,testing ![image](,[],['BUG'],['bug'],github,2026-01-27T11:29:26Z,,changing inline chat model does not persist in session testing ![image](,4.6,Critical,1.0,crash-like behavior
microsoft/vscode#290789,background agent does not work,"testing i started a background session and after 2 minutes it just stopped with nothing to show: ![image]( i have this error in the ouput which make it seem like it is mcp related: here are my mcp servers. see how they are all stopped, this should not be causing any mcp issues. also, if there is an issue with the mcp then it would be great to just continue without the mcp and show a message/error somewhere mentioning that the configured mcp could not be used.",[],['BUG'],"['bug', 'important', 'chat-background-agent']",github,2026-01-27T11:47:19Z,2026-01-28T03:38:01Z,"background agent does not work testing i started a background session and after 2 minutes it just stopped with nothing to show: ![image]( i have this error in the ouput which make it seem like it is mcp related: here are my mcp servers. see how they are all stopped, this should not be causing any mcp issues. also, if there is an issue with the mcp then it would be great to just continue without the mcp and show a message/error somewhere mentioning that the configured mcp could not be used.",2.204,Medium,0.721,functional impact
microsoft/vscode#290793,chat progress badge not useful and just distracts me,"right now we show a progress badge for chat that is running. this badge is not useful to me and distracting: * it does not mean i need to act on it * my chat is almost always running when i use vs code, which means i see this badge all the time * i would prefer to see a badge if chat asks for my input or i need to act on something",['Chat progress badge not useful and just distracts me (fix #290793) (#291200)'],['BUG'],"['bug', 'unreleased', 'chat-agents-view']",github,2026-01-27T11:50:32Z,2026-01-28T08:55:00Z,"chat progress badge not useful and just distracts me right now we show a progress badge for chat that is running. this badge is not useful to me and distracting: * it does not mean i need to act on it * my chat is almost always running when i use vs code, which means i see this badge all the time * i would prefer to see a badge if chat asks for my input or i need to act on something Chat progress badge not useful and just distracts me (fix #290793) (#291200)",2.465,Medium,0.78,functional impact
numpy/numpy#30732,bug: inplace updating not working as expected with advanced indexing.,"### describe the issue: i have an int64 array, mesh tuple (as constructed by ix_) and a boolean array. the shape of the integer array indexed by the mesh is the same as the boolean array. then we want to add 1 to all places where the boolean array. so: in the example, i have tried multiple ways and the update seems inconsistent to me. ### reproduce the code example: ### error message: ### python and numpy versions: i tried multiple versions with the same result: 2.4.1 3.13.9 (main, nov 19 2025, 22:47:49) [clang 21.1.4 ] 2.3.0 3.13.9 (main, nov 19 2025, 22:47:49) [clang 21.1.4 ] 2.2.0 3.13.9 (main, nov 19 2025, 22:47:49) [clang 21.1.4 ] ### runtime environment: 2.4.1: [{'numpy_version': '2.4.1', 'python': '3.13.9 (main, nov 19 2025, 22:47:49) [clang 21.1.4 ]', 'uname': uname_result(system='linux', node='rolfdebianlt', release='6.12.63+deb13-amd64', version=' smp preempt_dynamic debian 6.12.63-1 (2025-12-30)', machine='x86_64')}, {'simd_extensions': {'baseline': ['x86_v2'], 'found': ['x86_v3'], 'not_found': ['x86_v4', 'avx512_icl', 'avx512_spr']}}, {'ignore_floating_point_errors_in_matmul': false}, {'architecture': 'haswell', 'filepath': '/home/rolf/thunderstock/test/.venv/lib/python3.13/site-packages/numpy.libs/libscipy_openblas64_-fdde5778.so', 'internal_api': 'openblas', 'num_threads': 20, 'prefix': 'libscipy_openblas', 'threading_layer': 'pthreads', 'user_api': 'blas', 'version': '0.3.30'}] 2.3.0: [{'numpy_version': '2.3.0', 'python': '3.13.9 (main, nov 19 2025, 22:47:49) [clang 21.1.4 ]', 'uname': uname_result(system='linux', node='rolfdebianlt', release='6.12.63+deb13-amd64', version=' smp preempt_dynamic debian 6.12.63-1 (2025-12-30)', machine='x86_64')}, {'simd_extensions': {'baseline': ['sse', 'sse2', 'sse3'], 'found': ['ssse3', 'sse41', 'popcnt', 'sse42', 'avx', 'f16c', 'fma3', 'avx2'], 'not_found': ['avx512f', 'avx512cd', 'avx512_knl', 'avx512_knm', 'avx512_skx', 'avx512_clx', 'avx512_cnl', 'avx512_icl', 'avx512_spr']}}, {'architecture': 'haswell', 'filepath': '/home/rolf/thunderstock/test/.venv/lib/python3.13/site-packages/numpy.libs/libscipy_openblas64_-56d6093b.so', 'internal_api': 'openblas', 'num_threads': 20, 'prefix': 'libscipy_openblas', 'threading_layer': 'pthreads', 'user_api': 'blas', 'version': '0.3.29'}] 2.2.0: [{'numpy_version': '2.2.0', 'python': '3.13.9 (main, nov 19 2025, 22:47:49) [clang 21.1.4 ]', 'uname': uname_result(system='linux', node='rolfdebianlt', release='6.12.63+deb13-amd64', version=' smp preempt_dynamic debian 6.12.63-1 (2025-12-30)', machine='x86_64')}, {'simd_extensions': {'baseline': ['sse', 'sse2', 'sse3'], 'found': ['ssse3', 'sse41', 'popcnt', 'sse42', 'avx', 'f16c', 'fma3', 'avx2'], 'not_found': ['avx512f', 'avx512cd', 'avx512_knl', 'avx512_knm', 'avx512_skx', 'avx512_clx', 'avx512_cnl', 'avx512_icl']}}, {'architecture': 'haswell', 'filepath': '/home/rolf/thunderstock/test/.venv/lib/python3.13/site-packages/numpy.libs/libscipy_openblas64_-6bb31eeb.so', 'internal_api': 'openblas', 'num_threads': 20, 'prefix': 'libscipy_openblas', 'threading_layer': 'pthreads', 'user_api': 'blas', 'version': '0.3.28'}] ### how does this issue affect you or how did you find it: the code does work for my colleague on his macbook, but fails on my debian linux system, and also seems to fail on our cloud (aws lambda x86). i found it because our lambda was timing out, and testing locally i could find an infinite loop. the above operation was assumed to work, and it does work as expected till we found this specific example. my colleague could not find the bug locally, as it was not reproducable on his machine, it was reproducable on mine. so in this case we had an impact, but for now an easy work around is in place. but the above structure might also be in place in other calculations, which do not create infinite loops, but gives wrong calculations. so i cannot for sure say how much it does impact us right now.",[],['BUG'],['00 - Bug'],github,2026-01-27T12:05:33Z,,"bug: inplace updating not working as expected with advanced indexing. ### describe the issue: i have an int64 array, mesh tuple (as constructed by ix_) and a boolean array. the shape of the integer array indexed by the mesh is the same as the boolean array. then we want to add 1 to all places where the boolean array. so: in the example, i have tried multiple ways and the update seems inconsistent to me. ### reproduce the code example: ### error message: ### python and numpy versions: i tried multiple versions with the same result: 2.4.1 3.13.9 (main, nov 19 2025, 22:47:49) [clang 21.1.4 ] 2.3.0 3.13.9 (main, nov 19 2025, 22:47:49) [clang 21.1.4 ] 2.2.0 3.13.9 (main, nov 19 2025, 22:47:49) [clang 21.1.4 ] ### runtime environment: 2.4.1: [{'numpy_version': '2.4.1', 'python': '3.13.9 (main, nov 19 2025, 22:47:49) [clang 21.1.4 ]', 'uname': uname_result(system='linux', node='rolfdebianlt', release='6.12.63+deb13-amd64', version=' smp preempt_dynamic debian 6.12.63-1 (2025-12-30)', machine='x86_64')}, {'simd_extensions': {'baseline': ['x86_v2'], 'found': ['x86_v3'], 'not_found': ['x86_v4', 'avx512_icl', 'avx512_spr']}}, {'ignore_floating_point_errors_in_matmul': false}, {'architecture': 'haswell', 'filepath': '/home/rolf/thunderstock/test/.venv/lib/python3.13/site-packages/numpy.libs/libscipy_openblas64_-fdde5778.so', 'internal_api': 'openblas', 'num_threads': 20, 'prefix': 'libscipy_openblas', 'threading_layer': 'pthreads', 'user_api': 'blas', 'version': '0.3.30'}] 2.3.0: [{'numpy_version': '2.3.0', 'python': '3.13.9 (main, nov 19 2025, 22:47:49) [clang 21.1.4 ]', 'uname': uname_result(system='linux', node='rolfdebianlt', release='6.12.63+deb13-amd64', version=' smp preempt_dynamic debian 6.12.63-1 (2025-12-30)', machine='x86_64')}, {'simd_extensions': {'baseline': ['sse', 'sse2', 'sse3'], 'found': ['ssse3', 'sse41', 'popcnt', 'sse42', 'avx', 'f16c', 'fma3', 'avx2'], 'not_found': ['avx512f', 'avx512cd', 'avx512_knl', 'avx512_knm', 'avx512_skx', 'avx512_clx', 'avx512_cnl', 'avx512_icl', 'avx512_spr']}}, {'architecture': 'haswell', 'filepath': '/home/rolf/thunderstock/test/.venv/lib/python3.13/site-packages/numpy.libs/libscipy_openblas64_-56d6093b.so', 'internal_api': 'openblas', 'num_threads': 20, 'prefix': 'libscipy_openblas', 'threading_layer': 'pthreads', 'user_api': 'blas', 'version': '0.3.29'}] 2.2.0: [{'numpy_version': '2.2.0', 'python': '3.13.9 (main, nov 19 2025, 22:47:49) [clang 21.1.4 ]', 'uname': uname_result(system='linux', node='rolfdebianlt', release='6.12.63+deb13-amd64', version=' smp preempt_dynamic debian 6.12.63-1 (2025-12-30)', machine='x86_64')}, {'simd_extensions': {'baseline': ['sse', 'sse2', 'sse3'], 'found': ['ssse3', 'sse41', 'popcnt', 'sse42', 'avx', 'f16c', 'fma3', 'avx2'], 'not_found': ['avx512f', 'avx512cd', 'avx512_knl', 'avx512_knm', 'avx512_skx', 'avx512_clx', 'avx512_cnl', 'avx512_icl']}}, {'architecture': 'haswell', 'filepath': '/home/rolf/thunderstock/test/.venv/lib/python3.13/site-packages/numpy.libs/libscipy_openblas64_-6bb31eeb.so', 'internal_api': 'openblas', 'num_threads': 20, 'prefix': 'libscipy_openblas', 'threading_layer': 'pthreads', 'user_api': 'blas', 'version': '0.3.28'}] ### how does this issue affect you or how did you find it: the code does work for my colleague on his macbook, but fails on my debian linux system, and also seems to fail on our cloud (aws lambda x86). i found it because our lambda was timing out, and testing locally i could find an infinite loop. the above operation was assumed to work, and it does work as expected till we found this specific example. my colleague could not find the bug locally, as it was not reproducable on his machine, it was reproducable on mine. so in this case we had an impact, but for now an easy work around is in place. but the above structure might also be in place in other calculations, which do not create infinite loops, but gives wrong calculations. so i cannot for sure say how much it does impact us right now.",2.685,Medium,0.83,functional impact
microsoft/vscode#290800,edited files not visible in agent view for empty workspace,"* open repo in vscode * use background agent to make some changes in a worktree. * verify you can see the edited files at the end of the session * open empty workspace and go into agents view and select the above session * the session is displayed, however none of the edited files are visible its completely empty (no files are listed). this only happens for background sessions started in a workspace and then viewing the session in an empty workspace",[],['BUG'],"['bug', 'chat-background-agent']",github,2026-01-27T12:09:26Z,,"edited files not visible in agent view for empty workspace * open repo in vscode * use background agent to make some changes in a worktree. * verify you can see the edited files at the end of the session * open empty workspace and go into agents view and select the above session * the session is displayed, however none of the edited files are visible its completely empty (no files are listed). this only happens for background sessions started in a workspace and then viewing the session in an empty workspace",4.6,Critical,1.0,crash-like behavior
nodejs/node#61543,[util] allow to take in hex colors,"### what is the problem this feature will solve? many userland projects use manual hex codes for color printing to the terminal. 256 and true color printing in supported terminals is supported in several libraries that existed prior to [ ]( for example: * [ ]( supports 256 and truecolor support with fallbacks to base 16 colors * [ ]( has full 256 and truecolor support with fallbacks to terminal-supported levels * [ ]( dedicated library for terminal gradients (see their ['who uses gradient string']( for some pretty examples) however, only supports the [standard 16 colors]( ### what is the feature you are proposing to solve the problem? could take in hex codes? they could be detected as anything starting with . ### what alternatives have you considered? userland code could stick with raw hex code printing. but that's rather inconvenient and error-prone. plus, functions like respect the , and environment variables. was this discussed in the design of ? i searched around and couldn't find it.",[],['FEATURE'],['feature request'],github,2026-01-27T12:45:32Z,,"[util] allow to take in hex colors ### what is the problem this feature will solve? many userland projects use manual hex codes for color printing to the terminal. 256 and true color printing in supported terminals is supported in several libraries that existed prior to [ ]( for example: * [ ]( supports 256 and truecolor support with fallbacks to base 16 colors * [ ]( has full 256 and truecolor support with fallbacks to terminal-supported levels * [ ]( dedicated library for terminal gradients (see their ['who uses gradient string']( for some pretty examples) however, only supports the [standard 16 colors]( ### what is the feature you are proposing to solve the problem? could take in hex codes? they could be detected as anything starting with . ### what alternatives have you considered? userland code could stick with raw hex code printing. but that's rather inconvenient and error-prone. plus, functions like respect the , and environment variables. was this discussed in the design of ? i searched around and couldn't find it.",1.4,Low,0.538,localized low-impact
cockroachdb/cockroach#161856,roachtest: import/nodeshutdown/coordinator/distmerge=true/nodes=4 failed,roachtest.import/nodeshutdown/coordinator/distmerge=true/nodes=4 [failed]( with [artifacts]( on master @ [5b0bfce304d2475a3ede5c8f02cfc381347fcab9]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21027236-1769498019-115-n4cpu4-0001 | 34.139.5.184 | 10.142.3.15 | | teamcity-21027236-1769498019-115-n4cpu4-0002 | 35.243.166.166 | 10.142.3.11 | | teamcity-21027236-1769498019-115-n4cpu4-0003 | 34.139.55.185 | 10.142.3.8 | | teamcity-21027236-1769498019-115-n4cpu4-0004 | 34.26.200.24 | 10.142.1.147 | parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - fs=ext4 - localssd=true - metamorphicleases=leader - metamorphicwritebuffering=true - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( [this test on roachdash]( | [improve this report!]( jira issue: crdb-59122,[],['TESTING'],"['C-test-failure', 'O-robot', 'O-roachtest', 'branch-master', 'release-blocker', 'T-sql-queries']",github,2026-01-27T12:56:17Z,2026-01-27T19:49:49Z,roachtest: import/nodeshutdown/coordinator/distmerge=true/nodes=4 failed roachtest.import/nodeshutdown/coordinator/distmerge=true/nodes=4 [failed]( with [artifacts]( on master @ [5b0bfce304d2475a3ede5c8f02cfc381347fcab9]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21027236-1769498019-115-n4cpu4-0001 | 34.139.5.184 | 10.142.3.15 | | teamcity-21027236-1769498019-115-n4cpu4-0002 | 35.243.166.166 | 10.142.3.11 | | teamcity-21027236-1769498019-115-n4cpu4-0003 | 34.139.55.185 | 10.142.3.8 | | teamcity-21027236-1769498019-115-n4cpu4-0004 | 34.26.200.24 | 10.142.1.147 | parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - fs=ext4 - localssd=true - metamorphicleases=leader - metamorphicwritebuffering=true - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( [this test on roachdash]( | [improve this report!]( jira issue: crdb-59122,3.203,High,0.948,system-wide impact
cockroachdb/cockroach#161857,pkg/sql/hints/hints_test: testhintcachemultinode failed,pkg/sql/hints/hints_test.testhintcachemultinode [failed]( on release-26.1 @ [be336920639819feab95e8fc2620ba50e2c71bbf]( parameters: - attempt=1 - race=true - run=1 - shard=1 help see also: [how to investigate a go test failure \(internal\)]( [this test on roachdash]( | [improve this report!]( jira issue: crdb-59123,[],['TESTING'],"['C-test-failure', 'C-investigation', 'O-robot', 'T-sql-queries', 'branch-release-26.1']",github,2026-01-27T13:08:47Z,,pkg/sql/hints/hints_test: testhintcachemultinode failed pkg/sql/hints/hints_test.testhintcachemultinode [failed]( on release-26.1 @ [be336920639819feab95e8fc2620ba50e2c71bbf]( parameters: - attempt=1 - race=true - run=1 - shard=1 help see also: [how to investigate a go test failure \(internal\)]( [this test on roachdash]( | [improve this report!]( jira issue: crdb-59123,1.6,Low,0.584,localized low-impact
numpy/numpy#30733,bug: binary builds deadlock due to openblas threading issue,"### describe the issue: opening a new issue as requested in . the reproducer code hangs when run with numpy 2.4.1 installed via pip. it can be made to work by setting omp_num_threads=1 or omp_num_threads=2. details of exact process i followed to install numpy and run the tests are below, where numpytest.py is the reproducer code: ### reproduce the code example: ### error message: ### python and numpy versions: 2.4.1 3.12.0 (main, oct 5 2023, 15:44:07) [clang 14.0.3 (clang-1403.0.22.14.1)] mac os 13.6.7 ### runtime environment: _no response_ ### how does this issue affect you or how did you find it: _no response_ (formatted by )",[],['BUG'],"['00 - Bug', '27 - OpenBLAS']",github,2026-01-27T13:13:42Z,,"bug: binary builds deadlock due to openblas threading issue ### describe the issue: opening a new issue as requested in . the reproducer code hangs when run with numpy 2.4.1 installed via pip. it can be made to work by setting omp_num_threads=1 or omp_num_threads=2. details of exact process i followed to install numpy and run the tests are below, where numpytest.py is the reproducer code: ### reproduce the code example: ### error message: ### python and numpy versions: 2.4.1 3.12.0 (main, oct 5 2023, 15:44:07) [clang 14.0.3 (clang-1403.0.22.14.1)] mac os 13.6.7 ### runtime environment: _no response_ ### how does this issue affect you or how did you find it: _no response_ (formatted by )",4.6,Critical,1.0,crash-like behavior
envoyproxy/envoy#43184,tracing improvements (with a focus on opentelemetry),"**tracing improvements (with a focus on opentelemetry)** the opentelemetry tracer that currently exists in envoy needs some updating for both accuracy and flexibility. see the doc below for discussion. additionally, some of the core envoy tracing behavior could use improvements, especially around the interaction with async streams. [doc]( please feel free to request edit/comment access on the above doc.",[],['FEATURE'],"['area/observability', 'enhancement', 'area/tracing', 'help wanted']",github,2026-01-27T13:46:19Z,,"tracing improvements (with a focus on opentelemetry) **tracing improvements (with a focus on opentelemetry)** the opentelemetry tracer that currently exists in envoy needs some updating for both accuracy and flexibility. see the doc below for discussion. additionally, some of the core envoy tracing behavior could use improvements, especially around the interaction with async streams. [doc]( please feel free to request edit/comment access on the above doc.",1.4,Low,0.538,localized low-impact
cockroachdb/cockroach#161860,roachtest: ldr/conflict failed,"**note:** this build has runtime assertions enabled. if the same failure was hit in a run without assertions enabled, there should be a similar failure without this message. if there isn& ;t one, then this failure is likely due to an assertion violation or (assertion) timeout. roachtest.ldr/conflict [failed]( with [artifacts]( on release-26.1.0-rc @ [6c9b2f778bd29895383d493a2fa4490323499ecf]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21027401-1769498110-142-n7cpu4-0001 | 34.73.240.227 | 10.142.2.180 | | teamcity-21027401-1769498110-142-n7cpu4-0002 | 34.138.90.102 | 10.142.2.122 | | teamcity-21027401-1769498110-142-n7cpu4-0003 | 34.26.134.239 | 10.142.3.61 | | teamcity-21027401-1769498110-142-n7cpu4-0004 | 34.26.57.201 | 10.142.3.60 | | teamcity-21027401-1769498110-142-n7cpu4-0005 | 34.148.74.130 | 10.142.3.58 | | teamcity-21027401-1769498110-142-n7cpu4-0006 | 34.74.75.103 | 10.142.3.54 | | teamcity-21027401-1769498110-142-n7cpu4-0007 | 35.243.137.36 | 10.142.0.195 | parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - fs=ext4 - localssd=false - metamorphicleases=expiration - runtimeassertionsbuild=true help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( same failure on other branches - roachtest: ldr/conflict failed [a-disaster-recovery c-test-failure o-roachtest o-robot t-disaster-recovery branch-release-25.4.3-rc] - roachtest: ldr/conflict failed [a-disaster-recovery b-runtime-assertions-enabled c-test-failure o-roachtest o-robot t-disaster-recovery branch-release-26.1] - roachtest: ldr/conflict failed [a-disaster-recovery b-runtime-assertions-enabled c-test-failure o-roachtest o-robot p-2 t-disaster-recovery branch-release-25.4] - roachtest: ldr/conflict failed [a-disaster-recovery c-test-failure o-roachtest o-robot t-disaster-recovery branch-master] /cc /disaster-recovery [this test on roachdash]( | [improve this report!]( jira issue: crdb-59124",[],['TESTING'],"['C-test-failure', 'O-robot', 'A-disaster-recovery', 'O-roachtest', 'release-blocker', 'T-disaster-recovery', 'B-runtime-assertions-enabled', 'branch-release-26.1.0-rc']",github,2026-01-27T13:52:51Z,,"roachtest: ldr/conflict failed **note:** this build has runtime assertions enabled. if the same failure was hit in a run without assertions enabled, there should be a similar failure without this message. if there isn& ;t one, then this failure is likely due to an assertion violation or (assertion) timeout. roachtest.ldr/conflict [failed]( with [artifacts]( on release-26.1.0-rc @ [6c9b2f778bd29895383d493a2fa4490323499ecf]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21027401-1769498110-142-n7cpu4-0001 | 34.73.240.227 | 10.142.2.180 | | teamcity-21027401-1769498110-142-n7cpu4-0002 | 34.138.90.102 | 10.142.2.122 | | teamcity-21027401-1769498110-142-n7cpu4-0003 | 34.26.134.239 | 10.142.3.61 | | teamcity-21027401-1769498110-142-n7cpu4-0004 | 34.26.57.201 | 10.142.3.60 | | teamcity-21027401-1769498110-142-n7cpu4-0005 | 34.148.74.130 | 10.142.3.58 | | teamcity-21027401-1769498110-142-n7cpu4-0006 | 34.74.75.103 | 10.142.3.54 | | teamcity-21027401-1769498110-142-n7cpu4-0007 | 35.243.137.36 | 10.142.0.195 | parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - fs=ext4 - localssd=false - metamorphicleases=expiration - runtimeassertionsbuild=true help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( same failure on other branches - roachtest: ldr/conflict failed [a-disaster-recovery c-test-failure o-roachtest o-robot t-disaster-recovery branch-release-25.4.3-rc] - roachtest: ldr/conflict failed [a-disaster-recovery b-runtime-assertions-enabled c-test-failure o-roachtest o-robot t-disaster-recovery branch-release-26.1] - roachtest: ldr/conflict failed [a-disaster-recovery b-runtime-assertions-enabled c-test-failure o-roachtest o-robot p-2 t-disaster-recovery branch-release-25.4] - roachtest: ldr/conflict failed [a-disaster-recovery c-test-failure o-roachtest o-robot t-disaster-recovery branch-master] /cc /disaster-recovery [this test on roachdash]( | [improve this report!]( jira issue: crdb-59124",2.999,Medium,0.902,functional impact
microsoft/vscode#290819,flip default of workbench.browser.openlocalhostlinks to true?,right now is false by default. should we change that to true? maybe in feb/mar+,[],['FEATURE'],"['feature-request', 'browser-integration']",github,2026-01-27T14:02:20Z,,flip default of workbench.browser.openlocalhostlinks to true? right now is false by default. should we change that to true? maybe in feb/mar+,3.6,Critical,1.0,crash-like behavior
pandas-dev/pandas#63899,bug: dataframe derived from index can mutate the index,"### pandas version checks - [x] i have checked that this issue has not already been reported. - [x] i have confirmed this bug exists on the [latest version]( of pandas. - [x] i have confirmed this bug exists on the [main branch]( of pandas. ### reproducible example ### issue description when creating a dataframe from the columns of another dataframe the underlying array is reused, even when calling . changes to the derived dataframe affect the original index. this also seems to break some internal invariants -- we experienced ipython kernel crashes when trying to display a dataframe whose columns had been modified this way. ### expected behavior the original df should not be affected. ### installed versions installed versions ------------------ commit : 98ffe4c45edbfb9100aa8ee982ebc3f1f1453ed7 python : 3.11.14 python-bits : 64 os : windows os-release : 10 version : 10.0.26100 machine : amd64 processor : intel64 family 6 model 142 stepping 10, genuineintel byteorder : little lc_all : none lang : none locale : english_united kingdom.1252 pandas : 3.1.0.dev0+37.g98ffe4c45e numpy : 2.3.5 dateutil : 2.9.0.post0 pip : 25.3 cython : 3.2.4 sphinx : 9.0.4 ipython : 9.9.0 adbc-driver-postgresql: none adbc-driver-sqlite : none bs4 : 4.14.3 bottleneck : 1.6.0 fastparquet : 2025.12.0 fsspec : 2026.1.0 html5lib : 1.1 hypothesis : 6.151.2 gcsfs : 2026.1.0 jinja2 : 3.1.6 lxml.etree : 6.0.2 matplotlib : 3.10.8 numba : 0.63.1 numexpr : 2.14.1 odfpy : none openpyxl : 3.1.5 psycopg2 : 2.9.11 pymysql : 1.4.6 pyarrow : 23.0.0 pyiceberg : 0.10.0 pyreadstat : 1.3.3 pytest : 9.0.2 python-calamine : none pytz : 2025.2 pyxlsb : 1.0.10 s3fs : 2026.1.0 scipy : 1.17.0 sqlalchemy : 2.0.46 tables : 3.10.2 tabulate : 0.9.0 xarray : 2025.12.0 xlrd : 2.0.2 xlsxwriter : 3.2.9 zstandard : 0.25.0 qtpy : none pyqt5 : none",[],['BUG'],"['Bug', 'Copy / view semantics']",github,2026-01-27T14:04:01Z,,"bug: dataframe derived from index can mutate the index ### pandas version checks - [x] i have checked that this issue has not already been reported. - [x] i have confirmed this bug exists on the [latest version]( of pandas. - [x] i have confirmed this bug exists on the [main branch]( of pandas. ### reproducible example ### issue description when creating a dataframe from the columns of another dataframe the underlying array is reused, even when calling . changes to the derived dataframe affect the original index. this also seems to break some internal invariants -- we experienced ipython kernel crashes when trying to display a dataframe whose columns had been modified this way. ### expected behavior the original df should not be affected. ### installed versions installed versions ------------------ commit : 98ffe4c45edbfb9100aa8ee982ebc3f1f1453ed7 python : 3.11.14 python-bits : 64 os : windows os-release : 10 version : 10.0.26100 machine : amd64 processor : intel64 family 6 model 142 stepping 10, genuineintel byteorder : little lc_all : none lang : none locale : english_united kingdom.1252 pandas : 3.1.0.dev0+37.g98ffe4c45e numpy : 2.3.5 dateutil : 2.9.0.post0 pip : 25.3 cython : 3.2.4 sphinx : 9.0.4 ipython : 9.9.0 adbc-driver-postgresql: none adbc-driver-sqlite : none bs4 : 4.14.3 bottleneck : 1.6.0 fastparquet : 2025.12.0 fsspec : 2026.1.0 html5lib : 1.1 hypothesis : 6.151.2 gcsfs : 2026.1.0 jinja2 : 3.1.6 lxml.etree : 6.0.2 matplotlib : 3.10.8 numba : 0.63.1 numexpr : 2.14.1 odfpy : none openpyxl : 3.1.5 psycopg2 : 2.9.11 pymysql : 1.4.6 pyarrow : 23.0.0 pyiceberg : 0.10.0 pyreadstat : 1.3.3 pytest : 9.0.2 python-calamine : none pytz : 2025.2 pyxlsb : 1.0.10 s3fs : 2026.1.0 scipy : 1.17.0 sqlalchemy : 2.0.46 tables : 3.10.2 tabulate : 0.9.0 xarray : 2025.12.0 xlrd : 2.0.2 xlsxwriter : 3.2.9 zstandard : 0.25.0 qtpy : none pyqt5 : none",4.6,Critical,1.0,crash-like behavior
cockroachdb/cockroach#161861,improve allocations for rarely used histograms,"during each tick of a histogram metric, the current and previous windows are rotated such that , ([see code here]( for rarely used histograms, its often the case that both and have not been modified since the last tick, so rotating the windows is unnecessary. ideally, we would avoid allocating a new histogram in this situation. since this tick happens frequently (every ~5s today i believe) for every histogram metric, this unnecessary allocation can significantly affect the overall number of allocs in the system. see for more context. jira issue: crdb-59125",[],['FEATURE'],"['C-enhancement', 'T-observability']",github,2026-01-27T14:20:17Z,,"improve allocations for rarely used histograms during each tick of a histogram metric, the current and previous windows are rotated such that , ([see code here]( for rarely used histograms, its often the case that both and have not been modified since the last tick, so rotating the windows is unnecessary. ideally, we would avoid allocating a new histogram in this situation. since this tick happens frequently (every ~5s today i believe) for every histogram metric, this unnecessary allocation can significantly affect the overall number of allocs in the system. see for more context. jira issue: crdb-59125",1.4,Low,0.538,localized low-impact
microsoft/vscode#290828,"move/remove ""review"" command from context menu","today, the context menu has ""generate code > fix | review | generate docs | generate tests""",[],['BUG'],"['bug', 'inline-chat']",github,2026-01-27T14:34:26Z,,"move/remove ""review"" command from context menu today, the context menu has ""generate code > fix | review | generate docs | generate tests""",2.623,Medium,0.816,functional impact
microsoft/vscode#290830,make copilot and other agentic clis recognized as shells to change the terminal title,for windows: rest:,[],"['FEATURE', 'UI']","['feature-request', 'terminal-tabs', 'terminal-profiles']",github,2026-01-27T14:38:18Z,,make copilot and other agentic clis recognized as shells to change the terminal title for windows: rest:,2.819,Medium,0.861,"user-visible issue, crash-like behavior"
microsoft/vscode#290831,command 'workbench.action.output.show.github.copilot-chat.github copilot chat' not found,"1. open chat debug view 2. ... > show output channel 3. error notification ""command 'workbench.action.output.show.github.copilot-chat.github copilot chat' not found"" no clue who owns this. starting with",[],['BUG'],"['bug', 'chat']",github,2026-01-27T14:38:45Z,,"command 'workbench.action.output.show.github.copilot-chat.github copilot chat' not found 1. open chat debug view 2. ... > show output channel 3. error notification ""command 'workbench.action.output.show.github.copilot-chat.github copilot chat' not found"" no clue who owns this. starting with",2.626,Medium,0.817,functional impact
microsoft/vscode#290834,explore associating recognized shells with an icon that changes,"we have shell detection, perhaps that should also tie the terminal tab icon to it. so when you open , it could change not just the title of the terminal (when lands) but also the icon?",[],"['FEATURE', 'UI']","['feature-request', 'terminal-tabs', 'terminal-profiles']",github,2026-01-27T14:44:04Z,,"explore associating recognized shells with an icon that changes we have shell detection, perhaps that should also tie the terminal tab icon to it. so when you open , it could change not just the title of the terminal (when lands) but also the icon?",2.833,Medium,0.864,"user-visible issue, crash-like behavior"
python/cpython#144276,resourcewarning emitted when object yielded from nested contextmanager,"# bug report ### bug description: the following code triggers a resourcewarning the resourcewarning disappears if the textio object is first assigned to a variable before yielding it also disappears if the outer context is assigned to a variable like this i am not sure if this is a bug in cpython, but it is for sure a surprising behaviour that we do not fully understand. this is a full stack trace when enabling tracemalloc ### cpython versions tested on: 3.15, 3.9, 3.10, 3.11, 3.12, 3.13, 3.14 ### operating systems tested on: linux",[],['BUG'],"['type-bug', 'extension-modules', 'topic-IO']",github,2026-01-27T14:47:57Z,,"resourcewarning emitted when object yielded from nested contextmanager # bug report ### bug description: the following code triggers a resourcewarning the resourcewarning disappears if the textio object is first assigned to a variable before yielding it also disappears if the outer context is assigned to a variable like this i am not sure if this is a bug in cpython, but it is for sure a surprising behaviour that we do not fully understand. this is a full stack trace when enabling tracemalloc ### cpython versions tested on: 3.15, 3.9, 3.10, 3.11, 3.12, 3.13, 3.14 ### operating systems tested on: linux",4.2,Critical,1.0,system-wide impact
numpy/numpy#30734,doc: typo in c-api documentation for pyarray_sort,"### issue with current documentation: in the c-api documentation for , there is a small typo: > ""...an integer/enum specifying the **reguirements** of the sorting algorithm used."" the word **reguirements** should be corrected to **requirements**. ### location - **page:** [array api ‚Äî numpy v2.4 manual]( ### impact this is a minor typo, but correcting it will improve the readability and professionalism of the documentation. ### idea or request for content: _no response_",[],['DOCUMENTATION'],['04 - Documentation'],github,2026-01-27T14:53:13Z,2026-01-27T15:13:38Z,"doc: typo in c-api documentation for pyarray_sort ### issue with current documentation: in the c-api documentation for , there is a small typo: > ""...an integer/enum specifying the **reguirements** of the sorting algorithm used."" the word **reguirements** should be corrected to **requirements**. ### location - **page:** [array api ‚Äî numpy v2.4 manual]( ### impact this is a minor typo, but correcting it will improve the readability and professionalism of the documentation. ### idea or request for content: _no response_",1.2,Low,0.493,localized low-impact
python/cpython#144277,"inconsistent use of the ""free threading"" term","# documentation i noticed this [here]( there are currently 4 ways to refer to the ft build: - [""free threaded"" (11 uses)]( - [""free threading"" (10 uses)]( - [""free-threaded"" (9 uses)]( - [""free-threading"" (1 hit)]( ideally, we should be consistent. i guess there are two things to solve here: 1. should there be a hyphen? 2. should it be the ""free(-)threaded build"" or the ""free(-)threading build""?",[],['DOCUMENTATION'],"['docs', 'topic-free-threading']",github,2026-01-27T15:07:08Z,,"inconsistent use of the ""free threading"" term # documentation i noticed this [here]( there are currently 4 ways to refer to the ft build: - [""free threaded"" (11 uses)]( - [""free threading"" (10 uses)]( - [""free-threaded"" (9 uses)]( - [""free-threading"" (1 hit)]( ideally, we should be consistent. i guess there are two things to solve here: 1. should there be a hyphen? 2. should it be the ""free(-)threaded build"" or the ""free(-)threading build""?",1.2,Low,0.493,localized low-impact
microsoft/vscode#290853,"""new codex"" disappears after clicking it and installing","found while testing i installed the extension from this menu, but now the menu entry is gone:",[],['BUG'],['bug'],github,2026-01-27T15:10:08Z,,"""new codex"" disappears after clicking it and installing found while testing i installed the extension from this menu, but now the menu entry is gone:",2.665,Medium,0.826,functional impact
python/cpython#144278,allow disabling cache_tag at build,"this effectively disables automatic .pyc file generation and loading of files, which is stronger and more reliable than setting . historically, we have supported (it's throughout about half the relevant tests), but there's some work to do to get other tests to properly check. the only build-time option i'm proposing is a preprocessor define that isn't set by default, and when set will set to none rather than the default value. patching in a preprocessor define is much easier to maintain than replacing the code, and i expect anyone who needs this (such as myself) is likely already patching other parts of the source and can easily add a define. ### linked prs * gh-144293",[],"['BUG', 'UI']","['type-bug', 'build', 'topic-importlib']",github,2026-01-27T15:21:05Z,,"allow disabling cache_tag at build this effectively disables automatic .pyc file generation and loading of files, which is stronger and more reliable than setting . historically, we have supported (it's throughout about half the relevant tests), but there's some work to do to get other tests to properly check. the only build-time option i'm proposing is a preprocessor define that isn't set by default, and when set will set to none rather than the default value. patching in a preprocessor define is much easier to maintain than replacing the code, and i expect anyone who needs this (such as myself) is likely already patching other parts of the source and can easily add a define. ### linked prs * gh-144293",2.856,Medium,0.869,user-visible issue
openssl/openssl#29807,clean up cms code by removing multi-shot evp sign and verify calls,"part of the aim of (while adding support of md-less signature algs to cms and pkcs ) was to replace the needlessly complicated legacy multi-shot evp sign and verify calls by simple calls to . meanwhile, straightforward use of and was added by , but only for signature algorithms with implicit digest such as eddsa and ml-dsa. on the other hand, introduced to new internal functions and that (for some reason i did not attempt to understand) again make use of such detailed evp functions. i believe that for maintainability it would be good to switch to also for signature algs used with an explicit digest, as far as possible for cms (and potentially further openssl code).",[],['FEATURE'],['issue: feature request'],github,2026-01-27T15:23:30Z,,"clean up cms code by removing multi-shot evp sign and verify calls part of the aim of (while adding support of md-less signature algs to cms and pkcs ) was to replace the needlessly complicated legacy multi-shot evp sign and verify calls by simple calls to . meanwhile, straightforward use of and was added by , but only for signature algorithms with implicit digest such as eddsa and ml-dsa. on the other hand, introduced to new internal functions and that (for some reason i did not attempt to understand) again make use of such detailed evp functions. i believe that for maintainability it would be good to switch to also for signature algs used with an explicit digest, as far as possible for cms (and potentially further openssl code).",1.4,Low,0.538,localized low-impact
microsoft/vscode#290858,don't sort sessions by read/unread state,"type: bug sort them only by modification date. (outlook and others also do this.) vs code version: code - insiders 1.109.0-insider (e7a06c8eabf2915e2c383b1ce6d2b993d90e2e92, 2026-01-27t07:16:53.085z) os version: darwin arm64 24.6.0 system info |item|value| |---|---| |cpus|apple m1 max (10 x 2400)| |gpu status|2d_canvas: enabled gpu0: vendor= 0x106b [google inc. (apple)], device=0x0000 [angle (apple, angle metal renderer: apple m1 max, version 15.7.3 (build 24g419))], driver_vendor=apple, driver_version=15.7.3 *active* machine model name: macbookpro machine model version: 18.2 direct_rendering_display_compositor: disabled_off_ok gpu_compositing: enabled multiple_raster_threads: enabled_on opengl: enabled_on rasterization: enabled raw_draw: disabled_off_ok skia_graphite: disabled_off trees_in_viz: disabled_off video_decode: enabled video_encode: enabled webgl: enabled webgl2: enabled webgpu: enabled webnn: disabled_off| |load (avg)|32, 47, 47| |memory (system)|64.00gb (0.10gb free)| |process argv|--log trace --log github.copilot-chat=debug --crash-reporter-id 7aad610c-0e6b-48aa-ba87-b704b1b543cf| |screen reader|no| |vm|0%| connection to 'dev-container+7b22686f737450617468223a222f55736572732f6368726d617274692f446576656c6f706d656e742f7265706f732f736d6b747374222c226c6f63616c446f636b6572223a66616c73652c2273657474696e6773223a7b22636f6e74657874223a226465736b746f702d6c696e7578227d2c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a222f55736572732f6368726d617274692f446576656c6f706d656e742f7265706f732f736d6b7473742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f55736572732f6368726d617274692f446576656c6f706d656e742f7265706f732f736d6b7473742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d' could not be established extensions (75) extension|author (truncated)|version ---|---|--- tsl-problem-matcher|amo|0.6.2 claude-code|ant|2.1.20 openmatchingfiles|bca|0.5.4 unique-lines|bib|1.0.0 devcontainer-image-convert|bri|0.0.1 ruff|cha|2026.34.0 network-proxy-test|chr|0.0.22 regex|chr|0.6.0 esbuild-problem-matchers|con|0.0.3 vscode-eslint|dba|3.0.20 docker|doc|0.18.0 kusto|don|0.5.4 prettier-vscode|esb|12.3.0 codespaces|git|1.18.5 copilot-chat|git|0.37.2026012701 remotehub|git|0.64.0 vscode-pull-request-github|git|0.127.2026012704 gitlab-workflow|git|6.67.3 vscode-test-explorer|hbe|2.22.1 vscode-drawio|hed|1.9.0 rest-client|hum|0.25.1 template-string-converter|meg|0.6.1 regexsnippets|mon|1.0.2 vscode-azurefunctions|ms-|1.20.3 vscode-azureresourcegroups|ms-|0.11.7 vscode-containers|ms-|2.4.0 vscode-docker|ms-|2.0.0 vscode-language-pack-de|ms-|1.108.2026011409 vscode-language-pack-qps-ploc|ms-|1.108.2026012109 debugpy|ms-|2025.19.2026012701 python|ms-|2026.1.2026012301 vscode-pylance|ms-|2025.12.101 vscode-python-envs|ms-|1.17.10261013 jupyter|ms-|2025.10.2026010601 jupyter-hub|ms-|2024.10.1002831100 jupyter-keymap|ms-|1.1.2 jupyter-renderers|ms-|1.3.2025062701 vscode-ai|ms-|1.5.2026012208 vscode-ai-remote|ms-|1.5.2026012009 vscode-jupyter-cell-tags|ms-|0.1.9 vscode-jupyter-slideshow|ms-|0.1.6 remote-containers|ms-|0.440.0 remote-ssh|ms-|0.123.2026012215 remote-ssh-edit|ms-|0.87.0 remote-wsl|ms-|0.104.3 vscode-remote-extensionpack|ms-|0.26.0 azure-repos|ms-|0.40.0 azurecli|ms-|0.6.0 debug-value-editor|ms-|0.2.2 extension-test-runner|ms-|0.0.14 hexeditor|ms-|1.11.1 live-server|ms-|0.5.2026012601 remote-explorer|ms-|0.6.2025081809 remote-repositories|ms-|0.42.0 remote-server|ms-|1.6.2026011209 test-adapter-converter|ms-|0.2.1 ts-file-path-support|ms-|1.0.0 vscode-github-issue-notebooks|ms-|0.0.134 vscode-selfhost-test-provider|ms-|0.3.25 vscode-speech|ms-|0.16.0 vscode-speech-language-pack-de-de|ms-|0.5.0 vscode-websearchforcopilot|ms-|0.2.2026012701 web-editors|ms-|0.3.0 vscode-xml|red|0.29.2025112508 vscode-yaml|red|1.19.1 vscode-dall-toys|rob|0.5.0 rust-analyzer|rus|0.4.2768 eval|sto|0.0.6 vscode-open-in-github|sys|1.18.0 even-better-toml|tam|0.21.2 shellcheck|tim|0.38.6 native-preview|typ|0.20260127.1 vscode-lldb|vad|1.12.0 explorer|vit|1.38.1 autocomplete-english-word|wus|0.1.7 (1 theme extensions excluded) a/b experiments","[""Don't sort sessions by read/unread state (fix #290858) (#291207)""]",['BUG'],"['bug', 'unreleased', 'chat-agents-view']",github,2026-01-27T15:23:31Z,2026-01-28T08:55:13Z,"don't sort sessions by read/unread state type: bug sort them only by modification date. (outlook and others also do this.) vs code version: code - insiders 1.109.0-insider (e7a06c8eabf2915e2c383b1ce6d2b993d90e2e92, 2026-01-27t07:16:53.085z) os version: darwin arm64 24.6.0 system info |item|value| |---|---| |cpus|apple m1 max (10 x 2400)| |gpu status|2d_canvas: enabled gpu0: vendor= 0x106b [google inc. (apple)], device=0x0000 [angle (apple, angle metal renderer: apple m1 max, version 15.7.3 (build 24g419))], driver_vendor=apple, driver_version=15.7.3 *active* machine model name: macbookpro machine model version: 18.2 direct_rendering_display_compositor: disabled_off_ok gpu_compositing: enabled multiple_raster_threads: enabled_on opengl: enabled_on rasterization: enabled raw_draw: disabled_off_ok skia_graphite: disabled_off trees_in_viz: disabled_off video_decode: enabled video_encode: enabled webgl: enabled webgl2: enabled webgpu: enabled webnn: disabled_off| |load (avg)|32, 47, 47| |memory (system)|64.00gb (0.10gb free)| |process argv|--log trace --log github.copilot-chat=debug --crash-reporter-id 7aad610c-0e6b-48aa-ba87-b704b1b543cf| |screen reader|no| |vm|0%| connection to 'dev-container+7b22686f737450617468223a222f55736572732f6368726d617274692f446576656c6f706d656e742f7265706f732f736d6b747374222c226c6f63616c446f636b6572223a66616c73652c2273657474696e6773223a7b22636f6e74657874223a226465736b746f702d6c696e7578227d2c22636f6e66696746696c65223a7b22246d6964223a312c22667350617468223a222f55736572732f6368726d617274692f446576656c6f706d656e742f7265706f732f736d6b7473742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c2270617468223a222f55736572732f6368726d617274692f446576656c6f706d656e742f7265706f732f736d6b7473742f2e646576636f6e7461696e65722f646576636f6e7461696e65722e6a736f6e222c22736368656d65223a2266696c65227d7d' could not be established extensions (75) extension|author (truncated)|version ---|---|--- tsl-problem-matcher|amo|0.6.2 claude-code|ant|2.1.20 openmatchingfiles|bca|0.5.4 unique-lines|bib|1.0.0 devcontainer-image-convert|bri|0.0.1 ruff|cha|2026.34.0 network-proxy-test|chr|0.0.22 regex|chr|0.6.0 esbuild-problem-matchers|con|0.0.3 vscode-eslint|dba|3.0.20 docker|doc|0.18.0 kusto|don|0.5.4 prettier-vscode|esb|12.3.0 codespaces|git|1.18.5 copilot-chat|git|0.37.2026012701 remotehub|git|0.64.0 vscode-pull-request-github|git|0.127.2026012704 gitlab-workflow|git|6.67.3 vscode-test-explorer|hbe|2.22.1 vscode-drawio|hed|1.9.0 rest-client|hum|0.25.1 template-string-converter|meg|0.6.1 regexsnippets|mon|1.0.2 vscode-azurefunctions|ms-|1.20.3 vscode-azureresourcegroups|ms-|0.11.7 vscode-containers|ms-|2.4.0 vscode-docker|ms-|2.0.0 vscode-language-pack-de|ms-|1.108.2026011409 vscode-language-pack-qps-ploc|ms-|1.108.2026012109 debugpy|ms-|2025.19.2026012701 python|ms-|2026.1.2026012301 vscode-pylance|ms-|2025.12.101 vscode-python-envs|ms-|1.17.10261013 jupyter|ms-|2025.10.2026010601 jupyter-hub|ms-|2024.10.1002831100 jupyter-keymap|ms-|1.1.2 jupyter-renderers|ms-|1.3.2025062701 vscode-ai|ms-|1.5.2026012208 vscode-ai-remote|ms-|1.5.2026012009 vscode-jupyter-cell-tags|ms-|0.1.9 vscode-jupyter-slideshow|ms-|0.1.6 remote-containers|ms-|0.440.0 remote-ssh|ms-|0.123.2026012215 remote-ssh-edit|ms-|0.87.0 remote-wsl|ms-|0.104.3 vscode-remote-extensionpack|ms-|0.26.0 azure-repos|ms-|0.40.0 azurecli|ms-|0.6.0 debug-value-editor|ms-|0.2.2 extension-test-runner|ms-|0.0.14 hexeditor|ms-|1.11.1 live-server|ms-|0.5.2026012601 remote-explorer|ms-|0.6.2025081809 remote-repositories|ms-|0.42.0 remote-server|ms-|1.6.2026011209 test-adapter-converter|ms-|0.2.1 ts-file-path-support|ms-|1.0.0 vscode-github-issue-notebooks|ms-|0.0.134 vscode-selfhost-test-provider|ms-|0.3.25 vscode-speech|ms-|0.16.0 vscode-speech-language-pack-de-de|ms-|0.5.0 vscode-websearchforcopilot|ms-|0.2.2026012701 web-editors|ms-|0.3.0 vscode-xml|red|0.29.2025112508 vscode-yaml|red|1.19.1 vscode-dall-toys|rob|0.5.0 rust-analyzer|rus|0.4.2768 eval|sto|0.0.6 vscode-open-in-github|sys|1.18.0 even-better-toml|tam|0.21.2 shellcheck|tim|0.38.6 native-preview|typ|0.20260127.1 vscode-lldb|vad|1.12.0 explorer|vit|1.38.1 autocomplete-english-word|wus|0.1.7 (1 theme extensions excluded) a/b experiments Don't sort sessions by read/unread state (fix #290858) (#291207)",7.8,Critical,1.0,crash-like behavior
zaproxy/zaproxy#9229,missing csp header rule is sometimes slow,"### describe the bug: per gathered statistics etc it seems that the missing csp header rule ([10038]( can sometimes be quite slow. we believe this is due to the csp via meta tag check. the meta tag check could be moved so that it is only considered at [low threshold]( this may mean adding a note to the ""description"" or ""other info"" when not low threshold stating that meta tags were not checked for csp, purely headers. - unit tests will need to be updated/added - help should be updated. --- references: - [zap dev guide]( --- obviously if you can find/identify a live or unit test situation in which this actually does take more than 5 seconds reproducibly that would be super helpful. --- ### steps to reproduce the behavior: n/a ### expected behavior: the rule should not be noticeably slow. after the fix the stats should show increased performance/decreased scan time. ### software versions: 2.17 & current dev. passive scan rules release - v70 ### screenshots: n/a ### errors from the zap.log file: n/a ### additional context: none at this time. ### would you like to help fix this issue? - [ ] yes",[],['BUG'],"['bug', 'IdealFirstBug', 'add-on', 'good first issue', 'in:pscanrules']",github,2026-01-27T15:36:55Z,,"missing csp header rule is sometimes slow ### describe the bug: per gathered statistics etc it seems that the missing csp header rule ([10038]( can sometimes be quite slow. we believe this is due to the csp via meta tag check. the meta tag check could be moved so that it is only considered at [low threshold]( this may mean adding a note to the ""description"" or ""other info"" when not low threshold stating that meta tags were not checked for csp, purely headers. - unit tests will need to be updated/added - help should be updated. --- references: - [zap dev guide]( --- obviously if you can find/identify a live or unit test situation in which this actually does take more than 5 seconds reproducibly that would be super helpful. --- ### steps to reproduce the behavior: n/a ### expected behavior: the rule should not be noticeably slow. after the fix the stats should show increased performance/decreased scan time. ### software versions: 2.17 & current dev. passive scan rules release - v70 ### screenshots: n/a ### errors from the zap.log file: n/a ### additional context: none at this time. ### would you like to help fix this issue? - [ ] yes",3.8,Critical,1.0,system-wide impact
microsoft/vscode#290872,cloud session: checkout and apply still show when checking out the branch,"testing i wonder whether the checkout and apply actions should hide or be disabled when the user clicks . maybe there are other actions which make sense in that case? also, there is no approve pr action or similar. ![image](",[],['FEATURE'],"['feature-request', 'chat']",github,2026-01-27T15:44:44Z,,"cloud session: checkout and apply still show when checking out the branch testing i wonder whether the checkout and apply actions should hide or be disabled when the user clicks . maybe there are other actions which make sense in that case? also, there is no approve pr action or similar. ![image](",1.4,Low,0.538,localized low-impact
microsoft/vscode#290873,in stacked view filtering resets the more expansion making filtering hard to see,testing 1. show sessions stacked 2. click the unread filter 3. nothing appears to change 4. click more to see the rest of the unread sessions 5. unclick the filter and see more collapses when you have just unreads for example nothing really changes besides the number more shows making it a little challenging to see things are being filtered as i cannot expand and see the list filter down,[],['BUG'],"['bug', 'chat-agents-view']",github,2026-01-27T15:47:11Z,,in stacked view filtering resets the more expansion making filtering hard to see testing 1. show sessions stacked 2. click the unread filter 3. nothing appears to change 4. click more to see the rest of the unread sessions 5. unclick the filter and see more collapses when you have just unreads for example nothing really changes besides the number more shows making it a little challenging to see things are being filtered as i cannot expand and see the list filter down,4.6,Critical,1.0,crash-like behavior
microsoft/vscode#290874,should disable the status indicators,testing if i don't want sessions at all as i am a one man one chat type of guy then how do i hide all this ui? i unchecked show sessions but the status indicators still persist,[],['BUG'],"['bug', 'insiders-released']",github,2026-01-27T15:48:51Z,2026-01-27T23:12:16Z,should disable the status indicators testing if i don't want sessions at all as i am a one man one chat type of guy then how do i hide all this ui? i unchecked show sessions but the status indicators still persist,2.698,Medium,0.833,functional impact
rust-lang/rust#151748,1.93 regression satisfying send obligation,"i cannot share a reproduction right at this moment (closed source), i will work to minimise the error sometime soon. i will describe the error that i am seeing though, incase someone can figure out a minimum repro by the error and regression alone. * with 1.92, my code compiles. * with 1.93 dev profile, my code compiles. * with 1.93 release profile, my code no longer compiles. * with 1.93 release and debug-assertions, my code still doesn't compile. i see the following (truncated) error: i bisected this to the following commit f37aa9955f03bb1bc6fe08670cb1ecae534b5815 ( ). i've been told that there is a post-mono trait solver pass that can be influenced by mir transforms, which would explain why i see this issue only with . i was able to fix the issue with the following modifications before: after: i know there's not a lot to go off of here, i apologise for this. as i say, i hope to have some time eventually to minimise this regression.",[],['BUG'],"['T-compiler', 'regression-from-stable-to-stable', 'C-bug', 'A-const-eval', 'A-async-closures', 'I-prioritize', 'A-patterns', 'A-auto-traits', 'S-has-mcve', 'T-types', 'A-crates', 'I-cycle']",github,2026-01-27T15:57:06Z,,"1.93 regression satisfying send obligation i cannot share a reproduction right at this moment (closed source), i will work to minimise the error sometime soon. i will describe the error that i am seeing though, incase someone can figure out a minimum repro by the error and regression alone. * with 1.92, my code compiles. * with 1.93 dev profile, my code compiles. * with 1.93 release profile, my code no longer compiles. * with 1.93 release and debug-assertions, my code still doesn't compile. i see the following (truncated) error: i bisected this to the following commit f37aa9955f03bb1bc6fe08670cb1ecae534b5815 ( ). i've been told that there is a post-mono trait solver pass that can be influenced by mir transforms, which would explain why i see this issue only with . i was able to fix the issue with the following modifications before: after: i know there's not a lot to go off of here, i apologise for this. as i say, i hope to have some time eventually to minimise this regression.",4.2,Critical,1.0,system-wide impact
microsoft/vscode#290875,file never appears,"testing i followed steps, and can confirm that the extension is running, by adding breakpoint, however for some reason the files are not version: 1.109.0-insider (universal) commit: e7a06c8eabf2915e2c383b1ce6d2b993d90e2e92 date: 2026-01-27t07:16:53.085z (8 hrs ago) electron: 39.3.0 electronbuildid: 13168319 chromium: 142.0.7444.265 node.js: 22.21.1 v8: 14.2.231.22-electron.0 os: darwin arm64 24.6.0",[],['BUG'],"['bug', 'chat']",github,2026-01-27T15:59:16Z,2026-01-27T17:05:58Z,"file never appears testing i followed steps, and can confirm that the extension is running, by adding breakpoint, however for some reason the files are not version: 1.109.0-insider (universal) commit: e7a06c8eabf2915e2c383b1ce6d2b993d90e2e92 date: 2026-01-27t07:16:53.085z (8 hrs ago) electron: 39.3.0 electronbuildid: 13168319 chromium: 142.0.7444.265 node.js: 22.21.1 v8: 14.2.231.22-electron.0 os: darwin arm64 24.6.0",2.591,Medium,0.809,functional impact
cockroachdb/cockroach#161865,bulkmerge: reduce memory consumption during final merge iteration,"during the distributed merge pipeline, the final merge iteration writes directly to the kv. the spans used in this iteration cover the entire range of keys. the number of ssts that we need to merge in the final iteration determines how much memory is consumed. in some cases where we had > 3000 ssts, it cause oom issues. this was using the default rpc implementation. when run with the drpc, the entire system ground to a halt, probably because we don't multiplex the connections. this task is opened to reduce the memory requirement for the final phase. this will likely come out as a result of changes to the externalsstiterator. we are thinking that by including disjoint ssts on the same level that it will cut down on the amount of memory needed. more prototyping and analysis is needed confirm this though. jira issue: crdb-59126 epic crdb-48845",[],['FEATURE'],"['C-enhancement', 'T-sql-foundations']",github,2026-01-27T15:59:45Z,,"bulkmerge: reduce memory consumption during final merge iteration during the distributed merge pipeline, the final merge iteration writes directly to the kv. the spans used in this iteration cover the entire range of keys. the number of ssts that we need to merge in the final iteration determines how much memory is consumed. in some cases where we had > 3000 ssts, it cause oom issues. this was using the default rpc implementation. when run with the drpc, the entire system ground to a halt, probably because we don't multiplex the connections. this task is opened to reduce the memory requirement for the final phase. this will likely come out as a result of changes to the externalsstiterator. we are thinking that by including disjoint ssts on the same level that it will cut down on the amount of memory needed. more prototyping and analysis is needed confirm this though. jira issue: crdb-59126 epic crdb-48845",3.6,Critical,1.0,crash-like behavior
rust-lang/rust#151750,rust stdlib debug symbols for mingw cannot be loaded under gdb,"<!-- thank you for filing a bug report! üêõ please provide a short summary of the bug, along with any information you feel relevant to replicating the bug. --> hi all, i'm trying to ship debuginfo files for our (gstreamer) rust plugins. the current approach is to split debuginfo as follows: when i load these plugins within a session of gdb, i expect the load to succeed and the debugging information to be applied. instead, gdb complains about missing dwo files: i checked and these files do are present in the corresponding .rlibs for the target, but for some reason gdb expects them in the fixed locations mentioned above, instead of loading them from the corresponding dll's .dwp. ### meta <!-- if you're using the stable version of the compiler, you should also check if the bug also exists in the beta or nightly versions. --> : i attach the complete debugsession in lieu of a backtrace, as a separate file because it exceeds github's issue body limit: [mingw_log.txt](",[],['BUG'],"['A-debuginfo', 'T-compiler', 'O-windows-gnu', 'C-bug']",github,2026-01-27T16:04:15Z,,"rust stdlib debug symbols for mingw cannot be loaded under gdb <!-- thank you for filing a bug report! üêõ please provide a short summary of the bug, along with any information you feel relevant to replicating the bug. --> hi all, i'm trying to ship debuginfo files for our (gstreamer) rust plugins. the current approach is to split debuginfo as follows: when i load these plugins within a session of gdb, i expect the load to succeed and the debugging information to be applied. instead, gdb complains about missing dwo files: i checked and these files do are present in the corresponding .rlibs for the target, but for some reason gdb expects them in the fixed locations mentioned above, instead of loading them from the corresponding dll's .dwp. ### meta <!-- if you're using the stable version of the compiler, you should also check if the bug also exists in the beta or nightly versions. --> : i attach the complete debugsession in lieu of a backtrace, as a separate file because it exceeds github's issue body limit: [mingw_log.txt](",2.474,Medium,0.782,functional impact
cockroachdb/cockroach#161866,sql: improve progress reporting during index backfill of a distributed merge,"currently, when you use distributed merge with index backfill, the progress reporting will show something like this: running_status: pending: backfilling index (1 operation) ‚Äî postcommit phase (stage 2 of 7). fraction_complete: 0-1 this woks great during the initial map phase of distributed merge. the % complete slowly gets to 100% when it completes the stage. however, it will then go onto the 2 more merge iterations. what you will see is the running status stay the same and the fraction complete will go back to 0% and stay there. there is no indication that it's doing: - merge iteration n - % complete in the merge iteration this task is opened to improve on that so that we have clarify as to what stage we are in and the % complete in that stage. jira issue: crdb-59127 epic crdb-48845",[],['FEATURE'],"['C-enhancement', 'T-sql-foundations']",github,2026-01-27T16:04:32Z,,"sql: improve progress reporting during index backfill of a distributed merge currently, when you use distributed merge with index backfill, the progress reporting will show something like this: running_status: pending: backfilling index (1 operation) ‚Äî postcommit phase (stage 2 of 7). fraction_complete: 0-1 this woks great during the initial map phase of distributed merge. the % complete slowly gets to 100% when it completes the stage. however, it will then go onto the 2 more merge iterations. what you will see is the running status stay the same and the fraction complete will go back to 0% and stay there. there is no indication that it's doing: - merge iteration n - % complete in the merge iteration this task is opened to improve on that so that we have clarify as to what stage we are in and the % complete in that stage. jira issue: crdb-59127 epic crdb-48845",3.092,High,0.923,functional impact
rust-lang/rust#151752,rustc could warn if a feature is unused,"code: expected behavior: a warning that is unused. actual behavior: no warning. this will be slightly tricky to track, but i think it's possible if were better integrated with the query system. right now, that's a single query that returns all features at once. if there were instead one query per-feature, the query system would know in detail which features were used and emit a warning if the feature was enabled but never queried. (it might also be possible to do this in a simpler way using refcell.)",[],['FEATURE'],"['A-diagnostics', 'T-compiler', 'L-unused_features']",github,2026-01-27T16:20:00Z,,"rustc could warn if a feature is unused code: expected behavior: a warning that is unused. actual behavior: no warning. this will be slightly tricky to track, but i think it's possible if were better integrated with the query system. right now, that's a single query that returns all features at once. if there were instead one query per-feature, the query system would know in detail which features were used and emit a warning if the feature was enabled but never queried. (it might also be possible to do this in a simpler way using refcell.)",1.4,Low,0.538,localized low-impact
microsoft/vscode#290885,"chat: anonymous access fails with ""language model unavailable""",testing: steps to reproduce: 1. run 2. configure to be 3. send a chat message => üêõ you see a dialog asking to login //cc -99,[],['BUG'],"['bug', 'important', 'chat']",github,2026-01-27T16:45:59Z,,"chat: anonymous access fails with ""language model unavailable"" testing: steps to reproduce: 1. run 2. configure to be 3. send a chat message => üêõ you see a dialog asking to login //cc -99",2.65,Medium,0.822,functional impact
microsoft/vscode#290888,tool calls should be immediately streamed to the user,steps to reproduce: 1. ask the model to create a file with 500 console.log lines without loops or any shortcuts 2. üêõ chat just spins with working for a long time we do receive tool call data via the stream we should show users progress as quickly as possible to improve upon user perceived latency,[],['FEATURE'],"['feature-request', 'verification-needed', 'chat']",github,2026-01-27T16:51:42Z,,tool calls should be immediately streamed to the user steps to reproduce: 1. ask the model to create a file with 500 console.log lines without loops or any shortcuts 2. üêõ chat just spins with working for a long time we do receive tool call data via the stream we should show users progress as quickly as possible to improve upon user perceived latency,4.6,Critical,1.0,system-wide impact
facebook/react#35644,[compiler bug]: eslint-plugin-react-hooks silent bailout when try/catch/finally block in the same component body,"### what kind of issue is this? - [ ] react compiler core (the js output is incorrect, or your app works incorrectly after optimization) - [ ] babel-plugin-react-compiler (build issue installing or using the babel plugin) - [x] eslint-plugin-react-hooks (build issue installing or using the eslint plugin) - [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script) ### link to repro ### repro steps 1. install eslint-plugin-react-hooks .0.1 (or latest). 2. create a react component with a clear rule violation (e.g., usememo with no return value). 3. observe that the linter correctly reports the error. 4. add a try/catch or try/catch/finally block anywhere inside the component body. 5. bug: the linter stops reporting the usememo error (and all other compiler errors) for that component. **note**: in the react playground, when recreating the example, i see that the the try/catch/finally block actually throws an error, but the usememo one is still ignored ### expected behavior the linter should flag the usememo error regardless of the try/catch block present in the component. ### actual behavior the linter reports 0 errors. removing the try/catch block immediately causes the usememo error to reappear. ### how often does this bug happen? every time ### what version of react are you using? 19.1.1 ### what version of react compiler are you using? i don't use react compiler, but i use eslint-plugin-react-hooks version 7.0.1",[],['BUG'],"['Type: Bug', 'Status: Unconfirmed']",github,2026-01-27T16:52:55Z,,"[compiler bug]: eslint-plugin-react-hooks silent bailout when try/catch/finally block in the same component body ### what kind of issue is this? - [ ] react compiler core (the js output is incorrect, or your app works incorrectly after optimization) - [ ] babel-plugin-react-compiler (build issue installing or using the babel plugin) - [x] eslint-plugin-react-hooks (build issue installing or using the eslint plugin) - [ ] react-compiler-healthcheck (build issue installing or using the healthcheck script) ### link to repro ### repro steps 1. install eslint-plugin-react-hooks .0.1 (or latest). 2. create a react component with a clear rule violation (e.g., usememo with no return value). 3. observe that the linter correctly reports the error. 4. add a try/catch or try/catch/finally block anywhere inside the component body. 5. bug: the linter stops reporting the usememo error (and all other compiler errors) for that component. **note**: in the react playground, when recreating the example, i see that the the try/catch/finally block actually throws an error, but the usememo one is still ignored ### expected behavior the linter should flag the usememo error regardless of the try/catch block present in the component. ### actual behavior the linter reports 0 errors. removing the try/catch block immediately causes the usememo error to reappear. ### how often does this bug happen? every time ### what version of react are you using? 19.1.1 ### what version of react compiler are you using? i don't use react compiler, but i use eslint-plugin-react-hooks version 7.0.1",2.639,Medium,0.82,functional impact
docker/docker#51949,"network named ""container"" can be created but not used","### description #### current behavior users can create a network named ""container"" via , but cannot use it when starting containers. this causes confusion. #### root cause in [ ]( has special logic for the literal string ""container"", but this creates an inconsistency: network creation allows ""container"" as a valid name container startup rejects it as invalid a fixme comment already exists at line 8 acknowledging this issue. #### proposed solution 1. reserve ""container"" as a network name during creation 2. also reserve names starting with ""container:"" to avoid ambiguity 3. update validation logic to handle this consistently **note**: this doesn't break backward compatibility because networks named ""container"" are already unusable in practice. ### reproduce **case 1: network named ""container"" (without colon)** **case 2: network named ""container:"" (with colon only)** ### expected behavior the network name ""container"" (and variants like ""container:"") should be reserved and rejected during network creation to prevent ambiguity: ### docker version ### docker info ### additional info - related fixme comment: - this issue affects all platforms (linux, macos, windows) - impact: low frequency but high confusion when encountered",[],['BUG'],"['status/0-triage', 'kind/bug']",github,2026-01-27T16:54:28Z,,"network named ""container"" can be created but not used ### description #### current behavior users can create a network named ""container"" via , but cannot use it when starting containers. this causes confusion. #### root cause in [ ]( has special logic for the literal string ""container"", but this creates an inconsistency: network creation allows ""container"" as a valid name container startup rejects it as invalid a fixme comment already exists at line 8 acknowledging this issue. #### proposed solution 1. reserve ""container"" as a network name during creation 2. also reserve names starting with ""container:"" to avoid ambiguity 3. update validation logic to handle this consistently **note**: this doesn't break backward compatibility because networks named ""container"" are already unusable in practice. ### reproduce **case 1: network named ""container"" (without colon)** **case 2: network named ""container:"" (with colon only)** ### expected behavior the network name ""container"" (and variants like ""container:"") should be reserved and rejected during network creation to prevent ambiguity: ### docker version ### docker info ### additional info - related fixme comment: - this issue affects all platforms (linux, macos, windows) - impact: low frequency but high confusion when encountered",2.649,Medium,0.822,functional impact
openssl/openssl#29808,make issue on 3.5.5 solaris64-sparcv9-gcc,"encountered compile issues for openssl 3.5.5 on solaris sunos 5.11 11.4.54.138.1 sun4v sparc sun4v see make crash below. i successfully compiled prior releases 3.0.18 without any issues using the configure below on the same server. ./configure solaris64-sparcv9-gcc --openssldir=/usr/local/apache/ssl shared --prefix=/usr/local/apache/ssl make debug: all keys: applinkdir, bindir, cmakeconfigdir, enginesdir, includedir, ldlibs, libdir, modulesdir, pkgconfigdir, prefix, version, libdir use of uninitialized value in concatenation (.) or string at util/mkinstallvars.pl line 55. debug: libdir = , libdir = => use of uninitialized value in concatenation (.) or string at util/mkinstallvars.pl line 57. libdir = no value given for cmakeconfigdir no value given for pkgconfigdir no value given for libdir debug: prefix = . => prefix = /usr/local/build_36_0/install_apache_php/openssl-3.5.5 use of uninitialized value in concatenation (.) or string at util/mkinstallvars.pl line 72. debug: libdir = => use of uninitialized value in concatenation (.) or string at util/mkinstallvars.pl line 75. libdir = debug: bindir = apps => bindir = /usr/local/build_36_0/install_apache_php/openssl-3.5.5/apps, bindir_rel_prefix = apps debug: libdir = use of uninitialized value in print at util/mkinstallvars.pl line 91. => use of uninitialized value in concatenation (.) or string at util/mkinstallvars.pl line 108. libdir = /usr/local/build_36_0/install_apache_php/openssl-3.5.5, libdir_rel_prefix = debug: includedir = [ include, ./include ] => includedir = [ /usr/local/build_36_0/install_apache_php/openssl-3.5.5/include, /usr/local/build_36_0/install_apache_php/openssl-3.5.5/include ], includedir_rel_prefix = [ include, ./include ] debug: applinkdir = ms => applinkdir = /usr/local/build_36_0/install_apache_php/openssl-3.5.5/ms, applinkdir_rel_prefix = ms debug: enginesdir = engines => enginesdir = /usr/local/build_36_0/install_apache_php/openssl-3.5.5/engines, enginesdir_rel_libdir = engines debug: modulesdir = providers => modulesdir = /usr/local/build_36_0/install_apache_php/openssl-3.5.5/providers, modulesdir_rel_libdir = providers debug: pkgconfigdir = . => pkgconfigdir = /usr/local/build_36_0/install_apache_php/openssl-3.5.5, pkgconfigdir_rel_libdir = . debug: cmakeconfigdir = . => cmakeconfigdir = /usr/local/build_36_0/install_apache_php/openssl-3.5.5, cmakeconfigdir_rel_libdir = . use of uninitialized value in join or string at util/mkinstallvars.pl line 156. debug: all keys: applinkdir, bindir, cmakeconfigdir, enginesdir, includedir, ldlibs, libdir, modulesdir, pkgconfigdir, prefix, version, libdir debug: libdir = lib/64, libdir = /usr/local/apache/ssl/lib/64 => libdir = lib/64 debug: prefix = /usr/local/apache/ssl => prefix = /usr/local/apache/ssl debug: libdir = /usr/local/apache/ssl/lib/64 => libdir = /usr/local/apache/ssl/lib/64 debug: bindir = bin => bindir = /usr/local/apache/ssl/bin, bindir_rel_prefix = bin debug: libdir = lib/64 => libdir = /usr/local/apache/ssl/lib/64, libdir_rel_prefix = lib/64 debug: includedir = include => includedir = /usr/local/apache/ssl/include, includedir_rel_prefix = include debug: applinkdir = include/openssl => applinkdir = /usr/local/apache/ssl/include/openssl, applinkdir_rel_prefix = include/openssl debug: enginesdir = /usr/local/apache/ssl/lib/64/engines-3 => enginesdir = /usr/local/apache/ssl/lib/64/engines-3, enginesdir_rel_libdir = engines-3 debug: modulesdir = /usr/local/apache/ssl/lib/64/ossl-modules => modulesdir = /usr/local/apache/ssl/lib/64/ossl-modules, modulesdir_rel_libdir = ossl-modules debug: pkgconfigdir = /usr/local/apache/ssl/lib/64/pkgconfig => pkgconfigdir = /usr/local/apache/ssl/lib/64/pkgconfig, pkgconfigdir_rel_libdir = pkgconfig debug: cmakeconfigdir = /usr/local/apache/ssl/lib/64/cmake/openssl => cmakeconfigdir = /usr/local/apache/ssl/lib/64/cmake/openssl, cmakeconfigdir_rel_libdir = cmake/openssl /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 528: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 547: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 703: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 922: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 973: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 1271: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 1360: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 1659: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 1752: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 1771: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 1997: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 2324: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 2343: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 2471: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 2490: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 2646: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 2865: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 2916: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 3214: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 3303: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 3602: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 3723: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 3942: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 3962: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 3981: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 4207: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 4230: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 4249: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 4475: error: statement syntax make[1]: *** [makefile:15049: crypto/aes/libcrypto-lib-aest4-sparcv9.o] error 1 make: *** [makefile:3891: build_sw] error 2 /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 528: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 547: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 703: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 922: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 973: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 1271: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 1360: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 1659: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 1752: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 1771: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 1997: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 2324: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 2343: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 2471: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 2490: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 2646: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 2865: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 2916: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 3214: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 3303: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 3602: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 3723: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 3942: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 3962: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 3981: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 4207: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 4230: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 4249: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 4475: error: statement syntax make[1]: *** [makefile:15049: crypto/aes/libcrypto-lib-aest4-sparcv9.o] error 1 make: *** [makefile:3894: build_libs] error 2",[],['BUG'],"['help wanted', 'triaged: bug']",github,2026-01-27T17:17:35Z,,"make issue on 3.5.5 solaris64-sparcv9-gcc encountered compile issues for openssl 3.5.5 on solaris sunos 5.11 11.4.54.138.1 sun4v sparc sun4v see make crash below. i successfully compiled prior releases 3.0.18 without any issues using the configure below on the same server. ./configure solaris64-sparcv9-gcc --openssldir=/usr/local/apache/ssl shared --prefix=/usr/local/apache/ssl make debug: all keys: applinkdir, bindir, cmakeconfigdir, enginesdir, includedir, ldlibs, libdir, modulesdir, pkgconfigdir, prefix, version, libdir use of uninitialized value in concatenation (.) or string at util/mkinstallvars.pl line 55. debug: libdir = , libdir = => use of uninitialized value in concatenation (.) or string at util/mkinstallvars.pl line 57. libdir = no value given for cmakeconfigdir no value given for pkgconfigdir no value given for libdir debug: prefix = . => prefix = /usr/local/build_36_0/install_apache_php/openssl-3.5.5 use of uninitialized value in concatenation (.) or string at util/mkinstallvars.pl line 72. debug: libdir = => use of uninitialized value in concatenation (.) or string at util/mkinstallvars.pl line 75. libdir = debug: bindir = apps => bindir = /usr/local/build_36_0/install_apache_php/openssl-3.5.5/apps, bindir_rel_prefix = apps debug: libdir = use of uninitialized value in print at util/mkinstallvars.pl line 91. => use of uninitialized value in concatenation (.) or string at util/mkinstallvars.pl line 108. libdir = /usr/local/build_36_0/install_apache_php/openssl-3.5.5, libdir_rel_prefix = debug: includedir = [ include, ./include ] => includedir = [ /usr/local/build_36_0/install_apache_php/openssl-3.5.5/include, /usr/local/build_36_0/install_apache_php/openssl-3.5.5/include ], includedir_rel_prefix = [ include, ./include ] debug: applinkdir = ms => applinkdir = /usr/local/build_36_0/install_apache_php/openssl-3.5.5/ms, applinkdir_rel_prefix = ms debug: enginesdir = engines => enginesdir = /usr/local/build_36_0/install_apache_php/openssl-3.5.5/engines, enginesdir_rel_libdir = engines debug: modulesdir = providers => modulesdir = /usr/local/build_36_0/install_apache_php/openssl-3.5.5/providers, modulesdir_rel_libdir = providers debug: pkgconfigdir = . => pkgconfigdir = /usr/local/build_36_0/install_apache_php/openssl-3.5.5, pkgconfigdir_rel_libdir = . debug: cmakeconfigdir = . => cmakeconfigdir = /usr/local/build_36_0/install_apache_php/openssl-3.5.5, cmakeconfigdir_rel_libdir = . use of uninitialized value in join or string at util/mkinstallvars.pl line 156. debug: all keys: applinkdir, bindir, cmakeconfigdir, enginesdir, includedir, ldlibs, libdir, modulesdir, pkgconfigdir, prefix, version, libdir debug: libdir = lib/64, libdir = /usr/local/apache/ssl/lib/64 => libdir = lib/64 debug: prefix = /usr/local/apache/ssl => prefix = /usr/local/apache/ssl debug: libdir = /usr/local/apache/ssl/lib/64 => libdir = /usr/local/apache/ssl/lib/64 debug: bindir = bin => bindir = /usr/local/apache/ssl/bin, bindir_rel_prefix = bin debug: libdir = lib/64 => libdir = /usr/local/apache/ssl/lib/64, libdir_rel_prefix = lib/64 debug: includedir = include => includedir = /usr/local/apache/ssl/include, includedir_rel_prefix = include debug: applinkdir = include/openssl => applinkdir = /usr/local/apache/ssl/include/openssl, applinkdir_rel_prefix = include/openssl debug: enginesdir = /usr/local/apache/ssl/lib/64/engines-3 => enginesdir = /usr/local/apache/ssl/lib/64/engines-3, enginesdir_rel_libdir = engines-3 debug: modulesdir = /usr/local/apache/ssl/lib/64/ossl-modules => modulesdir = /usr/local/apache/ssl/lib/64/ossl-modules, modulesdir_rel_libdir = ossl-modules debug: pkgconfigdir = /usr/local/apache/ssl/lib/64/pkgconfig => pkgconfigdir = /usr/local/apache/ssl/lib/64/pkgconfig, pkgconfigdir_rel_libdir = pkgconfig debug: cmakeconfigdir = /usr/local/apache/ssl/lib/64/cmake/openssl => cmakeconfigdir = /usr/local/apache/ssl/lib/64/cmake/openssl, cmakeconfigdir_rel_libdir = cmake/openssl /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 528: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 547: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 703: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 922: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 973: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 1271: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 1360: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 1659: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 1752: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 1771: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 1997: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 2324: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 2343: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 2471: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 2490: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 2646: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 2865: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 2916: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 3214: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 3303: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 3602: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 3723: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 3942: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 3962: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 3981: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 4207: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 4230: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 4249: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//cczbbqfl.s"", line 4475: error: statement syntax make[1]: *** [makefile:15049: crypto/aes/libcrypto-lib-aest4-sparcv9.o] error 1 make: *** [makefile:3891: build_sw] error 2 /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 528: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 547: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 703: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 922: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 973: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 1271: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 1360: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 1659: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 1752: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 1771: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 1997: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 2324: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 2343: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 2471: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 2490: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 2646: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 2865: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 2916: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 3214: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 3303: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 3602: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 3723: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 3942: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 3962: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 3981: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 4207: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 4230: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 4249: error: statement syntax /usr/ccs/bin/as: ""/var/tmp//ccfpkyrr.s"", line 4475: error: statement syntax make[1]: *** [makefile:15049: crypto/aes/libcrypto-lib-aest4-sparcv9.o] error 1 make: *** [makefile:3894: build_libs] error 2",4.6,Critical,1.0,crash-like behavior
rust-lang/rust#151755,-receiving trait function behaves different than when gats are involved,"here's an interesting issue i ran into when writing code that involves gats. [(playground)]( which emits two errors - one saying that does not live long enough for the call to , - and another saying that it cannot move out of at because it is still borrowed. the whole diagnostic is included further down. if you comment out the line with and instead add it compiles. as far as i know, these should behave exactly the same as the free function is nothing special. the only difference i could see was the bound on in (1.95 nightly, see below), but this can hardly be it, can it? somewhat tangentially: while minimizing this from my actual code, i didn't have the bound initially. i noticed that it was needed only after i removed an associated type from which was used in the return type of . the additional diagnostic that is shown when you remove the bound is not emitted when the return type of more complex. i don't know if this is directly related to this problem, but [here's an example]( of it. ### meta i initially ran into this on 1.89 stable: i've also tested this with 1.95 nightly: complete diagnostic the line numbers may be off, i added locally.",[],['BUG'],"['C-bug', 'needs-triage', 'A-GATs']",github,2026-01-27T17:28:56Z,,"-receiving trait function behaves different than when gats are involved here's an interesting issue i ran into when writing code that involves gats. [(playground)]( which emits two errors - one saying that does not live long enough for the call to , - and another saying that it cannot move out of at because it is still borrowed. the whole diagnostic is included further down. if you comment out the line with and instead add it compiles. as far as i know, these should behave exactly the same as the free function is nothing special. the only difference i could see was the bound on in (1.95 nightly, see below), but this can hardly be it, can it? somewhat tangentially: while minimizing this from my actual code, i didn't have the bound initially. i noticed that it was needed only after i removed an associated type from which was used in the return type of . the additional diagnostic that is shown when you remove the bound is not emitted when the return type of more complex. i don't know if this is directly related to this problem, but [here's an example]( of it. ### meta i initially ran into this on 1.89 stable: i've also tested this with 1.95 nightly: complete diagnostic the line numbers may be off, i added locally.",2.26,Medium,0.734,functional impact
llvm/llvm-project#178262,shtest-encoding.py breaks with python 3.14,"the llvm-21-tools package ships a test file that contains non-utf-8 characters without an encoding declaration, causing package installation to fail with python 3.14. **affected package** llvm-21-tools 1:21.1.8-1ubuntu1 **problematic file** /usr/lib/llvm-21/build/utils/lit/tests/shtest-encoding.py **error** when python 3.14 is the default interpreter, installing llvm-21-tools triggers: syntaxerror: non-utf-8 code starting with '\xc2' on line 3, but no encoding declared this occurs during package post-installation when python attempts to byte-compile the file. the error causes llvm-21-tools configuration to fail, which blocks installation of: - clang-tidy-21 - python3-pygments (via rtupdate hooks) - any package build-depending on llvm 21 toolchain **impact** this is a python 3.14 transition blocker affecting the entire ubuntu archive. any package that build-depends on clang-21, clang-tidy-21, clang-tools-21, llvm-21-dev, or llvm-21-tools will fail to build when python 3.14 is default. **technical details** line 3 of shtest-encoding.py contains byte 0xc2 (non-utf-8 character) but lacks a pep 263 encoding declaration (e.g., # -*- coding: utf-8 -*-). python 3.14 enforces pep 263 strictly and fails to compile such files, while python 3.13 is more lenient. verification: $ python3.14 -m py_compile /usr/lib/llvm-21/build/utils/lit/tests/shtest-encoding.py file ""/usr/lib/llvm-21/build/utils/lit/tests/shtest-encoding.py"", line 3 syntaxerror: non-utf-8 code starting with '\xc2' $ sed -n '3p' /usr/lib/llvm-21/build/utils/lit/tests/shtest-encoding.py | xxd (shows 0xc2 byte) debian bug report:",[],['UI'],"['test-suite', 'tools:llvm-lit']",github,2026-01-27T17:33:16Z,,"shtest-encoding.py breaks with python 3.14 the llvm-21-tools package ships a test file that contains non-utf-8 characters without an encoding declaration, causing package installation to fail with python 3.14. **affected package** llvm-21-tools 1:21.1.8-1ubuntu1 **problematic file** /usr/lib/llvm-21/build/utils/lit/tests/shtest-encoding.py **error** when python 3.14 is the default interpreter, installing llvm-21-tools triggers: syntaxerror: non-utf-8 code starting with '\xc2' on line 3, but no encoding declared this occurs during package post-installation when python attempts to byte-compile the file. the error causes llvm-21-tools configuration to fail, which blocks installation of: - clang-tidy-21 - python3-pygments (via rtupdate hooks) - any package build-depending on llvm 21 toolchain **impact** this is a python 3.14 transition blocker affecting the entire ubuntu archive. any package that build-depends on clang-21, clang-tidy-21, clang-tools-21, llvm-21-dev, or llvm-21-tools will fail to build when python 3.14 is default. **technical details** line 3 of shtest-encoding.py contains byte 0xc2 (non-utf-8 character) but lacks a pep 263 encoding declaration (e.g., # -*- coding: utf-8 -*-). python 3.14 enforces pep 263 strictly and fails to compile such files, while python 3.13 is more lenient. verification: $ python3.14 -m py_compile /usr/lib/llvm-21/build/utils/lit/tests/shtest-encoding.py file ""/usr/lib/llvm-21/build/utils/lit/tests/shtest-encoding.py"", line 3 syntaxerror: non-utf-8 code starting with '\xc2' $ sed -n '3p' /usr/lib/llvm-21/build/utils/lit/tests/shtest-encoding.py | xxd (shows 0xc2 byte) debian bug report:",3.6,Critical,1.0,user-visible issue
microsoft/vscode#290898,"terminal tool says ""ran"" while still in progress","testing there's just a lot going on here and it feels a bit heavy in this state with one tool- working, ""ran sleep 20"" while the command isn't done, it's still going. then the command name again, with an empty output container. i think the empty terminal output looks a bit off, maybe it shouldn't be shown until there is output? i'm not sure. then the ""computing"" label.",[],"['BUG', 'UI']","['bug', 'ux']",github,2026-01-27T17:34:27Z,,"terminal tool says ""ran"" while still in progress testing there's just a lot going on here and it feels a bit heavy in this state with one tool- working, ""ran sleep 20"" while the command isn't done, it's still going. then the command name again, with an empty output container. i think the empty terminal output looks a bit off, maybe it shouldn't be shown until there is output? i'm not sure. then the ""computing"" label.",2.078,Medium,0.692,user-visible issue
cockroachdb/cockroach#161871,opt: edge case bug in new inline rule,discovered a bug in the [recently merged]( inlineanyprojectset norm rule: it results in an incorrect result for a array. ex: we can fix this by checking if the array is null directly: (replace with an arbitrary array-typed scalar) jira issue: crdb-59128,[],['BUG'],"['C-bug', 'A-sql-optimizer', 'T-sql-queries']",github,2026-01-27T17:41:37Z,,opt: edge case bug in new inline rule discovered a bug in the [recently merged]( inlineanyprojectset norm rule: it results in an incorrect result for a array. ex: we can fix this by checking if the array is null directly: (replace with an arbitrary array-typed scalar) jira issue: crdb-59128,2.324,Medium,0.748,functional impact
microsoft/vscode#290905,sessions buttons: preference on buttons,"testing just a ux nit but lowkey i think this looks better, since the chevron is on the very right.",[],['UI'],"['ux', 'under-discussion']",github,2026-01-27T17:47:38Z,,"sessions buttons: preference on buttons testing just a ux nit but lowkey i think this looks better, since the chevron is on the very right.",1.8,Low,0.629,user-visible issue
cockroachdb/cockroach#161873,sentry: panic.go:770: panic: √ó (1) attached stack trace -- stack trace: | runtime.gopanic | goroot/src/runtime/panic.go:770 | github.com/cockroachdb/cockroach/pkg/raft.asserttrue | pkg/ra...,this issue was auto filed by sentry. it represents a crash or reported error on a live cluster with telemetry enabled. sentry link: [ panic message: stacktrace (expand for inline code snippets): [src/runtime/asm_amd64.s#l1694-l1696](src/runtime/asm_amd64.s#l1694-l1696) [pkg/util/stop/stopper.go#l497-l499](pkg/util/stop/stopper.go#l497-l499) [pkg/kv/kvserver/scheduler.go#l318-l320](pkg/kv/kvserver/scheduler.go#l318-l320) [pkg/kv/kvserver/scheduler.go#l396-l398](pkg/kv/kvserver/scheduler.go#l396-l398) [pkg/kv/kvserver/store_raft.go#l624-l626](pkg/kv/kvserver/store_raft.go#l624-l626) [pkg/kv/kvserver/store_raft.go#l361-l363](pkg/kv/kvserver/store_raft.go#l361-l363) [pkg/kv/kvserver/store_raft.go#l626-l628](pkg/kv/kvserver/store_raft.go#l626-l628) [pkg/kv/kvserver/store_raft.go#l406-l408](pkg/kv/kvserver/store_raft.go#l406-l408) [pkg/kv/kvserver/replica_raft.go#l639-l641](pkg/kv/kvserver/replica_raft.go#l639-l641) [pkg/kv/kvserver/replica_raft.go#l2352-l2354](pkg/kv/kvserver/replica_raft.go#l2352-l2354) [pkg/kv/kvserver/replica_raft.go#l2315-l2317](pkg/kv/kvserver/replica_raft.go#l2315-l2317) [pkg/kv/kvserver/replica_raft.go#l713-l715](pkg/kv/kvserver/replica_raft.go#l713-l715) [pkg/raft/rawnode.go#l139-l141](pkg/raft/rawnode.go#l139-l141) [pkg/raft/raft.go#l1608-l1610](pkg/raft/raft.go#l1608-l1610) [pkg/raft/raft.go#l1276-l1278](pkg/raft/raft.go#l1276-l1278) [pkg/raft/raft.go#l1035-l1037](pkg/raft/raft.go#l1035-l1037) [pkg/raft/util.go#l351-l353](pkg/raft/util.go#l351-l353) [goroot/src/runtime/panic.go#l769-l771](goroot/src/runtime/panic.go#l769-l771) ### tags | tag | value | | --- | --- | | command | server | | environment | v24.3.22 | | go version | go1.22.12 x:nocoverageredesign | | platform | linux amd64 | | distribution | ccl | | cockroach release | v24.3.22 | | cockroach sha | 303aedc82bbb8ef1b70f10cd91fcfc01d967d799 | | # of cpus | 32 | | # of goroutines | 8665 | jira issue: crdb-59129,[],['BUG'],"['C-bug', 'O-sentry', 'X-blathers-triaged', 'T-kv', 'branch-release-24.3']",github,2026-01-27T17:56:17Z,,sentry: panic.go:770: panic: √ó (1) attached stack trace -- stack trace: | runtime.gopanic | goroot/src/runtime/panic.go:770 | github.com/cockroachdb/cockroach/pkg/raft.asserttrue | pkg/ra... this issue was auto filed by sentry. it represents a crash or reported error on a live cluster with telemetry enabled. sentry link: [ panic message: stacktrace (expand for inline code snippets): [src/runtime/asm_amd64.s#l1694-l1696](src/runtime/asm_amd64.s#l1694-l1696) [pkg/util/stop/stopper.go#l497-l499](pkg/util/stop/stopper.go#l497-l499) [pkg/kv/kvserver/scheduler.go#l318-l320](pkg/kv/kvserver/scheduler.go#l318-l320) [pkg/kv/kvserver/scheduler.go#l396-l398](pkg/kv/kvserver/scheduler.go#l396-l398) [pkg/kv/kvserver/store_raft.go#l624-l626](pkg/kv/kvserver/store_raft.go#l624-l626) [pkg/kv/kvserver/store_raft.go#l361-l363](pkg/kv/kvserver/store_raft.go#l361-l363) [pkg/kv/kvserver/store_raft.go#l626-l628](pkg/kv/kvserver/store_raft.go#l626-l628) [pkg/kv/kvserver/store_raft.go#l406-l408](pkg/kv/kvserver/store_raft.go#l406-l408) [pkg/kv/kvserver/replica_raft.go#l639-l641](pkg/kv/kvserver/replica_raft.go#l639-l641) [pkg/kv/kvserver/replica_raft.go#l2352-l2354](pkg/kv/kvserver/replica_raft.go#l2352-l2354) [pkg/kv/kvserver/replica_raft.go#l2315-l2317](pkg/kv/kvserver/replica_raft.go#l2315-l2317) [pkg/kv/kvserver/replica_raft.go#l713-l715](pkg/kv/kvserver/replica_raft.go#l713-l715) [pkg/raft/rawnode.go#l139-l141](pkg/raft/rawnode.go#l139-l141) [pkg/raft/raft.go#l1608-l1610](pkg/raft/raft.go#l1608-l1610) [pkg/raft/raft.go#l1276-l1278](pkg/raft/raft.go#l1276-l1278) [pkg/raft/raft.go#l1035-l1037](pkg/raft/raft.go#l1035-l1037) [pkg/raft/util.go#l351-l353](pkg/raft/util.go#l351-l353) [goroot/src/runtime/panic.go#l769-l771](goroot/src/runtime/panic.go#l769-l771) ### tags | tag | value | | --- | --- | | command | server | | environment | v24.3.22 | | go version | go1.22.12 x:nocoverageredesign | | platform | linux amd64 | | distribution | ccl | | cockroach release | v24.3.22 | | cockroach sha | 303aedc82bbb8ef1b70f10cd91fcfc01d967d799 | | # of cpus | 32 | | # of goroutines | 8665 | jira issue: crdb-59129,6.0,Critical,1.0,crash-like behavior
microsoft/vscode#290910,vs code 1.108.2 crashes on wayland (rhel 8),"type: bug environment os: red hat enterprise linux 8.x desktop environment: gnome display server: wayland (crashes) / x11 (works) vs code version: 1.108.2 installation method: rpm (microsoft official repo) session type: wayland ‚Üí crash x11/xorg ‚Üí works as expected reproduction steps log into a gnome wayland session launch vs code (code) perform any ui interaction, for example: save a file (ctrl+s) open a file or folder close a tab open help ‚Üí about vs code exits immediately (hard crash, no ui error) workarounds run vs code with wayland disabled: env -u wayland_display -u xdg_session_type code",[],['PERFORMANCE'],"['freeze-slow-crash-leak', 'under-discussion']",github,2026-01-27T18:02:48Z,,"vs code 1.108.2 crashes on wayland (rhel 8) type: bug environment os: red hat enterprise linux 8.x desktop environment: gnome display server: wayland (crashes) / x11 (works) vs code version: 1.108.2 installation method: rpm (microsoft official repo) session type: wayland ‚Üí crash x11/xorg ‚Üí works as expected reproduction steps log into a gnome wayland session launch vs code (code) perform any ui interaction, for example: save a file (ctrl+s) open a file or folder close a tab open help ‚Üí about vs code exits immediately (hard crash, no ui error) workarounds run vs code with wayland disabled: env -u wayland_display -u xdg_session_type code",5.4,Critical,1.0,"performance degradation, crash-like behavior"
pandas-dev/pandas#63903,"bug: pandas converts nullable int to float, even when this loses data","### pandas version checks - [x] i have checked that this issue has not already been reported. - [x] i have confirmed this bug exists on the [latest version]( of pandas. - [x] i have confirmed this bug exists on the [main branch]( of pandas. ### reproducible example ### issue description anytime there's a null value in a series when using .apply() (or creating a dataframe with no dtype specified), it will be converted to a float, even if the data type it already is can support null values, and is a better fit for the data, and this results in loss of precision compared to not converting possibly related to ### expected behavior pandas should not convert data types to a float when this loses data and is a worse fit for the data. ### installed versions installed versions ------------------ commit : 366ccdfcd8ed1e5543bfb6d4ee0c9bc519898670 python : 3.14.2 python-bits : 64 os : windows os-release : 11 version : 10.0.26200 machine : amd64 processor : intel64 family 6 model 165 stepping 2, genuineintel byteorder : little lc_all : none lang : en_us.utf-8 locale : english_canada.1252 pandas : 3.0.0 numpy : 2.4.1 dateutil : 2.9.0.post0 pip : 25.3 cython : none sphinx : none ipython : none adbc-driver-postgresql: none adbc-driver-sqlite : none bs4 : none bottleneck : none fastparquet : none fsspec : none html5lib : none hypothesis : none gcsfs : none jinja2 : none lxml.etree : none matplotlib : none numba : none numexpr : none odfpy : none openpyxl : none psycopg2 : none pymysql : none pyarrow : none pyiceberg : none pyreadstat : none pytest : none python-calamine : none pytz : none pyxlsb : none s3fs : none scipy : none sqlalchemy : none tables : none tabulate : none xarray : none xlrd : none xlsxwriter : none zstandard : none qtpy : none pyqt5 : none",[],['BUG'],"['Bug', 'Needs Triage']",github,2026-01-27T18:04:37Z,,"bug: pandas converts nullable int to float, even when this loses data ### pandas version checks - [x] i have checked that this issue has not already been reported. - [x] i have confirmed this bug exists on the [latest version]( of pandas. - [x] i have confirmed this bug exists on the [main branch]( of pandas. ### reproducible example ### issue description anytime there's a null value in a series when using .apply() (or creating a dataframe with no dtype specified), it will be converted to a float, even if the data type it already is can support null values, and is a better fit for the data, and this results in loss of precision compared to not converting possibly related to ### expected behavior pandas should not convert data types to a float when this loses data and is a worse fit for the data. ### installed versions installed versions ------------------ commit : 366ccdfcd8ed1e5543bfb6d4ee0c9bc519898670 python : 3.14.2 python-bits : 64 os : windows os-release : 11 version : 10.0.26200 machine : amd64 processor : intel64 family 6 model 165 stepping 2, genuineintel byteorder : little lc_all : none lang : en_us.utf-8 locale : english_canada.1252 pandas : 3.0.0 numpy : 2.4.1 dateutil : 2.9.0.post0 pip : 25.3 cython : none sphinx : none ipython : none adbc-driver-postgresql: none adbc-driver-sqlite : none bs4 : none bottleneck : none fastparquet : none fsspec : none html5lib : none hypothesis : none gcsfs : none jinja2 : none lxml.etree : none matplotlib : none numba : none numexpr : none odfpy : none openpyxl : none psycopg2 : none pymysql : none pyarrow : none pyiceberg : none pyreadstat : none pytest : none python-calamine : none pytz : none pyxlsb : none s3fs : none scipy : none sqlalchemy : none tables : none tabulate : none xarray : none xlrd : none xlsxwriter : none zstandard : none qtpy : none pyqt5 : none",2.21,Medium,0.722,functional impact
python/cpython#144285,suggestions: check for name collisions between modules and items in a module,"this suggestion is pretty confusing, particularly if you don't understand that is a thing. when the suggestion finds a match like this, it should also check to make sure the module name and the suggestion don't clash. perhaps it should also be able to suggest ? ### linked prs * gh-144299",[],['FEATURE'],"['type-feature', 'stdlib']",github,2026-01-27T18:16:18Z,,"suggestions: check for name collisions between modules and items in a module this suggestion is pretty confusing, particularly if you don't understand that is a thing. when the suggestion finds a match like this, it should also check to make sure the module name and the suggestion don't clash. perhaps it should also be able to suggest ? ### linked prs * gh-144299",1.4,Low,0.538,localized low-impact
microsoft/vscode#290916,folders are not hidden when explorer.excludegitignore is true,testing basic case-insensitive folder exclusion 1. create a new folder and open it in vs code 2. create a .gitignore file with content: node_modules 3. create a folder named node_modules or node_modules 4. enable explorer.excludegitignore in settings 5. expected: the folder should be hidden from explorer actual: the folders are grayed out but they are not hidden.,[],['BUG'],['bug'],github,2026-01-27T18:17:15Z,,folders are not hidden when explorer.excludegitignore is true testing basic case-insensitive folder exclusion 1. create a new folder and open it in vs code 2. create a .gitignore file with content: node_modules 3. create a folder named node_modules or node_modules 4. enable explorer.excludegitignore in settings 5. expected: the folder should be hidden from explorer actual: the folders are grayed out but they are not hidden.,2.418,Medium,0.77,functional impact
microsoft/vscode#290919,support terminal color scheme reporting,xterm.js:,[],"['FEATURE', 'UI']","['feature-request', 'upstream', 'terminal', 'upstream-issue-linked']",github,2026-01-27T18:20:37Z,,support terminal color scheme reporting xterm.js:,1.6,Low,0.584,user-visible issue
rust-lang/rust#151757,gcc codegen backend fails to compile diesel,"<!-- thank you for filing a bug report! üêõ please provide a short summary of the bug, along with any information you feel relevant to replicating the bug. --> to put this upfront: i'm well aware that the gcc backend is nowhere near to be production ready. i'm mostly filling this bug to let you know, not to get this fixed soon. i tried this code: i expected to see this happen: compilation succeeds instead, this happened: compilation fails with the following error if that's helpful: dsl_auto_type is a local proc-macro crate ### meta <!-- if you're using the stable version of the compiler, you should also check if the bug also exists in the beta or nightly versions. --> :",[],['BUG'],"['T-compiler', 'C-bug', 'A-gcc']",github,2026-01-27T18:22:05Z,,"gcc codegen backend fails to compile diesel <!-- thank you for filing a bug report! üêõ please provide a short summary of the bug, along with any information you feel relevant to replicating the bug. --> to put this upfront: i'm well aware that the gcc backend is nowhere near to be production ready. i'm mostly filling this bug to let you know, not to get this fixed soon. i tried this code: i expected to see this happen: compilation succeeds instead, this happened: compilation fails with the following error if that's helpful: dsl_auto_type is a local proc-macro crate ### meta <!-- if you're using the stable version of the compiler, you should also check if the bug also exists in the beta or nightly versions. --> :",2.51,Medium,0.791,functional impact
python/cpython#144287,os.scandir incorrectly reports symlink as file,"# bug report ### bug description: i started rewriting my code to replace os.stat for every file with a smarter / more efficient os.scandir. nice. thank you. however: os.scandir() entries report symlinks as files, -- which is *absolutely incorrect* and breaks everything. to reproduce: as one can see here, 'bar' is treated as a file. the unix find cannot find it with the option '-type f'. it is not a file. please fix. ### cpython versions tested on: 3.9 ### operating systems tested on: macos",[],['BUG'],"['type-bug', 'pending']",github,2026-01-27T18:24:52Z,2026-01-27T18:59:10Z,"os.scandir incorrectly reports symlink as file # bug report ### bug description: i started rewriting my code to replace os.stat for every file with a smarter / more efficient os.scandir. nice. thank you. however: os.scandir() entries report symlinks as files, -- which is *absolutely incorrect* and breaks everything. to reproduce: as one can see here, 'bar' is treated as a file. the unix find cannot find it with the option '-type f'. it is not a file. please fix. ### cpython versions tested on: 3.9 ### operating systems tested on: macos",2.253,Medium,0.732,functional impact
electron/electron#49549,accelerators broken after toggling on macos,"### preflight checklist - [x] i have read the [contributing guidelines]( for this project. - [x] i agree to follow the [code of conduct]( that this project adheres to. - [x] i have searched the [issue tracker]( for a bug report that matches the one i want to file, without success. ### electron version 40.0.0, 39.1.0 ### what operating system(s) are you using? macos ### operating system version macos tahoe 26.2 ### what arch are you using? arm64 (including apple silicon) ### last known working electron version 39.0.0 ### does the issue also appear in chromium / google chrome? i don't know how to test ### expected behavior keyboard accelerators, e.g. cmd+o, cmd+s for file open/save, work after selecting item from native menu. ### actual behavior if select open from menu then cmd+s will not work. if select save from menu then cmd+o will not work. ### testcase gist url ### additional information this is not an issue on windows. electron 39.0.0 works but 39.1.0 and above does not.",[],['BUG'],"['platform/macOS', 'component/menu', 'bug :beetle:', 'bug/regression :leftwards_arrow_with_hook:', 'status/confirmed', 'has-repro-comment', '39-x-y', '40-x-y']",github,2026-01-27T18:30:06Z,,"accelerators broken after toggling on macos ### preflight checklist - [x] i have read the [contributing guidelines]( for this project. - [x] i agree to follow the [code of conduct]( that this project adheres to. - [x] i have searched the [issue tracker]( for a bug report that matches the one i want to file, without success. ### electron version 40.0.0, 39.1.0 ### what operating system(s) are you using? macos ### operating system version macos tahoe 26.2 ### what arch are you using? arm64 (including apple silicon) ### last known working electron version 39.0.0 ### does the issue also appear in chromium / google chrome? i don't know how to test ### expected behavior keyboard accelerators, e.g. cmd+o, cmd+s for file open/save, work after selecting item from native menu. ### actual behavior if select open from menu then cmd+s will not work. if select save from menu then cmd+o will not work. ### testcase gist url ### additional information this is not an issue on windows. electron 39.0.0 works but 39.1.0 and above does not.",2.367,Medium,0.758,functional impact
rust-lang/rust#151758,new vecdeque::splice() method in nightly produce incorrect result,"when i run the following code on the latest nightly channel, it prints incorrect result: i would expect the output of this simple program to be: but instead the following got print: as you can see, the last element got remvoed, and two extra elements got inserted. in my actual application (not in this simple example), the extra elements contain random values instead of just zeros.",[],"['BUG', 'UI']","['I-unsound', 'C-bug', 'requires-nightly', 'T-libs', 'S-has-mcve', 'needs-triage']",github,2026-01-27T18:45:32Z,,"new vecdeque::splice() method in nightly produce incorrect result when i run the following code on the latest nightly channel, it prints incorrect result: i would expect the output of this simple program to be: but instead the following got print: as you can see, the last element got remvoed, and two extra elements got inserted. in my actual application (not in this simple example), the extra elements contain random values instead of just zeros.",2.377,Medium,0.76,user-visible issue
cockroachdb/cockroach#161877,sql: resume of index backfill with distributed merge can hit terminal error,"currently, if you resume an index backfill that used distributed merge, we assume that all of the nodes that were previously up are still available. this comes from writing temporary ssts to nodelocal storage. those files are only accessible as long as the node that owns them stays up. if a node goes down and isn‚Äôt restarted quickly, the backfill can no longer make progress. when this happens, the failure mode isn‚Äôt very obvious. we surface some generic rpc error when trying to talk to the node that holds the nodelocal files. the goal of this task is to make that failure much more explicit. we likely want to keep retrying the job in case the node outage is transient, but we should probably cap the retries or add some form of backoff. one possible approach is to detect this earlier when we plan the merge processor. we can compare the set of nodes in the plan with the nodes referenced in the sst manifest and fail fast if they don‚Äôt line up. we may also want to consider falling back to the old, non-distributed merge backfill algorithm in this case, though that needs more thought. jira issue: crdb-59130 epic crdb-48845",[],['FEATURE'],"['C-enhancement', 'T-sql-foundations']",github,2026-01-27T18:46:10Z,,"sql: resume of index backfill with distributed merge can hit terminal error currently, if you resume an index backfill that used distributed merge, we assume that all of the nodes that were previously up are still available. this comes from writing temporary ssts to nodelocal storage. those files are only accessible as long as the node that owns them stays up. if a node goes down and isn‚Äôt restarted quickly, the backfill can no longer make progress. when this happens, the failure mode isn‚Äôt very obvious. we surface some generic rpc error when trying to talk to the node that holds the nodelocal files. the goal of this task is to make that failure much more explicit. we likely want to keep retrying the job in case the node outage is transient, but we should probably cap the retries or add some form of backoff. one possible approach is to detect this earlier when we plan the merge processor. we can compare the set of nodes in the plan with the nodes referenced in the sst manifest and fail fast if they don‚Äôt line up. we may also want to consider falling back to the old, non-distributed merge backfill algorithm in this case, though that needs more thought. jira issue: crdb-59130 epic crdb-48845",1.4,Low,0.538,localized low-impact
microsoft/vscode#290938,new session button theme key/fallback?,testing this button might not be using a good fallback? are these themes just missing a common theme key? sapphire red solarized dark,[],['BUG'],"['bug', 'themes']",github,2026-01-27T18:57:12Z,,new session button theme key/fallback? testing this button might not be using a good fallback? are these themes just missing a common theme key? sapphire red solarized dark,2.456,Medium,0.778,functional impact
cockroachdb/cockroach#161879,crosscluster: create o11y helper,currently a user connected to the source cluster of ldr and pcr doesn't have a user friendly way to see what is being replicated. could display jira issue: crdb-59132 epic crdb-59131,[],['FEATURE'],"['C-enhancement', 'E-starter']",github,2026-01-27T19:06:16Z,,crosscluster: create o11y helper currently a user connected to the source cluster of ldr and pcr doesn't have a user friendly way to see what is being replicated. could display jira issue: crdb-59132 epic crdb-59131,3.008,High,0.904,functional impact
microsoft/vscode#290947,prompted to trust twice when selecting a untrusted folder in the background picker,"testing maybe we should generically ask for workspace trust on that folder when it gets picked, and cancel if untrusted?",[],['BUG'],"['bug', 'chat-background-agent']",github,2026-01-27T19:10:54Z,2026-01-28T05:59:42Z,"prompted to trust twice when selecting a untrusted folder in the background picker testing maybe we should generically ask for workspace trust on that folder when it gets picked, and cancel if untrusted?",2.228,Medium,0.726,functional impact
flutter/flutter#181561,investigate potential use of dart shared native memory in flutter,"dart is getting shared native memory support soon. the functionality is described in the it would be interesting to see if there are any use cases in flutter where use of shared native memory could improve framework performance. potentially, data that is currently copied and messaged between dart framework code and native android/ios could instead be made available to dart as shared memory and accessed from dart directly, with access guarded to ensure integrity.",[],['FEATURE'],"['c: new feature', 'framework', 'engine', 'dependency: dart', 'c: proposal', 'team-engine']",github,2026-01-27T19:11:45Z,,"investigate potential use of dart shared native memory in flutter dart is getting shared native memory support soon. the functionality is described in the it would be interesting to see if there are any use cases in flutter where use of shared native memory could improve framework performance. potentially, data that is currently copied and messaged between dart framework code and native android/ios could instead be made available to dart as shared memory and accessed from dart directly, with access guarded to ensure integrity.",2.723,Medium,0.839,functional impact
llvm/llvm-project#178277,document current status of preserve_most/all/none cc,"documentation for states that the feature is experimental and slated for use in objective-c, but that note was added in 2016 ( > this calling convention will be used by a future version of the objective-c runtime and should therefore still be considered experimental at this time. although this convention was created to optimize certain runtime calls to the objective-c runtime, it is not limited to this runtime and might be used by other runtimes in the future too. the current implementation only supports x86-64, but the intention is to support more architectures in the future. there are a few other places aspects in which documentation should be updated to reflect 2026, including: * updating the docs to reflect the current intended usage in objective-c. * re-evaluating the current ‚Äúexperimental‚Äù status of to ensure it‚Äôs up to date. i‚Äôm unsure when a feature in clang qualifies for promotion out of experimental. it‚Äôs been around since 2016, though it does have relatively recent bug fixes. i did some basic searching, and fwiw it‚Äôs used in the linux kernel for cold validation functions [1], some parts of v8 for error handling [2], tcmalloc for an initializer [3], and in protobuf' Œºpb [4]. * documenting the intended behavior for flags registers -- * addressing to some degree. i‚Äôve sent a patch for glibc's plt resolution [5], but it‚Äôs still pending viability. if it can't/won't be fixed in popular dynamic linkers, then we should at least note this pitfall in the documentation or provide warnings etc, as it seems like many folks rediscover this footgun. -rh has good example documentation warning about plt interactions in hi , , i see you were the original authors of the docs (though now ~10y ago). i'm not seeing references in do you know whether objective-c runtime relies or intends to rely on ? or could you help me find the right people? thanks! [1] [2] [3] [4] [5]",[],['DOCUMENTATION'],['documentation'],github,2026-01-27T19:12:18Z,,"document current status of preserve_most/all/none cc documentation for states that the feature is experimental and slated for use in objective-c, but that note was added in 2016 ( > this calling convention will be used by a future version of the objective-c runtime and should therefore still be considered experimental at this time. although this convention was created to optimize certain runtime calls to the objective-c runtime, it is not limited to this runtime and might be used by other runtimes in the future too. the current implementation only supports x86-64, but the intention is to support more architectures in the future. there are a few other places aspects in which documentation should be updated to reflect 2026, including: * updating the docs to reflect the current intended usage in objective-c. * re-evaluating the current ‚Äúexperimental‚Äù status of to ensure it‚Äôs up to date. i‚Äôm unsure when a feature in clang qualifies for promotion out of experimental. it‚Äôs been around since 2016, though it does have relatively recent bug fixes. i did some basic searching, and fwiw it‚Äôs used in the linux kernel for cold validation functions [1], some parts of v8 for error handling [2], tcmalloc for an initializer [3], and in protobuf' Œºpb [4]. * documenting the intended behavior for flags registers -- * addressing to some degree. i‚Äôve sent a patch for glibc's plt resolution [5], but it‚Äôs still pending viability. if it can't/won't be fixed in popular dynamic linkers, then we should at least note this pitfall in the documentation or provide warnings etc, as it seems like many folks rediscover this footgun. -rh has good example documentation warning about plt interactions in hi , , i see you were the original authors of the docs (though now ~10y ago). i'm not seeing references in do you know whether objective-c runtime relies or intends to rely on ? or could you help me find the right people? thanks! [1] [2] [3] [4] [5]",3.4,High,0.993,crash-like behavior
microsoft/vscode#290952,"""tool simplified the command"" with preprended space seems unnecessary","testing it seems to warn the agent on every command that a space was prepended. since this doesn't change the output, it seems like unnecessary detail.",[],"['BUG', 'UI']","['bug', 'chat-terminal']",github,2026-01-27T19:13:11Z,,"""tool simplified the command"" with preprended space seems unnecessary testing it seems to warn the agent on every command that a space was prepended. since this doesn't change the output, it seems like unnecessary detail.",3.274,High,0.964,"user-visible issue, crash-like behavior"
microsoft/vscode#290954,running in an untrusted folder seems to break,"testing 1. try to run in an untrusted folder 2. you get 3x prompts to trust 3. if you cancel those, you get a message that the workspace isn't trusted. the agent still responds, although it seems to not be in the right folder",[],['BUG'],"['bug', 'chat-background-agent']",github,2026-01-27T19:14:02Z,2026-01-28T02:34:21Z,"running in an untrusted folder seems to break testing 1. try to run in an untrusted folder 2. you get 3x prompts to trust 3. if you cancel those, you get a message that the workspace isn't trusted. the agent still responds, although it seems to not be in the right folder",2.587,Medium,0.808,functional impact
microsoft/vscode#290958,non-merged agent.md files aren't shown as cloud custom agent,"testing learned from josh that the list is populated based on custom agents pushed to a repo so newly created agents that haven't been committed to the repo wont show up 1. create a new agent.md locally 2. check to try and find it in the cloud agent dropdown, üêõ it isn't there 3. reload, üêõ it isn't there 4. push changes and merge into main 5. then the agent shows up would be nice to have some sort of indication that the agent wont work since its local (so i knew what was happening and that i had to push my agent.md to use it)",[],['BUG'],['bug'],github,2026-01-27T19:18:42Z,,"non-merged agent.md files aren't shown as cloud custom agent testing learned from josh that the list is populated based on custom agents pushed to a repo so newly created agents that haven't been committed to the repo wont show up 1. create a new agent.md locally 2. check to try and find it in the cloud agent dropdown, üêõ it isn't there 3. reload, üêõ it isn't there 4. push changes and merge into main 5. then the agent shows up would be nice to have some sort of indication that the agent wont work since its local (so i knew what was happening and that i had to push my agent.md to use it)",4.6,Critical,1.0,crash-like behavior
microsoft/vscode#290960,"""pick folder"" being required could be more clear","testing not directly on the welcome view, but: 1. have a session in a worktree folder 2. hit ""new session"" in the sessions view (this resets the folder picker) 3. try to put send a new chat message 4. üêõ ""send"" is disabled and there is no indication i need to pick a folder first i think it would probably be good to reuse the last session's folder when hitting ""new session."" if i want to change folders, that's something i would conciously think to do.",[],['BUG'],"['bug', 'chat-background-agent']",github,2026-01-27T19:19:54Z,2026-01-27T22:01:09Z,"""pick folder"" being required could be more clear testing not directly on the welcome view, but: 1. have a session in a worktree folder 2. hit ""new session"" in the sessions view (this resets the folder picker) 3. try to put send a new chat message 4. üêõ ""send"" is disabled and there is no indication i need to pick a folder first i think it would probably be good to reuse the last session's folder when hitting ""new session."" if i want to change folders, that's something i would conciously think to do.",4.6,Critical,1.0,crash-like behavior
microsoft/vscode#290965,several weird things when opening welcome view after having been in a chat,"testing 1. have a chat 2. trigger ""open welcome view"" 3. send a new chat message without touching anything else 4. several weird things happen: - it runs a local session even though i have selected a background session - even though i have no model or folder selected, it runs in the last worktree folder - i'm asked to trust the worktree even though i already trusted the primary repo folder itself - even if i don't trust the folder, the agent still starts running in the background",[],['BUG'],['bug'],github,2026-01-27T19:25:14Z,,"several weird things when opening welcome view after having been in a chat testing 1. have a chat 2. trigger ""open welcome view"" 3. send a new chat message without touching anything else 4. several weird things happen: - it runs a local session even though i have selected a background session - even though i have no model or folder selected, it runs in the last worktree folder - i'm asked to trust the worktree even though i already trusted the primary repo folder itself - even if i don't trust the folder, the agent still starts running in the background",2.33,Medium,0.75,functional impact
microsoft/vscode#290983,terminal awaiting input feature does not work with background terminals,"## description the ""terminal awaiting input"" feature does not work when a terminal has been pushed to the background. prompt: the prompt runs a background terminal, but the ""terminal is awaiting input"" confirmation does not appear. i'd expect it to since it does work with a foregroun terminal: ## related - tpi: ## environment - os: macos",[],"['BUG', 'UI']","['bug', 'terminal']",github,2026-01-27T19:45:33Z,,"terminal awaiting input feature does not work with background terminals ## description the ""terminal awaiting input"" feature does not work when a terminal has been pushed to the background. prompt: the prompt runs a background terminal, but the ""terminal is awaiting input"" confirmation does not appear. i'd expect it to since it does work with a foregroun terminal: ## related - tpi: ## environment - os: macos",1.944,Low,0.662,user-visible issue
microsoft/vscode#290992,background agent runs in wrong folder and asks to migrate changes from non-git workspace,"testing 1. go to the welcome view, pick a background session 2. pick a folder that is a git repo 3. change your mind a pick a folder that's not a git repo 4. üêõ you're asked to migrate changes 5. üêõ if you pick an option, the background agent then runs in your previously selected folder",[],['BUG'],"['bug', 'chat-background-agent']",github,2026-01-27T19:55:51Z,2026-01-28T04:09:23Z,"background agent runs in wrong folder and asks to migrate changes from non-git workspace testing 1. go to the welcome view, pick a background session 2. pick a folder that is a git repo 3. change your mind a pick a folder that's not a git repo 4. üêõ you're asked to migrate changes 5. üêõ if you pick an option, the background agent then runs in your previously selected folder",4.6,Critical,1.0,crash-like behavior
microsoft/vscode#290996,natural language search for agent sessions,"## problem i have a hard time finding the correct session i want to get back to in the ""agent sessions"" view. with many sessions accumulating over time, it becomes increasingly difficult to locate a specific conversation. ## proposed solution it would be great if i could ask the agent something along the lines of: ""open the chat session where i was recently working on feature x."" this could be implemented as: - a natural language search capability within the agent sessions view - a chat command or slash command like that uses semantic search to locate relevant past sessions - integration with the existing chat interface where the agent understands context about previous sessions ## use cases 1. finding a session where a specific feature was discussed 2. returning to a debugging session for a particular bug 3. locating a code review session for a specific pr 4. finding sessions related to a particular file or component ## additional context this would significantly improve the workflow when working on multiple features or projects, as users often need to context-switch and return to previous conversations.",[],['FEATURE'],"['feature-request', 'chat-agents-view']",github,2026-01-27T19:58:44Z,,"natural language search for agent sessions ## problem i have a hard time finding the correct session i want to get back to in the ""agent sessions"" view. with many sessions accumulating over time, it becomes increasingly difficult to locate a specific conversation. ## proposed solution it would be great if i could ask the agent something along the lines of: ""open the chat session where i was recently working on feature x."" this could be implemented as: - a natural language search capability within the agent sessions view - a chat command or slash command like that uses semantic search to locate relevant past sessions - integration with the existing chat interface where the agent understands context about previous sessions ## use cases 1. finding a session where a specific feature was discussed 2. returning to a debugging session for a particular bug 3. locating a code review session for a specific pr 4. finding sessions related to a particular file or component ## additional context this would significantly improve the workflow when working on multiple features or projects, as users often need to context-switch and return to previous conversations.",1.4,Low,0.538,localized low-impact
microsoft/vscode#290997,install popup from cloud agent session,testing i tried to click on the link to the pr the cloud agent supplies to then open the wip pr. the uri is long and is a bit confusing to understand what is happening? not sure maybe this required to show to full uri but i would prefer it to say i was attempting to open the pr and use the name.,[],['BUG'],"['bug', 'chat']",github,2026-01-27T20:01:18Z,,install popup from cloud agent session testing i tried to click on the link to the pr the cloud agent supplies to then open the wip pr. the uri is long and is a bit confusing to understand what is happening? not sure maybe this required to show to full uri but i would prefer it to say i was attempting to open the pr and use the name.,2.579,Medium,0.806,functional impact
cockroachdb/cockroach#161885,"sqlstats: move , and from metadata to statistics","whether a statement execution was vectorized, distributed, and performed a full scan is pulled off the a plan's struct and recorded into statement stats. these are recorded as boolean flags in a statement statistics and ultimately stored in the metadata column in . it doesn't make sense to store these fields as boolean flags in this object because its possible for these to be different (true / false) per execution for the same statement statement key (fingerprintid, plan hash, transaction fingerprint id). as a result, the value of these flags on the first execution is that which is saved to . instead of saving it in the metadata, it should be stored as a counter that is incremented when recorded, similar to how we handle [generic count]( note that this will also require changes to db console where we show these as (yes / no) on statement fingerprint overview and statement fingerprint explain plans page. jira issue: crdb-59134",[],['FEATURE'],"['C-enhancement', 'T-observability']",github,2026-01-27T20:09:21Z,,"sqlstats: move , and from metadata to statistics whether a statement execution was vectorized, distributed, and performed a full scan is pulled off the a plan's struct and recorded into statement stats. these are recorded as boolean flags in a statement statistics and ultimately stored in the metadata column in . it doesn't make sense to store these fields as boolean flags in this object because its possible for these to be different (true / false) per execution for the same statement statement key (fingerprintid, plan hash, transaction fingerprint id). as a result, the value of these flags on the first execution is that which is saved to . instead of saving it in the metadata, it should be stored as a counter that is incremented when recorded, similar to how we handle [generic count]( note that this will also require changes to db console where we show these as (yes / no) on statement fingerprint overview and statement fingerprint explain plans page. jira issue: crdb-59134",3.6,Critical,1.0,crash-like behavior
microsoft/vscode#291003,"add ""new tab"" command somewhere","testing once i have a browser tab open, i might want to quickly open another one. i would suggest contributing a + button to the editor toolbar (top right corner) which would only appear when a browser is already open. or it could go under a menu",[],['FEATURE'],"['feature-request', 'verification-needed', 'insiders-released', 'browser-integration']",github,2026-01-27T20:12:32Z,2026-01-27T22:06:45Z,"add ""new tab"" command somewhere testing once i have a browser tab open, i might want to quickly open another one. i would suggest contributing a + button to the editor toolbar (top right corner) which would only appear when a browser is already open. or it could go under a menu",1.4,Low,0.538,localized low-impact
istio/istio#58926,buffer filter incompatible with http upgrades,"### is this the right place to submit this? - [x] this is not a security vulnerability or a crashing bug - [x] this is not a question about how to use istio ### bug description enabling `envoy.filters. is incompatible with http upgrades, which causes websocket requests to fail. > buffering is generally not compatible with upgrades, so if the [buffer filter]( is configured in the default http filter chain it should probably be excluded for upgrades by using [upgrade filters]( and not including the buffer filter in that list. i believe istio should allow us to specify a filter for the http upgrade extension or should implement some way to allow these two extensions to work together. ### version ### additional information _no response_",[],['NETWORK'],['area/networking'],github,2026-01-27T20:19:10Z,,"buffer filter incompatible with http upgrades ### is this the right place to submit this? - [x] this is not a security vulnerability or a crashing bug - [x] this is not a question about how to use istio ### bug description enabling `envoy.filters. is incompatible with http upgrades, which causes websocket requests to fail. > buffering is generally not compatible with upgrades, so if the [buffer filter]( is configured in the default http filter chain it should probably be excluded for upgrades by using [upgrade filters]( and not including the buffer filter in that list. i believe istio should allow us to specify a filter for the http upgrade extension or should implement some way to allow these two extensions to work together. ### version ### additional information _no response_",7.0,Critical,1.0,"affects communication layer, crash-like behavior"
python/cpython#144289,clean up free-threading specific specialization code,"now that the specializing interpreter works with free threading (gh-115999), we can remove the free-threading specific macro and decorator: * * ### linked prs * gh-144290",[],['CLEANUP'],"['topic-free-threading', 'type-refactor']",github,2026-01-27T20:22:56Z,2026-01-27T23:03:14Z,"clean up free-threading specific specialization code now that the specializing interpreter works with free threading (gh-115999), we can remove the free-threading specific macro and decorator: * * ### linked prs * gh-144290",1.2,Low,0.493,localized low-impact
cockroachdb/cockroach#161887,bulkmerge: avoid degrading sorted data,"currently, when we run an index backfill with distributed merge on already-sorted data, performance is worse. on a 5-billion row dataset, the full backfill is about 20% slower. this run goes through three phases: map, local-only merge, and final merge into kv. there are a couple of options here: 1. if we know the data is sorted, which is straightforward for indexes since we know the ordering from the primary key, fall back to the non-distributed merge path. 2. detect that the data is sorted before the local-only merge and skip that phase entirely. this would need some perf validation to confirm that it actually eliminates the ~20% regression. jira issue: crdb-59135 epic crdb-48845",[],['FEATURE'],"['C-enhancement', 'T-sql-foundations']",github,2026-01-27T20:26:30Z,,"bulkmerge: avoid degrading sorted data currently, when we run an index backfill with distributed merge on already-sorted data, performance is worse. on a 5-billion row dataset, the full backfill is about 20% slower. this run goes through three phases: map, local-only merge, and final merge into kv. there are a couple of options here: 1. if we know the data is sorted, which is straightforward for indexes since we know the ordering from the primary key, fall back to the non-distributed merge path. 2. detect that the data is sorted before the local-only merge and skip that phase entirely. this would need some perf validation to confirm that it actually eliminates the ~20% regression. jira issue: crdb-59135 epic crdb-48845",2.803,Medium,0.857,functional impact
microsoft/vscode#291005,rendered text was not passed in with the attachment,"testing 1. navigate to in the integrated browser 2. add the container element for the entire blog text 3. click the attachment, see that the text was all included 4. ask a question about the text üêõ agent can't see the text debug logs confirm that it is only css in the request.",[],['BUG'],"['bug', 'browser-integration']",github,2026-01-27T20:35:10Z,,"rendered text was not passed in with the attachment testing 1. navigate to in the integrated browser 2. add the container element for the entire blog text 3. click the attachment, see that the text was all included 4. ask a question about the text üêõ agent can't see the text debug logs confirm that it is only css in the request.",2.209,Medium,0.722,functional impact
envoyproxy/envoy#43188,ubuntu noble install instruction fails,"the instruction for ubuntu noble is failing, there is no pakage for noble in the archive ==> so, how to install envoy (with script) in a ubuntu 24.0* os?",[],"['BUG', 'DOCUMENTATION']","['bug', 'area/distribution', 'area/docs']",github,2026-01-27T20:38:40Z,,"ubuntu noble install instruction fails the instruction for ubuntu noble is failing, there is no pakage for noble in the archive ==> so, how to install envoy (with script) in a ubuntu 24.0* os?",1.8,Low,0.629,localized low-impact
cockroachdb/cockroach#161889,sql: support explain execute,"currently, we only support of (as well as ""hacked in"" ) whereas pg supports too. we had support of it in the past, but it was removed long time ago when the optimizer was being actively developed (see for context). we should look into supporting it now that the optimizer is very stable. jira issue: crdb-59136",[],"['COMPATIBILITY', 'FEATURE']","['C-enhancement', 'A-sql-pgcompat', 'O-support', 'A-sql-explain', 'T-sql-queries', 'P-3']",github,2026-01-27T20:45:34Z,,"sql: support explain execute currently, we only support of (as well as ""hacked in"" ) whereas pg supports too. we had support of it in the past, but it was removed long time ago when the optimizer was being actively developed (see for context). we should look into supporting it now that the optimizer is very stable. jira issue: crdb-59136",2.0,Medium,0.675,localized low-impact
microsoft/vscode#291025,checkout button idles for a little bit,"testing small nit but looks like the checkout button can take a bit of time, should we just disable it after clicking on it for a little bit or at least until the callback execution is finished?",[],['BUG'],"['bug', 'unreleased', 'chat']",github,2026-01-27T21:24:01Z,2026-01-28T11:32:24Z,"checkout button idles for a little bit testing small nit but looks like the checkout button can take a bit of time, should we just disable it after clicking on it for a little bit or at least until the callback execution is finished?",2.469,Medium,0.781,functional impact
cockroachdb/cockroach#161892,ui: custom time interval leads to error page,"**describe the problem** the time picker selection for no longer opens a new dropdown, instead it leads to the error page immediately. **to reproduce** what did you do? describe in your own words. if possible, provide steps to reproduce the behavior: 1. start a v26.2.0-alpha cluster 2. visit either the metrics page or the sql activity page, wherever a time picker is present, e.g. 3. click on past 10 minutes 4. click custom time interval 5. observe the error page **additional context** it seems to be introduced after this change jira issue: crdb-59137",[],['BUG'],"['C-bug', 'branch-master', 'T-observability']",github,2026-01-27T21:28:52Z,,"ui: custom time interval leads to error page **describe the problem** the time picker selection for no longer opens a new dropdown, instead it leads to the error page immediately. **to reproduce** what did you do? describe in your own words. if possible, provide steps to reproduce the behavior: 1. start a v26.2.0-alpha cluster 2. visit either the metrics page or the sql activity page, wherever a time picker is present, e.g. 3. click on past 10 minutes 4. click custom time interval 5. observe the error page **additional context** it seems to be introduced after this change jira issue: crdb-59137",4.6,Critical,1.0,crash-like behavior
microsoft/vscode#291026,comments are not highlighted in 2026 light theme,current: other themes:,[],['UI'],"['ux', 'polish', '2026-themes']",github,2026-01-27T21:29:01Z,,comments are not highlighted in 2026 light theme current: other themes:,1.8,Low,0.629,user-visible issue
zaproxy/zaproxy#9230,memory leak using javascript scripts,"### describe the bug: hey zap team - starting a few days ago, i noticed out-of-memory errors during active scans. it's happening during the . swaggersecretdetector.js is the only script with type in my installation. it looks like maybe that script was supposed to be [added as an alpha rule]( but it seems to be running for a regular non-alpha rule scan. disabling the swaggersecretdetector.js script appears to fix the issue. ### steps to reproduce the behavior: 1. started w/ a fresh install of 2.17.0 2. help > check for updates... > update all 3. after update the swaggersecretdetector.js script is listed under the active rules scripts. 4. observe oom during active scan during the activescriptscanrule ### expected behavior: for zap active scans to maintain a similar memory profile after updating the latest add-on versions ### software versions: zap version: 2.17.0 installed add-ons: [[id=alertfilters, version=26.0.0], [id=ascanrules, version=79.0.0], [id=authhelper, version=0.34.0], [id=automation, version=0.58.0], [id=bruteforce, version=20.0.0], [id=callhome, version=0.20.0], [id=client, version=0.20.0], [id=commonlib, version=1.39.0], [id=custompayloads, version=0.16.0], [id=database, version=0.9.0], [id=diff, version=18.0.0], [id=directorylistv1, version=9.0.0], [id=domxss, version=23.0.0], [id=encoder, version=1.8.0], [id=exim, version=0.16.0], [id=formhandler, version=6.8.0], [id=fuzz, version=13.16.0], [id=gettingstarted, version=20.0.0], [id=graaljs, version=0.12.0], [id=graphql, version=0.29.0], [id=help, version=22.0.0], [id=hud, version=0.19.0], [id=importurls, version=9.0.0], [id=insights, version=0.1.0], [id=invoke, version=17.0.0], [id=network, version=0.25.0], [id=oast, version=0.24.0], [id=onlinemenu, version=15.0.0], [id=openapi, version=50.0.0], [id=postman, version=0.9.0], [id=pscan, version=0.6.0], [id=pscanrules, version=70.0.0], [id=quickstart, version=53.0.0], [id=replacer, version=21.0.0], [id=reports, version=0.43.0], [id=requester, version=7.9.0], [id=retest, version=0.11.0], [id=retire, version=0.53.0], [id=reveal, version=10.0.0], [id=saverawmessage, version=7.0.0], [id=savexmlmessage, version=0.3.0], [id=scanpolicies, version=0.7.0], [id=scripts, version=45.17.0], [id=selenium, version=15.43.0], [id=sequence, version=9.0.0], [id=soap, version=29.0.0], [id=spider, version=0.18.0], [id=spiderajax, version=23.29.0], [id=tips, version=16.0.0], [id=wappalyzer, version=21.52.0], [id=webdrivermacos, version=174.0.0], [id=websocket, version=35.0.0], [id=zest, version=48.11.0]] operating system: mac os x architecture: x86_64 cpu cores: 8 max memory: 4 gb java version: eclipse adoptium 17.0.17 system's locale: en_us display locale: en_gb format locale: en_us default charset: utf-8 look and feel: flatlaf light (com.formdev.flatlaf.flatlightlaf) ### screenshots: ### errors from the zap.log file: ### additional context: _no response_ ### would you like to help fix this issue? - [ ] yes",[],['BUG'],['bug'],github,2026-01-27T21:33:20Z,,"memory leak using javascript scripts ### describe the bug: hey zap team - starting a few days ago, i noticed out-of-memory errors during active scans. it's happening during the . swaggersecretdetector.js is the only script with type in my installation. it looks like maybe that script was supposed to be [added as an alpha rule]( but it seems to be running for a regular non-alpha rule scan. disabling the swaggersecretdetector.js script appears to fix the issue. ### steps to reproduce the behavior: 1. started w/ a fresh install of 2.17.0 2. help > check for updates... > update all 3. after update the swaggersecretdetector.js script is listed under the active rules scripts. 4. observe oom during active scan during the activescriptscanrule ### expected behavior: for zap active scans to maintain a similar memory profile after updating the latest add-on versions ### software versions: zap version: 2.17.0 installed add-ons: [[id=alertfilters, version=26.0.0], [id=ascanrules, version=79.0.0], [id=authhelper, version=0.34.0], [id=automation, version=0.58.0], [id=bruteforce, version=20.0.0], [id=callhome, version=0.20.0], [id=client, version=0.20.0], [id=commonlib, version=1.39.0], [id=custompayloads, version=0.16.0], [id=database, version=0.9.0], [id=diff, version=18.0.0], [id=directorylistv1, version=9.0.0], [id=domxss, version=23.0.0], [id=encoder, version=1.8.0], [id=exim, version=0.16.0], [id=formhandler, version=6.8.0], [id=fuzz, version=13.16.0], [id=gettingstarted, version=20.0.0], [id=graaljs, version=0.12.0], [id=graphql, version=0.29.0], [id=help, version=22.0.0], [id=hud, version=0.19.0], [id=importurls, version=9.0.0], [id=insights, version=0.1.0], [id=invoke, version=17.0.0], [id=network, version=0.25.0], [id=oast, version=0.24.0], [id=onlinemenu, version=15.0.0], [id=openapi, version=50.0.0], [id=postman, version=0.9.0], [id=pscan, version=0.6.0], [id=pscanrules, version=70.0.0], [id=quickstart, version=53.0.0], [id=replacer, version=21.0.0], [id=reports, version=0.43.0], [id=requester, version=7.9.0], [id=retest, version=0.11.0], [id=retire, version=0.53.0], [id=reveal, version=10.0.0], [id=saverawmessage, version=7.0.0], [id=savexmlmessage, version=0.3.0], [id=scanpolicies, version=0.7.0], [id=scripts, version=45.17.0], [id=selenium, version=15.43.0], [id=sequence, version=9.0.0], [id=soap, version=29.0.0], [id=spider, version=0.18.0], [id=spiderajax, version=23.29.0], [id=tips, version=16.0.0], [id=wappalyzer, version=21.52.0], [id=webdrivermacos, version=174.0.0], [id=websocket, version=35.0.0], [id=zest, version=48.11.0]] operating system: mac os x architecture: x86_64 cpu cores: 8 max memory: 4 gb java version: eclipse adoptium 17.0.17 system's locale: en_us display locale: en_gb format locale: en_us default charset: utf-8 look and feel: flatlaf light (com.formdev.flatlaf.flatlightlaf) ### screenshots: ### errors from the zap.log file: ### additional context: _no response_ ### would you like to help fix this issue? - [ ] yes",5.6,Critical,1.0,system-wide impact
pandas-dev/pandas#63904,bug: assert_frame_equal changes behavior in 3.0 for nested arrays without documentation,### pandas version checks - [x] i have checked that this issue has not already been reported. - [x] i have confirmed this bug exists on the [latest version]( of pandas. - [ ] i have confirmed this bug exists on the [main branch]( of pandas. ### reproducible example ### issue description this code above raises in 3.0.0 and passes in 2.3.x. i can understand the behavior but there's no docs in what's new of 3.0.0 about this. is this considered a bug fix? is the current behavior expected? ### expected behavior i don't know. i can live with either but it's nice if it's stable and documented. ### installed versions installed versions ------------------ commit : 9c8bc3e55188c8aff37207a74f1dd144980b8874 python : 3.12.12 python-bits : 64 os : darwin os-release : 25.2.0 version : darwin kernel version 25.2.0: tue nov 18 21:09:56 pst 2025; root:xnu-12377.61.12~1/release_arm64_t6041 machine : arm64 processor : arm byteorder : little lc_all : none lang : c.utf-8 locale : c.utf-8 pandas : 2.3.3 numpy : 2.3.4 pytz : 2025.2 dateutil : 2.9.0.post0 pip : 25.0.1 cython : none sphinx : 4.5.0 ipython : 9.6.0 adbc-driver-postgresql: none adbc-driver-sqlite : none bs4 : 4.14.2 blosc : none bottleneck : none dataframe-api-compat : none fastparquet : none fsspec : 2025.9.0 html5lib : none hypothesis : none gcsfs : none jinja2 : 3.1.6 lxml.etree : 6.0.2 matplotlib : 3.10.7 numba : none numexpr : none odfpy : none openpyxl : 3.1.5 pandas_gbq : none psycopg2 : none pymysql : none pyarrow : 21.0.0 pyreadstat : none pytest : 8.4.2 python-calamine : none pyxlsb : none s3fs : none scipy : 1.16.3 sqlalchemy : 2.0.44 tables : none tabulate : none xarray : none xlrd : none xlsxwriter : none zstandard : 0.25.0 tzdata : 2025.2 qtpy : none pyqt5 : none vs installed versions ------------------ commit : 366ccdfcd8ed1e5543bfb6d4ee0c9bc519898670 python : 3.12.12 python-bits : 64 os : darwin os-release : 25.2.0 version : darwin kernel version 25.2.0: tue nov 18 21:09:56 pst 2025; root:xnu-12377.61.12~1/release_arm64_t6041 machine : arm64 processor : arm byteorder : little lc_all : none lang : c.utf-8 locale : c.utf-8 pandas : 3.0.0 numpy : 2.3.4 dateutil : 2.9.0.post0 pip : 25.0.1 cython : none sphinx : 4.5.0 ipython : 9.6.0 adbc-driver-postgresql: none adbc-driver-sqlite : none bs4 : 4.14.2 bottleneck : none fastparquet : none fsspec : 2025.9.0 html5lib : none hypothesis : none gcsfs : none jinja2 : 3.1.6 lxml.etree : 6.0.2 matplotlib : 3.10.7 numba : none numexpr : none odfpy : none openpyxl : 3.1.5 psycopg2 : none pymysql : none pyarrow : 21.0.0 pyiceberg : none pyreadstat : none pytest : 8.4.2 python-calamine : none pytz : 2025.2 pyxlsb : none s3fs : none scipy : 1.16.3 sqlalchemy : 2.0.44 tables : none tabulate : none xarray : none xlrd : none xlsxwriter : none zstandard : 0.25.0 qtpy : none pyqt5 : none,[],['BUG'],"['Bug', 'Needs Triage']",github,2026-01-27T21:40:21Z,,bug: assert_frame_equal changes behavior in 3.0 for nested arrays without documentation ### pandas version checks - [x] i have checked that this issue has not already been reported. - [x] i have confirmed this bug exists on the [latest version]( of pandas. - [ ] i have confirmed this bug exists on the [main branch]( of pandas. ### reproducible example ### issue description this code above raises in 3.0.0 and passes in 2.3.x. i can understand the behavior but there's no docs in what's new of 3.0.0 about this. is this considered a bug fix? is the current behavior expected? ### expected behavior i don't know. i can live with either but it's nice if it's stable and documented. ### installed versions installed versions ------------------ commit : 9c8bc3e55188c8aff37207a74f1dd144980b8874 python : 3.12.12 python-bits : 64 os : darwin os-release : 25.2.0 version : darwin kernel version 25.2.0: tue nov 18 21:09:56 pst 2025; root:xnu-12377.61.12~1/release_arm64_t6041 machine : arm64 processor : arm byteorder : little lc_all : none lang : c.utf-8 locale : c.utf-8 pandas : 2.3.3 numpy : 2.3.4 pytz : 2025.2 dateutil : 2.9.0.post0 pip : 25.0.1 cython : none sphinx : 4.5.0 ipython : 9.6.0 adbc-driver-postgresql: none adbc-driver-sqlite : none bs4 : 4.14.2 blosc : none bottleneck : none dataframe-api-compat : none fastparquet : none fsspec : 2025.9.0 html5lib : none hypothesis : none gcsfs : none jinja2 : 3.1.6 lxml.etree : 6.0.2 matplotlib : 3.10.7 numba : none numexpr : none odfpy : none openpyxl : 3.1.5 pandas_gbq : none psycopg2 : none pymysql : none pyarrow : 21.0.0 pyreadstat : none pytest : 8.4.2 python-calamine : none pyxlsb : none s3fs : none scipy : 1.16.3 sqlalchemy : 2.0.44 tables : none tabulate : none xarray : none xlrd : none xlsxwriter : none zstandard : 0.25.0 tzdata : 2025.2 qtpy : none pyqt5 : none vs installed versions ------------------ commit : 366ccdfcd8ed1e5543bfb6d4ee0c9bc519898670 python : 3.12.12 python-bits : 64 os : darwin os-release : 25.2.0 version : darwin kernel version 25.2.0: tue nov 18 21:09:56 pst 2025; root:xnu-12377.61.12~1/release_arm64_t6041 machine : arm64 processor : arm byteorder : little lc_all : none lang : c.utf-8 locale : c.utf-8 pandas : 3.0.0 numpy : 2.3.4 dateutil : 2.9.0.post0 pip : 25.0.1 cython : none sphinx : 4.5.0 ipython : 9.6.0 adbc-driver-postgresql: none adbc-driver-sqlite : none bs4 : 4.14.2 bottleneck : none fastparquet : none fsspec : 2025.9.0 html5lib : none hypothesis : none gcsfs : none jinja2 : 3.1.6 lxml.etree : 6.0.2 matplotlib : 3.10.7 numba : none numexpr : none odfpy : none openpyxl : 3.1.5 psycopg2 : none pymysql : none pyarrow : 21.0.0 pyiceberg : none pyreadstat : none pytest : 8.4.2 python-calamine : none pytz : 2025.2 pyxlsb : none s3fs : none scipy : 1.16.3 sqlalchemy : 2.0.44 tables : none tabulate : none xarray : none xlrd : none xlsxwriter : none zstandard : 0.25.0 qtpy : none pyqt5 : none,4.6,Critical,1.0,crash-like behavior
microsoft/vscode#291038,nes rename fails for relatively simple rename,testing,[],['BUG'],"['bug', 'rename', 'NES']",github,2026-01-27T21:51:06Z,,nes rename fails for relatively simple rename testing,2.578,Medium,0.806,functional impact
microsoft/vscode#291043,applications folder icon missing,testing,[],['BUG'],"['bug', 'info-needed', 'macos']",github,2026-01-27T22:00:41Z,,applications folder icon missing testing,2.662,Medium,0.825,functional impact
cockroachdb/cockroach#161898,ui: fine-grained statement latency time chart,"we are looking for a way to troubleshoot sql latency spikes more effectively, specifically to support a new canary statistics rollout feature. currently, the ""statement times"" bar chart under the statement details page uses a default 1-hour aggregation interval, which is too coarse to detect immediate performance regressions caused by new table statistics (an a/b testing scenario where some statements use canary stats and others use stable stats). the goal is to provide a real-time, fine-grained latency line chart‚Äîideally with 1-5 minute intervals‚Äîto allow users to identify slowness and trigger rollbacks or refreshes before system-wide impact occurs. to avoid the storage overhead and reduced retention periods associated with a global reduction in aggregation intervals, we propose a tiered compaction strategy for sql statistics: - **high-fidelity buffering**: collect and persist fine-grained statistics (e.g., 10-minute buckets) only for the **most recent hour** or for specific opt-in statement fingerprints. - **automated roll-ups**: periodically ""compact"" these granular records into standard 1-hour buckets for long-term storage, and eventually into 1-day buckets for historical analysis. - **dual-line visualization**: enhance the statement details ui to support a line chart showing a side-by-side comparison of latency (e.g., canary vs. stable stats) powered by this short-term, high-resolution data. jira issue: crdb-59138",[],['FEATURE'],"['C-enhancement', 'T-observability']",github,2026-01-27T22:10:43Z,,"ui: fine-grained statement latency time chart we are looking for a way to troubleshoot sql latency spikes more effectively, specifically to support a new canary statistics rollout feature. currently, the ""statement times"" bar chart under the statement details page uses a default 1-hour aggregation interval, which is too coarse to detect immediate performance regressions caused by new table statistics (an a/b testing scenario where some statements use canary stats and others use stable stats). the goal is to provide a real-time, fine-grained latency line chart‚Äîideally with 1-5 minute intervals‚Äîto allow users to identify slowness and trigger rollbacks or refreshes before system-wide impact occurs. to avoid the storage overhead and reduced retention periods associated with a global reduction in aggregation intervals, we propose a tiered compaction strategy for sql statistics: - **high-fidelity buffering**: collect and persist fine-grained statistics (e.g., 10-minute buckets) only for the **most recent hour** or for specific opt-in statement fingerprints. - **automated roll-ups**: periodically ""compact"" these granular records into standard 1-hour buckets for long-term storage, and eventually into 1-day buckets for historical analysis. - **dual-line visualization**: enhance the statement details ui to support a line chart showing a side-by-side comparison of latency (e.g., canary vs. stable stats) powered by this short-term, high-resolution data. jira issue: crdb-59138",2.857,Medium,0.869,functional impact
microsoft/vscode#291053,"agent should know about its customizations (instructions, skills, custom agents, etc) to create, update and review them","right now it's a hit or miss when i ask the agent to update my customizations like skills, instructions, or custom agents. it's simply not aware of what the rules and best practices are. in the worst case, it will try to search on the internet, find some other information, or get confused in our own vs code documentation, like reading the coding agent docs when it needs to create a custom agent. as a user, i should just be able to ask the agent about any of these primitives. i expect that the agent is fully aware of how its primitives work, where they need to be created, and if they align with best practices.",[],['FEATURE'],"['feature-request', 'chat-prompts']",github,2026-01-27T22:20:33Z,2026-01-27T22:22:00Z,"agent should know about its customizations (instructions, skills, custom agents, etc) to create, update and review them right now it's a hit or miss when i ask the agent to update my customizations like skills, instructions, or custom agents. it's simply not aware of what the rules and best practices are. in the worst case, it will try to search on the internet, find some other information, or get confused in our own vs code documentation, like reading the coding agent docs when it needs to create a custom agent. as a user, i should just be able to ask the agent about any of these primitives. i expect that the agent is fully aware of how its primitives work, where they need to be created, and if they align with best practices.",1.4,Low,0.538,localized low-impact
microsoft/vscode#291056,"tip references ""add element to chat"" but i can't findit",testing i don't see that command accessible anywhere.,[],"['FEATURE', 'UI']","['feature-request', 'ux', 'browser-integration']",github,2026-01-27T22:26:00Z,,"tip references ""add element to chat"" but i can't findit testing i don't see that command accessible anywhere.",1.6,Low,0.584,user-visible issue
cockroachdb/cockroach#161901,sentry: memo.go:527: cannot overwrite √ó / ({0.02 {%!g(bool=false) %!g(bool=false) %!g(bool=false)} {%!g(uint8=000000000)}}) with √ó / (0.02) (1) assertion failure wraps: (2) attached stack trace ...,this issue was auto filed by sentry. it represents a crash or reported error on a live cluster with telemetry enabled. sentry link: [ panic message: stacktrace (expand for inline code snippets): [pkg/sql/pgwire/server.go#l1196-l1198](pkg/sql/pgwire/server.go#l1196-l1198) [pkg/sql/pgwire/conn.go#l251-l253](pkg/sql/pgwire/conn.go#l251-l253) [pkg/sql/conn_executor.go#l1031-l1033](pkg/sql/conn_executor.go#l1031-l1033) [pkg/sql/conn_executor.go#l2365-l2367](pkg/sql/conn_executor.go#l2365-l2367) [pkg/sql/conn_executor.go#l2557-l2559](pkg/sql/conn_executor.go#l2557-l2559) [pkg/sql/conn_executor.go#l2555-l2557](pkg/sql/conn_executor.go#l2555-l2557) [pkg/sql/conn_executor_exec.go#l320-l322](pkg/sql/conn_executor_exec.go#l320-l322) [pkg/sql/conn_executor_exec.go#l169-l171](pkg/sql/conn_executor_exec.go#l169-l171) [pkg/sql/conn_executor_exec.go#l4492-l4494](pkg/sql/conn_executor_exec.go#l4492-l4494) [pkg/sql/conn_executor_exec.go#l170-l172](pkg/sql/conn_executor_exec.go#l170-l172) [pkg/sql/conn_executor_exec.go#l1085-l1087](pkg/sql/conn_executor_exec.go#l1085-l1087) [pkg/sql/conn_executor_exec.go#l2867-l2869](pkg/sql/conn_executor_exec.go#l2867-l2869) [pkg/sql/conn_executor_exec.go#l3320-l3322](pkg/sql/conn_executor_exec.go#l3320-l3322) [pkg/sql/plan_opt.go#l259-l261](pkg/sql/plan_opt.go#l259-l261) [pkg/sql/plan_opt.go#l802-l804](pkg/sql/plan_opt.go#l802-l804) [pkg/sql/plan_opt.go#l743-l745](pkg/sql/plan_opt.go#l743-l745) [pkg/sql/plan_opt.go#l578-l580](pkg/sql/plan_opt.go#l578-l580) [pkg/sql/opt/xform/optimizer.go#l270-l272](pkg/sql/opt/xform/optimizer.go#l270-l272) [pkg/sql/opt/xform/optimizer.go#l832-l834](pkg/sql/opt/xform/optimizer.go#l832-l834) [pkg/sql/opt/xform/optimizer.go#l832-l834](pkg/sql/opt/xform/optimizer.go#l832-l834) [pkg/sql/opt/xform/optimizer.go#l832-l834](pkg/sql/opt/xform/optimizer.go#l832-l834) [pkg/sql/opt/xform/optimizer.go#l832-l834](pkg/sql/opt/xform/optimizer.go#l832-l834) [pkg/sql/opt/xform/optimizer.go#l832-l834](pkg/sql/opt/xform/optimizer.go#l832-l834) [pkg/sql/opt/xform/optimizer.go#l832-l834](pkg/sql/opt/xform/optimizer.go#l832-l834) [pkg/sql/opt/xform/optimizer.go#l832-l834](pkg/sql/opt/xform/optimizer.go#l832-l834) [pkg/sql/opt/xform/optimizer.go#l832-l834](pkg/sql/opt/xform/optimizer.go#l832-l834) [pkg/sql/opt/xform/optimizer.go#l832-l834](pkg/sql/opt/xform/optimizer.go#l832-l834) [pkg/sql/opt/xform/optimizer.go#l832-l834](pkg/sql/opt/xform/optimizer.go#l832-l834) [pkg/sql/opt/xform/optimizer.go#l832-l834](pkg/sql/opt/xform/optimizer.go#l832-l834) [pkg/sql/opt/xform/optimizer.go#l832-l834](pkg/sql/opt/xform/optimizer.go#l832-l834) [pkg/sql/opt/xform/optimizer.go#l849-l851](pkg/sql/opt/xform/optimizer.go#l849-l851) [pkg/sql/opt/memo/memo.go#l526-l528](pkg/sql/opt/memo/memo.go#l526-l528) ### tags | tag | value | | --- | --- | | command | server | | environment | v25.2.2 | | go version | go1.23.7 x:nocoverageredesign | | platform | linux amd64 | | distribution | ccl | | cockroach release | v25.2.2 | | cockroach sha | 0780d511bd527bfd9d02f4c7ca5ecc19115d9f5e | | # of cpus | 8 | | # of goroutines | 2272 | jira issue: crdb-59139,[],['BUG'],"['C-bug', 'O-sentry', 'X-blathers-triaged', 'T-sql-queries', 'branch-release-25.2']",github,2026-01-27T22:44:58Z,2026-01-27T22:50:35Z,sentry: memo.go:527: cannot overwrite √ó / ({0.02 {%!g(bool=false) %!g(bool=false) %!g(bool=false)} {%!g(uint8=000000000)}}) with √ó / (0.02) (1) assertion failure wraps: (2) attached stack trace ... this issue was auto filed by sentry. it represents a crash or reported error on a live cluster with telemetry enabled. sentry link: [ panic message: stacktrace (expand for inline code snippets): [pkg/sql/pgwire/server.go#l1196-l1198](pkg/sql/pgwire/server.go#l1196-l1198) [pkg/sql/pgwire/conn.go#l251-l253](pkg/sql/pgwire/conn.go#l251-l253) [pkg/sql/conn_executor.go#l1031-l1033](pkg/sql/conn_executor.go#l1031-l1033) [pkg/sql/conn_executor.go#l2365-l2367](pkg/sql/conn_executor.go#l2365-l2367) [pkg/sql/conn_executor.go#l2557-l2559](pkg/sql/conn_executor.go#l2557-l2559) [pkg/sql/conn_executor.go#l2555-l2557](pkg/sql/conn_executor.go#l2555-l2557) [pkg/sql/conn_executor_exec.go#l320-l322](pkg/sql/conn_executor_exec.go#l320-l322) [pkg/sql/conn_executor_exec.go#l169-l171](pkg/sql/conn_executor_exec.go#l169-l171) [pkg/sql/conn_executor_exec.go#l4492-l4494](pkg/sql/conn_executor_exec.go#l4492-l4494) [pkg/sql/conn_executor_exec.go#l170-l172](pkg/sql/conn_executor_exec.go#l170-l172) [pkg/sql/conn_executor_exec.go#l1085-l1087](pkg/sql/conn_executor_exec.go#l1085-l1087) [pkg/sql/conn_executor_exec.go#l2867-l2869](pkg/sql/conn_executor_exec.go#l2867-l2869) [pkg/sql/conn_executor_exec.go#l3320-l3322](pkg/sql/conn_executor_exec.go#l3320-l3322) [pkg/sql/plan_opt.go#l259-l261](pkg/sql/plan_opt.go#l259-l261) [pkg/sql/plan_opt.go#l802-l804](pkg/sql/plan_opt.go#l802-l804) [pkg/sql/plan_opt.go#l743-l745](pkg/sql/plan_opt.go#l743-l745) [pkg/sql/plan_opt.go#l578-l580](pkg/sql/plan_opt.go#l578-l580) [pkg/sql/opt/xform/optimizer.go#l270-l272](pkg/sql/opt/xform/optimizer.go#l270-l272) [pkg/sql/opt/xform/optimizer.go#l832-l834](pkg/sql/opt/xform/optimizer.go#l832-l834) [pkg/sql/opt/xform/optimizer.go#l832-l834](pkg/sql/opt/xform/optimizer.go#l832-l834) [pkg/sql/opt/xform/optimizer.go#l832-l834](pkg/sql/opt/xform/optimizer.go#l832-l834) [pkg/sql/opt/xform/optimizer.go#l832-l834](pkg/sql/opt/xform/optimizer.go#l832-l834) [pkg/sql/opt/xform/optimizer.go#l832-l834](pkg/sql/opt/xform/optimizer.go#l832-l834) [pkg/sql/opt/xform/optimizer.go#l832-l834](pkg/sql/opt/xform/optimizer.go#l832-l834) [pkg/sql/opt/xform/optimizer.go#l832-l834](pkg/sql/opt/xform/optimizer.go#l832-l834) [pkg/sql/opt/xform/optimizer.go#l832-l834](pkg/sql/opt/xform/optimizer.go#l832-l834) [pkg/sql/opt/xform/optimizer.go#l832-l834](pkg/sql/opt/xform/optimizer.go#l832-l834) [pkg/sql/opt/xform/optimizer.go#l832-l834](pkg/sql/opt/xform/optimizer.go#l832-l834) [pkg/sql/opt/xform/optimizer.go#l832-l834](pkg/sql/opt/xform/optimizer.go#l832-l834) [pkg/sql/opt/xform/optimizer.go#l832-l834](pkg/sql/opt/xform/optimizer.go#l832-l834) [pkg/sql/opt/xform/optimizer.go#l849-l851](pkg/sql/opt/xform/optimizer.go#l849-l851) [pkg/sql/opt/memo/memo.go#l526-l528](pkg/sql/opt/memo/memo.go#l526-l528) ### tags | tag | value | | --- | --- | | command | server | | environment | v25.2.2 | | go version | go1.23.7 x:nocoverageredesign | | platform | linux amd64 | | distribution | ccl | | cockroach release | v25.2.2 | | cockroach sha | 0780d511bd527bfd9d02f4c7ca5ecc19115d9f5e | | # of cpus | 8 | | # of goroutines | 2272 | jira issue: crdb-59139,6.0,Critical,1.0,crash-like behavior
microsoft/vscode#291067,auto-approval information not viewable with screenreader,"1. ask the agent to `#fetch 2. hover over the checkbox next to the word ""fetched"" in the output. 3. confirm that the auto-approval reason shows up. 4. try tabbing to the element with a screenreader. 5. üêõ the auto-approval reason isn't read.",[],['BUG'],['bug'],github,2026-01-27T22:45:27Z,,"auto-approval information not viewable with screenreader 1. ask the agent to `#fetch 2. hover over the checkbox next to the word ""fetched"" in the output. 3. confirm that the auto-approval reason shows up. 4. try tabbing to the element with a screenreader. 5. üêõ the auto-approval reason isn't read.",2.324,Medium,0.748,functional impact
microsoft/vscode#291077,claude agent is using the wrong icon in the session details,"when you create a session with the claude agent, the session details use a different icon then is used in the session list or the agent drop-down. for consistency, it'd be great if this uses the same ""code"" icon, as opposed to the sparkle icon.",[],['BUG'],"['bug', 'chat-external-agent']",github,2026-01-27T22:59:32Z,2026-01-28T03:44:49Z,"claude agent is using the wrong icon in the session details when you create a session with the claude agent, the session details use a different icon then is used in the session list or the agent drop-down. for consistency, it'd be great if this uses the same ""code"" icon, as opposed to the sparkle icon.",2.614,Medium,0.814,functional impact
microsoft/vscode#291080,justified panel opening breaks the maximized agent session (2nd side bar),testing 1. set panel alignment to justify 2. open agent session window 3. open the panel üêõ 2nd sidebar is not justified,[],['BUG'],"['bug', 'layout', 'confirmed']",github,2026-01-27T23:02:45Z,,justified panel opening breaks the maximized agent session (2nd side bar) testing 1. set panel alignment to justify 2. open agent session window 3. open the panel üêõ 2nd sidebar is not justified,2.576,Medium,0.805,functional impact
microsoft/vscode#291083,background agent session allows switching directory midway,"testing 1. new session 2. background, agent, opus, foldera 3. send a message 4. folder picker is present, so i switch to another folder üêõ send message still thinks its in the old worktree (which i expect, i don't expect this dropdown to work)",[],['BUG'],"['bug', 'chat-background-agent']",github,2026-01-27T23:06:54Z,,"background agent session allows switching directory midway testing 1. new session 2. background, agent, opus, foldera 3. send a message 4. folder picker is present, so i switch to another folder üêõ send message still thinks its in the old worktree (which i expect, i don't expect this dropdown to work)",2.445,Medium,0.776,functional impact
microsoft/vscode#291087,multiroot workspace selection for background agents in this view is very broken,"testing 1. create a new session 2. background, agent, opus, multiroot 3. ask it to make changes üêõ => it isn't allowed to read the files, it runs commands that fail etc. if i do this same flow after having run a successful background agent on a folder (not workspace), the session will be reverted to the folder, not the multiroot",[],['UI'],"['workbench-multiroot', 'chat-agents-view', 'chat-background-agent']",github,2026-01-27T23:13:05Z,,"multiroot workspace selection for background agents in this view is very broken testing 1. create a new session 2. background, agent, opus, multiroot 3. ask it to make changes üêõ => it isn't allowed to read the files, it runs commands that fail etc. if i do this same flow after having run a successful background agent on a folder (not workspace), the session will be reverted to the folder, not the multiroot",4.0,Critical,1.0,"user-visible issue, crash-like behavior"
microsoft/vscode#291091,"the expand button on the ""working"" group tab in the cloud agents chat panel is non-functional.",testing,[],['BUG'],['bug'],github,2026-01-27T23:21:14Z,,"the expand button on the ""working"" group tab in the cloud agents chat panel is non-functional. testing",2.429,Medium,0.772,functional impact
microsoft/vscode#291097,typing in the diff editor from an agent edited file does weird stuff,"steps to reproduce: 1. in agent mode, have the agent edit a file 2. click on the file in the changed file set to view the diff 3. in the added text, edit the content of what the agent has done üêõ => see below where i only edited the ""summary"" line and it started mucking with the source_batch line",[],['BUG'],['bug'],github,2026-01-27T23:27:38Z,,"typing in the diff editor from an agent edited file does weird stuff steps to reproduce: 1. in agent mode, have the agent edit a file 2. click on the file in the changed file set to view the diff 3. in the added text, edit the content of what the agent has done üêõ => see below where i only edited the ""summary"" line and it started mucking with the source_batch line",6.4,Critical,1.0,crash-like behavior
microsoft/vscode#291099,agent session mode breaks the tri-state toggle of the chat icon in the title bar,"testing 1. enable agent session mode 2. click the sparkle chat icon üêõ => nothing happens, even if the chat view is not maximized",['Agent session mode breaks the tri-state toggle of the chat icon in the title bar (fix #291099) (#291243)'],['BUG'],"['bug', 'unreleased', 'chat-agents-view']",github,2026-01-27T23:31:52Z,2026-01-28T10:56:51Z,"agent session mode breaks the tri-state toggle of the chat icon in the title bar testing 1. enable agent session mode 2. click the sparkle chat icon üêõ => nothing happens, even if the chat view is not maximized Agent session mode breaks the tri-state toggle of the chat icon in the title bar (fix #291099) (#291243)",2.277,Medium,0.737,functional impact
microsoft/vscode#291101,support local sessions in agent sessions window,"testing i don't select a folder in this case, so what does it do? also i get this dialog",[],['FEATURE'],"['feature-request', 'chat-agent', 'chat-agents-window']",github,2026-01-27T23:41:05Z,,"support local sessions in agent sessions window testing i don't select a folder in this case, so what does it do? also i get this dialog",1.4,Low,0.538,localized low-impact
cockroachdb/cockroach#161905,sentry: panic.go:791: runtime error: invalid memory address or nil pointer dereference (1) attached stack trace -- stack trace: | runtime.gopanic | goroot/src/runtime/panic.go:791 | runtime.p...,this issue was auto filed by sentry. it represents a crash or reported error on a live cluster with telemetry enabled. sentry link: [ panic message: stacktrace (expand for inline code snippets): [src/runtime/asm_amd64.s#l1699-l1701](src/runtime/asm_amd64.s#l1699-l1701) [pkg/util/stop/stopper.go#l499-l501](pkg/util/stop/stopper.go#l499-l501) [pkg/kv/kvserver/queue.go#l609-l611](pkg/kv/kvserver/queue.go#l609-l611) [pkg/kv/kvserver/queue.go#l622-l624](pkg/kv/kvserver/queue.go#l622-l624) [pkg/kv/kvserver/queue.go#l570-l572](pkg/kv/kvserver/queue.go#l570-l572) [pkg/kv/kvserver/queue.go#l676-l678](pkg/kv/kvserver/queue.go#l676-l678) [pkg/kv/kvserver/consistency_queue.go#l119-l121](pkg/kv/kvserver/consistency_queue.go#l119-l121) [pkg/kv/kvserver/consistency_queue.go#l148-l150](pkg/kv/kvserver/consistency_queue.go#l148-l150) [pkg/kv/kvserver/consistency_queue.go#l123-l125](pkg/kv/kvserver/consistency_queue.go#l123-l125) [pkg/kv/kvserver/replica.go#l1842-l1844](pkg/kv/kvserver/replica.go#l1842-l1844) [pkg/storage/mvcc.go#l1103-l1105](pkg/storage/mvcc.go#l1103-l1105) [pkg/storage/mvcc.go#l1407-l1409](pkg/storage/mvcc.go#l1407-l1409) [pkg/storage/mvcc.go#l1495-l1497](pkg/storage/mvcc.go#l1495-l1497) [pkg/storage/mvcc.go#l1351-l1353](pkg/storage/mvcc.go#l1351-l1353) [pkg/storage/pebble.go#l1297-l1299](pkg/storage/pebble.go#l1297-l1299) [pkg/storage/pebble_iterator.go#l98-l100](pkg/storage/pebble_iterator.go#l98-l100) [external/com_github_cockroachdb_pebble/db.go#l1597-l1599](external/com_github_cockroachdb_pebble/db.go#l1597-l1599) [external/com_github_cockroachdb_pebble/db.go#l1128-l1130](external/com_github_cockroachdb_pebble/db.go#l1128-l1130) [external/com_github_cockroachdb_pebble/db.go#l1160-l1162](external/com_github_cockroachdb_pebble/db.go#l1160-l1162) [external/com_github_cockroachdb_pebble/db.go#l1457-l1459](external/com_github_cockroachdb_pebble/db.go#l1457-l1459) [goroot/src/runtime/signal_unix.go#l916-l918](goroot/src/runtime/signal_unix.go#l916-l918) [goroot/src/runtime/panic.go#l261-l263](goroot/src/runtime/panic.go#l261-l263) [goroot/src/runtime/panic.go#l790-l792](goroot/src/runtime/panic.go#l790-l792) ### tags | tag | value | | --- | --- | | command | start-single-node | | environment | v25.2.4 | | go version | go1.23.7 x:nocoverageredesign | | platform | linux amd64 | | distribution | ccl | | cockroach release | v25.2.4 | | cockroach sha | db39678e142ec9f40d40b7139c3f821a94fcb80f | | # of cpus | 100 | | # of goroutines | 1080 | jira issue: crdb-59140,[],['BUG'],"['C-bug', 'A-storage', 'O-sentry', 'X-blathers-triaged', 'T-storage', 'branch-release-25.2']",github,2026-01-27T23:43:00Z,,sentry: panic.go:791: runtime error: invalid memory address or nil pointer dereference (1) attached stack trace -- stack trace: | runtime.gopanic | goroot/src/runtime/panic.go:791 | runtime.p... this issue was auto filed by sentry. it represents a crash or reported error on a live cluster with telemetry enabled. sentry link: [ panic message: stacktrace (expand for inline code snippets): [src/runtime/asm_amd64.s#l1699-l1701](src/runtime/asm_amd64.s#l1699-l1701) [pkg/util/stop/stopper.go#l499-l501](pkg/util/stop/stopper.go#l499-l501) [pkg/kv/kvserver/queue.go#l609-l611](pkg/kv/kvserver/queue.go#l609-l611) [pkg/kv/kvserver/queue.go#l622-l624](pkg/kv/kvserver/queue.go#l622-l624) [pkg/kv/kvserver/queue.go#l570-l572](pkg/kv/kvserver/queue.go#l570-l572) [pkg/kv/kvserver/queue.go#l676-l678](pkg/kv/kvserver/queue.go#l676-l678) [pkg/kv/kvserver/consistency_queue.go#l119-l121](pkg/kv/kvserver/consistency_queue.go#l119-l121) [pkg/kv/kvserver/consistency_queue.go#l148-l150](pkg/kv/kvserver/consistency_queue.go#l148-l150) [pkg/kv/kvserver/consistency_queue.go#l123-l125](pkg/kv/kvserver/consistency_queue.go#l123-l125) [pkg/kv/kvserver/replica.go#l1842-l1844](pkg/kv/kvserver/replica.go#l1842-l1844) [pkg/storage/mvcc.go#l1103-l1105](pkg/storage/mvcc.go#l1103-l1105) [pkg/storage/mvcc.go#l1407-l1409](pkg/storage/mvcc.go#l1407-l1409) [pkg/storage/mvcc.go#l1495-l1497](pkg/storage/mvcc.go#l1495-l1497) [pkg/storage/mvcc.go#l1351-l1353](pkg/storage/mvcc.go#l1351-l1353) [pkg/storage/pebble.go#l1297-l1299](pkg/storage/pebble.go#l1297-l1299) [pkg/storage/pebble_iterator.go#l98-l100](pkg/storage/pebble_iterator.go#l98-l100) [external/com_github_cockroachdb_pebble/db.go#l1597-l1599](external/com_github_cockroachdb_pebble/db.go#l1597-l1599) [external/com_github_cockroachdb_pebble/db.go#l1128-l1130](external/com_github_cockroachdb_pebble/db.go#l1128-l1130) [external/com_github_cockroachdb_pebble/db.go#l1160-l1162](external/com_github_cockroachdb_pebble/db.go#l1160-l1162) [external/com_github_cockroachdb_pebble/db.go#l1457-l1459](external/com_github_cockroachdb_pebble/db.go#l1457-l1459) [goroot/src/runtime/signal_unix.go#l916-l918](goroot/src/runtime/signal_unix.go#l916-l918) [goroot/src/runtime/panic.go#l261-l263](goroot/src/runtime/panic.go#l261-l263) [goroot/src/runtime/panic.go#l790-l792](goroot/src/runtime/panic.go#l790-l792) ### tags | tag | value | | --- | --- | | command | start-single-node | | environment | v25.2.4 | | go version | go1.23.7 x:nocoverageredesign | | platform | linux amd64 | | distribution | ccl | | cockroach release | v25.2.4 | | cockroach sha | db39678e142ec9f40d40b7139c3f821a94fcb80f | | # of cpus | 100 | | # of goroutines | 1080 | jira issue: crdb-59140,6.0,Critical,1.0,crash-like behavior
microsoft/vscode#291106,consider including a logo indicating the programming language run,testing it might be nice to include a little python/ruby/nodejs logo next to the executed code to indicate concisely what type of code it is,[],['FEATURE'],['feature-request'],github,2026-01-27T23:55:31Z,,consider including a logo indicating the programming language run testing it might be nice to include a little python/ruby/nodejs logo next to the executed code to indicate concisely what type of code it is,1.4,Low,0.538,localized low-impact
microsoft/vscode#291108,2026 themes feedback & bugs,"capturing self hosting feedback in issues labeled with 2026-themes. to track progress: check when pr submitted, close issue when in insiders. - [ ] - [ ] - [x] - [x] - [x] - [ ] - [ ] - [x] - [x] - [ ] - [ ] - [ ] - [ ] - [ ] - [ ]",[],['UI'],"['ux', 'polish', '2026-themes']",github,2026-01-27T23:59:34Z,,"2026 themes feedback & bugs capturing self hosting feedback in issues labeled with 2026-themes. to track progress: check when pr submitted, close issue when in insiders. - [ ] - [ ] - [x] - [x] - [x] - [ ] - [ ] - [x] - [x] - [ ] - [ ] - [ ] - [ ] - [ ] - [ ]",1.8,Low,0.629,user-visible issue
microsoft/vscode#291109,clicking links in vs code should show integrated browser option,"testing i had the browser open right there, it would have been nice to open the link i clicked directly in it.",[],['FEATURE'],"['feature-request', 'opener', 'browser-integration']",github,2026-01-28T00:01:54Z,,"clicking links in vs code should show integrated browser option testing i had the browser open right there, it would have been nice to open the link i clicked directly in it.",1.4,Low,0.538,localized low-impact
microsoft/vscode#291113,integrated browser: can't tell which browser tabs share sessions,"testing after i changed to i went back to my original tab (opened with global datastorage) and saw that it still had the 'clear cache (global)'. for some reason i thought that changed existing browsers to no longer cache info. having a global and ephemeral tab open next to each other, you can't easily tell which is which. perhaps we need a copilot incognito style icon for ephemeral browser windows? üòé | global | ephemeral | |--------|-----------| | | |",[],"['FEATURE', 'UI']","['feature-request', 'ux', 'browser-integration']",github,2026-01-28T00:13:58Z,,"integrated browser: can't tell which browser tabs share sessions testing after i changed to i went back to my original tab (opened with global datastorage) and saw that it still had the 'clear cache (global)'. for some reason i thought that changed existing browsers to no longer cache info. having a global and ephemeral tab open next to each other, you can't easily tell which is which. perhaps we need a copilot incognito style icon for ephemeral browser windows? üòé | global | ephemeral | |--------|-----------| | | |",2.586,Medium,0.808,"user-visible issue, crash-like behavior"
microsoft/vscode#291114,2026 themes: chat input buttons are not foreground color,both themes show dim buttons when compared to other themes: default theme:,[],['UI'],"['ux', 'polish', '2026-themes']",github,2026-01-28T00:15:16Z,,2026 themes: chat input buttons are not foreground color both themes show dim buttons when compared to other themes: default theme:,1.8,Low,0.629,user-visible issue
microsoft/vscode#291115,2026 themes: selected and highlighted files are the same color,from 1. open a file in the explorer 2. right click another file in the explorer both files appear selected. this is problematic since the context actions will only affect the file the user has right clicked on,[],['UI'],"['ux', 'polish', '2026-themes']",github,2026-01-28T00:20:32Z,,2026 themes: selected and highlighted files are the same color from 1. open a file in the explorer 2. right click another file in the explorer both files appear selected. this is problematic since the context actions will only affect the file the user has right clicked on,1.8,Low,0.629,user-visible issue
openssl/openssl#29811,x509_attribute accessors aren't const-correct,"found an old todo in our code and thought i'd pass it along in case you all are interested in doing anything with it for 4.0. the accessors aren't -correct and don't let you usefully interact with a . these functions: should instead be: (maybe should also be const. i dunno, that parameter is unused. no idea why it's there.)",[],['BUG'],['triaged: bug'],github,2026-01-28T00:22:59Z,,"x509_attribute accessors aren't const-correct found an old todo in our code and thought i'd pass it along in case you all are interested in doing anything with it for 4.0. the accessors aren't -correct and don't let you usefully interact with a . these functions: should instead be: (maybe should also be const. i dunno, that parameter is unused. no idea why it's there.)",2.274,Medium,0.737,functional impact
microsoft/vscode#291116,2026 themes: split button hover background color,"from there's hardly any contrast between the two click areas of the button+dropdown combo, if you hover either side",[],['UI'],"['ux', 'polish', '2026-themes']",github,2026-01-28T00:23:16Z,,"2026 themes: split button hover background color from there's hardly any contrast between the two click areas of the button+dropdown combo, if you hover either side",1.8,Low,0.629,user-visible issue
microsoft/vscode#291118,2026 themes: boolean values have no syntax coloring,2026 light: default: cc:,[],['UI'],"['ux', 'polish', '2026-themes']",github,2026-01-28T00:25:15Z,,2026 themes: boolean values have no syntax coloring 2026 light: default: cc:,1.8,Low,0.629,user-visible issue
microsoft/vscode#291123,2026 themes: terminal border overlaps completions,border overlaps terminal completions on hover. cc:,[],['UI'],"['ux', 'polish', '2026-themes']",github,2026-01-28T00:30:53Z,,2026 themes: terminal border overlaps completions border overlaps terminal completions on hover. cc:,1.8,Low,0.629,user-visible issue
electron/electron#49550,usage of in default app triggers deprecation warning,"### preflight checklist - [x] i have read the [contributing guidelines]( for this project. - [x] i agree to follow the [code of conduct]( that this project adheres to. - [x] i have searched the [issue tracker]( for a bug report that matches the one i want to file, without success. ### electron version 40.0.0 ### what operating system(s) are you using? macos ### operating system version latest ### what arch are you using? arm64 (including apple silicon) ### last known working electron version _no response_ ### does the issue also appear in chromium / google chrome? no ### expected behavior the default app should not trigger deprecation warnings ### actual behavior which is triggered by this: ### testcase gist url _no response_ ### additional information _no response_",[],['BUG'],"['platform/macOS', 'bug :beetle:', '40-x-y']",github,2026-01-28T00:39:47Z,,"usage of in default app triggers deprecation warning ### preflight checklist - [x] i have read the [contributing guidelines]( for this project. - [x] i agree to follow the [code of conduct]( that this project adheres to. - [x] i have searched the [issue tracker]( for a bug report that matches the one i want to file, without success. ### electron version 40.0.0 ### what operating system(s) are you using? macos ### operating system version latest ### what arch are you using? arm64 (including apple silicon) ### last known working electron version _no response_ ### does the issue also appear in chromium / google chrome? no ### expected behavior the default app should not trigger deprecation warnings ### actual behavior which is triggered by this: ### testcase gist url _no response_ ### additional information _no response_",2.431,Medium,0.772,functional impact
microsoft/vscode#291134,browser content moves when being grayed out,testing 1) open integrated browser 2) open panel 3) hover over the panel tabs -> when the hover open the content of the browser moves up 1-2 pixels see this recording:,[],['BUG'],"['bug', 'browser-integration']",github,2026-01-28T01:36:07Z,,browser content moves when being grayed out testing 1) open integrated browser 2) open panel 3) hover over the panel tabs -> when the hover open the content of the browser moves up 1-2 pixels see this recording:,2.566,Medium,0.803,functional impact
microsoft/vscode#291138,meta: gaps between claude code extension,a collection of issues that i think we could do to bridge gaps between the claude agent experience & the claude code extension,[],['FEATURE'],"['feature-request', 'chat-external-agent']",github,2026-01-28T01:42:49Z,,meta: gaps between claude code extension a collection of issues that i think we could do to bridge gaps between the claude agent experience & the claude code extension,1.4,Low,0.538,localized low-impact
rust-lang/rust#151774,build enzyme on mingw in ci,"status quo: we are able to build enzyme on macos and linux in ci, and libload (dlopen) it into rustc. tried to help me with getting windows (mingw) builds up. msvc ins't supported due to some black magic in enzyme. on linux, we have rustc linking dynamically against llvm, so libenzyme also dynamically links against it. on apple, we use static linking, so at first libenzyme also tried to statically link llvm, and we ended up with two llvm copies, which causes bugs. we solved this via [cmake flags]( telling libenzyme to look for llvm symbols only at dlopen time. this way libenzyme reuses the llvm symbols from rustc. on mingw, we also use static linking, and due to the limited number of public symbols per library, we can't really change that from my understanding. unlike macos, we can not tell the linker to resolve llvm symbols at dlopen time (the flags used for macos aren't recognized), but we also can't ship a second llvm copy. so, from our limited understanding of linkers, there isn't really a way around statically linking enzyme into rustc and just always distributing it? i got permission from the infra team to do that a while ago, even though it is not the preferred solution. we also had static linking of rustc and enzyme work in the past, so it should be doable to get it to work again. i just hate that we have 3 different approaches on 3 systems, but i guess that's just the way it is. does that sound sensible to you, or do you see a better solution for mingw? fyi i don't necessarily intend to block nightly release on mingw, but since we're waiting on anyway, i thought it would be nice to try to get windows in as well. ### summary <!-- provide a brief description of the problem you are experiencing. --> ### command used ### expected behaviour <!-- describe what you expected to happen. --> ### actual behaviour a variety of errors based on the concrete setup. when trying to copy the macos solution, kiran ended up with: ### bootstrap configuration (bootstrap.toml) ### operating system <!-- windows, mingw --> ### head <!-- output of command, or content of the file if using a tarball source. --> ### additional context <!-- include any other relevant information (e.g., if you have custom patches or modifications on the project). --> <!-- include the complete build log in the section below. enable backtrace and verbose mode if possible for more detailed information e.g., with . --> build log",[],['BUG'],"['T-bootstrap', 'C-bug', 'A-CI', 'F-autodiff']",github,2026-01-28T01:43:23Z,,"build enzyme on mingw in ci status quo: we are able to build enzyme on macos and linux in ci, and libload (dlopen) it into rustc. tried to help me with getting windows (mingw) builds up. msvc ins't supported due to some black magic in enzyme. on linux, we have rustc linking dynamically against llvm, so libenzyme also dynamically links against it. on apple, we use static linking, so at first libenzyme also tried to statically link llvm, and we ended up with two llvm copies, which causes bugs. we solved this via [cmake flags]( telling libenzyme to look for llvm symbols only at dlopen time. this way libenzyme reuses the llvm symbols from rustc. on mingw, we also use static linking, and due to the limited number of public symbols per library, we can't really change that from my understanding. unlike macos, we can not tell the linker to resolve llvm symbols at dlopen time (the flags used for macos aren't recognized), but we also can't ship a second llvm copy. so, from our limited understanding of linkers, there isn't really a way around statically linking enzyme into rustc and just always distributing it? i got permission from the infra team to do that a while ago, even though it is not the preferred solution. we also had static linking of rustc and enzyme work in the past, so it should be doable to get it to work again. i just hate that we have 3 different approaches on 3 systems, but i guess that's just the way it is. does that sound sensible to you, or do you see a better solution for mingw? fyi i don't necessarily intend to block nightly release on mingw, but since we're waiting on anyway, i thought it would be nice to try to get windows in as well. ### summary <!-- provide a brief description of the problem you are experiencing. --> ### command used ### expected behaviour <!-- describe what you expected to happen. --> ### actual behaviour a variety of errors based on the concrete setup. when trying to copy the macos solution, kiran ended up with: ### bootstrap configuration (bootstrap.toml) ### operating system <!-- windows, mingw --> ### head <!-- output of command, or content of the file if using a tarball source. --> ### additional context <!-- include any other relevant information (e.g., if you have custom patches or modifications on the project). --> <!-- include the complete build log in the section below. enable backtrace and verbose mode if possible for more detailed information e.g., with . --> build log",6.4,Critical,1.0,crash-like behavior
microsoft/vscode#291139,meta: gaps between claude agent & agent mode,a collection of issues that i think we could do to bridge gaps between the claude agent experience & the claude code extension,[],['FEATURE'],"['feature-request', 'chat-external-agent']",github,2026-01-28T01:45:37Z,,meta: gaps between claude agent & agent mode a collection of issues that i think we could do to bridge gaps between the claude agent experience & the claude code extension,1.4,Low,0.538,localized low-impact
microsoft/vscode#291141,agent sessions need to indicate which folder or repo they target unless single-folder workspace,"type: bug 1) open vs code 2) open a new emprt window (cmd+shift+n on mac) 3) open the agent sessions view 4) scan the session list -> no indication of folder or repo for any agent type 5) open a session -> no indication of folder or repo vs code version: code - insiders 1.109.0-insider (07483cec7c6bc583a449c0d444d3187f6261b911, 2026-01-27t17:05:27.861z) os version: darwin arm64 25.2.0 modes: system info |item|value| |---|---| |cpus|apple m4 (10 x 2400)| |gpu status|2d_canvas: enabled gpu0: vendor= 0x106b [google inc. (apple)], device=0x0000 [angle (apple, angle metal renderer: apple m4, version 26.2 (build 25c56))], driver_vendor=apple, driver_version=26.2 *active* machine model name: mac machine model version: 16.12 direct_rendering_display_compositor: disabled_off_ok gpu_compositing: enabled multiple_raster_threads: enabled_on opengl: enabled_on rasterization: enabled raw_draw: disabled_off_ok skia_graphite: disabled_off trees_in_viz: disabled_off video_decode: enabled video_encode: enabled webgl: enabled webgl2: enabled webgpu: enabled webnn: disabled_off| |load (avg)|5, 6, 7| |memory (system)|32.00gb (0.35gb free)| |process argv|--enable-proposed-api lszomoru.lszomoru-proposed-api-sample --enable-proposed-api donjayamanne.kusto --crash-reporter-id ef10dfea-3602-4c74-ba1a-109ce7da7825| |screen reader|no| |vm|0%| extensions (78) extension|author (truncated)|version ---|---|--- vscode-track-build-errors|aes|0.0.3 git-branch|ale|1.7.0 tsl-problem-matcher|amo|0.6.2 claude-code|ant|2.1.5 github-markdown-preview|bie|0.3.0 markdown-checkbox|bie|0.4.0 markdown-emoji|bie|0.3.1 markdown-footnotes|bie|0.1.1 markdown-mermaid|bie|1.29.0 markdown-preview-github-styles|bie|2.2.0 markdown-yaml-preamble|bie|0.1.0 mermaid-markdown-syntax-highlighting|bpr|1.7.6 cerebras-chat|cer|0.1.19 vscode-opennewinstance|chr|0.0.15 network-proxy-test|chr|0.0.22 esbuild-problem-matchers|con|0.0.3 vscode-svgviewer|css|2.0.0 vscode-eslint|dba|3.0.20 docker|doc|0.18.0 typescript-notebook|don|2.0.6 prettier-vscode|esb|12.3.0 graphviz-markdown-preview|gee|0.0.8 codespaces|git|1.18.5 copilot-chat|git|0.37.2026012702 remotehub|git|0.65.2026012601 vscode-pull-request-github|git|0.127.2026012717 mdmath|goe|2.7.4 vscode-mocha-test-adapter|hbe|2.14.4 vscode-test-explorer|hbe|2.22.1 kotlin|mat|1.7.1 rainbow-csv|mec|3.24.1 pyrefly|met|0.50.0 theme-monokai-pro-vscode|mon|2.0.12 compare-folders|mos|0.25.3 vscode-containers|ms-|2.4.0 vscode-docker|ms-|2.0.0 vscode-language-pack-es|ms-|1.108.2026012109 black-formatter|ms-|2025.3.11831009 debugpy|ms-|2025.19.2026012701 isort|ms-|2025.1.13251007 python|ms-|2026.1.2026012301 vscode-pylance|ms-|2025.12.101 vscode-python-envs|ms-|1.17.10261013 jupyter|ms-|2025.10.2026010601 jupyter-keymap|ms-|1.1.2 jupyter-renderers|ms-|1.3.2025062701 vscode-jupyter-cell-tags|ms-|0.1.9 vscode-jupyter-slideshow|ms-|0.1.6 remote-containers|ms-|0.440.0 remote-ssh|ms-|0.123.2026012215 remote-ssh-edit|ms-|0.87.0 azure-repos|ms-|0.41.2026012601 copilot-mermaid-diagram|ms-|0.0.2026012701 debug-value-editor|ms-|0.2.2 extension-test-runner|ms-|0.0.14 hexeditor|ms-|1.11.1 js-debug-nightly|ms-|2026.1.2217 remote-explorer|ms-|0.6.2025081809 remote-repositories|ms-|0.43.2026012601 remote-server|ms-|1.6.2026011209 test-adapter-converter|ms-|0.2.1 vscode-github-issue-notebooks|ms-|0.0.134 vscode-js-profile-flame|ms-|1.0.9 vscode-markdown-notebook|ms-|0.0.26 vscode-selfhost-test-provider|ms-|0.3.25 vscode-speech|ms-|0.16.0 vsliveshare|ms-|1.0.5959 chatgpt|ope|0.5.67 material-icon-theme|pki|5.31.0 excalidraw-editor|pom|3.9.0 vscode-commons|red|0.0.6 vscode-xml|red|0.29.0 ruby-lsp|sho|0.10.0 code-spell-checker|str|4.4.0 plan-tooling|und|0.1.0 vscode-lldb|vad|1.11.6 explorer|vit|1.38.1 codetour|vsl|0.0.61 (5 theme extensions excluded) a/b experiments",[],"['FEATURE', 'UI']","['feature-request', 'ux', 'chat-agents-view']",github,2026-01-28T01:55:56Z,,"agent sessions need to indicate which folder or repo they target unless single-folder workspace type: bug 1) open vs code 2) open a new emprt window (cmd+shift+n on mac) 3) open the agent sessions view 4) scan the session list -> no indication of folder or repo for any agent type 5) open a session -> no indication of folder or repo vs code version: code - insiders 1.109.0-insider (07483cec7c6bc583a449c0d444d3187f6261b911, 2026-01-27t17:05:27.861z) os version: darwin arm64 25.2.0 modes: system info |item|value| |---|---| |cpus|apple m4 (10 x 2400)| |gpu status|2d_canvas: enabled gpu0: vendor= 0x106b [google inc. (apple)], device=0x0000 [angle (apple, angle metal renderer: apple m4, version 26.2 (build 25c56))], driver_vendor=apple, driver_version=26.2 *active* machine model name: mac machine model version: 16.12 direct_rendering_display_compositor: disabled_off_ok gpu_compositing: enabled multiple_raster_threads: enabled_on opengl: enabled_on rasterization: enabled raw_draw: disabled_off_ok skia_graphite: disabled_off trees_in_viz: disabled_off video_decode: enabled video_encode: enabled webgl: enabled webgl2: enabled webgpu: enabled webnn: disabled_off| |load (avg)|5, 6, 7| |memory (system)|32.00gb (0.35gb free)| |process argv|--enable-proposed-api lszomoru.lszomoru-proposed-api-sample --enable-proposed-api donjayamanne.kusto --crash-reporter-id ef10dfea-3602-4c74-ba1a-109ce7da7825| |screen reader|no| |vm|0%| extensions (78) extension|author (truncated)|version ---|---|--- vscode-track-build-errors|aes|0.0.3 git-branch|ale|1.7.0 tsl-problem-matcher|amo|0.6.2 claude-code|ant|2.1.5 github-markdown-preview|bie|0.3.0 markdown-checkbox|bie|0.4.0 markdown-emoji|bie|0.3.1 markdown-footnotes|bie|0.1.1 markdown-mermaid|bie|1.29.0 markdown-preview-github-styles|bie|2.2.0 markdown-yaml-preamble|bie|0.1.0 mermaid-markdown-syntax-highlighting|bpr|1.7.6 cerebras-chat|cer|0.1.19 vscode-opennewinstance|chr|0.0.15 network-proxy-test|chr|0.0.22 esbuild-problem-matchers|con|0.0.3 vscode-svgviewer|css|2.0.0 vscode-eslint|dba|3.0.20 docker|doc|0.18.0 typescript-notebook|don|2.0.6 prettier-vscode|esb|12.3.0 graphviz-markdown-preview|gee|0.0.8 codespaces|git|1.18.5 copilot-chat|git|0.37.2026012702 remotehub|git|0.65.2026012601 vscode-pull-request-github|git|0.127.2026012717 mdmath|goe|2.7.4 vscode-mocha-test-adapter|hbe|2.14.4 vscode-test-explorer|hbe|2.22.1 kotlin|mat|1.7.1 rainbow-csv|mec|3.24.1 pyrefly|met|0.50.0 theme-monokai-pro-vscode|mon|2.0.12 compare-folders|mos|0.25.3 vscode-containers|ms-|2.4.0 vscode-docker|ms-|2.0.0 vscode-language-pack-es|ms-|1.108.2026012109 black-formatter|ms-|2025.3.11831009 debugpy|ms-|2025.19.2026012701 isort|ms-|2025.1.13251007 python|ms-|2026.1.2026012301 vscode-pylance|ms-|2025.12.101 vscode-python-envs|ms-|1.17.10261013 jupyter|ms-|2025.10.2026010601 jupyter-keymap|ms-|1.1.2 jupyter-renderers|ms-|1.3.2025062701 vscode-jupyter-cell-tags|ms-|0.1.9 vscode-jupyter-slideshow|ms-|0.1.6 remote-containers|ms-|0.440.0 remote-ssh|ms-|0.123.2026012215 remote-ssh-edit|ms-|0.87.0 azure-repos|ms-|0.41.2026012601 copilot-mermaid-diagram|ms-|0.0.2026012701 debug-value-editor|ms-|0.2.2 extension-test-runner|ms-|0.0.14 hexeditor|ms-|1.11.1 js-debug-nightly|ms-|2026.1.2217 remote-explorer|ms-|0.6.2025081809 remote-repositories|ms-|0.43.2026012601 remote-server|ms-|1.6.2026011209 test-adapter-converter|ms-|0.2.1 vscode-github-issue-notebooks|ms-|0.0.134 vscode-js-profile-flame|ms-|1.0.9 vscode-markdown-notebook|ms-|0.0.26 vscode-selfhost-test-provider|ms-|0.3.25 vscode-speech|ms-|0.16.0 vsliveshare|ms-|1.0.5959 chatgpt|ope|0.5.67 material-icon-theme|pki|5.31.0 excalidraw-editor|pom|3.9.0 vscode-commons|red|0.0.6 vscode-xml|red|0.29.0 ruby-lsp|sho|0.10.0 code-spell-checker|str|4.4.0 plan-tooling|und|0.1.0 vscode-lldb|vad|1.11.6 explorer|vit|1.38.1 codetour|vsl|0.0.61 (5 theme extensions excluded) a/b experiments",3.4,High,0.993,"user-visible issue, crash-like behavior"
microsoft/vscode#291149,agent sessions welcome view stealing focus,- open the agent sessions welcome view - open the agent sessions - now hit the new session button - the focus goes into the input box on the welcome view,[],['BUG'],['bug'],github,2026-01-28T02:24:56Z,,agent sessions welcome view stealing focus - open the agent sessions welcome view - open the agent sessions - now hit the new session button - the focus goes into the input box on the welcome view,2.627,Medium,0.817,functional impact
microsoft/vscode#291150,background folder/repo option not disabled when opening existing sessions,"* open vs code folder * start a background session with the prompt * wait for completion * open an untitled workspace and go into background sessions view * open the above session * verify you can see the details problem: the dropdown that display as the folder (on the bottom right of chat input) is now editable, it should be readonly not allowing user to change this.",[],['BUG'],"['bug', 'chat-background-agent']",github,2026-01-28T02:25:00Z,2026-01-28T04:00:03Z,"background folder/repo option not disabled when opening existing sessions * open vs code folder * start a background session with the prompt * wait for completion * open an untitled workspace and go into background sessions view * open the above session * verify you can see the details problem: the dropdown that display as the folder (on the bottom right of chat input) is now editable, it should be readonly not allowing user to change this.",4.6,Critical,1.0,crash-like behavior
cockroachdb/cockroach#161909,sentry: restore_planning.go:623: function descriptor seen when restoring tables (1) wraps: (2) assertion failure wraps: (3) attached stack trace -- stack trace: | github.com/cockroachdb/cockroach/...,this issue was auto filed by sentry. it represents a crash or reported error on a live cluster with telemetry enabled. sentry link: [ panic message: stacktrace (expand for inline code snippets): [src/runtime/asm_arm64.s#l1221-l1223](src/runtime/asm_arm64.s#l1221-l1223) [pkg/util/stop/stopper.go#l497-l499](pkg/util/stop/stopper.go#l497-l499) [pkg/sql/planhook.go#l193-l195](pkg/sql/planhook.go#l193-l195) [pkg/ccl/backupccl/restore_planning.go#l1429-l1431](pkg/ccl/backupccl/restore_planning.go#l1429-l1431) [pkg/ccl/backupccl/restore_planning.go#l2023-l2025](pkg/ccl/backupccl/restore_planning.go#l2023-l2025) [pkg/ccl/backupccl/restore_planning.go#l857-l859](pkg/ccl/backupccl/restore_planning.go#l857-l859) [pkg/ccl/backupccl/restore_planning.go#l622-l624](pkg/ccl/backupccl/restore_planning.go#l622-l624) ### tags | tag | value | | --- | --- | | command | server | | environment | v24.3.25 | | go version | go1.22.12 x:nocoverageredesign | | platform | linux arm64 | | distribution | ccl | | cockroach release | v24.3.25 | | cockroach sha | eecd7c2c316ed213daab1c664e0d854fc937a746 | | # of cpus | 16 | | # of goroutines | 659 | jira issue: crdb-59152,[],['BUG'],"['C-bug', 'A-disaster-recovery', 'O-sentry', 'X-blathers-triaged', 'T-disaster-recovery', 'branch-release-24.3']",github,2026-01-28T02:44:36Z,,sentry: restore_planning.go:623: function descriptor seen when restoring tables (1) wraps: (2) assertion failure wraps: (3) attached stack trace -- stack trace: | github.com/cockroachdb/cockroach/... this issue was auto filed by sentry. it represents a crash or reported error on a live cluster with telemetry enabled. sentry link: [ panic message: stacktrace (expand for inline code snippets): [src/runtime/asm_arm64.s#l1221-l1223](src/runtime/asm_arm64.s#l1221-l1223) [pkg/util/stop/stopper.go#l497-l499](pkg/util/stop/stopper.go#l497-l499) [pkg/sql/planhook.go#l193-l195](pkg/sql/planhook.go#l193-l195) [pkg/ccl/backupccl/restore_planning.go#l1429-l1431](pkg/ccl/backupccl/restore_planning.go#l1429-l1431) [pkg/ccl/backupccl/restore_planning.go#l2023-l2025](pkg/ccl/backupccl/restore_planning.go#l2023-l2025) [pkg/ccl/backupccl/restore_planning.go#l857-l859](pkg/ccl/backupccl/restore_planning.go#l857-l859) [pkg/ccl/backupccl/restore_planning.go#l622-l624](pkg/ccl/backupccl/restore_planning.go#l622-l624) ### tags | tag | value | | --- | --- | | command | server | | environment | v24.3.25 | | go version | go1.22.12 x:nocoverageredesign | | platform | linux arm64 | | distribution | ccl | | cockroach release | v24.3.25 | | cockroach sha | eecd7c2c316ed213daab1c664e0d854fc937a746 | | # of cpus | 16 | | # of goroutines | 659 | jira issue: crdb-59152,6.0,Critical,1.0,crash-like behavior
microsoft/vscode#291154,search subagent shows up in tool picker when it's disabled by setting,"the search subagent should be hidden in vs code when its setting is disabled, not just disabled in the agent's list at runtime. do this by using a clause on the tool contribution in package.json cc can you take care of this?",[],['BUG'],"['bug', 'chat-subagents']",github,2026-01-28T02:45:54Z,,"search subagent shows up in tool picker when it's disabled by setting the search subagent should be hidden in vs code when its setting is disabled, not just disabled in the agent's list at runtime. do this by using a clause on the tool contribution in package.json cc can you take care of this?",2.298,Medium,0.742,functional impact
microsoft/vscode#291159,subagent response includes more text than it should,"- use a subagent with a claude model and multiple tool calls - see that all the plain text responses are included in the subagent result, not just the final response from after the last tool call",[],['BUG'],"['bug', 'important', 'chat-subagents']",github,2026-01-28T03:17:33Z,,"subagent response includes more text than it should - use a subagent with a claude model and multiple tool calls - see that all the plain text responses are included in the subagent result, not just the final response from after the last tool call",2.364,Medium,0.757,functional impact
envoyproxy/envoy#43193,support separate timeout configurations for http response headers (ttft) and full response body in llm api scenarios,"*title*: *support separate timeout configurations for http response headers (ttft) and full response body in llm api scenarios* *description*: in large language model (llm) api applications, two key performance metrics are: 1. time to first token (ttft): the latency between sending a request and receiving the first byte of the response (typically the response headers or the first chunk in streaming responses). 2. total response time: the time to receive the complete response stream, which can be significantly longer for long-generated content. currently, http timeout configurations lack a clear separation between these phases. the route-level timeout applies to the entire response (from request completion to response completion), while per_try_timeout is designed for retries and not suitable for ttft monitoring. this forces developers to choose a single timeout that either risks premature timeout for long streams or tolerates slow ttft unnecessarily. this feature enables: use cases: 1. ttft-sensitive applications: llm chatbots and real-time assistants require fast first token delivery (e.g., <2 seconds) for user perception, but may tolerate longer total response times (e.g., 30 seconds) for full responses. monitoring and alerting on slow ttft without penalizing long but valid responses. 2. streaming llm responses: distinguish between timeout for establishing the stream (headers/first token) and timeout for the ongoing stream completion. example: enforce a 5-second timeout for receiving the first token (response_headers_timeout), but allow up to 60 seconds for the full server-sent events (sse) stream (timeout). 3. fallback and retry logic: quickly detect upstream llm provider delays (via ttft timeout) and trigger fallback to a backup provider or model. avoid unnecessary retries for long-running but healthy streams. proposed implementation: 1. add a response_headers_timeout field to route configurations, defining the maximum wait time for response headers (or first token in streaming). 2. retain the existing timeout field for the full response (headers + body). 3. router logic updates: - start response_headers_timeout timer when the request is forwarded upstream. - on header receipt, cancel the ttft timer and start the timeout timer for the full response. - if headers are not received within response_headers_timeout, abort the request and trigger retry/fallback if configured. 4. backward compatibility: if response_headers_timeout is unset, use timeout for both phases (current behavior).",[],['FEATURE'],['enhancement'],github,2026-01-28T03:27:51Z,,"support separate timeout configurations for http response headers (ttft) and full response body in llm api scenarios *title*: *support separate timeout configurations for http response headers (ttft) and full response body in llm api scenarios* *description*: in large language model (llm) api applications, two key performance metrics are: 1. time to first token (ttft): the latency between sending a request and receiving the first byte of the response (typically the response headers or the first chunk in streaming responses). 2. total response time: the time to receive the complete response stream, which can be significantly longer for long-generated content. currently, http timeout configurations lack a clear separation between these phases. the route-level timeout applies to the entire response (from request completion to response completion), while per_try_timeout is designed for retries and not suitable for ttft monitoring. this forces developers to choose a single timeout that either risks premature timeout for long streams or tolerates slow ttft unnecessarily. this feature enables: use cases: 1. ttft-sensitive applications: llm chatbots and real-time assistants require fast first token delivery (e.g., <2 seconds) for user perception, but may tolerate longer total response times (e.g., 30 seconds) for full responses. monitoring and alerting on slow ttft without penalizing long but valid responses. 2. streaming llm responses: distinguish between timeout for establishing the stream (headers/first token) and timeout for the ongoing stream completion. example: enforce a 5-second timeout for receiving the first token (response_headers_timeout), but allow up to 60 seconds for the full server-sent events (sse) stream (timeout). 3. fallback and retry logic: quickly detect upstream llm provider delays (via ttft timeout) and trigger fallback to a backup provider or model. avoid unnecessary retries for long-running but healthy streams. proposed implementation: 1. add a response_headers_timeout field to route configurations, defining the maximum wait time for response headers (or first token in streaming). 2. retain the existing timeout field for the full response (headers + body). 3. router logic updates: - start response_headers_timeout timer when the request is forwarded upstream. - on header receipt, cancel the ttft timer and start the timeout timer for the full response. - if headers are not received within response_headers_timeout, abort the request and trigger retry/fallback if configured. 4. backward compatibility: if response_headers_timeout is unset, use timeout for both phases (current behavior).",4.6,Critical,1.0,system-wide impact
microsoft/vscode#291173,ux nit: chat working spinner animation is wobbly,does this issue occur when all extensions are disabled?: yes report issue' dialog can assist with this. --> just noticed while working on a tpi that the chat working spinner animation is wobbly. would be great to make sure to the center point of the animation is correctly aligned so we don't have the wobbly effect. steps to reproduce: 1. start an agent chat the includes some amount of thinking/working/reasoning 2. look closely at spinner animation that shows in thinking parts,[],['BUG'],"['bug', 'chat']",github,2026-01-28T05:13:48Z,,ux nit: chat working spinner animation is wobbly does this issue occur when all extensions are disabled?: yes report issue' dialog can assist with this. --> just noticed while working on a tpi that the chat working spinner animation is wobbly. would be great to make sure to the center point of the animation is correctly aligned so we don't have the wobbly effect. steps to reproduce: 1. start an agent chat the includes some amount of thinking/working/reasoning 2. look closely at spinner animation that shows in thinking parts,2.225,Medium,0.726,functional impact
python/cpython#144295,possible data race in in,"# bug report ### bug description: during fuzzing, i found this code produce tsan warning on no-gil build. reproducer: tsan report: version: python 3.15.0a5+ free-threading build (heads/main:48795b6460e, jan 21 2026, 04:16:17) [clang 18.1.3 (1ubuntu1)] ### cpython versions tested on: 3.15, cpython main branch ### operating systems tested on: linux",[],['BUG'],"['type-bug', 'interpreter-core', 'topic-free-threading']",github,2026-01-28T05:23:09Z,,"possible data race in in # bug report ### bug description: during fuzzing, i found this code produce tsan warning on no-gil build. reproducer: tsan report: version: python 3.15.0a5+ free-threading build (heads/main:48795b6460e, jan 21 2026, 04:16:17) [clang 18.1.3 (1ubuntu1)] ### cpython versions tested on: 3.15, cpython main branch ### operating systems tested on: linux",2.513,Medium,0.791,functional impact
cockroachdb/cockroach#161919,ccl/workloadccl/allccl: testallregisteredimportfixture failed,ccl/workloadccl/allccl.testallregisteredimportfixture [failed]( on master @ [f142ecb6d44fe1b8b68fe8c465e2f6e8ffc21507]( parameters: - attempt=1 - run=1 - shard=1 help see also: [how to investigate a go test failure \(internal\)]( /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-59153,[],['TESTING'],"['C-test-failure', 'O-robot', 'branch-master', 'release-blocker', 'T-testeng']",github,2026-01-28T05:41:39Z,,ccl/workloadccl/allccl: testallregisteredimportfixture failed ccl/workloadccl/allccl.testallregisteredimportfixture [failed]( on master @ [f142ecb6d44fe1b8b68fe8c465e2f6e8ffc21507]( parameters: - attempt=1 - run=1 - shard=1 help see also: [how to investigate a go test failure \(internal\)]( /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-59153,1.6,Low,0.584,localized low-impact
rust-lang/rust#151776,fuchsia targets building error: unresolved import,compile with for all fuchsia targets there is no problem in,[],['BUG'],"['C-bug', 'O-fuchsia']",github,2026-01-28T05:55:16Z,,fuchsia targets building error: unresolved import compile with for all fuchsia targets there is no problem in,2.236,Medium,0.728,functional impact
kubernetes/kubernetes#136586,"kubectl apply does not remove manually added env vars, causing permanent drift between manifest and live deployment","### what would you like to be added? what happened? we manage deployments declaratively using kubectl apply (server-side apply) from gitops pipelines. in production, a user manually edited a deployment (kubectl edit deployment) and added an extra environment variable directly under: spec.template.spec.containers[].env later, the deployment was redeployed using kubectl apply -f with a manifest that does not contain this manually added env var. despite successful apply and no reported conflicts, the manually added environment variable remains present in the live deployment. this results in permanent configuration drift between the manifest and the cluster state. ### why is this needed? we expected the live deployment to exactly match the applied manifest, such that: environment variables not present in the manifest are removed from the deployment kubectl apply enforces declarative state, even for mergeable lists like env the applied manifest becomes the source of truth in short: after apply, the cluster state should be fully in sync with the manifest. **how can we reproduce it** 1. create a deployment apiversion: apps/v1 kind: deployment metadata: name: sample-app spec: replicas: 1 selector: matchlabels: app: sample template: metadata: labels: app: sample spec: containers: - name: app image: nginx env: - name: testing value: ""working"" kubectl apply -f deployment.yaml 2. manually edit the deployment kubectl edit deployment sample-app env: - name: testing2 value: workingnot kubectl describe deployment sample-app 3. re-apply the original manifest kubectl apply -f deployment.yaml 4. observe the result kubectl describe deployment sample-app ‚ùå testing2=workingnot still exists, even though it is not in the manifest. **anything else we need to know?** even tried with the below commands kubectl apply -f deployment.yaml --server-side --force-conflicts --field-manager=gitops still it is not able to remove the additional env variables. there is currently no supported way to say: ‚Äúthis list must exactly match the manifest.‚Äù using kubectl apply **kubernetes version** 1.32.6 **cloud provider** azure kubernetes service (aks)",[],['FEATURE'],"['kind/feature', 'sig/cli', 'needs-triage']",github,2026-01-28T06:12:23Z,,"kubectl apply does not remove manually added env vars, causing permanent drift between manifest and live deployment ### what would you like to be added? what happened? we manage deployments declaratively using kubectl apply (server-side apply) from gitops pipelines. in production, a user manually edited a deployment (kubectl edit deployment) and added an extra environment variable directly under: spec.template.spec.containers[].env later, the deployment was redeployed using kubectl apply -f with a manifest that does not contain this manually added env var. despite successful apply and no reported conflicts, the manually added environment variable remains present in the live deployment. this results in permanent configuration drift between the manifest and the cluster state. ### why is this needed? we expected the live deployment to exactly match the applied manifest, such that: environment variables not present in the manifest are removed from the deployment kubectl apply enforces declarative state, even for mergeable lists like env the applied manifest becomes the source of truth in short: after apply, the cluster state should be fully in sync with the manifest. **how can we reproduce it** 1. create a deployment apiversion: apps/v1 kind: deployment metadata: name: sample-app spec: replicas: 1 selector: matchlabels: app: sample template: metadata: labels: app: sample spec: containers: - name: app image: nginx env: - name: testing value: ""working"" kubectl apply -f deployment.yaml 2. manually edit the deployment kubectl edit deployment sample-app env: - name: testing2 value: workingnot kubectl describe deployment sample-app 3. re-apply the original manifest kubectl apply -f deployment.yaml 4. observe the result kubectl describe deployment sample-app ‚ùå testing2=workingnot still exists, even though it is not in the manifest. **anything else we need to know?** even tried with the below commands kubectl apply -f deployment.yaml --server-side --force-conflicts --field-manager=gitops still it is not able to remove the additional env variables. there is currently no supported way to say: ‚Äúthis list must exactly match the manifest.‚Äù using kubectl apply **kubernetes version** 1.32.6 **cloud provider** azure kubernetes service (aks)",3.05,High,0.913,functional impact
microsoft/vscode#291179,adopt screenelement from xterm,,[],['UI'],"['debt', 'terminal']",github,2026-01-28T06:16:52Z,,adopt screenelement from xterm,1.8,Low,0.629,user-visible issue
cockroachdb/cockroach#161920,sentry: conn_executor.go:1117: panic: √ó (1) attached stack trace -- stack trace: | github.com/cockroachdb/cockroach/pkg/sql.(*server).serveconn.func1 | pkg/sql/conn_executor.go:1117 | [...re...,this issue was auto filed by sentry. it represents a crash or reported error on a live cluster with telemetry enabled. sentry link: [ panic message: stacktrace (expand for inline code snippets): [pkg/sql/conn_executor_exec.go#l4506-l4508](pkg/sql/conn_executor_exec.go#l4506-l4508) [pkg/sql/conn_executor_exec.go#l177-l179](pkg/sql/conn_executor_exec.go#l177-l179) [pkg/sql/conn_executor_exec.go#l1087-l1089](pkg/sql/conn_executor_exec.go#l1087-l1089) [pkg/sql/conn_executor_exec.go#l2950-l2952](pkg/sql/conn_executor_exec.go#l2950-l2952) [pkg/sql/conn_executor_exec.go#l3451-l3453](pkg/sql/conn_executor_exec.go#l3451-l3453) [pkg/sql/distsql_running.go#l2043-l2045](pkg/sql/distsql_running.go#l2043-l2045) [pkg/sql/distsql_running.go#l2040-l2042](pkg/sql/distsql_running.go#l2040-l2042) [pkg/sql/distsql_running.go#l2341-l2343](pkg/sql/distsql_running.go#l2341-l2343) [pkg/sql/distsql_running.go#l1075-l1077](pkg/sql/distsql_running.go#l1075-l1077) [pkg/sql/colflow/vectorized_flow.go#l316-l318](pkg/sql/colflow/vectorized_flow.go#l316-l318) [pkg/sql/colflow/flow_coordinator.go#l268-l270](pkg/sql/colflow/flow_coordinator.go#l268-l270) [pkg/sql/colflow/flow_coordinator.go#l234-l236](pkg/sql/colflow/flow_coordinator.go#l234-l236) [pkg/sql/colexecerror/error.go#l161-l163](pkg/sql/colexecerror/error.go#l161-l163) [pkg/sql/colflow/flow_coordinator.go#l235-l237](pkg/sql/colflow/flow_coordinator.go#l235-l237) [pkg/sql/colflow/stats.go#l104-l106](pkg/sql/colflow/stats.go#l104-l106) [pkg/sql/colexecerror/error.go#l161-l163](pkg/sql/colexecerror/error.go#l161-l163) [pkg/sql/colflow/stats.go#l95-l97](pkg/sql/colflow/stats.go#l95-l97) [pkg/sql/colexec/columnarizer.go#l177-l179](pkg/sql/colexec/columnarizer.go#l177-l179) [pkg/sql/plan_node_to_row_source.go#l208-l210](pkg/sql/plan_node_to_row_source.go#l208-l210) [pkg/sql/plan.go#l557-l559](pkg/sql/plan.go#l557-l559) [pkg/sql/values.go#l85-l87](pkg/sql/values.go#l85-l87) [pkg/sql/sem/eval/expr.go#l21-l23](pkg/sql/sem/eval/expr.go#l21-l23) [bazel-out/k8-opt/bin/pkg/sql/sem/tree/eval_expr_generated.go#l290-l292](bazel-out/k8-opt/bin/pkg/sql/sem/tree/eval_expr_generated.go#l290-l292) [pkg/sql/sem/eval/expr.go#l501-l503](pkg/sql/sem/eval/expr.go#l501-l503) [pkg/sql/sem/builtins/builtins.go#l5996-l5998](pkg/sql/sem/builtins/builtins.go#l5996-l5998) [pkg/sql/colexecerror/error.go#l316-l318](pkg/sql/colexecerror/error.go#l316-l318) [goroot/src/runtime/panic.go#l782-l784](goroot/src/runtime/panic.go#l782-l784) [pkg/sql/colexecerror/error.go#l136-l138](pkg/sql/colexecerror/error.go#l136-l138) [goroot/src/runtime/panic.go#l782-l784](goroot/src/runtime/panic.go#l782-l784) [pkg/sql/colexecerror/error.go#l136-l138](pkg/sql/colexecerror/error.go#l136-l138) [goroot/src/runtime/panic.go#l782-l784](goroot/src/runtime/panic.go#l782-l784) [pkg/sql/conn_executor.go#l1116-l1118](pkg/sql/conn_executor.go#l1116-l1118) ### tags | tag | value | | --- | --- | | command | demo | | environment | v26.1.0 | | go version | go1.25.5 | | platform | linux amd64 | | distribution | ccl | | cockroach release | v26.1.0 | | cockroach sha | 505f4c50f2d87ebbc151fd28e6b589c2ed05b7cd | | # of cpus | 8 | | # of goroutines | 456 | jira issue: crdb-59154,[],['BUG'],"['C-bug', 'O-sentry', 'branch-release-26.1']",github,2026-01-28T06:29:13Z,2026-01-28T06:29:15Z,sentry: conn_executor.go:1117: panic: √ó (1) attached stack trace -- stack trace: | github.com/cockroachdb/cockroach/pkg/sql.(*server).serveconn.func1 | pkg/sql/conn_executor.go:1117 | [...re... this issue was auto filed by sentry. it represents a crash or reported error on a live cluster with telemetry enabled. sentry link: [ panic message: stacktrace (expand for inline code snippets): [pkg/sql/conn_executor_exec.go#l4506-l4508](pkg/sql/conn_executor_exec.go#l4506-l4508) [pkg/sql/conn_executor_exec.go#l177-l179](pkg/sql/conn_executor_exec.go#l177-l179) [pkg/sql/conn_executor_exec.go#l1087-l1089](pkg/sql/conn_executor_exec.go#l1087-l1089) [pkg/sql/conn_executor_exec.go#l2950-l2952](pkg/sql/conn_executor_exec.go#l2950-l2952) [pkg/sql/conn_executor_exec.go#l3451-l3453](pkg/sql/conn_executor_exec.go#l3451-l3453) [pkg/sql/distsql_running.go#l2043-l2045](pkg/sql/distsql_running.go#l2043-l2045) [pkg/sql/distsql_running.go#l2040-l2042](pkg/sql/distsql_running.go#l2040-l2042) [pkg/sql/distsql_running.go#l2341-l2343](pkg/sql/distsql_running.go#l2341-l2343) [pkg/sql/distsql_running.go#l1075-l1077](pkg/sql/distsql_running.go#l1075-l1077) [pkg/sql/colflow/vectorized_flow.go#l316-l318](pkg/sql/colflow/vectorized_flow.go#l316-l318) [pkg/sql/colflow/flow_coordinator.go#l268-l270](pkg/sql/colflow/flow_coordinator.go#l268-l270) [pkg/sql/colflow/flow_coordinator.go#l234-l236](pkg/sql/colflow/flow_coordinator.go#l234-l236) [pkg/sql/colexecerror/error.go#l161-l163](pkg/sql/colexecerror/error.go#l161-l163) [pkg/sql/colflow/flow_coordinator.go#l235-l237](pkg/sql/colflow/flow_coordinator.go#l235-l237) [pkg/sql/colflow/stats.go#l104-l106](pkg/sql/colflow/stats.go#l104-l106) [pkg/sql/colexecerror/error.go#l161-l163](pkg/sql/colexecerror/error.go#l161-l163) [pkg/sql/colflow/stats.go#l95-l97](pkg/sql/colflow/stats.go#l95-l97) [pkg/sql/colexec/columnarizer.go#l177-l179](pkg/sql/colexec/columnarizer.go#l177-l179) [pkg/sql/plan_node_to_row_source.go#l208-l210](pkg/sql/plan_node_to_row_source.go#l208-l210) [pkg/sql/plan.go#l557-l559](pkg/sql/plan.go#l557-l559) [pkg/sql/values.go#l85-l87](pkg/sql/values.go#l85-l87) [pkg/sql/sem/eval/expr.go#l21-l23](pkg/sql/sem/eval/expr.go#l21-l23) [bazel-out/k8-opt/bin/pkg/sql/sem/tree/eval_expr_generated.go#l290-l292](bazel-out/k8-opt/bin/pkg/sql/sem/tree/eval_expr_generated.go#l290-l292) [pkg/sql/sem/eval/expr.go#l501-l503](pkg/sql/sem/eval/expr.go#l501-l503) [pkg/sql/sem/builtins/builtins.go#l5996-l5998](pkg/sql/sem/builtins/builtins.go#l5996-l5998) [pkg/sql/colexecerror/error.go#l316-l318](pkg/sql/colexecerror/error.go#l316-l318) [goroot/src/runtime/panic.go#l782-l784](goroot/src/runtime/panic.go#l782-l784) [pkg/sql/colexecerror/error.go#l136-l138](pkg/sql/colexecerror/error.go#l136-l138) [goroot/src/runtime/panic.go#l782-l784](goroot/src/runtime/panic.go#l782-l784) [pkg/sql/colexecerror/error.go#l136-l138](pkg/sql/colexecerror/error.go#l136-l138) [goroot/src/runtime/panic.go#l782-l784](goroot/src/runtime/panic.go#l782-l784) [pkg/sql/conn_executor.go#l1116-l1118](pkg/sql/conn_executor.go#l1116-l1118) ### tags | tag | value | | --- | --- | | command | demo | | environment | v26.1.0 | | go version | go1.25.5 | | platform | linux amd64 | | distribution | ccl | | cockroach release | v26.1.0 | | cockroach sha | 505f4c50f2d87ebbc151fd28e6b589c2ed05b7cd | | # of cpus | 8 | | # of goroutines | 456 | jira issue: crdb-59154,7.8,Critical,1.0,crash-like behavior
cilium/cilium#44045,cfp: support for ipfamilypolicy & ipfamilies in helm chart,## cilium feature proposal **is your proposed feature related to a problem?** currently it's not possible to set & in the cilium helm chart via values. there is a related issue **describe the feature you'd like** the ability to set & for at least the cilium-ingress service. **notify relevant community channels** **(optional) describe your proposed solution** add & to all services in the cilium helm chart or at least the cilium-ingress service.,[],['FEATURE'],"['kind/feature', 'kind/cfp']",github,2026-01-28T06:45:01Z,,cfp: support for ipfamilypolicy & ipfamilies in helm chart ## cilium feature proposal **is your proposed feature related to a problem?** currently it's not possible to set & in the cilium helm chart via values. there is a related issue **describe the feature you'd like** the ability to set & for at least the cilium-ingress service. **notify relevant community channels** **(optional) describe your proposed solution** add & to all services in the cilium helm chart or at least the cilium-ingress service.,1.4,Low,0.538,localized low-impact
openssl/openssl#29815,fails to build on big-endian ppc64 by trying to link elfv1 to elfv2 code,"starting with openssl 3.5.5, the package in debian started to fail to build from source with what seems to be a problem with the build system that tries to link elfv1 and elfv2 code: full build log available here: also reported to ibm's linuxppc issue tracker: and in debian:",[],['BUG'],"['help wanted', 'triaged: bug', 'severity: regression', 'branch: 3.3', 'branch: 3.4', 'branch: 3.5', 'branch: 3.6']",github,2026-01-28T07:23:32Z,,"fails to build on big-endian ppc64 by trying to link elfv1 to elfv2 code starting with openssl 3.5.5, the package in debian started to fail to build from source with what seems to be a problem with the build system that tries to link elfv1 and elfv2 code: full build log available here: also reported to ibm's linuxppc issue tracker: and in debian:",4.2,Critical,1.0,system-wide impact
cockroachdb/cockroach#161923,roachtest: backup-restore/online-restore failed,roachtest.backup-restore/online-restore [failed]( with [artifacts]( on release-26.1.0-rc @ [505f4c50f2d87ebbc151fd28e6b589c2ed05b7cd]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21033433-1769584130-11-n4cpu4-0001 | 34.139.205.62 | 10.142.2.28 | | teamcity-21033433-1769584130-11-n4cpu4-0002 | 34.139.47.12 | 10.142.2.36 | | teamcity-21033433-1769584130-11-n4cpu4-0003 | 34.74.4.157 | 10.142.2.37 | | teamcity-21033433-1769584130-11-n4cpu4-0004 | 35.231.96.72 | 10.142.2.52 | parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=true - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( same failure on other branches - roachtest: backup-restore/online-restore failed [error getting random table name: relation does not exist] [b-runtime-assertions-enabled c-test-failure o-roachtest o-robot p-2 t-sql-foundations branch-release-26.1] /cc /sql-foundations [this test on roachdash]( | [improve this report!]( jira issue: crdb-59155,[],['TESTING'],"['C-test-failure', 'O-robot', 'O-roachtest', 'release-blocker', 'T-sql-foundations', 'branch-release-26.1.0-rc']",github,2026-01-28T07:41:29Z,,roachtest: backup-restore/online-restore failed roachtest.backup-restore/online-restore [failed]( with [artifacts]( on release-26.1.0-rc @ [505f4c50f2d87ebbc151fd28e6b589c2ed05b7cd]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21033433-1769584130-11-n4cpu4-0001 | 34.139.205.62 | 10.142.2.28 | | teamcity-21033433-1769584130-11-n4cpu4-0002 | 34.139.47.12 | 10.142.2.36 | | teamcity-21033433-1769584130-11-n4cpu4-0003 | 34.74.4.157 | 10.142.2.37 | | teamcity-21033433-1769584130-11-n4cpu4-0004 | 35.231.96.72 | 10.142.2.52 | parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=true - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( same failure on other branches - roachtest: backup-restore/online-restore failed [error getting random table name: relation does not exist] [b-runtime-assertions-enabled c-test-failure o-roachtest o-robot p-2 t-sql-foundations branch-release-26.1] /cc /sql-foundations [this test on roachdash]( | [improve this report!]( jira issue: crdb-59155,2.903,Medium,0.88,functional impact
tensorflow/tensorflow#109015,[xla] inconsistent error detection: tf.config() callable error caught in xla compilation but not in eager execution,"### issue type bug ### have you reproduced the bug with tensorflow nightly? yes ### source source ### tensorflow version 2.20.0 ### custom code yes ### os platform and distribution linux ubuntu 24.04 ### mobile device _no response_ ### python version 3.12 ### bazel version _no response_ ### gcc/compiler version _no response_ ### cuda/cudnn version _no response_ ### gpu model and memory _no response_ ### current behavior? tensorflow shows inconsistent error detection when incorrectly calling as a function. the error is caught during xla compilation but not during eager execution, leading to different behaviors in different execution modes. ### standalone code to reproduce the issue ### relevant log output",[],['BUG'],['type:bug'],github,2026-01-28T07:53:43Z,,"[xla] inconsistent error detection: tf.config() callable error caught in xla compilation but not in eager execution ### issue type bug ### have you reproduced the bug with tensorflow nightly? yes ### source source ### tensorflow version 2.20.0 ### custom code yes ### os platform and distribution linux ubuntu 24.04 ### mobile device _no response_ ### python version 3.12 ### bazel version _no response_ ### gcc/compiler version _no response_ ### cuda/cudnn version _no response_ ### gpu model and memory _no response_ ### current behavior? tensorflow shows inconsistent error detection when incorrectly calling as a function. the error is caught during xla compilation but not during eager execution, leading to different behaviors in different execution modes. ### standalone code to reproduce the issue ### relevant log output",4.2,Critical,1.0,system-wide impact
microsoft/vscode#291193,tasks: leaked disposable,,[],['PERFORMANCE'],['freeze-slow-crash-leak'],github,2026-01-28T07:58:01Z,,tasks: leaked disposable,3.212,High,0.95,performance degradation
envoyproxy/envoy#43197,newer release available : v4.2.3 (current: v4.2.1),package name: fips_cmake_linux_aarch64 .2.1 current version: v4.2.1 -12-08 available version: v4.2.3 -01-27 upstream releases:,[],['UI'],"['dependencies', 'no stalebot', 'area/build']",github,2026-01-28T08:11:01Z,,newer release available : v4.2.3 (current: v4.2.1) package name: fips_cmake_linux_aarch64 .2.1 current version: v4.2.1 -12-08 available version: v4.2.3 -01-27 upstream releases:,1.8,Low,0.629,user-visible issue
envoyproxy/envoy#43196,newer release available : v0.1.6 (current: v0.1.5),package name: envoy_examples .1.5 current version: v0.1.5 -01-03 available version: v0.1.6 -01-27 upstream releases:,[],['UI'],"['dependencies', 'no stalebot', 'area/build']",github,2026-01-28T08:11:01Z,,newer release available : v0.1.6 (current: v0.1.5) package name: envoy_examples .1.5 current version: v0.1.5 -01-03 available version: v0.1.6 -01-27 upstream releases:,1.8,Low,0.629,user-visible issue
envoyproxy/envoy#43198,newer release available : v4.2.3 (current: v4.2.1),package name: fips_cmake_linux_x86_64 .2.1 current version: v4.2.1 -12-08 available version: v4.2.3 -01-27 upstream releases:,[],['UI'],"['dependencies', 'no stalebot', 'area/build']",github,2026-01-28T08:11:03Z,,newer release available : v4.2.3 (current: v4.2.1) package name: fips_cmake_linux_x86_64 .2.1 current version: v4.2.1 -12-08 available version: v4.2.3 -01-27 upstream releases:,1.8,Low,0.629,user-visible issue
envoyproxy/envoy#43199,newer release available : 1.8.3 (current: 1.7.0),package name: rules_python .7.0 current version: 1.7.0 -11-14 available version: 1.8.3 -01-28 upstream releases:,[],['UI'],"['dependencies', 'no stalebot', 'area/build']",github,2026-01-28T08:11:05Z,,newer release available : 1.8.3 (current: 1.7.0) package name: rules_python .7.0 current version: 1.7.0 -11-14 available version: 1.8.3 -01-28 upstream releases:,1.8,Low,0.629,user-visible issue
tensorflow/tensorflow#109016,[xla] inconsistent parameter type acceptance in tf.image.extract_patches: tensor parameters work in eager mode but fail in xla compilation,### issue type bug ### have you reproduced the bug with tensorflow nightly? yes ### source source ### tensorflow version 2.20.0 ### custom code yes ### os platform and distribution linux ubuntu 24.04 ### mobile device _no response_ ### python version 3.12 ### bazel version _no response_ ### gcc/compiler version _no response_ ### cuda/cudnn version _no response_ ### gpu model and memory _no response_ ### current behavior? the operation shows inconsistent behavior when the parameter contains tensor expressions. the operation works correctly in eager execution mode but fails during xla compilation with a type error. ### standalone code to reproduce the issue ### relevant log output,[],['BUG'],['type:bug'],github,2026-01-28T08:12:10Z,,[xla] inconsistent parameter type acceptance in tf.image.extract_patches: tensor parameters work in eager mode but fail in xla compilation ### issue type bug ### have you reproduced the bug with tensorflow nightly? yes ### source source ### tensorflow version 2.20.0 ### custom code yes ### os platform and distribution linux ubuntu 24.04 ### mobile device _no response_ ### python version 3.12 ### bazel version _no response_ ### gcc/compiler version _no response_ ### cuda/cudnn version _no response_ ### gpu model and memory _no response_ ### current behavior? the operation shows inconsistent behavior when the parameter contains tensor expressions. the operation works correctly in eager execution mode but fails during xla compilation with a type error. ### standalone code to reproduce the issue ### relevant log output,4.2,Critical,1.0,system-wide impact
llvm/llvm-project#178380,[llvm] build error when linking static z3 library,"when i build llvm 15.0.7 on centos 7.9 using following commands (static library with static z3) the linker complains about undefined symbol: the compile command is as follows: the question is the is located after . the relevant cmake code is: 1. linking pthread: [ 2. linking z3: [ i think this maybe a bug in cmake script, and i have successfully built after i fix it locally. could you check it out for other platform? many thanks.",[],['UI'],['build-problem'],github,2026-01-28T08:28:19Z,,"[llvm] build error when linking static z3 library when i build llvm 15.0.7 on centos 7.9 using following commands (static library with static z3) the linker complains about undefined symbol: the compile command is as follows: the question is the is located after . the relevant cmake code is: 1. linking pthread: [ 2. linking z3: [ i think this maybe a bug in cmake script, and i have successfully built after i fix it locally. could you check it out for other platform? many thanks.",1.8,Low,0.629,user-visible issue
microsoft/vscode#291213,chat sessions: add mark all read action,"not sure if this is specific to us testing sessions, but i at some point wished i could just mark all sessions as ""read"".",['Chat Sessions: Add Mark All Read action (fix #291213) (#291244)'],['FEATURE'],"['feature-request', 'unreleased', 'chat-agents-view']",github,2026-01-28T08:39:28Z,2026-01-28T10:44:46Z,"chat sessions: add mark all read action not sure if this is specific to us testing sessions, but i at some point wished i could just mark all sessions as ""read"". Chat Sessions: Add Mark All Read action (fix #291213) (#291244)",1.4,Low,0.538,localized low-impact
microsoft/vscode#291214,chat sessions: clicking on the read/unread indicator should toggle state,currently this opens the session which is a tedious way to mark it as read when i'm not interested in it.,[],"['FEATURE', 'UI']","['feature-request', 'ux', 'chat-agents-view']",github,2026-01-28T08:40:41Z,,chat sessions: clicking on the read/unread indicator should toggle state currently this opens the session which is a tedious way to mark it as read when i'm not interested in it.,1.6,Low,0.584,user-visible issue
microsoft/vscode#291216,state of actions when url is empty is incorrect,testing when url is empty i observe: the following could be disabled when url is empty: - add to chat - find in page (when page is empty) - open in external browser,[],['BUG'],['bug'],github,2026-01-28T08:51:47Z,,state of actions when url is empty is incorrect testing when url is empty i observe: the following could be disabled when url is empty: - add to chat - find in page (when page is empty) - open in external browser,2.567,Medium,0.803,functional impact
microsoft/vscode#291218,app icon on the tab flashes during in-page navigation,"testing open code.visualstudio.com/updates/v1_108 page click on the toc on the right (accessibility, editor experience etc) the tab icon will flash on every click even though the page is not changing. expected: it should not flash.",[],['BUG'],['bug'],github,2026-01-28T09:03:22Z,,"app icon on the tab flashes during in-page navigation testing open code.visualstudio.com/updates/v1_108 page click on the toc on the right (accessibility, editor experience etc) the tab icon will flash on every click even though the page is not changing. expected: it should not flash.",4.6,Critical,1.0,crash-like behavior
microsoft/vscode#291219,enable ctrl+f5 shortcut in the integrated browser,"testing i know f5 works, but i use ctrl+f5 most of the time and it brings up debugger in the browser which is unexpected.",[],['FEATURE'],['feature-request'],github,2026-01-28T09:04:53Z,,"enable ctrl+f5 shortcut in the integrated browser testing i know f5 works, but i use ctrl+f5 most of the time and it brings up debugger in the browser which is unexpected.",1.4,Low,0.538,localized low-impact
microsoft/vscode#291220,some address auto-complete please,testing at least remember the addresses i've already typed and complete from those.,[],['FEATURE'],['feature-request'],github,2026-01-28T09:06:19Z,,some address auto-complete please testing at least remember the addresses i've already typed and complete from those.,1.4,Low,0.538,localized low-impact
cockroachdb/cockroach#161926,roachtest: import/nodeshutdown/worker/distmerge=true/nodes=4 failed,roachtest.import/nodeshutdown/worker/distmerge=true/nodes=4 [failed]( with [artifacts]( on master @ [e01438c215ca00c6c017f4c092f8cb321aa53a7a]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21033268-1769585242-07-n4cpu4-0001 | 34.148.144.6 | 10.142.3.41 | | teamcity-21033268-1769585242-07-n4cpu4-0002 | 34.73.221.59 | 10.142.3.50 | | teamcity-21033268-1769585242-07-n4cpu4-0003 | 104.196.176.202 | 10.142.3.49 | | teamcity-21033268-1769585242-07-n4cpu4-0004 | 34.26.39.187 | 10.142.3.47 | parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=true - metamorphicleases=epoch - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( [this test on roachdash]( | [improve this report!]( jira issue: crdb-59156,[],['TESTING'],"['C-test-failure', 'O-robot', 'O-roachtest', 'branch-master', 'release-blocker', 'T-sql-queries']",github,2026-01-28T09:08:20Z,,roachtest: import/nodeshutdown/worker/distmerge=true/nodes=4 failed roachtest.import/nodeshutdown/worker/distmerge=true/nodes=4 [failed]( with [artifacts]( on master @ [e01438c215ca00c6c017f4c092f8cb321aa53a7a]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21033268-1769585242-07-n4cpu4-0001 | 34.148.144.6 | 10.142.3.41 | | teamcity-21033268-1769585242-07-n4cpu4-0002 | 34.73.221.59 | 10.142.3.50 | | teamcity-21033268-1769585242-07-n4cpu4-0003 | 104.196.176.202 | 10.142.3.49 | | teamcity-21033268-1769585242-07-n4cpu4-0004 | 34.26.39.187 | 10.142.3.47 | parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=true - metamorphicleases=epoch - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( [this test on roachdash]( | [improve this report!]( jira issue: crdb-59156,3.288,High,0.967,system-wide impact
cockroachdb/cockroach#161927,roachtest: failover/chaos/read-write/lease=expiration failed,roachtest.failover/chaos/read-write/lease=expiration [failed]( with [artifacts]( on release-24.1 @ [06d16acbcac8363e20deaa903ea2988fa79c4da6]( parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=2 - encrypted=false - fs=ext4 - localssd=false - runtimeassertionsbuild=false - ssd=0 help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( /cc /kv-triage [this test on roachdash]( | [improve this report!]( jira issue: crdb-59157,[],['TESTING'],"['C-test-failure', 'O-robot', 'O-roachtest', 'release-blocker', 'T-kv', 'branch-release-24.1']",github,2026-01-28T09:13:47Z,,roachtest: failover/chaos/read-write/lease=expiration failed roachtest.failover/chaos/read-write/lease=expiration [failed]( with [artifacts]( on release-24.1 @ [06d16acbcac8363e20deaa903ea2988fa79c4da6]( parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=2 - encrypted=false - fs=ext4 - localssd=false - runtimeassertionsbuild=false - ssd=0 help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( /cc /kv-triage [this test on roachdash]( | [improve this report!]( jira issue: crdb-59157,3.086,High,0.921,functional impact
microsoft/vscode#291230,"automatic task setting was updated in global user settings, not workspace","testing per step 4. click 'allow and run'.: automatic tasks should run immediately and the setting should be updated to allow future automatic tasks in this workspace. what happened is this line was added to global user settings.json, not workspace one (which does not exist): also, upon removing the setting and reopening the workspace, i don't see the notification again, but i think that's logged as a separate issue.",[],['BUG'],['bug'],github,2026-01-28T09:32:37Z,,"automatic task setting was updated in global user settings, not workspace testing per step 4. click 'allow and run'.: automatic tasks should run immediately and the setting should be updated to allow future automatic tasks in this workspace. what happened is this line was added to global user settings.json, not workspace one (which does not exist): also, upon removing the setting and reopening the workspace, i don't see the notification again, but i think that's logged as a separate issue.",2.664,Medium,0.825,functional impact
cilium/cilium#44046,cfp: oci registry for vault helm chart,"## cilium feature proposal do you plan to have the oci registry somewhen? **describe the feature you'd like** i need an oci registry were i can pull the clilium helm chart. currently, as far as i can see, it's only uploaded in a traditional helm repository in **notify relevant community channels** - /helm - /ci-structure **(optional) describe your proposed solution** thanks",[],['FEATURE'],"['kind/feature', 'kind/cfp']",github,2026-01-28T09:33:01Z,,"cfp: oci registry for vault helm chart ## cilium feature proposal do you plan to have the oci registry somewhen? **describe the feature you'd like** i need an oci registry were i can pull the clilium helm chart. currently, as far as i can see, it's only uploaded in a traditional helm repository in **notify relevant community channels** - /helm - /ci-structure **(optional) describe your proposed solution** thanks",1.4,Low,0.538,localized low-impact
microsoft/vscode#291231,not clear how to make a decision on running tasks,"testing in step `open a workspace with automatic tasks and click 'open file(s)' in the prompt. the 'tasks.json' file should open for review, and automatic tasks should not run until a decision is made. ` it's not clear how the user is supposed to make the decision after the file(s) are opened.",[],['BUG'],['bug'],github,2026-01-28T09:39:21Z,,"not clear how to make a decision on running tasks testing in step `open a workspace with automatic tasks and click 'open file(s)' in the prompt. the 'tasks.json' file should open for review, and automatic tasks should not run until a decision is made. ` it's not clear how the user is supposed to make the decision after the file(s) are opened.",2.312,Medium,0.745,functional impact
cockroachdb/cockroach#161928,sentry: signal_unix.go:917: runtime error: invalid memory address or nil pointer dereference (1) attached stack trace -- stack trace: | runtime.sigpanic | goroot/src/runtime/signal_unix.go:917 ...,this issue was auto filed by sentry. it represents a crash or reported error on a live cluster with telemetry enabled. sentry link: [ panic message: stacktrace (expand for inline code snippets): [src/runtime/asm_amd64.s#l1699-l1701](src/runtime/asm_amd64.s#l1699-l1701) [goroot/src/runtime/proc.go#l271-l273](goroot/src/runtime/proc.go#l271-l273) [pkg/cmd/cockroach/main.go#l20-l22](pkg/cmd/cockroach/main.go#l20-l22) [cli.go#l64-l66](cli.go#l64-l66) [cli.go#l139-l141](cli.go#l139-l141) [cli.go#l300-l302](cli.go#l300-l302) [external/com_github_spf13_cobra/command.go#l967-l969](external/com_github_spf13_cobra/command.go#l967-l969) [external/com_github_spf13_cobra/command.go#l1043-l1045](external/com_github_spf13_cobra/command.go#l1043-l1045) [external/com_github_spf13_cobra/command.go#l915-l917](external/com_github_spf13_cobra/command.go#l915-l917) [pkg/cli/clierrorplus/decorate_error.go#l66-l68](pkg/cli/clierrorplus/decorate_error.go#l66-l68) [sql_shell_cmd.go#l59-l61](sql_shell_cmd.go#l59-l61) [pkg/cli/clisqlcfg/context.go#l199-l201](pkg/cli/clisqlcfg/context.go#l199-l201) [pkg/cli/clisqlclient/conn.go#l229-l231](pkg/cli/clisqlclient/conn.go#l229-l231) [pkg/cli/clisqlclient/conn.go#l455-l457](pkg/cli/clisqlclient/conn.go#l455-l457) [pkg/cli/clisqlclient/conn.go#l286-l288](pkg/cli/clisqlclient/conn.go#l286-l288) [external/com_github_jackc_pgx_v5/conn.go#l419-l421](external/com_github_jackc_pgx_v5/conn.go#l419-l421) [goroot/src/runtime/signal_unix.go#l916-l918](goroot/src/runtime/signal_unix.go#l916-l918) ### tags | tag | value | | --- | --- | | command | sql | | environment | v25.4.3 | | go version | go1.23.12 x:nocoverageredesign | | platform | darwin amd64 | | distribution | ccl | | cockroach release | v25.4.3 | | cockroach sha | 71d853623f0d1f589fd5c727a1c4aec8a43e62e0 | | # of cpus | 12 | | # of goroutines | 9 | jira issue: crdb-59158,[],['BUG'],"['C-bug', 'O-sentry', 'X-blathers-triaged', 'T-sql-foundations', 'branch-release-25.4']",github,2026-01-28T10:01:50Z,,sentry: signal_unix.go:917: runtime error: invalid memory address or nil pointer dereference (1) attached stack trace -- stack trace: | runtime.sigpanic | goroot/src/runtime/signal_unix.go:917 ... this issue was auto filed by sentry. it represents a crash or reported error on a live cluster with telemetry enabled. sentry link: [ panic message: stacktrace (expand for inline code snippets): [src/runtime/asm_amd64.s#l1699-l1701](src/runtime/asm_amd64.s#l1699-l1701) [goroot/src/runtime/proc.go#l271-l273](goroot/src/runtime/proc.go#l271-l273) [pkg/cmd/cockroach/main.go#l20-l22](pkg/cmd/cockroach/main.go#l20-l22) [cli.go#l64-l66](cli.go#l64-l66) [cli.go#l139-l141](cli.go#l139-l141) [cli.go#l300-l302](cli.go#l300-l302) [external/com_github_spf13_cobra/command.go#l967-l969](external/com_github_spf13_cobra/command.go#l967-l969) [external/com_github_spf13_cobra/command.go#l1043-l1045](external/com_github_spf13_cobra/command.go#l1043-l1045) [external/com_github_spf13_cobra/command.go#l915-l917](external/com_github_spf13_cobra/command.go#l915-l917) [pkg/cli/clierrorplus/decorate_error.go#l66-l68](pkg/cli/clierrorplus/decorate_error.go#l66-l68) [sql_shell_cmd.go#l59-l61](sql_shell_cmd.go#l59-l61) [pkg/cli/clisqlcfg/context.go#l199-l201](pkg/cli/clisqlcfg/context.go#l199-l201) [pkg/cli/clisqlclient/conn.go#l229-l231](pkg/cli/clisqlclient/conn.go#l229-l231) [pkg/cli/clisqlclient/conn.go#l455-l457](pkg/cli/clisqlclient/conn.go#l455-l457) [pkg/cli/clisqlclient/conn.go#l286-l288](pkg/cli/clisqlclient/conn.go#l286-l288) [external/com_github_jackc_pgx_v5/conn.go#l419-l421](external/com_github_jackc_pgx_v5/conn.go#l419-l421) [goroot/src/runtime/signal_unix.go#l916-l918](goroot/src/runtime/signal_unix.go#l916-l918) ### tags | tag | value | | --- | --- | | command | sql | | environment | v25.4.3 | | go version | go1.23.12 x:nocoverageredesign | | platform | darwin amd64 | | distribution | ccl | | cockroach release | v25.4.3 | | cockroach sha | 71d853623f0d1f589fd5c727a1c4aec8a43e62e0 | | # of cpus | 12 | | # of goroutines | 9 | jira issue: crdb-59158,6.0,Critical,1.0,crash-like behavior
envoyproxy/envoy#43200,"suggest supporting session affinity, session isolation, and full session lifecycle management with envoy","in real-world deployments, many applications require strong session semantics. for example, ai inference services, serverless platforms, and multi-tenant web applications often rely on logical sessions that span multiple requests. these sessions require: stable session affinity to ensure consistency isolation between sessions for security and resource protection explicit lifecycle management, including ttl, expiration, and cleanup currently, istio does not provide native abstractions for these requirements, forcing users to rely on custom envoy filters or external systems. currently, stateful sessions only support session affinity and lack session isolation, so the functionality needs to be enhanced.",[],['FEATURE'],"['enhancement', 'triage']",github,2026-01-28T10:05:02Z,,"suggest supporting session affinity, session isolation, and full session lifecycle management with envoy in real-world deployments, many applications require strong session semantics. for example, ai inference services, serverless platforms, and multi-tenant web applications often rely on logical sessions that span multiple requests. these sessions require: stable session affinity to ensure consistency isolation between sessions for security and resource protection explicit lifecycle management, including ttl, expiration, and cleanup currently, istio does not provide native abstractions for these requirements, forcing users to rely on custom envoy filters or external systems. currently, stateful sessions only support session affinity and lack session isolation, so the functionality needs to be enhanced.",3.483,High,1.0,system-wide impact
kubernetes/kubernetes#136591,newunmounter always returns error while deleting pod after node reboot,"### what happened? 1. node reboot 2. kubelet starts and reconstructs a csi volume 3. kubelet calls and gets , this is a final error, then kubelet calls to remove the volume directory and vol_data.json 5. the pod is deleting 6. kubelet starts to unmount the volume, but the volume dir has been deleted, would always return error ### what did you expect to happen? newunmounter can ignore errors for volume dir no-exists ### how can we reproduce it (as minimally and precisely as possible)? above ### anything else we need to know? _no response_ ### kubernetes version ### cloud provider ### os version ### install tools ### container runtime (cri) and version (if applicable) ### related plugins (cni, csi, ...) and versions (if applicable)",[],['BUG'],"['kind/bug', 'sig/storage', 'needs-triage']",github,2026-01-28T10:13:55Z,,"newunmounter always returns error while deleting pod after node reboot ### what happened? 1. node reboot 2. kubelet starts and reconstructs a csi volume 3. kubelet calls and gets , this is a final error, then kubelet calls to remove the volume directory and vol_data.json 5. the pod is deleting 6. kubelet starts to unmount the volume, but the volume dir has been deleted, would always return error ### what did you expect to happen? newunmounter can ignore errors for volume dir no-exists ### how can we reproduce it (as minimally and precisely as possible)? above ### anything else we need to know? _no response_ ### kubernetes version ### cloud provider ### os version ### install tools ### container runtime (cri) and version (if applicable) ### related plugins (cni, csi, ...) and versions (if applicable)",2.426,Medium,0.771,functional impact
cockroachdb/cockroach#161929,roachtest: jepsen/register/subcritical-skews-start-kill-2 failed,roachtest.jepsen/register/subcritical-skews-start-kill-2 [failed]( with [artifacts]( on release-25.4.4-rc @ [688cda9084f2924ec4e22044c861aa0a3f34e956]( parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - encrypted=false - fs=ext4 - localssd=true - metamorphicbufferedsender=true - metamorphicleases=leader - metamorphicwritebuffering=true - runtimeassertionsbuild=false - ssd=0 help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-59159,[],['TESTING'],"['C-test-failure', 'O-robot', 'O-roachtest', 'release-blocker', 'T-testeng', 'branch-release-25.4.4-rc']",github,2026-01-28T11:49:37Z,,roachtest: jepsen/register/subcritical-skews-start-kill-2 failed roachtest.jepsen/register/subcritical-skews-start-kill-2 [failed]( with [artifacts]( on release-25.4.4-rc @ [688cda9084f2924ec4e22044c861aa0a3f34e956]( parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - encrypted=false - fs=ext4 - localssd=true - metamorphicbufferedsender=true - metamorphicleases=leader - metamorphicwritebuffering=true - runtimeassertionsbuild=false - ssd=0 help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( /cc /test-eng [this test on roachdash]( | [improve this report!]( jira issue: crdb-59159,2.932,Medium,0.886,functional impact
cockroachdb/cockroach#161931,roachtest: import/pause/distmerge=true/nodes=4 failed,roachtest.import/pause/distmerge=true/nodes=4 [failed]( with [artifacts]( on master @ [e01438c215ca00c6c017f4c092f8cb321aa53a7a]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21033268-1769585242-115-n4cpu4-0001 | 34.138.221.192 | 10.142.2.86 | | teamcity-21033268-1769585242-115-n4cpu4-0002 | 35.196.41.250 | 10.142.2.103 | | teamcity-21033268-1769585242-115-n4cpu4-0003 | 35.227.28.189 | 10.142.2.214 | | teamcity-21033268-1769585242-115-n4cpu4-0004 | 35.227.55.212 | 10.142.2.190 | parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=true - fs=ext4 - localssd=true - metamorphicleases=epoch - metamorphicwritebuffering=true - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( [this test on roachdash]( | [improve this report!]( jira issue: crdb-59160,[],['TESTING'],"['C-test-failure', 'O-robot', 'O-roachtest', 'branch-master', 'release-blocker', 'T-sql-queries']",github,2026-01-28T12:57:59Z,,roachtest: import/pause/distmerge=true/nodes=4 failed roachtest.import/pause/distmerge=true/nodes=4 [failed]( with [artifacts]( on master @ [e01438c215ca00c6c017f4c092f8cb321aa53a7a]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21033268-1769585242-115-n4cpu4-0001 | 34.138.221.192 | 10.142.2.86 | | teamcity-21033268-1769585242-115-n4cpu4-0002 | 35.196.41.250 | 10.142.2.103 | | teamcity-21033268-1769585242-115-n4cpu4-0003 | 35.227.28.189 | 10.142.2.214 | | teamcity-21033268-1769585242-115-n4cpu4-0004 | 35.227.55.212 | 10.142.2.190 | parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=true - fs=ext4 - localssd=true - metamorphicleases=epoch - metamorphicwritebuffering=true - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( [this test on roachdash]( | [improve this report!]( jira issue: crdb-59160,3.128,High,0.931,functional impact
envoyproxy/envoy#43202,propagating build choices (eg ssl/crypto),"historically we have had various patches that allow us to propagate and enforce our build choices for certain libs. the classic example of this and the most important is the ssl/crypto libs - propagating these to use the ones we selects ensure that when eg envoy builds with fips-mode all of the components are likewise built with a compliant library. see eg apart from the endemic hassle of patch-maintenance the problem with this pattern is that it will not work with bzlmod for the reason that (in the example above) will not be visible to the grpc repo. so the alternative is to make these targets configurable (via label_flag) either by patch or preferably by upstreaming this (see eg with this in place using the fips build as an example our config will become something like this this works fine when building _in_ workspace because and are visible to it it _can_ also work for someone building from the outside _if_ they specifically configure those repos in their workspace to make them visible. the consequence being that _everything_ that needs to be configured must be then made visible. the better bazel approach to this is to use a ""transition"". in this case we can wrap certain deps such that any time they are used they provide/enforce certain flags - potentially on detecting another with this set up even our existing define might work - but still the better way is to use a label_flag ourselves - so eg we might set and then the transition would always pass to the deps whatever ssl lib we have set. for external builders - they just set the same flag and all should magically work (and of course they _could_ wrap us i a similar transition to effectively chain the choice) ive used the ssl/crypto libs as an example here - but it also applies to a few other - like eg zlib - and is more generally a common pattern/challenge",[],"['FEATURE', 'UI']","['enhancement', 'area/build', 'bazel', 'bzlmod']",github,2026-01-28T13:54:10Z,,"propagating build choices (eg ssl/crypto) historically we have had various patches that allow us to propagate and enforce our build choices for certain libs. the classic example of this and the most important is the ssl/crypto libs - propagating these to use the ones we selects ensure that when eg envoy builds with fips-mode all of the components are likewise built with a compliant library. see eg apart from the endemic hassle of patch-maintenance the problem with this pattern is that it will not work with bzlmod for the reason that (in the example above) will not be visible to the grpc repo. so the alternative is to make these targets configurable (via label_flag) either by patch or preferably by upstreaming this (see eg with this in place using the fips build as an example our config will become something like this this works fine when building _in_ workspace because and are visible to it it _can_ also work for someone building from the outside _if_ they specifically configure those repos in their workspace to make them visible. the consequence being that _everything_ that needs to be configured must be then made visible. the better bazel approach to this is to use a ""transition"". in this case we can wrap certain deps such that any time they are used they provide/enforce certain flags - potentially on detecting another with this set up even our existing define might work - but still the better way is to use a label_flag ourselves - so eg we might set and then the transition would always pass to the deps whatever ssl lib we have set. for external builders - they just set the same flag and all should magically work (and of course they _could_ wrap us i a similar transition to effectively chain the choice) ive used the ssl/crypto libs as an example here - but it also applies to a few other - like eg zlib - and is more generally a common pattern/challenge",2.318,Medium,0.747,user-visible issue
cockroachdb/cockroach#161933,roachtest: schemachange/random-load failed,roachtest.schemachange/random-load [failed]( with [artifacts]( on release-26.1.0-rc @ [505f4c50f2d87ebbc151fd28e6b589c2ed05b7cd]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21033433-1769584130-193-n3cpu4-geo-0001 | 34.139.54.64 | 10.142.0.211 | | teamcity-21033433-1769584130-193-n3cpu4-geo-0002 | 34.82.174.128 | 10.138.0.41 | | teamcity-21033433-1769584130-193-n3cpu4-geo-0003 | 35.246.111.11 | 10.154.0.68 | parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - fs=ext4 - localssd=true - metamorphicleases=expiration - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( same failure on other branches - roachtest: schemachange/random-load failed [relation does not exist] [c-test-failure o-roachtest o-robot p-2 t-sql-foundations branch-release-26.1] /cc /sql-foundations [this test on roachdash]( | [improve this report!]( jira issue: crdb-59161,[],['TESTING'],"['C-test-failure', 'O-robot', 'O-roachtest', 'release-blocker', 'T-sql-foundations', 'branch-release-26.1.0-rc']",github,2026-01-28T14:26:18Z,,roachtest: schemachange/random-load failed roachtest.schemachange/random-load [failed]( with [artifacts]( on release-26.1.0-rc @ [505f4c50f2d87ebbc151fd28e6b589c2ed05b7cd]( failed with: cluster node to ip mapping: | node | public ip | private ip | | --- | --- | --- | | teamcity-21033433-1769584130-193-n3cpu4-geo-0001 | 34.139.54.64 | 10.142.0.211 | | teamcity-21033433-1769584130-193-n3cpu4-geo-0002 | 34.82.174.128 | 10.138.0.41 | | teamcity-21033433-1769584130-193-n3cpu4-geo-0003 | 35.246.111.11 | 10.154.0.68 | parameters: - arch=amd64 - cloud=gce - coveragebuild=false - cpu=4 - diskcount=0 - encrypted=false - fs=ext4 - localssd=true - metamorphicleases=expiration - runtimeassertionsbuild=false help see: [roachtest readme]( see: [how to investigate \(internal\)]( see: [grafana]( same failure on other branches - roachtest: schemachange/random-load failed [relation does not exist] [c-test-failure o-roachtest o-robot p-2 t-sql-foundations branch-release-26.1] /cc /sql-foundations [this test on roachdash]( | [improve this report!]( jira issue: crdb-59161,5.2,Critical,1.0,crash-like behavior
cockroachdb/cockroach#161934,pkg/sql/sqlstats/insights/integration/integration_test: testinsightspriorityintegration failed,pkg/sql/sqlstats/insights/integration/integration_test.testinsightspriorityintegration [failed]( on release-26.1 @ [0c5fb4f2971b5f61eba1946662b00a65249c1b2f]( parameters: - attempt=1 - race=true - run=3 - shard=1 help see also: [how to investigate a go test failure \(internal\)]( same failure on other branches - pkg/sql/sqlstats/insights/integration/integration_test: testinsightspriorityintegration failed [c-test-failure o-robot t-observability branch-release-26.1.0-rc] - pkg/sql/sqlstats/insights/integration/integration_test: testinsightspriorityintegration failed [c-test-failure o-robot t-observability branch-release-25.4.3-rc release-blocker] /cc /obs-prs [this test on roachdash]( | [improve this report!]( jira issue: crdb-59162,[],['TESTING'],"['C-test-failure', 'O-robot', 'release-blocker', 'T-observability', 'branch-release-26.1']",github,2026-01-28T14:29:22Z,,pkg/sql/sqlstats/insights/integration/integration_test: testinsightspriorityintegration failed pkg/sql/sqlstats/insights/integration/integration_test.testinsightspriorityintegration [failed]( on release-26.1 @ [0c5fb4f2971b5f61eba1946662b00a65249c1b2f]( parameters: - attempt=1 - race=true - run=3 - shard=1 help see also: [how to investigate a go test failure \(internal\)]( same failure on other branches - pkg/sql/sqlstats/insights/integration/integration_test: testinsightspriorityintegration failed [c-test-failure o-robot t-observability branch-release-26.1.0-rc] - pkg/sql/sqlstats/insights/integration/integration_test: testinsightspriorityintegration failed [c-test-failure o-robot t-observability branch-release-25.4.3-rc release-blocker] /cc /obs-prs [this test on roachdash]( | [improve this report!]( jira issue: crdb-59162,1.6,Low,0.584,localized low-impact
cockroachdb/cockroach#161935,pkg/sql/sqlstats/insights/integration/integration_test: testtransactioninsightsfailoncommit failed,pkg/sql/sqlstats/insights/integration/integration_test.testtransactioninsightsfailoncommit [failed]( on release-26.1 @ [0c5fb4f2971b5f61eba1946662b00a65249c1b2f]( parameters: - attempt=1 - race=true - run=3 - shard=1 help see also: [how to investigate a go test failure \(internal\)]( /cc /obs-prs [this test on roachdash]( | [improve this report!](,[],['TESTING'],"['C-test-failure', 'O-robot', 'release-blocker', 'T-observability', 'branch-release-26.1']",github,2026-01-28T14:29:24Z,,pkg/sql/sqlstats/insights/integration/integration_test: testtransactioninsightsfailoncommit failed pkg/sql/sqlstats/insights/integration/integration_test.testtransactioninsightsfailoncommit [failed]( on release-26.1 @ [0c5fb4f2971b5f61eba1946662b00a65249c1b2f]( parameters: - attempt=1 - race=true - run=3 - shard=1 help see also: [how to investigate a go test failure \(internal\)]( /cc /obs-prs [this test on roachdash]( | [improve this report!](,1.6,Low,0.584,localized low-impact
